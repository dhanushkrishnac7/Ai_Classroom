--- Page 15 ---
Publish - subscribe A messaging pattern used in software architecture to facilitate asynchronous communication between different components or systems. The Publish-Subscribe (Pub-Sub) pattern is a messaging pattern where senders (Publishers) send messages without knowing who will receive them, and receivers (Subscribers) receive messages without knowing who sent them.

--- Page 1 ---
Design Pattern - Types · Creational Design Pattern · Structural Design Pattern · Behavioral Design Pattern

--- Page 2 ---
Creational Design Patterns · Focuses on the process of object creation or problems related to object creation. · Helps in making a system independent of how its objects are created, composed and represented.

--- Page 7 ---
01 Observer Pattern 02 Strategy Pattern 03 State Pattern 04 Command Pattern 05 Chain of Responsibility Behavioral Design Patterns Template Pattern 06 Interpreter Pattern 07 Visitor Pattern 08 Mediator Pattern 09 Memento Pattern 10 se

--- Page 8 ---
MVC( Model - View Controller) · Software design Pattern · Separates application into three components as: Model View Controller

--- Page 13 ---
How MVC Works . User interacts with the View (e.g., clicks a button) · View sends the request to the Controller · Controller processes input and updates the Model . Model changes and notifies the View · View updates the Ul accordingly

--- Page 14 ---
Design Principles of MVC · Divide and conquer · Increase cohesion · Reduce coupling · Increase reuse · Design for flexibility

--- Page 9 ---
Model [Data and Logic layer ] · Manages Data and Business Logics · Does not directly interconnected with the User · Sends notification to "View" , incase of any changes in the data. Ex) User Model -> Updates, fetches & validates information from database.

--- Page 10 ---
View [UI layer ] · Displays data from the Model in a user-friendly way. · Only renders information, without modifying data or business logic. · Updates automatically when the Model changes. Ex) Web Page

--- Page 3 ---
Singleton Pattern 01 Factory Method Pattern 02 Creational Design Pattern Abstract Factory Pattern 05 C Prototype Pattern 04 Builder Pattern

--- Page 4 ---
Structural Design Patterns · Solves problems related to how classes and objects are composed/assembled to form larger structures. · Uses inheritance to compose interfaces or implementations.

--- Page 11 ---
Controller [ Input and Logic Layer ] · Handles user input and communicates between Model and View. · Updates the Model when an action is performed. · Instructs the View to refresh when the Model changes. Ex) Login Controller - validates user credentials and updates the session

--- Page 12 ---
Model 0 View · Handles data logic . Interacts with Database Fetch Data Request O o - Database · Handles data presentation Dynamically rendered Fetch presentation . Handles request flow · Never handles data logic O Response End User Controller DG

--- Page 5 ---
Adapter Pattern 01 07 Bridge Pattern Structural Design Patterns 02 Composite Pattern 03 04 9e Flyweight Pattern 06 Proxy Pattern 05 Facade Pattern Decorator Pattern

--- Page 6 ---
Behavioral Design Patterns · Concerned with algorithms and the assignment of responsibilities between objects. · Describe not just patterns of objects or classes but also the patterns of communication between them.

--- Page 39 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V The Downside of Dropout Although dropout is clearly a highly effective tool, it comes with certain drawbacks. A network with dropout can take 2 - 3 times longer to train than a standard network. One way to attain the benefits of dropout without slowing down training is by finding a regularizer that is essentially equivalent to a dropout layer. For linear regression, this regularizer has been proven to be a modified form of L2 regularization. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) nirf 175º Rank N33

--- Page 33 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V When a model performs well on the training data and does not perform well on the testing data, then the model is said to have high generalization error. In other words, in such a scenario, the model has low bias and high variance and is too complex. This is called overfitting. Overfitting means that the model is a good fit on the train data compared to the data, as illustrated in the graph above. Overfitting is also a result of the model being too complex. What Is Regularization in Machine Learning? Regularization is one of the key concepts in Machine learning as it helps choose a simple model rather than a complex one. We want our model to perform well both on the train and the new unseen data, meaning the model must have the ability to be generalized. Generalization error is -a measure of how accurately an algorithm can predict outcome values for previously unseen data.l Regularization refers to the modifications that can be made to a learning algorithm that helps to reduce this generalization error and not the training error. It reduces by ignoring the less important features. It also helps prevent overfitting, making the model more robust and decreasing the complexity of a model. How Does Regularization Work? Regularization works by shrinking the beta coefficients of a linear regression model. To understand why we need to shrink the coefficients, let us see the below example: Salary Slope is b1 + b3 if X2 = 1 if x2 = 0 bo + b2 bo Total years of experience y = bo + b2×2 + b1×1 + b3×2×1 Fig. 5.18. In the above graph, the two lines represent the relationship between total years of experience and salary, where salary is the target variable. These are slopes indicating CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 34 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V the change in salary per unit change in total years of experience. As the slope b1 + b3 decreases to slope b1, we see that the salary is less sensitive to the total years of experience. By decreasing the slope, the target variable (salary) became less sensitive to the change in the independent X variables, which increases the bias into the model. Remember, bias is the difference between the predicted and the actual values. With the increase in bias to the model, the variance (which is the difference between the predictions when the model fits different datasets.) decreases. And, by decreasing the variance, the overfitting gets reduced. The models having the higher variance leads to overfitting, and we saw above, we will shrink or reduce the beta coefficients to overcome the overfitting. The beta coefficients or the weights of the features converge towards zero, which is known as shrinkage. What Is the Regularization Parameter? For linear regression, the regularization has two terms in the loss function: The Ordinary Least Squares (OLS) function, and The penalty term It becomes : Loss function regularization = Loss function + Penalty term ols The goal of the linear regression model is to minimize the loss function. Now for Regularization, the goal becomes to minimize the following cost function: n ≥ (yact - ypred)2 + penalty i = 1 Where, the penalty term comprises the regularization parameter and the weights associated with the variables. Hence, the penalty term is: penalty = ^ * w where, 2 = Regularization parameter w = weight associated with the variables; generally considered to be the L - p norms The regularization parameter in machine learning is A : It imposes a higher penalty on the variable having higher values, and hence, it controls the strength of the penalty term. This tuning parameter controls the bias-variance trade-off. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 29 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Drawback: GridSearch CV will go through all the intermediate combinations of hyperparameters which makes grid search computationally very expensive. 9. Randomized Search CV Randomized Search CV solves the drawbacks of GridSearch CV, as it goes through only a fixed number of hyperparameter settings. It moves within the grid in a random fashion to find the best set of hyperparameters. This approach reduces unnecessary computation. The following code illustrates how to use RandomizedSearch CV # Necessary imports from scipy.stats import randint from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import RandomizedSearchCV # Creating the hyperparameter grid param_dist = {"max_depth": [3, None], "max_features": randint(1, 9), "min_samples_leaf": randint(1, 9), "criterion": ["gini", "entropy"]} # Instantiating Decision Tree classifier tree = Decision TreeClassifier() # Instantiating RandomizedSearchCV object tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5) tree_cv.fit(X, y) # Print the tuned parameters and score print("Tuned Decision Tree Parameters: { }".format(tree_cv.best_params_)) print("Best score is { }".format(tree_cv.best_score_)) CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 30 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Output: Tuned Decision Tree Parameters: {_min_samples_leaf': 5, _max_depth': 3, max features': 5, _criterion': _ gini'} Best score is 0.7265625 5.9. BATCH NORMALIZATION Normalization is a data pre-processing tool used to bring the numerical data to a common scale without distorting its shape. - Generally, when we input the data to a machine or deep learning algorithm we tend to change the values to a balanced scale. The reason we normalize is partly to ensure that our model can generalize appropriately. Now coming back to Batch normalization, it is a process to make neural networks faster and more stable through adding extra layers in a deep neural network. The new layer performs the standardizing and normalizing operations on the input of a layer coming from a previous layer. A typical neural network is trained using a collected set of input data called batch. Similarly, the normalizing process in batch normalization takes place in batches, not as a single input. X1 X2 X3 X4 W1 W2 W3 W4 W o L = Number of layers Bias = 0 Activation Function : Sigmoid Initially, our inputs X1, X2, X3, X4 are in normalized form as they are coming from the pre-processing stage. When the input passes through the first layer, it transforms, as a sigmoid function applied over the dot product of input X and the weight matrix W X1 X2 O X3 X4 W1 W2 W3 WL W4 h1 = o(W1X) INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 3 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V Bias, net sum, and an activation function. The perceptron model begins with the multiplication of all input values and their weights, then adds these values together to create the weighted sum. Then this weighted sum is applied to the activation function 'f to obtain the desired output. This activation function is also known as the step function and is represented by 'f'. 1 Wo Error X1 W1 output X2 W2 Xm : Wm Net input function Activation function Perceptron rule Fig. 5.3. Perceptron model works in two important steps as follows: Step-1 In the first step first, multiply all input values with corresponding weight values and then add them to determine the weighted sum. Mathematically, we can calculate the weighted sum as follows: Ew; * xi = X1 * W1 + X2 * W2 + ... Wn * xn Add a special term called bias 'b' to this weighted sum to improve the model's performance. Zwi * x; + b Step-2 In the second step, an activation function is applied with the above-mentioned weighted sum, which gives us output either in binary form or a continuous value as follows: Y = f(> wi * x¡ + b) Perceptron Function Perceptron function "f (x)" can be achieved as output by multiplying the input 'x' with the learned weight coefficient 'w'. CHENNAI INSTITUTE . TECHNOLOGY LEN CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank

--- Page 4 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Mathematically, we can express it as follows: f(x) = 1; if w . x + b > 0 otherwise, f (x) = 0 * 'w' represents real-valued weights vector 'b' represents the bias 'x' represents a vector of input x values. 5.1. MULTI - LAYERED PERCEPTRON MODEL Multi-Layer perceptron defines the most complex architecture of artificial neural networks. It is substantially formed from multiple layers of the perceptron. Tensor Flow is a very popular deep learning framework released by, and this notebook will guide to build a neural network with this library. If we want to understand what is a Multi-layer perceptron, we have to develop a multi-layer perceptron from scratch using Numpy. The pictorial representation of multi-layer perceptron learning is as shown below- Input Layer Hidden Layer Output Layer · Fig. 5.4. MLP networks are used for supervised learning format. A typical learning algorithm for MLP networks is also called back propagation's algorithm. A multilayer perceptron (MLP) is a feed forward artificial neural network that generates a set of outputs from a set of inputs. An MLP is characterized by several layers of input nodes connected as a directed graph between the input nodes CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N33 nirf 1750 Rank

--- Page 23 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V exponentially as we propagate down to the initial layers. A small gradient means that the weights and biases of the initial layers will not be updated effectively with each training session. Since these initial layers are often crucial to recognizing the core elements of the input data, it can lead to overall inaccuracy of the whole network. Solution: The simplest solution is to use other activation functions, such as ReLU, which doesn't cause a small derivative. Residual networks are another solution, as they provide residual connections straight to earlier layers. The residual connection directly adds the value at the beginning of the block, x, to the end of the block (F(x) + x). This residual connection doesn't go through activation functions that -squashesl the derivatives, resulting in a higher overall derivative of the block. X Weight layer F(x) Weight layer X identity F(x) + x + relu Fig. 5.14. A residual block 0.9 - Sigmoid --- Derivative Sigmoid 0.8 0.7 0.6 0.5 0.4 Ø3 02 0.1 -10 -8 -6 -4 -2 0 2 4 6 8 10 Fig. 5.15. Sigmoid function with restricted inputs INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 24 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Finally, batch normalization layers can also resolve the issue. As stated before, the problem arises when a large input space is mapped to a small one, causing the derivatives to disappear. In Image 1, this is most clearly seen at when |x | is big. Batch normalization reduces this problem by simply normalizing the input so |x | doesn't reach the outer edges of the sigmoid function. As seen in Image 3, it normalizes the input so that most of it falls in the green region, where the derivative isn't too small. What is an activation function? Activation function is a simple mathematical function that transforms the given input to the required output that has a certain range. From their name they activate the neuron when output reaches the set threshold value of the function. Basically they are responsible for switching the neuron ON/OFF. The neuron receives the sum of the product of inputs and randomly initialized weights along with a static bias for each layer. The activation function is applied on to this sum, and an output is generated. Activation functions introduce a non-linearity, so as to make the network learn complex patterns in the data such as in the case of images, text, videos or sounds. Without an activation function our model is going to behave like a linear regression model that has limited learning capacity. 5.7. RELU The rectified linear activation unit, or ReLU, is one of the few landmarks in the deep learning revolution. It's simple, yet it's far superior to previous activation functions like sigmoid or tanh. ReLU formula is : f (x) = max(0, x) y = x 3 2 1 y = 0 -3 -2 -1 0 1 2 3 Fig. 5.16. INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 21 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Types of Backpropagation There are two types of Backpropagation which are as follows - Static Back Propagation - In this type of backpropagation, the static output is created because of the mapping of static input. It is used to resolve static classification problems like optical character recognition. Recurrent Backpropagation - The Recurrent Propagation is directed forward or directed until a specific determined value or threshold value is acquired. After the certain value, the error is evaluated and propagated backward. Key Points ❖ Simplifies the network structure by elements weighted links that have the least effect on the trained network * You need to study a group of input and activation values to develop the relationship between the input and hidden unit layers. * It helps to assess the impact that a given input variable has on a network output. The knowledge gained from this analysis should be represented in rules. * Backpropagation is especially useful for deep neural networks working on error-prone projects, such as image or speech recognition. * Backpropagation takes advantage of the chain and power rules allows backpropagation to function with any number of outputs. 5.6. UNIT SATURATION (AKA THE VANISHING GRADIENT PROBLEM) The vanishing gradient problem is an issue that sometimes arises when training machine learning algorithms through gradient descent. This most often occurs in neural networks that have several neuronal layers such as in a deep learning system, but also occurs in recurrent neural networks. The key point is that the calculated partial derivatives used to compute the gradient as one goes deeper into the network. Since the gradients control how much the network learns during training, if the gradients are very small or zero, then little to no training can take place, leading to poor predictive performance. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 22 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V The problem: As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train. Why: Certain activation functions, like the sigmoid function, squishes a large input space into a small input space between 0 and 1. Therefore, a large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small. 0.9 - Sigmoid 0.8 --- Derivative Sigmoid 0.7 0.6 0.5 0.4 Ø3 02 0.1 -10 -8 -6 -4 -2 0 2 4 6 8 10 Fig. 5.13. The sigmoid function and its derivative As an example, Image 1 is the sigmoid function and its derivative. Note how when the inputs of the sigmoid function becomes larger or smaller (when |x | becomes bigger), the derivative becomes close to zero. Why it's significant: For shallow network with only a few layers that use these activations, this isn't a big problem. However, when more layers are used, it can cause the gradient to be too small for training to work effectively. Gradients of neural networks are found using backpropagation. Simply put, backpropagation finds the derivatives of the network by moving layer by layer from the final layer to the initial one. By the chain rule, the derivatives of each layer are multiplied down the network (from the final layer to the initial) to compute the derivatives of the initial layers. However, when n hidden layers use an activation like the sigmoid function, n small derivatives are multiplied together. Thus, the gradient decreases INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 5 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V connected as a directed graph between the input and output layers. MLP uses backpropagation for training the network. MLP is a deep learning method. The algorithm for the MLP is as follows: 1. Just as with the perceptron, the inputs are pushed forward through the MLP by taking the dot product of the input with the weights that exist between the input layer and the hidden layer (WH). This dot product yields a value at the hidden layer. We do not push this value forward as we would with a perceptron though. 2. MLPs utilize activation functions at each of their calculated layers. There are many activation functions to discuss: rectified linear units (ReLU), sigmoid function, tanh. Push the calculated output at the current layer through any of these activation functions. 3. Once the calculated output at the hidden layer has been pushed through the activation function, push it to the next layer in the MLP by taking the dot product with the corresponding weights. 4. Repeat steps two and three until the output layer is reached. 5. At the output layer, the calculations will either be used for a back propagation algorithm that corresponds to the activation function that was selected for the MLP (in the case of training) or a decision will be made based on the output (in the case of testing). Like a single-layer perceptron model, a multi-layer perceptron model also has the same model structure but has a greater number of hidden layers. The multi-layer perceptron model is also known as the Back propagation algorithm, which executes in two stages as follows: ❖ Forward Stage: Activation functions start from the input layer in the forward stage and terminate on the output layer. ❖ Backward Stage: In the backward stage, weight and bias values are modified as per the model's requirement. In this stage, the error between actual output and demanded originated backward on the output layer and ended on the input layer. Hence, a multi-layered perceptron model has considered as multiple artificial neural networks having various layers in which activation function does not remain CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N39 nirf 175° Rank

--- Page 6 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V linear, similar to a single layer perceptron model. Instead of linear, activation function can be executed as sigmoid, TanH, ReLU, etc., for deployment. A multi-layer perceptron model has greater processing power and can process linear and non-linear patterns. Further, it can also implement logic gates such as AND, OR, XOR, NAND, NOT, XNOR, NOR. Advantages of Multi-Layer Perceptron: ❖ A multi-layered perceptron model can be used to solve complex non-linear problems. * It works well with both small and large input data. It helps us to obtain quick predictions after the training. # It helps to obtain the same accuracy ratio with large as well as small data. Disadvantages of Multi-Layer Perceptron: * In Multi-layer perceptron, computations are difficult and time-consuming. * In multi-layer Perceptron, it is difficult to predict how much the dependent variable affects each independent variable. ❖ The model functioning depends on the quality of the training. 5.2. ACTIVATION FUNCTIONS Inputs Weights ×1 W1 ×2 W2 ×3 W3j · . . . . xn Wnj CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt] INSTITUTE . TECHNOLOGY N39 nirf 175" Rank net input netj Σ transfer function Fig. 5.5. activation function - Oj activation Oj threshold

--- Page 25 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Both the ReLU function and its derivative are monotonic. If the function receives any negative input, it returns 0; however, if the function receives any positive value x, it returns that value. As a result, the output has a range of 0 to infinite. ReLU is the most often used activation function in neural networks, especially CNNs, and is utilized as the default activation function. Implementing ReLU function in Python We can implement a simple ReLU function with Python code using an if-else statement as, def ReLU(x): if x>0: return x else: The positive value is returned as it is and for values less than (negative values) or equal to zero, 0.0 is returned. Now, we'll test out function by giving some input values and plot our result using pyplot from matplotlib library. The input range of values is from - 5 to 10. We apply our defined function on this set of input values. from matplotlib import pyplot def relu(x): return max(0.0, x) input = [x for x in range(-5, 10)] # apply relu on each input output = [relu(x) for x in input] # plot our result pyplot.plot(series_in, series_out) pyplot.show() We see from the plot that all the negative values have been set to zero, and the positive values are returned as it is. Note that we've given a set of consecutively increasing numbers as input, so we've a linear output with an increasing slope. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY nirf 1750 Rank N32

--- Page 26 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V 8 6 4 - 2 0 -4 -2 0 2 4 6 8 Fig. 5.17. Advantages of ReLU: ReLU is used in the hidden layers instead of Sigmoid or tanh as using sigmoid or tanh in the hidden layers leads to the infamous problem of "Vanishing Gradient". The "Vanishing Gradient" prevents the earlier layers from learning important information when the network is backpropagating. The sigmoid which is a logistic function is more preferrable to be used in regression or binary classification related problems and that too only in the output layer, as the output of a sigmoid function ranges from 0 to 1. Also Sigmoid and tanh saturate and have lesser sensitivity. Some of the advantages of ReLU are: # Simpler Computation: Derivative remains constant i.e 1 for a positive input and thus reduces the time taken for the model to learn and in minimizing the errors. # Representational Sparsity: It is capable of outputting a true zero value. * Linearity: Linear activation functions are easier to optimize and allow for a smooth flow. So, it is best suited for supervised tasks on large sets of labelled data. Disadvantages of ReLU: ❖ Exploding Gradient: This occurs when the gradient gets accumulated, this causes a large differences in the subsequent weight updates. This as a INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 1 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V UNIT V NEURAL NETWORKS Perceptron - Multilayer perceptron, activation functions, network training - gradient descent optimization - stochastic gradient descent, error backpropagation, from shallow networks to deep networks - Unit saturation (aka the vanishing gradient problem) - ReLU, hyperparameter tuning, batch normalization, regularization, dropout. What is the Perceptron Model in Machine Learning? Perceptron is Machine Learning algorithm for supervised learning of various binary classification tasks. Further, Perceptron is also understood as an Artificial Neuron or neural network unit that helps to detect certain input data computations in business intelligence. Perceptron model is also treated as one of the best and simplest types of Artificial Neural networks. However, it is a supervised learning algorithm of binary classifiers. Hence, we can consider it as a single-layer neural network with four main parameters, i.e., input values, weights and Bias, net sum, and an activation function. Basic Components of Perceptron Mr. Frank Rosenblatt invented the perceptron model as a binary classifier which contains three main components. These are as follows: Inputs Weights 1 Net input function Activation function Wo X1 W1 X2 W2 IT 5 IT - output ... Xm Wm Fig. 5.1. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N39 nirf 175° Rank

--- Page 2 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Input Nodes or Input Layer: This is the primary component of Perceptron which accepts the initial data into the system for further processing. Each input node contains a real numerical value. Wight and Bias: Weight parameter represents the strength of the connection between units. This is another most important parameter of Perceptron components. Weight is directly proportional to the strength of the associated input neuron in deciding the output. Further, Bias can be considered as the line of intercept in a linear equation. Activation Function: These are the final and important components that help to determine whether the neuron will fire or not. Activation Function can be considered primarily as a step function. Types of Activation functions: # Sign function # Step function, and * Sigmoid function a ai ai +1 +1 +1 t in; in; in -1 Step Function Sign Function Sigmoid Function Fig. 5.2. The data scientist uses the activation function to take a subjective decision based on various problem statements and forms the desired outputs. Activation function may differ (e.g., Sign, Step, and Sigmoid) in perceptron models by checking whether the learning process is slow or has vanishing or exploding gradients. How does Perceptron work? In Machine Learning, Perceptron is considered as a single-layer neural network that consists of four main parameters named input values (Input nodes), weights and CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY LEN nirf 175° Rank

--- Page 37 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Weight decay: incentivize the network to use smaller weights by adding a penalty to the loss function (this ensures that the norms of the weights are relatively evenly distributed amongst all the weights in the networks, which prevents just a few weights from heavily influencing network output) Noise: allow some random fluctuations in the data through augmentation (which makes the network robust to a larger distribution of inputs and hence improves generalization) Model combination: average the outputs of separately trained neural networks (requires a lot of computational power, data, and time) Dropout remains an extremely popular protective measure against overfitting because of its efficiency and effectiveness. How Does Dropout Work? When we apply dropout to a neural network, we're creating a -thinnedl network with unique combinations of the units in the hidden layers being dropped randomly at different points in time during training. Each time the gradient of our model is updated, we generate a new thinned neural network with different units dropped based on a probability hyperparameter p. Training a network using dropout can thus be viewed as training loads of different thinned neural networks and merging them into one network that picks up the key properties of each thinned network. This process allows dropout to reduce the overfitting of models on training data. This graph, taken from the paper -Dropout: A Simple Way to Prevent Neural Networks from Overfittingl by Srivastava et al., compares the change in classification error of models without dropout to the same models with dropout (keeping all other hyperparameters constant). All the models have been trained on the MNIST dataset. It is observed that the models with dropout had a lower classification error than the same models without dropout at any given point in time. A similar trend was observed when the models were used to train other datasets in vision, as well as speech recognition and text analysis. The lower error is because dropout helps prevent overfitting on the training data by reducing the reliance of each unit in the hidden layer on other units in the hidden layers. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY nirf 1750 Rank N32

--- Page 38 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V 2.5 2.0 Without dropout Classification Error % 1.5 With dropout 1.0 0 200000 400000 Number of Weight updates 600000 800000 1000000 Fig. 5.20. (a) Without dropout (b) Dropout with p = 0.5. Fig. 5.21. It can be observed in figure a that the units don't seem to pick up on any meaningful feature, whereas in figure b, the units seem to have picked up on distinct edges and spots in the data provided to them. This indicates that dropout helps break co- adaptations among units, and each unit can act more independently when dropout regularization is used. In other words, without dropout, the network would never be able to catch a unit A compensating for another unit B's flaws. With dropout, at some point unit A would be ignored and the training accuracy would decrease as a result, exposing the inaccuracy of unit B. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 31 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Similarly, this transformation will take place for the second layer and go till the last layer L as shown in the following image. X1 X2 X3 X4 W1 W2 W3 WL W4 O Normalize the inputs h1 = 0(W1X) h2 = @(W2h1)=0(W20(W1X)) O = 0(WL hL -1) Although, our input X was normalized with time the output will no longer be on the same scale. As the data go through multiple layers of the neural network and L activation functions are applied, it leads to an internal co-variate shift in the data. How does Batch Normalization work? Since by now we have a clear idea of why we need Batch normalization, let's understand how it works. It is a two-step process. First, the input is normalized, and later rescaling and offsetting is performed. Normalization of the Input Normalization is the process of transforming the data to have a mean zero and standard deviation one. In this step we have our batch input from layer h, first, we need to calculate the mean of this hidden activation. 3 = 2-1 M h m i Here, m is the number of neurons at layer h. Once we have meant at our end, the next step is to calculate the standard deviation of the hidden activations. [1 7 1/2 0 = m E (h; - u)2| Further, as we have the mean and the standard deviation ready. We will normalize the hidden activations using these values. For this, we will subtract the mean from CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 32 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V each input and divide the whole value with the sum of standard deviation and the smoothing term (¿). The smoothing term() assures numerical stability within the operation by stopping a division by a zero value. hi(norm) + (hị - ) Advantages of Batch Normalization Now let's look into the advantages the BN process offers. Speed Up the Training By Normalizing the hidden layer activation the Batch normalization speeds up the training process. Handles internal covariate shift It solves the problem of internal covariate shift. Through this, we ensure that the input for every layer is distributed around the same mean and standard deviation. If you are unaware of what is an internal covariate shift, look at the following example. Internal covariate shift Suppose we are training an image classification model, that classifies the images into Dog or Not Dog. Let's say we have the images of white dogs only, these images will have certain distribution as well. Using these images model will update its parameters. Smoothens the Loss Function Batch normalization smoothens the loss function that in turn by optimizing the model parameters improves the training speed of the model. 5.10. REGULARIZATION The Problem of Overfitting So, before diving into regularization, let's take a step back to understand what bias- variance is and its impact. Bias is the deviation between the values predicted by the model and the actual values whereas, variance is the difference between the predictions when the model fits different datasets. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 13 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V When x > 0, our derivative greater than zero and we need to go in negative direction, when x < 0, the derivative less than zero, we need to go in positive direction. We always need to take a step in the direction which is opposite of derivative. Let's apply the same idea to gradient. Gradient is vector which points to some direction in space. It actually point to the direction of the steepest increase of the function. Since we want minimize our function, we'll take a step in the opposite direction of gradient. In neural network we think of inputs x, and outputs y as fixed numbers. The variable with respect to which we're going to be taking our derivatives are weights w, since these are the values we want to change to improve our network. If we compute the gradient of the loss function w.r.t our weights and take small steps in the opposite direction of gradient our loss will gradually decrease until it converges to some local minima. This algorithms is called Gradient Descent. The rule for updating weights on each iteration of Gradient Descent is the following: Wj = w; - Irô L a wi Ir in the notation above means learning rate. It's there to control how big of a step we're taking each iteration. It is the most important hyper-parameter to tune when training neural networks. 5.3.1. THE ARTIFICIAL NEURAL NETWORK To build a good Artificial Neural Network (ANN) you will need the following ingredients Ingredients: ❖ Artificial Neurons (processing node) composed of: ✓ (many) input neuron(s) connection(s) (dendrites) ✓ a computation unit (nucleus) composed of: a linear function (ax + b) " an activation function (equivalent to the the synapse) ✓ an output (axon) Preparation to get an ANN for image classification training: 1. Decide on the number of output classes (meaning the number of image classes - for example two for cat vs dog). CHENNAI CHENNAI INSTITUTE . TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 14 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V 2. Draw as many computation units as the number of output classes (congrats you just create the Output Layer of the ANN). 3. Add as many Hidden Layers as needed within the defined architecture. Hidden Layers are just a set of neighboured Compute Units, they are not linked together. 4. Stack those Hidden Layers to the Output Layer using Neural Connections 5. It is important to understand that the Input Layer is basically a layer of data ingestion 6. Add an Input Layer that is adapted to ingest your data (or you will adapt your data format to the pre-defined architecture) 7. Assemble many Artificial Neurons together in a way where the output (axon) an Neuron on a given Layer is (one) of the input of another Neuron on a subsequent Layer. As a consequence, the Input Layer is linked to the Hidden Layers which are then linked to the Output Layer (as shown in the picture below) using Neural Connections Training an Artificial Neural Network (ANN) requires just a few steps: 1. First an ANN will require a random weight initialization 2. Split the dataset in batches (batch size) 3. Send the batches 1 by 1 to the GPU 4. Calculate the forward pass (what would be the output with the current weights) 5. Compare the calculated output to the expected output (loss) 6. Adjust the weights (using the learning rate increment or decrement) according to the backward pass (backward gradient propagation). 5.4. STOCHASTIC GRADIENT DESCENT What is Gradient Descent? Gradient Descent is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea is to tweak parameters iteratively in order to minimize the cost function. An important parameter of CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 9 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V difference is the output interval. The output interval of tanh is 1, and the whole function is 0-centric, which is better than sigmoid. 2. The major advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. 1.0 - sigmoid - tanh 0.5 1 0.0 -2 -6 -4 0 2 4 6 -0.5- -1.0L Fig. 5.6. 3. ReLU (Rectified Linear Unit) Activation Function- output 5 -5 0 Fig. 5.7. 5 The ReLU is half rectified (from the bottom). f (z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero. max(0, x) a(x) = { i 0 x >=0 x<0 The ReLU (Rectified Linear Unit) function is an activation function that is currently more popular compared to other activation functions in deep learning. Compared with the sigmoid function and the tanh function, it has the following advantages: 1. When the input is positive, there is no gradient saturation problem. INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 10 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V 2. The calculation speed is much faster. The ReLU function has only a linear relationship. Whether it is forward or backward, it is much faster than sigmoid and tanh. (Sigmoid and tanh need to calculate the exponent, which will be slower.) 4. Leaky ReLU Activation Function An activation function specifically designed to compensate for the dying ReLU problem. f(y) 4 f(y) f(y) = y f(y) = y f(y) = 0 y y f(y) = ay Fig. 5.8. ReLU vs Leaky ReLU Why Leaky ReLU is better than ReLU? if y; > 0 f(yi) = İyi laiyi ify; ≤ 0 1. The leaky ReLU adjusts the problem of zero gradients for negative value, by giving a very small linear component of x to negative inputs(0.01 x). 2. The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so. 3. Range of the Leaky ReLU is (-infinity to infinity). 5. ELU (Exponential Linear Units) Function- ELU is also proposed to solve the problems of ReLU. In contrast to ReLUs, ELUs have negative values which pushes the mean of the activations closer to zero. Mean activations that are closer to zero enable faster learning as they bring the gradient closer to the natural gradient. xx>0 g(x)=ELU(x) = \a(ex-1) x≤0 Obviously, ELU has all the advantages of ReLU, and: ❖ No Dead ReLU issues, the mean of the output is close to 0, zero-centered. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY nirf 1750 Rank N32

--- Page 7 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V An activation function is a function that is added to an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron. The neuron doesn't really know how to bound to value and thus is not able to decide the firing pattern. Thus the activation function is an important part of an artificial neural network. They basically decide whether a neuron should be activated or not. Thus it bounds the value of the net input. The activation function is a non-linear transformation that we do over the input before sending it to the next layer of neurons or finalizing it as output. 5.2.1. TYPES OF ACTIVATION FUNCTIONS Several different types of activation functions are used in Deep Learning. Some of them are explained below: 1. Sigmoid Activation Function - Sigmoid Activation Function 1.2 10 0.8 0.6 8 0.4 0.2 0.0 -0.2 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0 X The Sigmoid Function looks like an S-shaped curve. Formula : f (z) = 1/(1+e^-z) Why and when do we use the Sigmoid Activation Function? 1. The output of a sigmoid function ranges between 0 and 1. Since, output values bound between 0 and 1, it normalizes the output of each neuron. LEN CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank

--- Page 8 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V 2. Specially used for models where we have to predict the probability as an output. Since the probability of anything exists only between the range of 0 and 1, sigmoid is the perfect choice. 3. Smooth gradient, preventing -jumps in output values. 4. The function is differentiable. That means, we can find the slope of the sigmoid curve at any two points. 5. Clear predictions, i.e very close to 1 or 0. 2. Tanh or Hyperbolic Tangent Activation Function - y 1 f(x) = tanh x X -2 -1 1 2 -1. The tanh activation function is also sort of sigmoidal (S-shaped). 2 f(x) = tanh (x) = 1+ e-2x -- 1 Formula of tanh activation function Tanh is a hyperbolic tangent function. The curves of tanh function and sigmoid function are relatively similar. But it has some advantage over the sigmoid function. Let's look at what it is. Why is tanh better compared to sigmoid activation function? 1. First of all, when the input is large or small, the output is almost smooth and the gradient is small, which is not conducive to weight update. The CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N33 nirf 1750 Rank

--- Page 15 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Gradient Descent (GD) is the size of the steps, determined by the learning rate hyperparameters. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time, and if it is too high we may jump the optimal value. What is the objective of Gradient Descent? Gradient, in plain terms means slope or slant of a surface. So gradient descent literally means descending a slope to reach the lowest point on that surface. Let us imagine a two dimensional graph, such as a parabola in the figure below. y y = x2 - 2x - 3 6 5 2 16 + 5 -4 - 3 - 2 -1 1 2 3 4 5 6 7 8 -x -3 -4 -5. Fig. 5.11. A parabolic function with two dimensions (x , y) In the above graph, the lowest point on the parabola occurs at x = 1. The objective of gradient descent algorithm is to find the value of -xl such that -yl is minimum. -yl here is termed as the objective function that the gradient descent algorithm operates upon, to descend to the lowest point. 5.4.1. TYPES OF GRADIENT DESCENT: Typically, there are three types of Gradient Descent: ❖ Batch Gradient Descent Stochastic Gradient Descent CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY nirf 1750 Rank N32

--- Page 16 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V ❖ Mini-batch Gradient Descent Stochastic Gradient Descent (SGD): In Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration. In Gradient Descent, there is a term called -batchl which denotes the total number of samples from a dataset that is used for calculating the gradient for each iteration. In typical Gradient Descent optimization, like Batch Gradient Descent, the batch is taken to be the whole dataset. Suppose, you have a million samples in your dataset, so if you use a typical Gradient Descent optimization technique, you will have to use all of the one million samples for completing one iteration while performing the Gradient Descent, and it has to be done for every iteration until the minima are reached. Hence, it becomes computationally very expensive to perform. This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration. Steps in SGD: 1. Find the slope of the objective function with respect to each parameter / feature. In other words, compute the gradient of the function. 2. Pick a random initial value for the parameters. (To clarify, in the parabola example, differentiate -yl with respect to -xl. If we had more features like X1, x2 etc., we take the partial derivative of -yl with respect to each of the features.) 3. Update the gradient function by plugging in the parameter values. 4. Calculate the step sizes for each feature as : step size = gradient * learning rate. 5. Calculate the new parameters as : new params = old params -step size 6. Repeat steps 3 to 5 until gradient is almost 0. Stochastic Gradient Descent using Python The SGD algorithm is used in several loss functions. Simply put, it is used to minimize a cost function by iterating a gradient-based weight update. Instead of looking at the full dataset, the weight update is applied to batches randomly extracted from it, which is why it is also known as mini-batch gradient descent. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 11 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V ❖ In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. * ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. 4 - ELU -LReLU - ReLU ₹ 2 0 -10 -5 0 X 5 Fig. 5.9. ELU vs Leaky ReLU vs ReLU 5.3. NETWORK TRAINING In the process of training, we want to start with a bad performing neural network and wind up with network with high accuracy. In terms of loss function, we want our loss function to much lower in the end of training. Improving the network is possible, because we can change its function by adjusting weights. We want to find another function that performs better than the initial one. The problem of training is equivalent to the problem of minimizing the loss function. Why minimize loss instead of maximizing? Turns out loss is much easier function to optimize. There are a lot of algorithms that optimize functions. These algorithms can gradient-based or not, in sense that they are not only using the information provided by the function, but also by its gradient. First, we need to remember what a derivative is with respect to some variable. Let's take some easy function f (x) = x. If we remember the rules of calculus from high school we know, that the derivative of that is one at every value of x. The CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE . TECHNOLOGY (Autonomous] LEN nirf 1750 Rank

--- Page 12 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V derivative is the rate of how fast our function is changing when we take infinitely small step in the positive direction. Mathematically it can be written as the following: VL ~ 8 L V x à xi i Which means: how much our function changes(left term) approximately equals to derivative of that function with respect to some variable x multiplied with how much we changed that variable. That approximation is going to be exact when we step we take is infinitely small and this is very important concept of the derivative. 8 6 4 2 0 -3 -2 -1 0 y = x2 1 2 3 Fig. 5.10. Let's go back to our function f(x) = x2. Obviously, the minimum of that function is at point x = 0, but how would a computer know it ? Suppose, we start off with some random value of x and this value is 2. The derivative of the function in that in x = 2 equals 4. Which means that is we take a step in positive direction our function will change proportionally to 4. Our derivative only guarantees that the function will decrease if take infinitely small step. Generally, you want to control how big of step you make with some kind of hyper-parameter. This hyper-parameter is called learning rate and I'll talk about it later. Let's now see what happens if we start at a point x = - 2. The derivative is now equals - 4, which means, that if take a small step in positive direction our function will change proportionally to - 4, thus it will decrease. That's exactly what we want. INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 19 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V 2 Hidden layer(s) Input layer x W output layer 3 W x W Difference in desired values X W 5 Backprop output layer Fig. 5.12. The General Algorithm The backpropagation algorithm proceeds in the following steps, assuming a suitable learning rate \ alpha a and random initialization of the parameters w_{ij}^k:wijk: Definition → 1. Calculate the forward phase for each input-output pair ( xa, ya ) and store the results y^ , ak and ok for each node j in layer k by proceeding from d j j layer 0, the input layer, to layer m, the output layer. → 2. Calculate the backward phase for each input-output pair ( Xa, Ya ) and wk for each weight wk connecting node in layer k - 1 store the results O Ed aw j to node j in layer k by proceeding from layer m, the output layer, to layer1, the input layer. (a) Evaluate the error term for the final layer 8'" by using the second equation. (b) Backpropogate the error terms for the hidden layers 8k , working backwards from the final hidden layer k = m - 1, by repeatedly using the third equation. (c) Evaluate the partial derivatives of the individual error Ed with respect to wk by using the first equation. INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 20 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V 3. Combine the individual gradients for each input-output pair aw j ,k O Ed to get OE(X, 0) the total gradient O wk ? for the entire set of input-output pairs X = { ( ij → → X1, y1 ), ... , (XN, YN )}by using the fourth equation (a simple average of the individual gradients). 4. Update the weights according to the learning rate « and total gradient 0E(X, 0) k by using the fifth equation (moving in the direction of the aw j negative gradient). How Backpropagation Algorithm Works Inputs X, arrive through the preconnected path 1. Input is modeled using real weights W. The weights are usually randomly selected. 2. Calculate the output for every neuron from the input layer, to the hidden layers, to the output layer. 3. Calculate the error in the outputs Error B = Actual Output - Desired Output 4. Travel back from the output layer to the hidden layer to adjust the weights such that the error is decreased. 5. Keep repeating the process until the desired output is achieved Why We Need Backpropagation? Most prominent advantages of Backpropagation are: ❖ Backpropagation is fast, simple and easy to program * It has no parameters to tune apart from the numbers of input It is a flexible method as it does not require prior knowledge about the network It is a standard method that generally works well It does not need any special mention of the features of the function to be learned. INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 27 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V result causes instability when converging to the global minima and causes instability in the learning too. ❖ Dying ReLU: The problem of "dead neurons" occurs when the neuron gets stuck in the negative side and constantly outputs zero. Because gradient of 0 is also 0, it's unlikely for the neuron to ever recover. This happens when the learning rate is too high or negative bias is quite large. 5.8. HYPERPARAMETER TUNING A Machine Learning model is defined as a mathematical model with a number of parameters that need to be learned from the data. By training a model with existing data, we are able to fit the model parameters. However, there is another kind of parameter, known as Hyperparameters, that cannot be directly learned from the regular training process. They are usually fixed before the actual training process begins. These parameters express important properties of the model such as its complexity or how fast it should learn. Some examples of model hyperparameters include: The penalty in Logistic Regression Classifier i.e. L1 or L2 regularization The learning rate for training a neural network. The C and sigma hyperparameters for support vector machines. The k in k-nearest neighbors. Models can have many hyperparameters and finding the best combination of parameters can be treated as a search problem. The two best strategies for Hyperparameter tuning are: Grid Search CV In GridSearchCV approach, the machine learning model is evaluated for a range of hyperparameter values. This approach is called GridSearchCV, because it searches for the best set of hyperparameters from a grid of hyperparameters values. For example, if we want to set two hyperparameters C and Alpha of the Logistic Regression Classifier model, with different sets of values. The grid search technique will construct many versions of the model with all possible combinations of hyperparameters and will return the best one. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 28 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V As in the image, for C = [0.1, 0.2, 0.3, 0.4, 0.5] and Alpha = [0.1, 0.2, 0.3, 0.4]. For a combination of C = 0.3 and Alpha = 0.2, the performance score comes out to be 0.726(Highest), therefore it is selected. 0.5 0.701 0.703 0.4 0.699 0.702 0.698 0.3 0.721 0.726 0.713 0.2 0.706 0.705 0.704 0.701 C 0.1 0.698 0.692 0.1 0.2 Alpha 0.697 0.696 0.702 0.703 0.688 0.675 0.3 0.5 The following code illustrates how to use GridSearchCV # Necessary imports from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Creating the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {'C': c_space} # Instantiating logistic regression classifier logreg = LogisticRegression() # Instantiating the GridSearchCV object logreg_cv = GridSearchCV(logreg, param_grid, cv = 5) logreg_cv.fit(X, y) # Print the tuned parameters and score print("Tuned Logistic { }".format(logreg_cv.best_params_)) Regression Parameters: print("Best score is { }".format(logreg_cv.best_score_)) Output: Tuned Logistic Regression Parameters: {_C': 3.7275937203149381} Best score is 0.7708333333333334 CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 35 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V À can take values 0 to infinity. If A = 0, then means there is no difference between a model with and without regularization. Regularization Techniques in Machine Learning Each of the following techniques uses different regularization norms (L-p) based on the mathematical methodology that creates different kinds of regularization. These methodologies have different effects on the beta coefficients of the features. The regularization techniques in machine learning as follows: (a) Ridge Regression The Ridge regression technique is used to analyze the model where the variables may be having multicollinearity. It reduces the insignificant independent variables though it does not remove them completely. This type of regularization uses the L2 norm for regularization. Cost function n = E (yact - Ypred) 2 + 2 . | w 12 i= 1 (b) Lasso Regression Least Absolute Shrinkage and Selection Operator (or LASSO) Regression penalizes the coefficients to the extent that it becomes zero. It eliminates the insignificant independent variables. This regularization technique uses the L1 norm for regularization. i = 1 Cost function = > (yact - ypred )2 + 2. | w Il1 n (c) Elastic Net Regression The Elastic Net Regression technique is a combination of the Ridge and Lasso regression technique. It is the linear combination of penalties for both the L1 -norm and L2 -norm regularization. The model using elastic net regression allows the learning of the sparse model where some of the points are zero, similar to Lasso regularization, and yet maintains the Ridge regression properties. Therefore, the model is trained on both the L1 and L2 norms. The cost function of Elastic Net Regression is: n Cost function i= 1 = E (Vact - Y pred)2 + bridge . Il w 11/2 + Masso . | | CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY N32 nirf 1750 Rank

--- Page 36 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V When to Use Which Regularization Technique? The regularization in machine learning is used in following scenarios: Ridge regression is used when it is important to consider all the independent variables in the model or when many interactions are present. That is where collinearity or codependency is present amongst the variables. Lasso regression is applied when there are many predictors available and would want the model to make feature selection as well for us. When many variables are present, and we can't determine whether to use Ridge or Lasso regression, then the Elastic-Net regression is your safe bet. 5.11. DROPOUT -Dropoutl in machine learning refers to the process of randomly ignoring certain nodes in a layer during training. In the figure below, the neural network on the left represents a typical neural network where all units are activated. On the right, the red units have been dropped out of the model - the values of their weights and biases are not considered during training. (a) Standard Neural Net (b) After applying dropout Fig. 5.19. Dropout is used as a regularization technique - it prevents overfitting by ensuring that no units are codependent (more on this later). Common Regularization Methods Common regularization techniques include: Early stopping: stop training automatically when a specific performance measure (eg. Validation loss, accuracy) stops improving INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 17 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Below is the process of the stochastic gradient descent algorithm: 1. The algorithm starts at a random point by initializing the weights with random values 2. Then it calculates the gradients at that random point 3. Then it moves in the opposite direction of the gradient 4. The process continues to repeat itself until it finds the point of minimum loss from sklearn.datasets import make_classification from sklearn.linear_model import SGDClassifier from sklearn.model_selection import cross_val_score samples = 500 x, y = make_classification(n_samples=samples, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1) SGD_classifier = SGDClassifier(loss="perceptron", learning_rate="optimal", n_iter_no_change=10) print(cross_val_score(SGD_classifier, x, y, scoring="accuracy", cv=10).mean()) 5.5. ERROR BACKPROPAGATION, FROM SHALLOW NETWORKS TO DEEP NETWORKS Backpropagation is one of the important concepts of a neural network. Our task is to classify our data best. For this, we have to update the weights of parameter and bias, but how can we do that in a deep neural network? In the linear regression model, we use gradient descent to optimize the parameter. Similarly here we also use gradient descent algorithm using Backpropagation. Backpropagation defines the whole process encompassing both the calculation of the gradient and its need in the stochastic gradient descent. Technically, CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 18 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V backpropagation is used to calculate the gradient of the error of the network concerning the network's modifiable weights. The characteristics of Backpropagation are the iterative, recursive and effective approach through which it computes the updated weight to increase the network until it is not able to implement the service for which it is being trained. Derivatives of the activation service to be known at network design time are needed for Backpropagation. Backpropagation is widely used in neural network training and calculates the loss function for the weights of the network. Its service with a multi-layer neural network and discover the internal description of input-output mapping. It is a standard form of artificial network training, which supports computing gradient loss function concerning all weights in the network. The backpropagation algorithm is used to train a neural network more effectively through a chain rule method. This gradient is used in a simple stochastic gradient descent algorithm to find weights that minimize the error. The error propagates backward from the output nodes to the inner nodes. The training algorithm of backpropagation involves four stages which are as follows * Initialization of weights - There are some small random values are assigned. # Feed-forward - Each unit X receives an input signal and transmits this signal to each of the hidden unit Z1, Z2, ... Zn. Each hidden unit calculates the activation function and sends its signal Zi to each output unit. The output unit calculates the activation function to form the response of the given input pattern. ❖ Backpropagation of errors - Each output unit compares activation Yx with the target value TR to determine the associated error for that unit. It is based on the error, the factor 881 (K = 1, m) is computed and is used to distribute the error at the output unit Yk back to all units in the previous layer. Similarly the factor 88;(j = 1, ... . p) is compared for each hidden unit Zj. ❖ It can update the weights and biases. Consider the following Back propagation neural network example diagram to understand: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY N32 nirf 1750 Rank

--- Page 29 ---
Unit: I Course Code/Title:CS3V15/Devops Enterprise services. AWS virtual private cloud. Outsourced cloud services. Salesforce User Controls Operating System, Runtime, Middleware, and Application data Others It is highly scalable and flexible. Microsoft Azure. Force.com, Gigaspaces. Data of the application It is highly scalable to suit the different businesses according to resources. Google cloud services Google offers a seven wide range of Services: · Compute · Networking · Storage and Databases · Big Data · Machine Learning · Identity & Security · Management and Developer Tools IBM cloud analysis. AWS, Terremark Nothing It is highly scalable to suit the small, mid and enterprise level business 1. Compute: GCP provides a scalable range of computing options you can tailor to match your needs. It provides highly customizable virtual machines. and the option to deploy your code directly or via containers. · Google Compute Engine · Google App Engine · Google Kubernetes Engine · Google Cloud Container Registry · Cloud Functions 2. Networking: The Storage domain includes services related to networking, it includes the following services • Google Virtual Private Cloud (VPC) . Google Cloud Load Balancing · Content Delivery Network · What is Google Cloud Connect · Google Cloud DNS · What is Google Cloud Web Hosting 3. Storage and Databases: The Storage domain includes services related to data storage, it includes the following services · Google Cloud Storage . Cloud SQL · Cloud Bigtable · Google Cloud Datastore • Persistent Disk TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 29 N32 nirf 1750 Rank

--- Page 30 ---
Unit: I Course Code/Title:CS3V15/Devops 4. Big Data: The Storage domain includes services related to big data, it includes the following services · Google BigQuery · Google Cloud Dataproc · Google Cloud Datalab · Google Cloud Pub/Sub 5. Cloud AI: The Storage domain includes services related to machine learning, it includes the following services · Cloud Machine Learning · Vision API · Speech API · Natural Language API · Translation API • Jobs API 6. Identity & Security: The Storage domain includes services related to security, it includes the following services · Cloud Resource Manager · Cloud IAM · Cloud Security Scanner · Cloud Platform Security 7. Management Tools: The Storage domain includes services related to monitoring and management, it includes the following services • Stackdriver · Monitoring · Logging · Error Reporting · Trace · Cloud Console 8. Developer Tools: The Storage domain includes services related to development, it includes the following services . Cloud SDK · Deployment Manager · Cloud Source Repositories · Cloud Test Lab AZURE Azure is Microsoft's cloud platform, just like Google has its Google Cloud and Amazon has its Amazon Web Service or AWS.000. Generally, it is a platform through which we can use Microsoft's resources. For example, to set up a huge server, we will require huge investment, effort, physical space, and so on. In such situations, Microsoft Azure comes to our rescue. It will provide us with virtual machines, fast processing of data, analytical and monitoring tools, and so on to make TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 30 N33 narf 1750 Rank

--- Page 7 ---
Course Code/Title:CS3V15/Devops Unit: I DevOps Tools 1. Git (GitLab, GitHub, Bitbucket) Git remains indispensable in software development and DevOps due to its pivotal role in version control, collaborative coding, and efficient project management. As technology has accelerated, the need for streamlined and organized code management has never been greater. Git empowers developers to collaborate on codebases, effortlessly creating and merging branches for new features and bug fixes. Its distributed nature ensures developers can work seamlessly offline, an increasingly valuable feature in today's remote and distributed work environments. Additionally, Git facilitates the tracking of code modifications, making it easier to identify when and why specific changes were made, a critical aspect of maintaining code quality and security. Software development is essential in driving innovation and advancing progress, and Git maintains its prominent position as the bedrock of efficient, cooperative, and secure coding methodologies. 2. Maven Due to its enduring significance in managing project dependencies, building, and project lifecycle management, Maven remains a pivotal tool in SD and DevOps. As a robust build automation and project management tool, Maven simplifies the complexities of Java-based project development by streamlining the compilation, testing, packaging, and distribution processes. It ensures consistent and reproducible builds, making it easier for development teams to collaborate efficiently and deliver high-quality software. Maven's role in managing dependencies and facilitating continuous integration and deployment remains crucial. Its ability to handle complex build scenarios and integrate seamlessly with modern DevOps practices makes it indispensable for ensuring software projects' reliability, maintainability, and scalability in 2024 and beyond. 3. Jenkins Its importance lies in its role as a powerful automation server that enables continuous integration and continuous delivery (CI/CD) pipelines. Jenkins streamlines software development by automating tasks such as building, testing, and deploying code changes, ensuring that software is delivered quickly and highly. With the growing complexity of modern applications, the need for efficient CI/CD processes has become even more paramount. Jenkins provides flexibility, extensibility, and a vast library of plugins that cater to a wide range of technologies and tools, making it adaptable to diverse development environments. As organizations prioritize speed, reliability, and collaboration in their software development practices, Jenkins stands as a cornerstone tool, enabling teams to achieve seamless automation and efficient delivery of software solutions. 4. Chef Chef, a powerful automation platform, is crucial in managing infrastructure as code. Chef empowers organizations to achieve scalability, reliability, and speed seamlessly. By allowing the automation of server provisioning, configuration, and maintenance, Chef enhances efficiency and consistency across the entire infrastructure, reducing manual errors and ensuring that infrastructure remains desired. Moreover, Chef integrates smoothly with various cloud providers, containerization technologies, and other DevOps tools, making it adaptable to the ever-evolving tech TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 7 N32 nirf 1750 Rank

--- Page 8 ---
Course Code/Title:CS3V15/Devops Unit: I landscape. As organizations prioritize agility and scalability, Chef remains a vital tool in automating complex infrastructure tasks and enabling DevOps teams to focus on innovation and delivery. 5. Puppet Puppet is essential because it simplifies the management and orchestration of complex IT infrastructures by allowing administrators to define infrastructure as code. It ensures consistency and repeatability in configuration across servers, cloud instances, and containers. Businesses increasingly rely on diverse, dynamic, and hybrid infrastructures. Puppet's importance lies in its ability to streamline provisioning, configuration, and continuous compliance, thus reducing operational complexity, minimizing errors, and accelerating software delivery. Puppet continues to empower organizations to efficiently manage and scale their infrastructure while maintaining high levels of security and compliance, making it a crucial tool for DevOps teams. 6. Ansible Ansible is a powerful and widely adopted automation and configuration management tool important in 2024 for several reasons. This tool stands out for its simplicity and versatility. It empowers organizations to automate repetitive tasks, provisioning of infrastructure, and configuration management across diverse environments, making it an invaluable asset for DevOps and IT teams. Furthermore, Ansible's agentless architecture, declarative language, and a vast library of pre- built modules make it accessible to both beginners and seasoned professionals. As organizations prioritize efficiency, scalability, and the rapid deployment of applications and services, Ansible remains an indispensable DevOps toolkit, helping teams streamline operations, enhance security, and maintain infrastructure at scale, all while reducing manual errors and increasing agility in a fast-paced technological landscape. 7. Docker Docker is crucial in modern software development and DevOps practices. It can simplify and streamline the management of applications across various environments. Docker containers encapsulate an app and its dependencies, ensuring consistent and reproducible deployments from development to production. This technology enhances portability and scalability, accelerates development cycles, and reduces the "it works on my machine" problem. In a rapidly evolving software landscape, Docker's containerization approach remains crucial for achieving efficient, isolated, and highly flexible application deployment, making it an essential component of DevOps and continuous delivery pipelines. 8. Kubernetes Kubernetes, often abbreviated as K8s, play a central role in modern software development and operations. Its importance lies in its ability to orchestrate, manage, and automate containerized applications at scale. As organizations increasingly embrace microservices architectures and containerization for their applications, Kubernetes provides the essential infrastructure for deploying, scaling, and maintaining these containers efficiently. The tool's resilience, self-healing capabilities, and support for hybrid and multi-cloud environments make it vital for achieving agility, reliability, and cost-effectiveness in application deployment. It serves as the backbone of cloud-native ecosystems, enabling organizations to accelerate software delivery, improve resource utilization, and respond effectively to the evolving demands of the digital landscape. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N32 nirf 1750 Rank

--- Page 57 ---
Course Code/Title:CS3V15/Devops · Go to the File option · In the drop-down menu, select New · Select the Project option Unit: II If you want to create a Java project, you can select the "Java Project" option. Since we are not creating a Java project specifically, we have chosen the "Project" option. eclipse-workspace - Eclipse File Edit Source Refactor Navigate Search Project Run Window Help New Open File ... G) Open Projects from File System ... Shift+Alt+N Close Close All Shift + Cb1+W Save Save As. Save All Shift+Ctrl+S Revert Move .:. Rename 2 Refresh ES Convert Line Delimiters To Print Import ... Export ... Ctrl+P de java Project ui Project ... El Package G Class Interface G Enum @ Annotation 69 Source Folder 18 Java Working Set Folder File Untitled Text File Task JUnit Test Case Example ... Other ... Ctrl+N The dialog box that appears on the screen will display different types of projects. · Select the Maven Project option . Click on Next New Project + Select a wizard Create a Maven Project Wizards: type filter text irroject Gradle Java 2º Java Project #Java Project from Existing Ant Buildfile - @Maven Lu Check out Maven Projects from SCM M& Maven Module Maven Project Examples ? < Back Next > X Cancel Finish A dialog box will appear. Select the default workspace. . Click on "Next" Several Group IDs, Artifact IDs, and Versions will then appear. · Select a plugin there and click on "Next" CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 16 N33 narf 1750 Rank

--- Page 58 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Select an Archetype M Catalog: All Catalogs Configure .. Biter: × Group Id Artifact id Version org.apache.maven.archetypes maven-archetype-archetype 1.0 org.apache.maven.archetypes maven-archetype-|2ee-simple 1.0 org.apache.maven archetypes maven-archetype-plugin 1.2 org.apache.maven.archetypes maven-archetype-plugin-site 1.1 org.apache.maven.archetypes maven-archetype-portlet 1.0.1 org.apache.maven.archetypes maven-archetype-profiles 1.0-alpha-4 org.apache.maven.archetypes maven-archetype-quickstart 1.1 org.apache.maven.archetypes maven-archetype-site 11 org anacha mauan archabenes masan.archchina.cito.cimnia 11 An archetype which contains a sample Maven project. Show the last version of Archetype only Include snapshot archetypes Add Archetype ... > Advanced ? < Back Next > Cancel Finish In the next dialog box that appears, you'll complete the following steps: · Enter the Group ID “com.xyz" · Enter the Artifact ID "mavenproject" · The version will appear on the screen These items can all be modified at a later time if needed. · Click on "Finish" CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 17 N33 nirf 1750 Rank

--- Page 3 ---
Course Code/Title:CS3V15/Devops Unit: I Key Goals and Benefits of DevOps Goals of DevOps The fast-paced growth of the IT industry and continuous advancements in technology make it critical to set DevOps goals that are experimental and challenging for companies to compete and thrive in the market. Here are the key goals and principles that every successful DevOps program has in common. 1. Ensures effective collaboration between teams: Effective collaboration in any process relies on shared ownership. During the development process, all those involved should embrace the fact that everyone is equally responsible for the entire development process. Whether it is development, testing, or deployment, each team member should be involved. They should understand that they have an equal stake in the final outcome. In the DevOps paradigm, passing of work from one team to another is completely defined and broken down. This accelerates the entire process of development since collaboration between all the teams involved is streamlined. 2. Creates scalable infrastructure platforms: The primary focus of DevOps is to create a sustainable infrastructure for applications that make them highly scalable. According to the demands of the modern-day business world, scalable apps have become an absolute necessity. In an ideal situation, the process of scaling should be reliable and fully automated. As a result, the app will have the ability to adapt to any situation when a marketing effort goes viral. With the app being scalable, it can adjust itself to large traffic volumes and provide an immaculate user experience. 3. Builds on-demand release capabilities: Companies must focus on keeping their software in a 'releasable' state. Continuous delivery will allow the software to add new features and go live at any stage. DevOps aims to automate the process of release management because it has a plethora of advantages. Automated release management is predictable, fast, and very consistent. Moreover, through automation, companies can release new versions as per their requirements. Automated release management also has complete and thorough audit trials, as these are essential for compliance purposes. 4. Provides faster feedback: Automating monotonous tasks such as testing and reporting will accelerate the process of rapid feedback. Since the development team will know what has to change, it can roll out the updated version faster. In addition, the team can better understand the impact of the changes that it has done in the software lifecycle. A concrete understanding of changes will assist team members in working efficiently in tandem. With rapid feedback, the operations team and developers can make better decisions collectively and enhance the app's performance. Benefits of DevOps DevOps helps organizations deliver added value to their customers. Here are some compelling benefits of DevOps. 1. Smarter work and faster release: With DevOps, your development team can release the required deliverables quickly. Faster release of deliverables will keep you miles ahead of your competitors, which is very important in today's cut-throat business realm. Businesses should understand that if their review cycle is not automated, it will slow down the release process. Moreover, the inclusion of disparate tools will lead to context switching and higher costs. Thus, DevOps can help rectify this worrisome business situation. 2. Quick resolution of issues: In a business world where speed and accuracy are paramount, a fast feedback loop will help you thrive. With DevOps, the communication process becomes seamless, and, as such, it minimizes the time required to solve issues. Without open communication, key issues can slip out of mind, which will have serious repercussions in the long run. DevOps fosters open communication that helps resolve issues, thus unblocking the release pipeline faster. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 3 N32 nirf 1750 Rank

--- Page 4 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Better collaboration between teams: DevOps paves the way for more dynamic and round-the- clock communication between teams. It renders an environment for mutual collaboration and integration among teams that are distributed globally. Eliminating the traditional departmental barriers between teams forms a new sense of ownership, wherein each team member feels equally responsible for meeting delivery timelines. This collaboration contributes to happier and more engaged employees. 4. Fostering innovative mindsets: With DevOps, deployment phases of the application are more relaxed as compared to traditional methods. This is because it streamlines the entire process, ensures that there are no lapses in quality, and allows on-time and efficient release. Thus, as everything is in order, the development team is more at peace. This allows it to think out of the box and provide additional value to the user. Having a development team with an innovative mindset is a boon for any business organization. An innovative approach, in itself, has immense scope and leads to better quality and resolution of issues at hand. Thus, through DevOps, the process of expanding the horizon of an app becomes much easier. 5. Faster threat detection: Automated and continuous testing of the code will make the process of threat detection faster. As developers can locate problem areas at an early stage, they can then resolve them faster. Thus, DevOps is a vital cog in maintaining and enhancing the quality and performance of an app. As the overall build of the app is in capable hands, teams working together are empowered to share feedback as and when necessary. 6. Increased customer satisfaction: Customer satisfaction is paramount in any day and age, irrespective of the business one is involved in. DevOps is known for enhancing customer experience, which ultimately increases the level of customer satisfaction. Dissatisfied customers are never a good sign for any business. Feedback loops are an important component of DevOps. These loops empower end users to track the progress of app development at various stages. 7. In addition, they can suggest changes (if any) or give their inputs to make the app more customer- centric. Due to their dynamic nature, feedback loops help developers and customers remain on the same page. Moreover, DevOps accelerates the process of app development, which eventually lessens the delivery timer. This has a positive impact on the customer satisfaction ratio. 8. Providing the much-needed edge: Along with staying true to their development process, companies need to ensure that they sustain themselves in the cut-throat competition. Implementing DevOps can be your trump card to provide your organization with that much- needed edge. Competitive advantage is necessary, as it can become the deciding factor in the popularity of an application in many cases. Some factors set expert businesses apart from mediocre ones: · Top-quality features · Quicker and timely software releases · Maximizing return on investments · Listening to constructive feedback Difference between Agile and DevOps Agile: Agile program advancement comprises different approaches to computer program improvement beneath which prerequisites and arrangements advance through the collaborative exertion of self- organizing and cross-functional groups and their customer/end client. DevOps: DevOps could be a set of hones that combines program improvement and information- technology operations which points to abbreviating the framework's advancement life cycle and giving nonstop conveyance with tall program quality. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 4 N32 nirf 1750 Rank

--- Page 5 ---
Unit: I Course Code/Title:CS3V15/Devops Example: Facebook's mobile app which is updated every two weeks effectively tells users you can have what you want and you can have it. Now ever wondered how Facebook was able to do social smoothing? It's the DevOps philosophy that helps facebook and sure that apps aren't outdated and that users get the best experience on Facebook. Facebook accomplishes this true code ownership model that makes its developers responsible that includes testing and supporting through production and delivery for each kernel of code. They write and update their true policies like this but Facebook has developed a DevOps culture and has successfully accelerated its development lifecycle. Difference between Agile and DevOps: S. No. Agile 1 It started in the year 2001. It started in the year 2007. 2 Invented by John Kern, and Martin Fowler. Invented by John Allspaw and Paul Hammond at Flickr, and the Phoenix Project by Gene Kim. 3 Agile is a method for creating software. It is not related to software development. Instead, the software that is used by DevOps is pre-built, dependable, and simple to deploy. 4 An advancement and administration approach. Typically a conclusion of administration related to designing. 5 The agile handle centers on consistent changes. DevOps centers on steady testing and conveyance. 6 A few of the finest steps embraced in Agile are recorded underneath - 1. Backlog Building 2.Sprint advancement DevOps to have a few best hones that ease the method - 1. Focus on specialized greatness. 2. Collaborate straightforwardly with clients and join their feedback 7 Agile relates generally to the way advancement is carried of, any division of the company can be spry in its hones. This may be accomplished through preparation. 8 All the group individuals working in a spry hone have a wide assortment of comparable ability sets. This is often one of the points of interest of having such a group since within the time of requirement any of the group individuals can loan help instead of holding up for the group leads or any pro impedances. 9 Spry accepts "smaller and concise". Littler the group superior it would be to convey with fewer complexities. 10 Since Agile groups are brief, a foreordained sum of time is there which are sprints. Tough, it happens that a sprint has endured longer than a month but regularly a week long. 11 A big team for your project is not required. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY DevOps DevOps centers more on program arrangement choosing the foremost dependable and most secure course. DevOps features a diverse approach and is very viable, most of the time it takes after "Divide and Conquer". Work partitioned among the improvement and operation groups. DevOps, on the other hand, accepts that "bigger is better". DevOps, on the other hand, prioritizes reliabilities. It is since of this behavior that they can center on a long-term plan that minimizes commerce's unsettling influences. It demands collaboration among different teams for the completion of work. 5 N33 nirf 1750 Rank

--- Page 6 ---
Unit: I Course Code/Title:CS3V15/Devops 12 Some of the Tools- · Bugzilla · JIRA · Kanboard and more. 13 It is suitable for managing complex projects in any department. It centers on the complete engineering process. 14 It does not focus on the automation. It focusses on automation. 15 Working system gets more significance in Agile than documentation. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Some of the Tools- · Puppet • Ansible • AWS · Chef · team City OpenStack and more. The process documentation is significant in DevOps. 6 N32 nirf 1750 Rank

--- Page 21 ---
Unit: I Course Code/Title:CS3V15/Devops · Gaming: AWS has been serving many gaming studios. Combining Amazon EC2 and S3 services with CloudFront enables gaming websites to deliver high-quality gaming experiences to their customers regardless of location. Use Cases of AWS · Netflix Netflix is an entertainment platform that started in the United States, but eventually, it expanded to many countries and soon became popular. However, once Netflix confronted the scalability problem because of the sudden increase in viewers. That made Netflix choose AWS services. Netflix reports that when it started using AWS services like DynamoDB and Cassandra for its distributed databases, it could handle the data easily. So, scalability is a great advantage of AWS. Netflix has adapted around 100,000 server instances from AWS for computing and storage databases, analytics, recommendation engines, and video transcoding as well. . McDonald's McDonald's is the world's largest fast-food company that serves around 64 million people per day. The growth of this company has gone to another level when it started home deliveries. By utilizing AWS services, McDonald's created a platform that integrates local restaurants with delivery partners such as Uber Eats. Scalability is also a reason for the company to choose AWS services. Moreover, with AWS Microservices Architecture, McDonald's platform can scale 20,000 orders per second and integrate with the global partners easily. · Airbnb Airbnb is an international online marketplace for rental homes. This platform connects people who are looking for rental accommodation with those who want to rent out their houses. Quite soon, Airbnb became unable to handle the constant streaming of data on the website from its customers. That is when it started using Amazon EC2 service and Elastic Load Balancing, which distributes incoming traffic to multiple Amazon EC2 instances. In this way, Airbnb could avoid traffic, and customers could use the online platform without any disruption. • Novartis Novartis is the best example for AWS use cases in healthcare. Novartis is one of the world's largest healthcare companies that provides solutions for patients' well-being. It adapted Amazon EC2 services and built a platform using other services such as Amazon Simple Storage Service, Amazon Elastic Block Store, and four availability zones. Data Analysts of Novartis are taking advantage of the AWS services and still implementing new solutions for the patients. · Expedia Expedia is a worldwide online travel agency that has always focused on the constant development and innovation of its platform to offer an extraordinary user experience for its clients. Since 2010, Expedia has been using AWS services to build a standard deployment model for better infrastructure as AWS offers the best data security through different availability zones. • Samsung If you are using Samsung mobile phones, then you may know about the Samsung app store. For setting up the apps stacked in its store, the company started using AWS services. Using AWS app development services, Samsung wanted to provide its customers with the facility to download the apps anywhere without any network traffic. • NASA NASA (National Aeronautics and Space Administration) has always wondered about creating a library to present people with all its achievements through pictures and videos of space. Later on, it created such platforms, but because it had 10 different NASA centers, it couldn't provide the best experience for viewers. So, all it wanted was to create an easy-access platform for TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 21 N32 nirf 1750 Rank

--- Page 22 ---
Unit: I Course Code/Title:CS3V15/Devops people to search for and view images and videos. Then, NASA started adopting many services from AWS to solve this problem, which included Amazon Elastic Compute Cloud, Elastic Load Balancing, Amazon Simple Storage Service, Amazon Simple Queue Service, etc. Among these, Amazon S3 helped the company store all the incoming data such as photos, videos, and audio files without any hassle. • Facebook Facebook, without a doubt, is a widespread social media platform. To build a scalable application, Facebook used services such as Amazon Elastic Compute Cloud, Amazon Simple Storage Service, Amazon Relational Database Service, Amazon SimpleDB, Amazon CloudFront, Amazon Simple Queue Service, etc. Amazon RDS helps the platform to make it easy to set up, operate, and scale the database in the cloud. Various Services offered by AWS · Amazon EC2 (Elastic Cloud computing) · Amazon RDS (Relational Database Services) · Bonus Service: Amazon Connect · Amazon S3 (Simple Storage Service) · Amazon Lambda · Amazon Cognito · Amazon Glacier · Amazon SNS (Simple Notification Service) · Bonus Service: Amazon Lex · Amazon Lightsail · Amazon VPC (Virtual Private Cloud) · Amazon Kinesis · Amazon Inspector · Amazon Auto-scaling · Amazon IAM (Identity and Access Management) · Dynamo DB · Amazon SQS (Simple Queue Service) · Amazon ElastiCache · Amazon Chime · AWS Athena · Code Catalyst · Web Application Firewall · AWS Amplify · AWS Rekognition · AWS QuickSight · AWS CloudFormation · AWS Management Console The Important Cloud Services according to various categories that are provided by AWS are given below : 1. Compute • Amazon EC2: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It allows organizations to obtain and configure virtual compute capacity in the cloud. You can select from a variety of operating systems and resource configurations like memory, CPU, and storage that are required for your TECHNOLOGY (Autonomous) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY 22 N33 narf 1750 Rank

--- Page 55 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Environment Variables Now remove active profile from maven settings.xml and update the test profile mentioned in pom.xml. Add activation element to profile element as shown below. The test profile will trigger when the system property "env" is specified with the value "test". Create an environment variable "env" and set its value as "test". <profile> <id>test</id> <activation> <property> <name>env</name> <value>test</value> </property> </activation> </profile> Let's open command console, go to the folder containing pom.xml and execute the following mvn command. C:\MVN\project>mvn test Profile Activation via Operating System Activation element to include os detail as shown below. This test profile will trigger when the system is windows XP. <profile> <id>test</id> <activation> <os> <name>Windows XP</name> <family>Windows</family> <arch>x86</arch> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 14 N32 narf 1750 Rank -

--- Page 56 ---
Course Code/Title:CS3V15/Devops Unit: II <version>5.1.2600</version> </os> </activation> </profile> Now open command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test Profile Activation via Present/Missing File Now activation element to include OS details as shown below. The test profile will triggerwhen target/generated-sources/axistools/wsdl2java/com/companyname/group is missing. <profile> <id>test</id> <activation> <file> <missing>target/generated-sources/axistools/wsdl2java/ com/companyname/group</missing> </file> </activation> </profile> Now open the command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test 8. Maven create and build artifacts: The first step is to open Eclipse, which comes with the integrated Maven environment. The Eclipse window opens on the screen. Complete the following steps: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 15 N32 nirf 175º Rank -

--- Page 11 ---
Course Code/Title:CS3V15/Devops Unit: I 17. Selenium It remains a vital tool in software testing and automation due to its enduring relevance in ensuring the quality of web applications. As technology evolves, web applications become increasingly complex, requiring thorough testing across various browsers and platforms. With its robust automation capabilities and extensive browser compatibility, Selenium allows developers and QA teams to automate repetitive testing tasks efficiently, conduct cross- browser testing, and ensure that web applications function flawlessly across diverse environments. Its open-source nature, active community support, and integration with other DevOps tools make Selenium a go-to choice for organizations striving for continuous delivery and the rapid deployment of high-quality software, a cornerstone of modern software development practices. 18. Gremlin Gremlin is an essential tool in chaos engineering, which has become increasingly critical for ensuring the resilience and reliability of modern software systems. As technology advances and complex distributed systems become the norm, the potential for unexpected failures and outages also rises. Gremlin allows organizations to proactively identify weaknesses and vulnerabilities in their infrastructure and applications by simulating controlled failures, such as network disruptions, service outages, and resource constraints. By intentionally inducing chaos and monitoring the system's response, teams can uncover weaknesses before they lead to costly downtime or security breaches. Gremlin facilitates organizations to build more robust, fault-tolerant systems that can withstand real-world challenges and deliver uninterrupted services to users. 19. ServiceNow ServiceNow is a vital platform for organizations seeking to streamline their IT service management and beyond. Its significance lies in its ability to provide a unified, cloud-based solution for automating and optimizing various business processes, including ITSM, ITOM, HR, customer service, and more. Due to the rapid digitization of services, remote work, and the growing complexity of technology infrastructures, ServiceNow offers a comprehensive approach to managing workflows, resolving issues, and delivering services efficiently. Its intelligent automation capabilities, analytics, and AI-driven insights empower organizations to enhance productivity, agility, and customer satisfaction while reducing operational costs. ServiceNow's role in orchestrating and integrating diverse systems and processes makes it an indispensable tool for driving digital transformation and ensuring smooth operations in the ever-evolving business landscape of 2024. 20. Status Service Updates: The Status Page "Status Service Updates: The Status Page" is a critical tool for organizations and businesses of all sizes. In today's world, where online services and applications are integral to operations, ensuring their availability and reliability is essential. It provides real-time information to users and stakeholders about the operational status of services, applications, and infrastructure. The Status Page plays a crucial role in transparency, trust-building, and customer satisfaction by promptly communicating service disruptions, planned maintenance, and incident resolutions. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 1750 Rank

--- Page 12 ---
Course Code/Title:CS3V15/Devops Unit: I Downtime can often lead to significant financial losses and damage to a company's reputation, so having a practical Status Page becomes not just a convenience but a necessity. It allows organizations to showcase their commitment to transparency and responsiveness in addressing service-related issues, ultimately fostering stronger customer relationships and trust. 21. ELK (Elasticsearch, Logstash and Kibana) ELK, which stands for Elasticsearch, Logstash, and Kibana, continues to shine in DevOps and IT operations. This powerful trio of tools remains essential for organizations seeking effective log management, monitoring, and data visualization. Elasticsearch is a highly scalable and fast search engine that enables real-time data indexing and search. Logstash facilitates the collection, processing, and transformation of log data from various sources, making it compatible with Elasticsearch. Kibana, on the other hand, provides a user- friendly interface for visualizing and analyzing data, offering customizable dashboards and powerful data exploration capabilities. ELK's significance in 2024 lies in its ability to empower organizations with comprehensive insights into their systems, applications, and infrastructure. It ultimately facilitates quick problem resolution, proactive monitoring, and data-driven decision-making in an increasingly complex and fast-paced technological landscape. 22. GitLab CI/CD GitLab CI/CD's significance lies in its ability to automate the complete software delivery pipeline, from code changes to deployment, in a single integrated environment. GitLab CI/CD ensures rapid and reliable delivery of software updates. It enables continuous integration (CI) by automatically building and testing code changes, allowing teams to catch issues early in the development cycle. Furthermore, the continuous deployment (CD) aspect automates the release and deployment process, reducing the risk of human errors and enabling organizations to deliver features and updates to users swiftly and confidently. GitLab CI/CD's importance is further accentuated as businesses seek to accelerate digital transformation efforts, respond rapidly to changing market demands, and maintain a competitive edge through efficient and automated software delivery practices. 23. Scripting Scripting remains vital due to its pivotal role in automating and streamlining various aspects of software development, system administration, and DevOps practices. Scripting languages like Python, Bash, and PowerShell empower tech professionals to write code that can execute repetitive tasks, manipulate data, and orchestrate complex processes efficiently. Scripting facilitates rapid prototyping, configuration management, and the creation of automated deployment pipelines. It enhances productivity, ensures consistency and reduces human error in tasks ranging from software testing and deployment to infrastructure provisioning and monitoring. As organizations increasingly embrace DevOps and cloud- native technologies, scripting stays competitive and adaptive in the tech landscape. 24. Terraform Terraform plays a crucial role in modern infrastructure provisioning and management. It allows organizations to define and deploy infrastructure as code, enabling the automated creation and configuration of cloud resources, containers, and other infrastructure components. Cloud computing, microservices, and containerization have become the norm in CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 12 N32 nirf 1750 Rank

--- Page 59 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Specify Archetype parameters M Group Id: com.simplileam Artifact Id: mavenproject Version: 0.0.1-SNAPSHOT Package: com,simplilearn.mavenproject . Properties available from archetype: Name Value Add ... Remove Advanced ? < Back Next > Cancel Finish The project is now created. · Open the pom.xml file You can see all the basic information that you have entered on the screen, such as the Artifact ID, Group ID, etc. You can see the junit dependencies have been added. This process takes place by default in Eclipse. There will also be some by default test cases. File Edit Source Navigate Search Project Run Window Help # Package Explorer E Jo JUnit - mavenproject/pom.xml 28 mavenproject > ( src/main/java ( src/test/java > = JRE System Library (|2SE-1:5] i Maven Dependencies target 7 <artifactIdaavenproject</artifactId> <version>0. 0. 1-SNAPSHOT</version>: <packaging>jar</packaging> <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 136 sproperties> 14 <project. build.sourceEncoding>UTF-8</project. build.sourceEncoding> </properties> 15 16 «dependenciess 18- <dependency> 19 <groupId>junit</grouptasI <artifactId=junit</artifactle> sversion>3.8.1</version> <scope>tests/scope> 21 22 223 </dependency> 24 </dependencies> 25 </project> Overview Dependencies Dependency Hierarchy |Effective POM pom.xml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 18 N32 nirf 175º Rank -

--- Page 60 ---
Course Code/Title:CS3V15/Devops Unit: II There you can find AppTest.java to be a default test case. When you click on that, you can see the test cases written in JUnit on your Eclipse screen. # Package Explorer 13 Ju JUnit mavenproject/pom.xml Z App.java AppTest.java 12 1 package com.simplilearn.mavenproject; - fl mavenproject + ( src/main/java - il com.simplileam.mavenproject U App.java = ( src/test/java il com.simplileam.mavenproject , E AppTest.java > > JRE System Library ||2SE-1.5] al Maven Dependencies - target pom.xml 3w import jumit. framework. Test:[] 6 76 / ** . Unit test for simple App. 10 public class AppTest extends TestCase 12 1 14 . Create the test case 15 16 * @param testName name of the test cole 17 18% public AppTest( String testNane ) 19 1 super( testName ); 28 21 1 22 :23% 24 * @return the suite of tests being tested 75 When it comes to adding more test cases, it will depend on the user, but these test cases and commands can easily be added in the workspace. If we try to remove certain dependencies from our file, we will receive error messages. To troubleshoot this, complete the following steps: · Go to another tab: mavenproject/pom.xml · Delete any dependencies · Save the file Immediately, there will be several error messages in the AppTest.java. R mavenproject/pom.xml App.java AppTest.java 83 1 package com.simplilearn.mavenproject; 2 3@ import junit. framework. Test;[] 6 mavenproject/src/test/java/com/simplileam/mavenproject/AppTest java 8 * Unit test for simple App. 9 18 public class AppTest extends TestCase 011 12 { 136 14 . Create the test case 15 16 * @param testName name of the test case 17 188 19 € public AppTest( String testName ) 28 super( testName ); 21 22 - 230 24 * @return the suite of tests being tested Problems E @ Javadoc ) Declaration History 7 errors, 1 warning, 0 others Description + @ Errors (7 items) a Test cannot be resolved to a type Resource Path Location AppTest.java /mavenproject/src/te line 26 Java Problem Type CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 19 N32 nirf 1750 Rank -

--- Page 39 ---
Course Code/Title:CS3V15/Devops Unit: I GITHUB GitHub is an increasingly popular programming resource used for code sharing. It's a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in October 2020, it is the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 56 million total developers. GitHub is a Git repository hosting service that provides a web-based graphical interface. It is the world's largest coding community. Putting a code or a project into GitHub brings it increased, widespread exposure. Programmers can find source codes in many different languages and use the command-line interface, Git, to make and keep track of any changes. GitHub helps every team member work together on a project from any location while facilitating collaboration. You can also review previous versions created at an earlier point in time. GitHub's Features? 1. Easy Project Management: GitHub is a place where project managers and developers come together to coordinate, track, and update their work so that projects are transparent and stay on schedule. 2. Increased Safety With Packages Packages can be published privately, within the team, or publicly to the open-source community. The packages can be used or reused by downloading them from GitHub. 3. Effective Team Management GitHub helps all the team members stay on the same page and organized. Moderation tools like Issue and Pull Request Locking help the team to focus on the code. 4. Improved Code Writing Pull requests help the organizations to review, develop, and propose new code. Team members can discuss any implementations and proposals through these before changing the source code. 5. Increased Code Safety GitHub uses dedicated tools to identify and analyze vulnerabilities to the code that other tools tend to miss. Development teams everywhere work together to secure the software supply chain, from start to finish. 6. Easy Code Hosting All the code and documentation are in one place. There are millions of repositories on GitHub, and each repository has its own tools to help you host and release code. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 39 N32 nirf 1750 Rank

--- Page 40 ---
Course Code/Title:CS3V15/Devops Unit: I HOSTING SERVICE FOR GIT REPOSITORY When it comes to hosting Git repositories, various platforms provide a robust infrastructure for collaborative development, version control, and project management. Choosing the right hosting service depends on factors like ease of use, collaboration features, and integration capabilities. Here are some popular Git hosting services widely utilized in the software development community: 1. GitHub: GitHub stands out as one of the most prevalent Git hosting platforms, offering a user- friendly interface, powerful collaboration features, and seamless integration with various tools. It serves as an ideal choice for open-source projects, private repositories, and team collaboration. 2. GitLab: GitLab is a comprehensive web-based Git repository manager that not only provides source code management but also includes features like continuous integration. It caters to both cloud-based and self-hosted solutions, giving users flexibility in hosting their repositories. 3. Bitbucket: Owned by Atlassian, Bitbucket is another popular Git repository hosting service. Supporting both Git and Mercurial repositories, it offers features like code collaboration, issue tracking, and continuous integration. Bitbucket is often preferred by teams using other Atlassian tools such as Jira and Confluence. 4. GitKraken Glo Boards: GitKraken Glo Boards is an integrated task and issue tracking service linked with GitKraken, a Git client. This platform allows teams to manage tasks directly associated with their Git repositories and provides a visual approach to monitoring project progress. 5. SourceForge: SourceForge, with a long history, hosts open-source software projects and offers version control, bug tracking, and project management tools. While not as prominent as some other options, it remains a viable choice for numerous projects. 6. AWS CodeCommit: As part of Amazon Web Services (AWS), AWS CodeCommit is a fully managed source control service. It seamlessly integrates with other AWS services and provides a secure and scalable environment for hosting Git repositories. Selecting the most suitable Git hosting service depends on your team's requirements, project size, and preferences for cloud-based or self-hosted solutions. Each platform has its strengths, catering to specific use cases within the software development landscape. Difference between GIT and GITHUB Git and GitHub are related concepts but serve different purposes in the context of version control and collaborative software development. Git: Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows. GitHub: GitHub is a web-based Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 40 N32 nirf 1750 Rank

--- Page 25 ---
Course Code/Title:CS3V15/Devops Unit: I · backups · automated multi-region replication in-memory caching · data export tools. Global Infrastructure of AWS · AWS is a cloud computing platform which is globally available. . Global infrastructure is a region around the world in which AWS is based. Global infrastructure is a bunch of high-level IT services which is shown below: AWS is available in 19 regions, and 57 availability zones in December 2018 and 5 more regions 15 more availability zones for 2019. The following are the components that make up the AWS infrastructure: o Availability Zones o Region o Edge locations o Regional Edge Caches Compnents of Global Infrastructure Availability zones Region Edge Locations Regional Edge Caches Availability zone as a Data Center An availability zone is a facility that can be somewhere in a country or in a city. Inside this facility, i.e., Data Centre, we can have multiple servers, switches, load balancing, firewalls. The things which interact with the cloud sits inside the data centers. o An availability zone can be a several data centers, but if they are close together, they are counted as 1 availability zone. Region o A region is a geographical area. Each region consists of 2 more availability zones. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 26 ---
Course Code/Title:CS3V15/Devops Unit: I o A region is a collection of data centers which are completely isolated from other regions. · A region consists of more than two availability zones connected to each other through links. AZ-a AZ-b AZ-c Availability zones are connected through redundant and isolated metro fibers. Edge Locations o Edge locations are the endpoints for AWS used for caching content. o Edge locations consist of CloudFront, Amazon's Content Delivery Network (CDN). o Edge locations are more than regions. Currently, there are over 150 edge locations. Edge location is not a region but a small location that AWS have. It is used for caching the content. o Edge locations are mainly located in most of the major cities to distribute the content to end users with reduced latency. o For example, some user accesses your website from Singapore; then this request would be redirected to the edge location closest to Singapore where cached data can be read. Regional Edge Cache o AWS announced a new type of edge location in November 2016, known as a Regional Edge Cache. o Regional Edge cache lies between CloudFront Origin servers and the edge locations. o A regional edge cache has a large cache than an individual edge location. o Data is removed from the cache at the edge location while the data is retained at the Regional Edge Caches. o When the user requests the data, then data is no longer available at the edge location. Therefore, the edge location retrieves the cached data from the Regional edge cache instead of the Origin servers that have high latency. What Is Cloud Computing? Cloud computing is the use of hardware and software components in an off-premises location to deliver a service to a network. Users can access files and applications from any device that can access the internet. Some features and capabilities include: · Cloud providers can pull the computing resources to provide services to multiple customers with the help of a multi-tenant model · Cloud computing proves an on-demand self-service, which helps administrators monitor performance CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 26 N32 nirf 1750 Rank -

--- Page 1 ---
Course Code/Title:CS3V15/Devops CHENNAI INSTITUTE OF TECHNOLOGY Transforming Lives CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) NAME OF THE PROGRAMME B. E (CSE) YEAR III SEMESTER V REGULATIONS 2022R COURSE CODE CS3V15 COURSE NAME DEVOPS FACULTY NAME (Prepared by) NAME OF SUBJECT EXPERT (Verified by) M.SUNDHARI CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Unit: I N33 INATIONAL BOARD . ACCREDITATION 100% Accreditation All eligible UG & PG Programs NIRF 151 - 200 Band Engineering 2023 A+ DE NAAC Contact 8681927167 Contact 1 N32 nirf 1750 Rank

--- Page 2 ---
Course Code/Title:CS3V15/Devops Unit: I UNIT - I Introduction to Devops Devops Essentials - Introduction To AWS, GCP, Azure - Version control systems: Git and Github. The word "DevOps" was coined in 2009 by Patrick Debois, who became one of its gurus. The term was formed by combining "development" and "operations," which provides a starting point for understanding exactly what people typically mean when they say "DevOps." Notably, DevOps isn't a process or a technology or a standard. Many devotees refer to DevOps as a "culture"-a viewpoint that New Relic favors. We also use the term "DevOps movement" when talking about topics such as adoption rates and trends for the future, and "DevOps environment" to refer to an IT organization that has adopted a DevOps culture. "DevOps represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile, lean practices in the context of a system-oriented approach. DevOps emphasizes people (and culture), and seeks to improve collaboration between operations and development teams. DevOps implementations utilize technology- especially automation tools that can leverage an increasingly programmable and dynamic infrastructure from a life cycle perspective." DevOps is defined as a combination of processes and tools created to facilitate organizations in delivering services and applications much faster than they can through conventional software development processes. It helps increase customers' confidence in the applications that an organization offers, thereby allowing the company to flourish and achieve its business goals faster. Development 3 3 DevOps 13 51 Operations Quality Assurance (QA) How DevOps Works? A DevOps process can be summarized as an infinite loop that comprises the following stages - build, test, and release through the delivery pipeline and plan and monitor through feedback, which resets the loop again. With such an amazing combination, teams use tech stack and tooling that assists them in reliably developing apps. Moreover, going away from the Under the DevOps model, development and operations teams work in constant cohesion throughout the entire project lifecycle, starting right from development to deployment. When security is the main focus, the quality assurance team is tightly knitted with the DevOps team throughout the app lifecycle. In this situation, some DevOps teams are also referred to as DevSecOps. Close coordination with the QA team ensures that no loopholes are left unchecked in the provided service/app. COMPANY HOW DEVOPS WORKS DELIVERY PIPELINE BUILD TEST RELEASE PLAN MONITOR FEEDBACK LOOP TOOLBOX" CUSTOMER norm, teams use automated processes here. DevOps tools also allow engineers to complete different tasks independently. Be it provisioning infrastructure or deploying code, they can accomplish these tasks without being dependent on one another. As such, the DevOps model accelerates the overall application development process. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 51 ---
Course Code/Title:CS3V15/Devops Unit: II 7. Maven Profiles: A Build profile is a set of configuration values, which can be used to set or override default values of Maven build. Using a build profile, you can customize build for different environments such as Production v/s Development environments. Types of Build Profile Build profiles are majorly of three types. Type Where it is defined Per Project Defined in the project POM file, pom.xml Per User Defined in Maven settings xml file (%USER_HOME%/.m2/settings.xml) Global Defined in Maven global settings xml file (%M2_HOME%/conf/settings.xml) Profile Activation A Maven Build Profile can be activated in various ways. · Explicitly using command console input. · Through maven settings. · Based on environment variables (User/System variables). · OS Settings (for example, Windows family). · Present/missing files. Now, under src/main/resources, there are three environment specific files - Sl.No. File Name & Description env.properties 1 default configuration used if no profile is mentioned. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 10 N33 nirf 1750 Rank

--- Page 52 ---
Course Code/Title:CS3V15/Devops Unit: II env.test.properties 2 test configuration when test profile is used. 3 env.prod.properties production configuration when prod profile is used. Explicit Profile Activation In the following example, we will attach maven-antrun-plugin:run goal to test the phase. This will allow us to echo text messages for different profiles. We will be using pom.xml to define different profiles and will activate profile at command console using maven command. Assume, we've created the following pom.xml in C:\MVN\project folder. <project xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation = "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.companyname.projectgroup</groupId> <artifactId>project</artifactId> <version>1.0</version> <profiles> <profile> <id>test</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <version>1.1</version> <executions> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 11 N33 nirf 1750 Rank

--- Page 17 ---
Course Code/Title:CS3V15/Devops Unit: I that failed in a test suite gets simpler. Also, we can schedule the execution of the test cases at predefined times. After testing, the code is continuously integrated with the existing code. 4) Continuous Monitoring Monitoring is a phase that involves all the operational factors of the entire DevOps process, where important information about the use of the software is recorded and carefully processed to find out trends and identify problem areas. Usually, the monitoring is integrated within the operational capabilities of the software application. 5) Continuous Feedback The application development is consistently improved by analyzing the results from the operations of the software. This is carried out by placing the critical phase of constant feedback between the operations and the development of the next version of the current software application. The continuity is the essential factor in the DevOps as it removes the unnecessary steps which are required to take a software application from development, using it to find out its issues and then producing a better version. It kills the efficiency that may be possible with the app and reduce the number of interested customers. 6) Continuous Deployment In this phase, the code is deployed to the production servers. Also, it is essential to ensure that the code is correctly used on all the servers. The new code is deployed continuously, and configuration management tools play an essential role in executing tasks frequently and quickly. Here are some popular tools which are used in this phase, such as Chef, Puppet, Ansible, and SaltStack. Containerization tools are also playing an essential role in the deployment phase. Vagrant and Docker are popular tools that are used for this purpose. These tools help to produce consistency across development, staging, testing, and production environment. They also help in scaling up and scaling down instances softly. Containerization tools help to maintain consistency across the environments where the application is tested, developed, and deployed. There is no chance of errors or failure in the production environment as they package and replicate the same dependencies and packages used in the testing, development, and staging environment. It makes the application easy to run on different computers. 7) Continuous Operations All DevOps operations are based on the continuity with complete automation of the release process and allow the organization to accelerate the overall time to market continuingly. It is clear from the discussion that continuity is the critical factor in the DevOps in removing steps that often distract the development, take it longer to detect issues and produce a better version of the product after several months. With DevOps, we can make any software product more efficient and increase the overall count of interested customers in your product. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 17 N32 nirf 1750 Rank

--- Page 18 ---
Course Code/Title:CS3V15/Devops Unit: I AWS AWS stands for Amazon Web Services, It is an expanded cloud computing platform provided by Amazon Company. AWS provides a wide range of services with a pay-as-per-use pricing model over the Internet such as Storage, Computing power, Databases, Machine Learning services, and much more. AWS facilitates for both businesses and individual users with effectively hosting the applications, storing the data securely, and making use of a wide variety of tools and services improving management flexibility for IT resources. Advantages & Features of AWS: 1. Cost savings: One of the biggest benefits of AWS is that it can help businesses save money. As mentioned previously, businesses can avoid the high upfront costs of traditional infrastructure with AWS and pay only for the resources they use. Traditionally, businesses had to invest in hardware and software upfront, which often led to overspending. Let's look at this for example - if a business needs to run a website that gets 1000 visitors per day, they would need to purchase and maintain enough servers to support this traffic. With AWS, the business only pays for the compute resources they use when someone visits their website. This can result in significant cost savings. 2. Flexibility: Another key benefit of AWS is its flexibility. Businesses are able to customize their virtual environment - whether the operating system, database, programming language, or something else - to meet their specific needs. Especially in today's climate, the migration process to the cloud should be as frictionless as possible - and AWS makes that possible. Regardless of your use case or industry, AWS can be tailored to fit your needs, whether you're looking for a single cloud- hosted application or an entire suite of integrated solutions. 3. Reliability: AWS is known for being reliable, with an uptime of 99.9%. This makes it a great platform for mission-critical applications that need to be available 24/7. AWS also offers the ability to deploy resources across multiple availability zones for even greater reliability. The cloud platform also has a number of features that make it easier to ensure reliability, such as autoscaling and auto-healing. Autoscaling allows businesses to automatically scale their resources up or down based on demand, while auto-healing enables them to quickly identify and replace any faulty components. 4. Security: Businesses can take advantage of advanced security features, such as identity and access management, to help protect their data. Their tough infrastructure with an end-to-end approach is designed to withstand attacks and AWS provides customers with tools to help them monitor and respond to threats. When it comes to storage, Amazon S3 provides customers with a secure and reliable way to store and access data. The service is designed to be highly scalable and resilient, with built-in redundancy. Fine-grain identity and access controls can be applied to S3 buckets and objects, giving customers control over who has access to their data. Security tasks can be automated with AWS CloudFormation, making it easier for businesses to manage their security policies. And, you can rest easy knowing that AWS takes privacy seriously, with comprehensive customer data protection and compliance measures. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 18 N32 nirf 1750 Rank

--- Page 13 ---
Course Code/Title:CS3V15/Devops Unit: I 2024. Terraform provides the agility and scalability required to keep up with the dynamic demands of modern applications. Terraform's importance lies in its ability to bring consistency, version control, and automation to infrastructure operations, thereby reducing manual errors, streamlining DevOps workflows, and facilitating applications' rapid and reliable deployment in an increasingly complex and cloud-centric environment. As organizations adopt cloud-native technologies, Terraform remains essential to ensure efficient and consistent infrastructure management. 25. Phantom Phantom enhances security automation and incident response capabilities. In today's rapidly evolving threat landscape, organizations face a constant barrage of cybersecurity incidents, and the ability to respond swiftly and effectively is necessary. It provides a platform for automating security workflows, from detecting and investigating potential threats to orchestrating responses and mitigating risks. Phantom's importance lies in its capacity to reduce response times, increase consistency in incident handling, and free up manual resources from repetitive tasks. With the growing complexity of cyber threats, Phantom empowers security teams to defend against attacks and safeguard critical assets proactively. 26. Nagios Nagios, an open-source monitoring and alerting system, remains vital due to its enduring significance in maintaining the reliability and performance of IT infrastructure and applications. Organizations increasingly rely on complex systems and services. Nagios plays a crucial role by providing real-time monitoring and alerting capabilities, allowing IT teams to detect and address issues before they impact users or cause system outages. Its versatility, extensibility, and support for both on-premises and cloud environments make Nagios a valuable tool for ensuring critical systems' availability, stability, and security, aligning perfectly with the demands of modern IT operations and DevOps practices. 27. Vagrant Vagrant continues to play a crucial role in software development and DevOps. It is a tool that simplifies creating and managing reproducible development environments. Its importance lies in its ability to provide developers and DevOps teams with a consistent and isolated environment for software development, testing, and deployment. With the ever-evolving complexity of software stacks, dependencies, and infrastructure configurations, Vagrant remains essential in ensuring these environments are easily shareable, scalable, and maintainable. It allows developers to work seamlessly across various operating systems and provides a standardized setup that minimizes compatibility issues. 28. Sentry Sentry plays a critical role in modern software development and DevOps practices. With software applications' increasing complexity and scale, identifying and addressing errors and issues has become crucial. Sentry is vital because it provides real-time error tracking and monitoring, allowing development teams to proactively detect and diagnose issues, whether they occur in production or during development. Its importance is minimizing downtime, improving user experience, and maintaining software systems' overall health and reliability. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nirf 1750 Rank

--- Page 14 ---
Course Code/Title:CS3V15/Devops Unit: I 29. Gradle Gradle continues to be a vital tool in software development and DevOps. Gradle is an advanced build automation system that plays a crucial role in managing dependencies, building projects, and orchestrating complex workflows efficiently. Its importance lies in its versatility and scalability, as it caters to various project sizes and types. Gradle's ability to easily handle multi-language, multi-project builds and its support for plugin-based customization make it indispensable in modern software development. As organizations increasingly adopt microservices architectures and cloud-native technologies, Gradle's capabilities are instrumental in managing the complexity of building, testing, and deploying applications across diverse environments. DevOps Architecture DevOps Architecture Plan Release Code Dep Deploy Development and operations both play essential roles in order to deliver applications. The deployment comprises analyzing the requirements, designing, developing, and testing of the software components or frameworks. The operation consists of the administrative processes, services, and support for the software. When both the development and operations cture is the solution to fix the gap between Test Monitor Build Operate ined with collaborating, then the DevOps al www deployment and operation terms; therefore, delivery can be faster. DevOps architecture is used for the applications hosted on the cloud platform and large distributed applications. Agile Development is used in the DevOps architecture so that integration and delivery can be contiguous. When the development and operations team works separately from each other, then it is time- consuming to design, test, and deploy. And if the terms are not in sync with each other, then it may cause a delay in the delivery. So DevOps enables the teams to change their shortcomings and increases productivity. Below are the various components that are used in the DevOps architecture: 1) Build: Without DevOps, the cost of the consumption of the resources was evaluated based on the pre-defined individual usage with fixed hardware allocation. And with DevOps, the usage of cloud, sharing of resources comes into the picture, and the build is dependent upon the user's need, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Build Code Test Plan DevOps Components Moniter Dev Ops Deploy Operate Release 14 N33 nirf 1750 Rank

--- Page 37 ---
Course Code/Title:CS3V15/Devops Unit: I GIT INSTALLATION Git for Windows stand-alone installer · Download the latest Git for Windows installer. · When you've successfully started the installer, you should see the Git Setup wizard screen. Follow the Next and Finish prompts to complete the installation. The default options are pretty sensible for most users. · Open a Command Prompt (or Git Bash if during installation you elected not to use Git from the Windows Command Prompt). . Run the following commands to configure your Git username and email using the following commands, replacing Emma's name with your own. These details will be associated with any commits that you create: $ git config -- global user.name "CIT_CHENNAI" $ git config -- global user.email CITCHENNAI@atlassian.com • Optional: Install the Git credential helper on Windows Bitbucket supports pushing and pulling over HTTP to your remote Git repositories on Bitbucket. Every time you interact with the remote repository, you must supply a username/password combination. You can store these credentials, instead of supplying the combination every time, with the Git Credential Manager for Windows. BASIC COMMANDS OF GIT: Some basic Git commands along with their syntax and examples: · Initialize a Repository: Syntax: `git init' Example: `git init' · Clone a Repository: Syntax: `git clone <repository_url> Example: `git clone https://github.com/example/repository.git' · Check Repository Status: Syntax: `git status' Example: `git status' · Add Changes to Staging Area: Syntax: `git add <file(s)>' Example: `git add file.txt' · Commit Changes: Syntax:`git commit -m "Commit message" Example:`git commit -m "Add new feature" · Create a New Branch: Syntax: `git branch <branch_name>' Example: `git CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY (Autonomous] N33 37 nirf 1750 Rank

--- Page 38 ---
Course Code/Title:CS3V15/Devops branch feature-branch' Unit: I • Switch to a Branch: Syntax: `git checkout <branch_name>' Example:`git checkout feature-branch' OR Syntax: `git switch <branch_name>' (Git version 2.23 and later) Example: `git switch feature-branch' · Create and Switch to a New Branch: Syntax: `git checkout -b <new_branch name>' Example: `git checkout -b new-feature' OR Syntax: `git switch -c <new_branch_name>' (Git version 2.23 and later) Example: `git switch -c new-feature' . Merge Changes from One Branch to Another: Syntax: `git merge <branch_name>' Example: `git merge feature-branch' · View the Commit History: Syntax: `git log' Example: `git log' · Push Changes to a Remote Repository: Syntax: `git push <remote_name> <branch_name>' Example:`git push origin master' · Pull Changes from a Remote Repository: Syntax: `git pull <remote_name> <branch_name>' Example:`git pull origin master' . Show the Differences Between Working Directory and Staging Area: Syntax: `git diff Example: `git diff · Show the Differences Between Staging Area and Last Commit: Syntax: `git diff -- cached' Example: `git diff -- cached' . Show the Differences Between Working Directory and Last Commit: Syntax: `git diff HEAD' Example: `git diff HEAD' CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 38 N32 nirf 1750 Rank

--- Page 31 ---
Course Code/Title:CS3V15/Devops Unit: I our work simpler. The pricing of Azure is also simpler and cost-effective. Popularly termed as "Pay As You Go", which means how much you use, pay only for that. Microsoft Azure Used for · Deployment Of applications: You can develop and deploy the application in the azure cloud by using the service called Azure App Service and Azure Functions after deploying the applications end users can access it. · Identity and Access Managment: The application and data which is deployed and stored in the Microsoft Azure can be secured with the help of Identity and Access Managment. It's commonly used for single sign-on, multi-factor authentication, and identity governance. · Data Storage and Databases: You can store the data in Microsoft azure in service like blob storage for unstructured data, table storage for NoSQL data, file storage, and Azure SQL Database for relational databases. The service can be scaled depending on the amount of data we are getting. · DevOps and Continuous Integration/Continuous Deployment (CI/CD): Azure DevOps will provide some tools like ncluding version control, build automation, release management, and application monitoring Following are some of the services Microsoft Azure offers: 1. Compute: Includes Virtual Machines, Virtual Machine Scale Sets, Functions for serverless computing, Batch for containerized batch workloads, Service Fabric for microservices and container orchestration, and Cloud Services for building cloud-based apps and APIs. 2. Networking: With Azure, you can use a variety of networking tools, like the Virtual Network, which can connect to on-premise data centers; Load Balancer; Application Gateway; VPN Gateway; Azure DNS for domain hosting, Content Delivery Network, Traffic Manager, ExpressRoute dedicated private network fiber connections; and Network Watcher monitoring and diagnostics 3. Storage: Includes Blob, Queue, File, and Disk Storage, as well as a Data Lake Store, Backup, and Site Recovery, among others. 4. Web + Mobile: Creating Web + Mobile applications is very easy as it includes several services for building and deploying applications. 5. Containers: Azure has a property that includes Container Service, which supports Kubernetes, DC/OS or Docker Swarm, and Container Registry, as well as tools for microservices. 6. Databases: Azure also included several SQL-based databases and related tools. 7. Data + Analytics: Azure has some big data tools like HDInsight for Hadoop Spark, R Server, HBase, and Storm clusters 8. AI + Cognitive Services: With Azure developing applications with artificial intelligence capabilities, like the Computer Vision API, Face API, Bing Web Search, Video Indexer, and Language Understanding Intelligent. 9. Internet of Things: Includes IoT Hub and IoT Edge services that can be combined with a variety of machine learning, analytics, and communications services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 31 N32 nirf 1750 Rank

--- Page 32 ---
Unit: I Course Code/Title:CS3V15/Devops 10. Security + Identity: Includes Security Center, Azure Active Directory, Key Vault, and Multi-Factor Authentication Services. 11. Developer Tools: Includes cloud development services like Visual Studio Team Services, Azure DevTest Labs, HockeyApp mobile app deployment and monitoring, Xamarin cross- platform mobile development, and more. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 32 N32 nirf 1750 Rank

--- Page 41 ---
Unit: I Course Code/Title:CS3V15/Devops S.No. Git 1 Git is a software. 2 Git is a command-line tool GitHub is a graphical user interface 3 Git is installed locally on the system GitHub is hosted on the web 4 Git is maintained by linux. GitHub is maintained by Microsoft. 5 Git is focused on version control and code sharing. GitHub is a hosting service for Git repositories. 6 Git is a version control system to manage source code history. 7 Git was first released in 2005. GHub was launched in 2008. 8 Git has no user management feature. 9 Git is open-source licensed. GitHub includes a free-tier and pay- for- use tier. 10 Git has minimal external tool configuration. 11 Git provides a Desktop interface named Git Gui. GitHub provides a Desktop interface named GitHub Desktop. 12 Git competes with CVS, Azure DevOps Server, Subversion, Mercurial, etc. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY GitHub GitHub is a service. GitHub is focused on centralized source code hosting. GitHub has a built-in user management feature. GitHub has an active marketplace for tool integration. GitHub competes with GitLab, Bit Bucket, AWS Code Commit, etc. 41 N32 nirf 1750 Rank

--- Page 42 ---
Course Code/Title:CS2V15/Devops Unit: II UNIT - II COMPILE AND BUILD USING MAVEN AND GRADLE Introduction, Installation of Maven, POM files, Maven Build lifecycle, Build phases(compile build, test, package) Maven Profiles, Maven repositories(local, central, global),Maven plugins, Maven create and build Artifacts, Dependency management, Installation of Gradle, Understand build using Gradle. 1. Introduction of Maven: Apache Maven is an automation tool. The tool is written in Java. It was initially released on 13 July 2004. It is developed by the Apache software foundation. It is part of the Jakarta Project. It is working on two aspects: how software is built, and its dependencies. It was created by Jason van Zyl. It is built by using a plugin-based architecture that allows it to make the use of any application controllable by standard input. It dynamically downloads Java libraries. Understanding the problem without Maven There are many problems that we face during the project development. They are discussed below: 1) Adding set of Jars in each project: In case of struts, spring, hibernate frameworks, we need to add set of jar files in each project. It must include all the dependencies of jars also. 2) Creating the right project structure: We must create the right project structure in servlet, struts etc, otherwise it will not be executed. 3) Building and Deploying the project: We must have to build and deploy the project so that it may work. Maven simplifies the above mentioned problems. It does mainly following tasks. 1. It makes a project easy to build 2. It provides uniform build process (maven project can be shared by all the maven projects) 3. It provides project information (log document, cross referenced sources, mailing list, dependency list, unit test reports etc.) 4. It is easy to migrate for new features of Maven Apache Maven helps to manage o Builds Documentation o Reporing o SCMs o Releases o Distribution What is Build Tool? A build tool is used for building a process. It does following: CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE TECHNOLOGY N32 nirf 1 1750 Rank

--- Page 45 ---
Course Code/Title:CS3V15/Devops Unit: II Example for pom.xml file <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> 4. Maven Repository A maven repository is a directory of packaged JAR file with pom.xml file. Maven searches for dependencies in the repositories. There are 3 types of maven repository: 1. Local Repository 2. Central Repository 3. Remote Repository Maven searches for the dependencies in the following order: Local repository then Central repository then Remote repository. Local Central Remote Repository Repository Repository CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 4 N32 nirf 1750 Rank -

--- Page 46 ---
Course Code/Title:CS3V15/Devops Unit: II If dependency is not found in these repositories, maven stops processing and throws an error. 1) Maven Local Repository Maven local repository is located in your local system. It is created by the maven when you run any maven command. By default, maven local repository is %USER_HOME%/.m2 directory. For example: C:\Users\SSS IT\.m2. Update location of Local Repository We can change the location of maven local repository by changing the settings.xml file. It is located in MAVEN_HOME/conf/settings.xml, for example: E:\apache-maven- 3.1.1\conf\settings.xml. 2) Maven Central Repository Maven central repository is located on the web. It has been created by the apache maven community itself. The path of central repository is: http://repo1.maven.org/maven2/. The central repository contains a lot of common libraries that can be viewed by this url http://search.maven.org/#browse. 3) Maven Remote Repository Maven remote repository is located on the web. Most of libraries can be missing from the central repository such as JBoss library etc, so we need to define remote repository in pom.xml file. Let's see the code to add the jUnit library in pom.xml file. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 5 N32 narf 1750 Rank -

--- Page 23 ---
Course Code/Title:CS3V15/Devops Unit: I application. Amazon EC2 enables you to increase or decrease capacity within minutes. You can use one or hundreds or even thousands of server instances simultaneously. Because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs. Amazon EC2 is integrated with most AWS services, such as Amazon Simple Storage Service (Amazon S3), Amazon Relational Database Service (Amazon RDS), and Amazon Virtual Private Cloud (Amazon VPC) to provide a complete, secure solution for computing applications. Amazon EC2 is an example of Infrastructure as a Service(IaaS). EC2 delivers secure, reliable, cost-effective compute and high-performance compute infrastructure so as to meet the needs of demanding businesses. Amazon EC2 is one of the easiest ways of providing servers on AWS Cloud and also the access to Operating system. · AWS Lambda: AWS Lambda is a serverless, event-driven compute service that allows you to run code without managing servers. You pay only for the compute time you consume and there is no charge when your code is not running. With AWS Lambda, you can run code for any type of application with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services, or you can call it directly from any web or mobile app. But triggering Lambda is possible with over 200 AWS services. You can only pay for what you have used. The compute time that you consume, you are needed to pay for it. You just only need to upload your code and everything required to run will take care of by Lambda and it automatically scales your code with high availability. · AWS Elastic Beanstalk: AWS Elastic Beanstalk is a Platform as a Service that facilitates quick deployment of your applications by providing all the application services that you need for your application. Beanstalk is a plug- and-play platform that allows working with multiple programming languages and environments. Elastic Beanstalk supports a large range of platforms like Node js, Java, PHP, Python, and Ruby. So, you can develop your application to meet your requirements and simply deploy it on Elastic Beanstalk. The main aim to use AWS Elastic Beanstalk is to allow you to focus on the deployment and management of your applications. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. 2. Networking · Amazon VPC: Amazon VPC is your network environment in the cloud. It allows you to create a private network within the AWS cloud that uses many of the same concepts and constructs as an on-premises network. Amazon VPC also gives you complete control of the network configuration. Customers can define normal networking configuration items such as IP address ranges, subnet creation, route table creation, network gateways, and security settings. Amazon VPC is an AWS foundational service and integrates with numerous AWS services. For instance, Amazon EC2 instances are deployed into your CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 24 ---
Course Code/Title:CS3V15/Devops Unit: I Amazon VPC. Similarly, Amazon Relational Database Service (Amazon RDS) database instances deploy into your Amazon VPC, where the database is protected by the structure of the network just like your on-premises network. You can easily launch AWS resources into a virtual network by Amazon Virtual Private Cloud. An isolated virtual network environment in the AWS cloud is created by Amazon VPC. . Amazon Route 53: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating human-readable names, such as www.geeksforgeeks.com, into the numeric IP addresses that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. 2. Storage · Amazon S3 (Simple Storage Service): Amazon Simple Storage Service (Amazon S3) is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the web. It is designed to provide an infinite amount of storage and it is delivered with 99.999999999% durability. You can use Amazon S3 as primary storage for cloud-native applications as a target for backup and recovery and disaster recovery. It offers industry-leading scalability, data availability, security, and performance. It's simple to move large volumes of data into or out of Amazon S3 with Amazon's cloud data migration options. Once data is stored in Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like Amazon S3 Standard - Infrequent Access and Amazon Glacier for archiving. · Amazon Glacier: Amazon Glacier is a secure, durable, and extremely low- cost storage service for data archiving and long-term backup. Data stored in Amazon Glacier takes several hours to retrieve, which is why it's ideal for archiving. The fastest access to your archive data is via Amazon Glacier. 3. Databases · Amazon RDS (Relational Database Service): Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost- efficient and resizable capacity while managing time-consuming database administration tasks, freeing you up to focus on your applications and business. You can find Amazon RDS is also available on several database instance types - optimized for memory, performance, or I/O. Amazon RDS provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. · Amazon DynamoDB (Non-Relational Database): Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. When you create a database table that can store and retrieve any amount of data you can simply use Amazon DynamoDB that will serve any level of requested traffic. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, Internet of Things (IoT), and many other applications. DynamoDB provides many features like · built-in security CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 24 N32 nirf 1750 Rank

--- Page 53 ---
Course Code/Title:CS3V15/Devops Unit: II <execution> <phase>test</phase> <goals> <goal>run</goal> </goals> <configuration> <tasks> <echo>Using env.test.properties</echo> <copy file="src/main/resources/env.test.properties" tofile="${project.build.outputDirectory}/env.properties"/> </tasks> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> Now open the command console, go to the folder containing pom.xml and execute the following mvn command. Pass the profile name as argument using -P option. C:\MVN\project>mvn test -Ptest Maven will start processing and displaying the result of test build profile. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 12 N32 nirf 1750 Rank

--- Page 54 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Maven Settings Open Maven settings.xml file available in %USER_HOME%/.m2 directory where %USER_HOME% represents the user home directory. If settings.xml file is not there, then create a new one. Add test profile as an active profile using active Profiles node as shown below in example. <settings xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"> <mirrors> <mirror> <id>maven.dev.snaponglobal.com</id> <name>Internal Artifactory Maven repository</name> <url>http://repo1.maven.org/maven2/</url> <mirrorOf> *< /mirrorOf> /mirror </mirrors> <activeProfiles> <activeProfile>test</activeProfile> </activeProfiles> </settings> Now open command console, go to the folder containing pom.xml and execute the following mvn command. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 13 N33 nirf 1750 Rank

--- Page 61 ---
Course Code/Title:CS3V15/Devops Unit: II Return to the previous screen and undo the deletion. The errors that occurred will disappear. *mavenproject/pom.xml & IApp.java AppTest.java 6 <artifactId>mavenproject</artifactId> 7 <version>0.0.1-SNAPSHOT</version> <packaging>jar</packaging> 8 9 16 <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 138 <properties> 14 15 16 17% <dependencies> 18 19 <groupId>junit</groupId> I 28 21 22 23 24 <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> cdependency> <artifactId>junit</artifactid> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> 25 </project> 26 9. Dependency Management: The dependencyManagement and dependencies are especially useful for multi-module projects. Dependency Management : This tag consists of a dependencies tag which itself might contain multiple dependency tags. Each dependency is supposed to have at least three maintags: groupId, artifactId, and version. For example: <dependencyManagement> <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 20 N33 nirf 1750 Rank -

--- Page 62 ---
Course Code/Title:CS3V15/Devops Unit: II The above code just declares the new artifact commons-lang3, but it doesn't really add it to the project dependency resource list. Dependencies: This tag contains a list of dependency tags. Each dependency is supposed to have at least two main tags, which are groupId and artifactId. For example: <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> The version and scope tags can be inherited implicitly if we have used the dependencyManagement tag before in the POM file. DependencyManagement is just a declaration, and it does not really add a dependency. Dependencies tag adds the actual dependency to the project. An example for adding the JUnit library dependency: <dependencyManagement> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.2</version> <scope>test</scope> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 21 N33 nirf 175º Rank

--- Page 19 ---
Course Code/Title:CS3V15/Devops Unit: I 5. Compliance: By compliance, we mean that certain businesses are required to follow specific regulations. Financial services companies in the United States, for example, must comply with the Sarbanes-Oxley Act, while healthcare, education, and energy companies must comply with HIPAA and other regulations. AWS provides a number of compliance-related features and services, such as data encryption and identity and access management, to help businesses meet these requirements. 6. High-Performance: Interested in delivering your applications quickly and efficiently? Taking advantage of AWS features such as auto-scaling and load balancing will help ensure your applications are always available and running optimally. AWS can help businesses improve their performance by offering a variety of cloud-based services, including Amazon Elastic Compute Cloud (EC2), which provides high-performance computing resources, and Amazon CloudFront, which delivers content quickly and securely to users around the world. Others include machine learning (ML) and analytics services, such as Amazon SageMaker and Amazon Athena. These services provide the tools businesses need to quickly and easily analyze their data for insights. Fast networking in the cloud is also possible with AWS, thanks to its Elastic Load Balancing (ELB) and Amazon Virtual Private Cloud (VPC). With ELB, businesses can balance their workloads across multiple instances for increased performance, while VPC allows businesses to create isolated private networks in the cloud. 7. Developer Tools: Developer tools are designed to make it easier for developers to create, deploy, and manage applications - and AWS provides developers with what they need to build applications quickly and easily. By leveraging developer tools, developers can save time and money by automating tedious tasks. They also benefit from access to AWS's extensive library of pre-built applications that can help them get their projects off the ground quickly. Services such as Amazon Elastic Beanstalk and Amazon CloudFormation can help them automate the process of creating and deploying applications. Other ways developers can improve productivity with AWS include using AWS Code Commit to store and manage source code. 8. Integration: Thanks to its many integrations with other Amazon services, as well as third-party services, AWS makes it easy for businesses to get started with cloud computing. AWS provides a wide range of services that can be easily integrated into existing business infrastructure. This allows businesses to add new features and capabilities without having to make major changes or invest in new hardware or software. For instance, if a business wants to add mobile capabilities to its website, it can take advantage of Amazon's Mobile SDK and Web Services. These tools allow businesses to quickly develop and deploy mobile apps that connect directly with their existing infrastructure. 9. Management Console: The AWS management console is a web-based interface that provides users with a simple way to interact with and manage their AWS resources - essentially a place where you can access and manage everything on the cloud. It provides a graphical view of all the resources associated with an account, as well as tools for creating and configuring new resources. Compared to traditional command- line interfaces, the AWS management console saves time and makes it easier for users to get the most out of their AWS services. Not only that, but your business gets access to 350+ free digital training courses through the AWS Academy, covering topics such as cloud fundamentals, DevOps, security, and big data. This means you can train your employees on how to use AWS, and in turn, help them become more efficient at their jobs. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 19 N32 nirf 1750 Rank

--- Page 20 ---
Course Code/Title:CS3V15/Devops Unit: I 10. Scalability: With an on-demand service, businesses can quickly spin up new servers as needed with just a few clicks. This makes it much easier to scale resources up or down as demand changes, allowing businesses to save costs and maintain performance even during peak periods. For example, if a business is expecting a sudden surge in traffic due to an advertising campaign or seasonal event, they can easily add more capacity to their server infrastructure to handle the increased load. Bru Textiles, a specialty textile company in Belgium, was able to quickly scale its infrastructure by leveraging AWS. Bru Textiles went digital to grow and offer new services. Embracing technology, they brought in digital twin technology to give their customers an idea of the texture and essence of their physical fabrics. AWS Applications · Storage and Backup: Storage and backup are important for any Cloud Computing service. AWS provides you with reliable storage services like Amazon Simple Storage Service to store large- scale data and backup services like AWS Backup to take backups of this data, which is stored in other AWS services. AWS stores the data in three different availability zones so that if one fails, you can still access your data. This makes AWS storage reliable and easily accessible. Therefore, companies with huge application data to store and backup securely can use AWS. · Big Data: One of the biggest challenges faced by companies these days is Big Data. The companies are struggling to store their large amounts of data using traditional methods. With AWS Big Data storage services, they can manage to store their data even if the data limit increases unexpectedly as AWS provides virtually unlimited data storage with scale-in and scale- out options. AWS offers easy access and faster data retrieval as well. For data processing, it offers services like EMR, with which the companies can easily set up, operate, and scale their big data. Therefore, efficiently storing and managing Big Data is among the top AWS applications. · Enterprise IT: AWS is a one-stop solution for any IT business. Many features of it such as secure storage, scalability, flexibility, and elasticity support companies to innovate faster than ever before. Using AWS for IT enterprises makes them profitable in terms of both money and time. As AWS maintains its cloud architecture, it need not waste time and money on professionals to do the same. · Social Networking: Social networking is essential for businesses in the present-day scenario where Digital Marketing is key, and it is easier with AWS. Companies can connect with customers and stakeholders and communicate through social networking sites and develop their business. Services like AWS social networking engine, which is powered by Turn Key GNU/Linux (HVM) AMI stack, are used for performance and scalability to help companies build a suitable social networking site and gain profits. · Mobile Apps: Mobile applications are embedded with day-to-day life. With AWS, you have the facility to create an app in your desired programming language. You can also keep up the applications that are consistently accessible and solid with high compute, storage, database, and application services. You can take advantage of AWS auto-scaling and managed relational database service for the better performance of your apps. · Websites: AWS offers a wide range of website hosting options to create the best website for customers. Its services like Amazon Light sail have everything, such as a virtual machine, SSD- based storage, data transfer, DNS management, and a static IP, to launch a website in such a way that the user can manage the website easily. Amazon EC2, AWS Lambda, Elastic Load Balancing, AWS Amplify, Amazon S3, etc. also help users build reliable and scalable websites. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 47 ---
Course Code/Title:CS3V15/Devops Unit: II <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> Any repository of maven is available in mvnrepository.com 5. Maven Plugins: The maven plugins are central part of maven framework, it is used to perform specific goal. According to Apache Maven, there are 2 types of maven plugins. 1. Build Plugins 2. Reporting Plugins Build Plugins These plugins are executed at the time of build. These plugins should be declared inside the <build> element. Reporting Plugins These plugins are executed at the time of site generation. These plugins should be declared inside the <reporting> element. Maven Core Plugins A list of maven core plugins are given below: Plugin Description clean clean up after build. compiler compiles java source code. deploy deploys the artifact to the remote repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY N32 nirf 6 175º Rank -

--- Page 48 ---
Course Code/Title:CS3V15/Devops Unit: II failsafe runs the JUnit integration tests in an isolated classloader. install installs the built artifact into the local repository. resources copies the resources to the output directory for including in the JAR. site generates a site for the current project. surefire runs the JUnit unit tests in an isolated classloader. verifier verifies the existence of certain conditions. It is useful for integration tests. Example for maven plugin: The compiler plugin is used to compile the source code of a Maven project. This plugin has two goals, which are already bound to specific phases of the default lifecycle: · compile - compile main source files · testCompile - compile test source files Here's the compiler plugin in the POM: <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.12.1</version> <configuration> <source>1.8</source> <target>1.8</target> </configuration> </plugin> 6. Maven Build Lifecycle: A Build Lifecycle is a well-defined sequence of phases, which define the order in which the goals are to be executed. Here phase represents a stage in life cycle. As an example, a typical Maven Build Lifecycle consists of the following sequence of phases. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 7 N33 nirf 175º Rank -

--- Page 35 ---
Unit: I Course Code/Title:CS3V15/Devops Push: Push is the operation that sends your committed changes to a remote repository, making them accessible to others. Git workflow v0.1 v0.2 v1.0 Master Hotfix Release Develop Feature Feature Git Flow is a structured branching model designed for projects with well- defined release cycles and a need for strict quality control. Branches: The branching model described is commonly known as the Gitflow Workflow. It's a branching strategy that defines a strict branching model designed to facilitate collaboration and streamline the release process. Let's go into detail about each branch: clone operation Working Copy edit, add, move files update operation Modify working copy status and diff operation Review changes commit and push operation push operation Commit changes push operation amend and push operation Review changes 1. Master Branch: The `master'branch represents the main codebase and contains production- ready code. This branch is typically stable and should only include thoroughly tested and approved changes. Each commit on the 'master' branch represents a new version or release of the software. 2. Develop Branch: The 'develop' branch is an integration branch where various feature branches are merged. It serves as a staging area for testing new features and ensuring they work well together before merging into the 'master' branch. This branch may have ongoing development work and is not necessarily always in a production- ready state. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 35 nirf 1750 Rank N33

--- Page 36 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Feature Branches: Feature branches are created for developing new features or implementing changes. These branches are typically based on the 'develop'branch. Once a feature is complete, the branch is merged back into the 'develop' branch. Feature branches allow developers to work on specific tasks without affecting the main codebase. 4. Release Branch: The 'release' branch is created when the 'develop' branch reaches a point where it is ready for a production release. This branch is used for final testing, bug fixes, and preparing the code for deployment. No new features should be added to the release branch. Once the release is deemed stable, it is merged into both the 'master' branch and the 'develop' branch. 5. Hotfix Branch: The 'hotfix' branch is used to quickly address critical issues or bugs in the production code. It is created directly from the 'master' branch. Hotfixes are intended to be small and focused on resolving the specific issue at hand. Once the hotfix is complete, it is merged into both the 'master'branch and the `develop'branch to ensure that the fix is applied to future releases. Here is the typical flow: · Developers work on feature branches based on the 'develop' branch. · Completed features are merged into the 'develop' branch. · When ready for a release, a 'release' branch is created from 'develop". · The release branch undergoes testing and bug fixes. · The release branch is merged into both 'master' and 'develop' once it's stable. · If a critical issue arises in production, a 'hotfix' branch is created from 'master'. · The hotfix is merged into both 'master' and 'develop to keep both branches in sync. · This Gitflow Workflow helps maintain a structured development process, ensuring that features are developed, tested, and released in a controlled manner. Example Scenario: · Imagine you're working on a large software project with a team of 10 developers. You have a major release planned for every six months. · You create a "feature/Ticket-Id" branch to develop a new login system. · Once the feature is complete, it's merged into the "develop" branch for integration and testing. · As the release date approaches, you create a "release/v1.0" branch to freeze code for the upcoming release. · Any critical issues discovered in the production environment are fixed in "hotfix" branch and merged into "master" and "develop." CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 36 N33 nirf 1750 Rank

--- Page 15 ---
Course Code/Title:CS3V15/Devops Unit: I which is a mechanism to control the usage of resources or capacity. 2) Code: Many good practices such as Git enables the code to be used, which ensures writing the code for business, helps to track changes, getting notified about the reason behind the difference in the actual and the expected output, and if necessary reverting to the original code developed. The code can be appropriately arranged in files, folders, etc. And they can be reused. 3) Test: The application will be ready for production after testing. In the case of manual testing, it consumes more time in testing and moving the code to the output. The testing can be automated, which decreases the time for testing so that the time to deploy the code to production can be reduced as automating the running of the scripts will remove many manual steps. 4) Plan: DevOps use Agile methodology to plan the development. With the operations and development team in sync, it helps in organizing the work to plan accordingly to increase productivity. 5) Monitor: Continuous monitoring is used to identify any risk of failure. Also, it helps in tracking the system accurately so that the health of the application can be checked. The monitoring becomes more comfortable with services where the log data may get monitored through many third-party tools such as Splunk. 6) Deploy: Many systems can support the scheduler for automated deployment. The cloud management platform enables users to capture accurate insights and view the optimization scenario, analytics on trends by the deployment of dashboards. 7) Operate: DevOps changes the way traditional approach of developing and testing separately. The teams operate in a collaborative way where both the teams actively participate throughout the service lifecycle. The operation team interacts with developers, and they come up with a monitoring plan which serves the IT and business requirements. 8) Release: Deployment to an environment can be done by automation. But when the deployment is made to the production environment, it is done by manual triggering. Many processes involved in release management commonly used to do the deployment in the production environment manually to lessen the impact on the customers. DevOps Lifecycle DevOps defines an agile relationship between operations and Development. It is a process that is practiced by the development team and operational engineers Learning DevOps is not complete without understanding the DevOps lifecycle phases. The DevOps lifecycle includes seven phases as given below: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 16 ---
Unit: I Course Code/Title:CS3V15/Devops together from beginning to the final stage of the product. Development Operations Integration Deployment DevOps Lidecycle Feedback 2) Continuous Integration Testing Monitering 1) Continuous Development This phase involves the planning and coding of the software. The vision of the project is decided during the planning phase. And the developers begin developing the code for the application. There are no DevOps tools that are required for planning, but there are several tools for maintaining the code. This stage is the heart of the entire DevOps lifecycle. It is a software development practice in which the developers require to commit changes to the source code more frequently. This may be on a daily or weekly basis. Then every commit is built, and this allows early detection of problems if they are present. Building code is not only involved compilation, but it also includes unit testing, integration testing, code review, and packaging. The code supporting new functionality is continuously integrated with the existing code. Therefore, there is continuous development of software. The updated code needs to be integrated continuously and smoothly with the systems to reflect changes to the end-users. Commit Build O O Test O Stage Deploy Dev/QA O O > Continuous Integration/Delivery Development </> git Code Commit = production server. 3) Continuous Testing Production Jenkins is a popular tool used in this phase. Whenever there is a change in the Git repository, then Jenkins fetches the updated code and prepares a build of that code, which is an executable file in the form of war or jar. Then this build is forwarded to the test server or the This phase, where the developed software is continuously testing for bugs. For constant testing, automation testing tools such as TestNG, JUnit, Selenium, etc are used. These tools allow QAs to test multiple code- bases thoroughly in parallel to ensure that there is no flaw in the functionality. In this phase, Docker Containers can be used for simulating the test environment. It may occur in the form of documentation files or maybe produce large-scale data about the application parameters when it is in a continuous use position. The system errors such as server not reachable, low memory, etc are resolved in this phase. It maintains the security and availability of the service. Automation testing saves a lot of time and effort for executing the tests instead of doing this manually. Apart from that, report generation is a big plus. The task of evaluating the test cases INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 narf 1750 Rank

--- Page 49 ---
Course Code/Title:CS3V15/Devops Unit: II Validate Deploy Compile Install Test Verify Package Integration Test · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. There are always pre and post phases to register goals, which must run prior to, or after a particular phase. When Maven starts building a project, it steps through a defined sequence of phases and executes goals, which are registered with each phase. Maven has the following three standard lifecycles - • default: This is the main lifecycle, as it's responsible for project deployment. CHENNAI CHENNAI INSTITUTE . TECHNOLOGY 8 clean: Handles project cleaning, ensuring that all artifacts generated by previous INSTITUTE OF TECHNOLOGY 175º Rank (Autonomous]

--- Page 50 ---
Course Code/Title:CS3V15/Devops Unit: II builds are removed. · site: Manages the creation of the project's site documentation. Default Lifecycle (default) Most Maven users will be familiar with the default lifecycle. It is a general model of a build process for a software application. The first phase is validate and the last phase is deploy. · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. Clean Lifecycle (clean) The first lifecycle in Maven. Running mvn clean invokes the clean lifecycle which consists of three lifecycle phases: · pre-clean : execute processes needed prior to the actual project cleaning · clean : remove all files generated by the previous build · post-clean : execute processes needed to finalize the project cleaning Site Lifecycle (site) Maven does more than build software artifacts from project, it can also generate project documentation and reports about the project, or a collection of projects. Project documentation and site generation have a dedicated lifecycle which contains four phases: 1. pre-site : execute processes needed prior to the actual project site generation 2. site : generate the project's site documentation 3. post-site : execute processes needed to finalize the site generation, and to prepare for site deployment 4. site-deploy: deploy the generated site documentation to the specified web server CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 43 ---
Course Code/Title:CS3V15/Devops Unit: II o Generates source code (if auto-generated code is used) o Generates documentation from source code o Compiles source code o Packages compiled code into JAR of ZIP file o Installs the packaged code in local repository, server repository, or central repository 2. Installation of Maven: To install maven on windows, you need to perform following steps: 1. Download maven and extract it 2. Add JAVA_HOME and MAVEN_HOME in environment variable 3. Add maven path in environment variable 4. Verify Maven 1) Download Maven To install maven on windows, you need to download apache maven first. Download Maven latest Maven software from Download latest version of Maven For example: apache-maven-3.1.1-bin.zip 2) Add MAVEN_HOME in environment variable Right click on MyComputer -> properties -> Advanced > Environment variables -> click new button System Settings - Now add MAVEN_HOME in variable name and path of maven in variable value. It must be the home directory of maven i.e. outer directory of bin. For example: E:\apache-maven-3.1.1 3) Add Maven Path in environment variable Click on new tab if path is not set, then set the path of maven. If it is set, edit the path and append the path of maven.Here, we have installed JDK and its path is set by default, so we are going to append the path of maven. The path of maven should be %maven home%/bin. For example, E:\apache-maven- 3.1.1\bin . 4) Verify maven To verify whether maven is installed or not, open the command prompt and write: mvn -version Now it will display the version of maven and jdk including the maven home and java home. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 2 N32 nirf 175º Rank

--- Page 44 ---
Unit: II Course Code/Title:CS3V15/Devops 3. POM FILES: POM is an acronym for Project Object Model. The pom.xml file contains information of project and configuration information for the maven to build the project such as dependencies, build directory, source directory, test source directory, plugin, goals etc. Maven reads the pom.xml file, then executes the goal. Before maven 2, it was named as project.xml file. But, since maven 2 (also in maven 3), it is renamed as pom.xml. Elements of maven pom.xml file For creating the simple pom.xml file, you need to have following elements: Element Description project It is the root element of pom.xml file. modelVersion It is the sub element of project. It specifies the modelVersion. It should be set to 4.0.0. groupId It is the sub element of project. It specifies the id for the project group. artifactId It is the sub element of project. It specifies the id for the artifact (project). An artifact is something that is either produced or used by a project. Examples of artifacts produced by Maven for a project include: JARs, source and binary distributions, and WARs. version It is the sub element of project. It specifies the version of the artifact under given group. Maven pom.xml file with additional elements Here, we are going to add other elements in pom.xml file such as: Element Description packaging defines packaging type such as jar, war etc. name defines name of the maven project. url defines url of the project. dependencies defines dependencies for this project. dependency defines a dependency. It is used inside dependencies. scope defines scope for this maven project. It can be compile, provided, runtime, test and system. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 3 N32 narf 1750 Rank -

--- Page 63 ---
Course Code/Title:CS3V15/Devops Unit: II 10. Introduction of Gradle: The list of features that Gradle provides, · Gradle is available with separate Domain Specific Language (DSL) based on Groovy language. · It provides the declarative language elements. Those elements also provide build-by- convention support for Java, Groovy, OSGI, Web and Scala. Language for dependency based programming The declarative language lies on a top of a general purpose task graph, which can be fully supported in the build. Structure your build Gradle allows you to apply common design principles to your build. It will give you a perfect structure for build, so that, you can design well-structured and easily maintained, comprehensible build. Deep API By using this API, you can monitor and customize its configuration and execution behavior to the core. Gradle scales Gradle can easily increase the productivity, from simple and single project builds to huge enterprise multi-project builds. Multi-project builds Gradle supports the multi-project builds and partial builds. If you build a subproject, Gradle takes care of building all the subprojects, that the subproject depends on. Different ways to manage your builds Gradle supports different strategies to manage your dependencies. Gradle is the first build integration tool Gradle is fully supported for your ANT tasks, Maven and lvy repository infrastructure for publishing and retrieving dependencies. It also provides a converter for turning a Maven pom.xml to Gradle script. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 22 N32 nirf 175º Rank -

--- Page 64 ---
Course Code/Title:CS3V15/Devops Unit: II Ease of migration Gradle can easily adapt to any structure. Therefore, you can always develop your Gradle build in the same branch, where you can build live script. Gradle Wrapper Gradle Wrapper allows you to execute the Gradle builds on machines, where Gradle is not installed. This is useful for continuous integration of servers. Free open source Gradle is an open source project, and licensed under the Apache Software License (ASL). Groovy Gradle's build script are written in Groovy programming language. The whole design of Gradle is oriented towards being used as a language and not as a rigid framework. Groovy allows you to write your own script with some abstractions. The whole Gradle API is fully designed in Groovy language. Installation of Gradle: Prerequisites to install Gradle JDK and Groovy are the prerequisites for Gradle installation. Gradle requires JDK version 6 or later to be installed in the system. It uses the JDK libraries which are installed, and sets to the JAVA_HOME environmental variable. Gradle carries its own Groovy library, therefore, we need not install Groovy explicitly. If it is installed, that is ignored by Gradle. The steps to install Gradle in your system are explained below. Step 1 - Verify JAVA Installation First of all, you need to have Java Software Development Kit (SDK) installed on your system. To verify this, execute Java -version command in any of the platform you are working on. In Windows Execute the following command to verify Java installation. I have installed JDK 1.8 in my system. C:\> java - version CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE . TECHNOLOGY 23 N32 narf 1750 Rank

--- Page 33 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between AWS (Amazon Web Services), Google Cloud, and Azure Technology EC2 (Elastic Compute Cloud) AWS Databases Supported Pricing Models Difficulties Many enterprises find it difficult to understand the company cost AWS fully relational and supports NoSQL databases and Big Data. Per hour - rounded up. On demand, reserved spot. structure. Google Cloud Google Engine(GCE) Compute Technologies pioneered by Google, like Big Query, Big Table, and Hadoop, are databases, and Big Data,naturally fully supported. Per minute - rounded up On demand sustained use. — Fewer services. features Azure VHD Disk) (Virtual Hard Azure supports relational and both NoSQL Windows and through AzureTable HDInsight. Per minute - rounded up. Per minute- rounded up commitments(Pre-paid or monthly) and Less "Enterprise-ready. Storage Services Simple Storage Service(S3) Elastic Block Storage. Elastic File storage. Machine Learing Sage maker. Lex. polly.And many more Blob Storage Queue Storage. File Storage Disk Storage. Data Lake Store Machine learning Azure Bot service Cognitive service GIT Cloud storage. Persistent Disk Transfer appliance. Cloud speech AI Cloud Video Intelligence. Cloud Machine learning engine Git is a distributed version control system (DVCS) that helps manage and track changes in source code during software development. It was created by Linus Torvalds in 2005 and has become one of the most widely used version control systems in the software development industry. Some key concepts and features of Git: · Version Control: Git allows developers to keep track of changes made to their code over time. This includes modifications, additions, and deletions of files. · Distributed System: Git is a distributed version control system, meaning that each developer has a complete copy of the entire repository, including its full history. This allows developers to work independently and merge their changes when necessary. · Branching: Git enables developers to create branches, which are essentially separate lines of development. This allows for the parallel development of features or bug fixes without affecting the main codebase. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) LEN 33 nirf 1750 Rank

--- Page 34 ---
Course Code/Title:CS3V15/Devops Unit: I · Merging: Git provides tools for merging changes from one branch into another. This is essential when multiple developers are working on different branches and need to bring their changes together. · History Tracking: Git maintains a complete history of changes made to the codebase. Developers can view, revert, or analyze changes made over time. · Remote Repositories: Git supports remote repositories, allowing developers to collaborate with others by pushing and pulling changes to and from a shared repository. Platforms like GitHub, GitLab, and Bitbucket provide hosting services for Git repositories. · Staging Area: Git uses a staging area (also known as the index) to prepare and review changes before committing them to the repository. This allows developers to selectively include or exclude specific changes. · Open Source: Git is an open-source tool, and its source code is freely available for modification and distribution. Various GIT Components: Git is composed of several key components that work together to enable version control and collaborative development. Here are the main components of Git: · Repository (Repo): A repository is a directory or storage space where your project and its version history are stored. It contains all the files and directories associated with your project, along with the metadata and configuration information. · Working Directory: The working directory is the directory on your local machine where you manipulate files and make changes to your project. It is essentially your local copy of the repository. · Index (Staging Area): The index, also known as the staging area, is a middle ground where changes are prepared before being committed to the repository. It allows you to selectively stage changes, which means you can choose which modifications to include in the next commit. · Commit: A commit is a snapshot of the changes made to the files in the repository. It represents a specific point in the project's history and is accompanied by a commit message that describes the changes. · Branch: A branch is a parallel line of development within a repository. It allows developers to work on different features or bug fixes simultaneously without affecting the main codebase. Branches can be merged to incorporate changes into other branches. . Head: HEAD is a reference to the latest commit in the currently checked-out branch. It essentially points to the tip of the branch you are currently on. · Remote: A remote is a version of the repository stored on a different server. Git supports collaboration by allowing developers to push and pull changes between their local repository and remote repositories. Platforms like GitHub, GitLab, and Bitbucket are examples of remote repositories. · Clone: Cloning is the process of creating a copy of a remote repository on your local machine. This allows you to start working on your own copy of a project. · Fetch: The fetch operation retrieves changes from a remote repository but does not automatically merge them into your working directory. It is useful for reviewing changes before deciding to merge. · Pull: Pull is a combination of fetch and merge. It retrieves changes from a remote repository and automatically merges them into your working directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 27 ---
Course Code/Title:CS3V15/Devops Unit: I · Servers are maintained easily and there is nearly zero downtime · Users can access cloud data and upload it on the cloud from any device with a solid internet connection · Cloud environments can be modified according to the user's requirements and is easily accessible · Clouds are highly secure, making data breaches more unlikely · Migrating to the cloud eliminates the need to buy on-premises infrastructure . It offers pay-as-you-go pricing, meaning you only pay for the resources you use 1. Infrastructure as a Service: IaaS delivers virtualized computing resources over the Internet. Users can rent virtual machines, storage, and networking infrastructure, allowing for easy scalability without investing in physical hardware. Examples include AWS EC2 and Azure Virtual Machines. 2. Platform as a Service: PaaS offers a robust platform for developers to build, deploy, and manage apps without worrying about the underlying infrastructure. It simplifies application development and deployment, with services like Google App Engine and Heroku leading the way. 3. Software as a Service: SaaS offers software applications on a subscription basis, accessible via a web browser. Users don't need to install or maintain software locally, making it ideal for collaboration tools (e.g., Microsoft 365, Google Workspace) and CRM systems (e.g., Salesforce). 4. Function as a Service: FaaS allows developers to execute code responding to events without managing servers. It's highly scalable and cost-efficient, exemplified by AWS Lambda and Azure Functions. FaaS is also known as serverless computing. 5. Container as a Service: CaaS enables the deployment and management of containerized applications using orchestration tools like Kubernetes. It provides portability and scalability for applications across different cloud environments. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 27 N33 nif 1750 Rank

--- Page 28 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between main cloud computing services Terms Stands for Uses Access Model Technical understanding. Popularity Percentage rise Usage Cloud services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY IAAS Infrastructure as a service. IAAS is used by network architects. IAAS gives access to the resources like virtual machines and virtual storage. It is a service model that provides virtualized computing resources over the internet. It requires technical knowledge. It is popular among developers and researchers. It has around a 12% increment. Used by the skilled developer to develop unique applications. Amazon Web Services, sun, vCloud Express. PAAS Platform as a service. PAAS is used by developers. PAAS gives access to run time environment to deployment and development tools for application. It is a cloud computing model that delivers tools that are used for the development of applications. Some knowledge is required for the basic setup. It is popular among developers who focus on the development of apps and scripts. It has around 32% increment. Used by mid-level developers to build applications. Facebook, and Google search engine. SAAS Software as a service. SAAS is used by the end user. SAAS gives access to the end user. It is a service model in cloud computing that hosts software to make it available to clients. There is no requirement about technicalities company handles everything. It is popular among consumers and companies, such as file sharing, email, and networking. It has about a 27 % rise in the cloud computing model. Used among the users of entertainment. MS Office web, Facebook and Google Apps. 28 N32 nirf 1750 Rank

--- Page 9 ---
Course Code/Title:CS3V15/Devops Unit: I 9. Slack Slack is a crucial tool for businesses and organizations worldwide. Its significance lies in facilitating seamless communication and collaboration among teams, whether working in the same office or remotely. Slack's real-time messaging, file sharing, and integration capabilities streamline workflow, enhance productivity and keep teams connected across different time zones and locations. As the work landscape evolves, with more companies embracing hybrid and remote work models, Slack is a vital hub for quick decision-making, project coordination, and knowledge sharing. With an ever- expanding ecosystem of integrations and features, Slack remains at the forefront of modern workplace communication, making it essential for businesses to stay agile, efficient, and competitive. 10. AWS Cloud Computing and Storage in DevOps AWS (Amazon Web Services) Cloud Computing and Storage are crucial in DevOps because they provide scalable, flexible, and cost-effective infrastructure for DevOps practices. AWS offers many services, including compute resources, databases, container orchestration, and serverless computing, which align perfectly with modern software development and deployment demands. Organizations adopt DevOps to accelerate software delivery. AWS provides the foundation for rapidly deploying and scaling applications, supporting continuous integration and continuous delivery (CI/CD) pipelines, and automating infrastructure provisioning through tools like AWS CloudFormation. Furthermore, AWS's storage solutions enable efficient data management, backup, and recovery, ensuring the resilience and reliability required for DevOps operations. As cloud technology evolves, AWS remains at the forefront, enabling DevOps teams to focus on innovation and efficiency. 11. Azure Cloud Computing and Storage in DevOps Azure Cloud Computing and Storage will be pivotal in DevOps practices in 2024 and beyond. Azure offers a comprehensive cloud ecosystem that enables organizations to scale their infrastructure, deploy applications, and store data efficiently. Azure provides essential services for continuous integration and continuous deployment (CI/CD), automation, monitoring, and security. Its cloud computing capabilities facilitate the provisioning of resources on demand, ensuring that development and testing environments are readily available. Azure's storage solutions, including Azure Blob Storage, Azure Files, and Azure SQL Database, enable secure data storage and retrieval, supporting the data-driven aspects of DevOps. Besides, Azure's integration with DevOps tools like Azure DevOps Services streamlines the software development lifecycle, enhancing collaboration and automation. 12. GCP Cloud Computing and Storage in DevOps Google Cloud Platform (GCP) offers robust cloud computing and storage solutions. GCP provides a scalable, reliable, and highly available infrastructure essential for modern DevOps practices. With its comprehensive set of services, including Google Compute Engine, Google Kubernetes Engine, Cloud Storage, and BigQuery, GCP empowers DevOps teams to build, deploy, and manage applications easily. Its emphasis on automation, infrastructure as code, and container orchestration aligns seamlessly with DevOps principles. Moreover, GCP's cutting-edge technologies, such as AI and machine learning capabilities, provide DevOps practitioners with advanced tools for monitoring, analytics, and automation, making it a powerful choice for organizations seeking to optimize their software development and delivery processes. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 10 ---
Course Code/Title:CS3V15/Devops Unit: I 13. Monitoring, Alerting, and Incident Response Tools: SignalFx Monitoring, alerting, and incident response tools like SignalFx are pivotal in DevOps and software development. As software systems become complex and distributed, the need for real-time visibility into performance and the ability to respond swiftly to incidents is significant. SignalFx excels in this regard by providing advanced monitoring and observability solutions that enable organizations to detect anomalies, trace issues across microservices proactively, and set up intelligent alerts. As applications scale, cloud-native architectures become the norm, and user expectations for reliability grow, SignalFx's capabilities are crucial. It empowers DevOps teams to ensure high availability, optimize resource utilization, and maintain a seamless user experience by identifying and addressing performance issues before they impact end-users. It is one of the most essential tools for modern software operations. 14. Appdynamics AppDynamics, a leading application performance management and monitoring platform, remains critically important as it ensures the optimal performance of modern digital businesses. As organizations rely on complex and distributed software systems, proactively monitoring, troubleshooting, and optimizing these applications becomes essential. AppDynamics provides real-time visibility into application performance, allowing businesses to swiftly identify bottlenecks, latency issues, and errors. With the ever-growing complexity of applications, the importance of AppDynamics lies in its ability to empower organizations to deliver exceptional user experiences, maintain application reliability, and swiftly respond to performance issues, thereby ensuring the continued success and competitiveness of digital businesses. 15. Raygun It is a crucial tool in software development and DevOps because it ensures application reliability and performance. Raygun is an application monitoring and error-tracking platform that empowers development teams to identify, diagnose, and resolve real-time issues. With software systems growing in complexity and the increased demand for seamless user experiences, Raygun's importance lies in providing actionable insights into application errors and performance bottlenecks. It enables organizations to proactively address issues, reduce downtime, and enhance user satisfaction, leading to higher software quality and improved customer experiences. Software is central to businesses across industries. Raygun's role in maintaining application health and facilitating rapid issue resolution makes it a fundamental tool for DevOps professionals and software developers. 16. Splunk Cloud Splunk Cloud helps organizations gain critical insights from the ever-expanding volume of data generated in today's digital landscape. As businesses increasingly rely on data- driven decision-making, Splunk Cloud stands out as a robust and scalable platform for monitoring, searching, analyzing, and visualizing machine-generated data. Its importance lies in providing real-time visibility into the health and performance of complex systems, applications, and infrastructures, enabling rapid incident detection and response. As cybersecurity threats evolve, Splunk Cloud's advanced security analytics and threat detection capabilities remain indispensable for safeguarding against cyberattacks and ensuring data integrity. In a world where data is a strategic asset, Splunk Cloud's role in harnessing the power of data for operational excellence and security cannot be overstated. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 10 N32 nirf 1750 Rank

--- Page 81 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Credentials: Add credentials if required. ○ Branch Specifier: Specify the branch (e.g., */main). 7. Build Triggers: ○ Build Periodically: Use CRON syntax to schedule builds. ○ Poll SCM: Jenkins will check the repository for changes at specified intervals. ○ Other Triggers: Configure other triggers such as GitHub hooks, upstream projects, etc. 8. Build Environment: o Configure the build environment settings like setting environment variables, running scripts before the build, etc. 9. Build Steps: ○ Click on "Add Build Step" and choose the appropriate build step (e.g., "Execute Shell", "Invoke Ant", "Invoke Gradle script"). ○ Example: For executing a shell script, add the necessary shell commands. 10. Post-build Actions: ○ Click on "Add post-build action" and choose the appropriate action (e.g., "Archive the artifacts", "Publish JUnit test result report"). ○ Configure the settings for each post-build action. 11. Save the Configuration: ○ Once you have configured all the necessary settings, click "Save" at the bottom of the page. 12. Run the Job: ○ On the job's main page, click "Build Now" to run the job immediately. 13. Monitor the Job: ○ Click on the build number in the "Build History" to view the detailed output and logs of the build process. ○ Check the console output for any errors or warnings. Practical Example Setting up a Freestyle Project for a Maven Build: 1. Create New Item: ○ Name: MyMavenProject o Type: Freestyle project 2. Source Code Management: ○ Git: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 82 ---
Unit: III Course Code/Title:CS3V15/Devops I Repository URL: https://github.com/example/my-maven-project.git · Branch Specifier: * /main 3. Build Triggers: ○ Poll SCM: H/15 * (poll every 15 minutes) 4. Build Steps: ○ Invoke Top-Level Maven Targets: Goals: clean install 5. Post-build Actions: ○ Publish JUnit test result report: I Test report XMLs: * /target/surefire-reports/ *. xml 6. Save and Build: ○ Click "Save". ○ Click "Build Now" on the job's main page. By following these steps, you can set up a Jenkins job to automate your build process for various types of projects. Jenkins provides extensive customization options, allowing you to tailor the job to meet your specific requirements. Configuring a Jenkins Job 1. Install Jenkins First, make sure Jenkins is installed. You can download it from the official Jenkins website. 2. Access the Jenkins Dashboard Open your web browser and go to http://your-server-ip: 8080 to access the Jenkins dashboard. 3. Create a New Job 1. Start a New Job: ○ Click on "New Item" on the left-hand menu. 2. Name the Job: ○ Enter a name for your job in the "Enter an item name" field. 3. Select Job Type: ○ Choose the type of job (e.g., Freestyle project, Pipeline). CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) - TECHNOLOGY N32 nirf 7 1750 Rank

--- Page 73 ---
Course Code/Title:CS3V15/Devops Unit: II init - Initializes a new Gradle build. wrapper - Generates Gradle wrapper files. Help tasks Build Environment - Displays all build script dependencies declared in root project 'eclipse-workspace'. components - Displays the components produced by root project 'eclipse-workspace '. [incubating] dependencies - Displays all dependencies declared in root project 'eclipse-works pace'. Dependency Insight - Displays the insight into a specific dependency in root project 'eclipse-workspace.' Dependent Components : It displays the dependent components of components in the root project 'eclipse-workspace.' [incubating] help - Displays a help message. model - Displays the configuration model of root project 'eclipse-workspace.' [incubating] projects - Displays the sub-projects of root project 'eclipse-workspace.' properties - Displays the properties of root project 'eclipse-workspace.' tasks - Displays the tasks runnable from root project 'eclipse-workspace.' To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> To list all the tasks of the project, run the below command: 1. gradle tasks -all CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 32 N33 nirf 1750 Rank

--- Page 74 ---
Course Code/Title:CS3V15/Devops Unit: II To display more details about a task, run the below command: 1. gradle help -- task Listing Dependencies In Gradle, we can list the dependencies which are broken down by the configuration. To list the dependencies, run the below command: 1. gradle -q dependencies Difference between Gradle and Maven Basis Based on Gradle is based on developing Maven is based on developing pure Java language-based software. Configuration It uses a Groovy-based Domain-specific language (DSL) for creating project structure. Focuses on Performance It performs better than maven as it optimized for tracking only current running task. It is necessary to compile. Java Compilation Usability It is a new tool , which requires users to spend a lot This tool is a known tool for many users and is easily Gradle domain-specific language projects. It uses Extensible Markup Language (XML) for creating project structure. Developing applications by adding new features to them . It avoids compilation. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] Maven Developing applications in a given time limit. It does not create local temporary files during software creation and is hence - slower . 33 N33 nirf 1750 Rank -

--- Page 85 ---
Course Code/Title:CS3V15/Devops Unit: III By following these steps, you can set up a Jenkins job to automate builds, integrate with your source code management system, and perform various actions based on build results. Introduction to Jenkins Plugins What Are Jenkins Plugins? Jenkins plugins are extensions that add extra functionality to Jenkins. They allow you to customize and extend Jenkins to better fit your specific needs. Plugins can provide integrations with other tools, add new features, and improve existing functionalities. Why Use Plugins? . Extend Functionality: Add features not available in the core Jenkins. . Integrate Tools: Seamlessly integrate with other tools and platforms (e.g., GitHub, Docker). . Improve Productivity: Automate more tasks and streamline your workflow. · Customize UI: Tailor the Jenkins interface to better suit your preferences and needs. How to Manage Plugins 1. Accessing the Plugin Manager 1. Go to Jenkins Dashboard: ○ Open your Jenkins dashboard. 2. Navigate to Plugin Manager: ○ Click on "Manage Jenkins". ○ Select "Manage Plugins". 2. Installing Plugins 1. Available Plugins: ○ Go to the "Available" tab to see a list of plugins that can be installed. 2. Search for Plugins: ○ Use the search bar to find specific plugins. 3. Select Plugins: ○ Check the box next to the plugins you want to install. 4. Install Plugins: ○ Click "Install without restart" or "Download now and install after restart". 3. Updating Plugins CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 narf 10 1750 Rank

--- Page 86 ---
Course Code/Title:CS3V15/Devops Unit: III 1. Go to Updates: ○ Navigate to the "Updates" tab to see plugins with available updates. 2. Select Plugins: ○ Check the box next to the plugins you want to update. 3. Update Plugins: ○ Click "Download now and install after restart". 4. Managing Installed Plugins 1. Installed Plugins: ○ Go to the "Installed" tab to see all installed plugins. 2. Uninstall Plugins: ○ Click the "Uninstall" button next to the plugin you want to remove. 3. Check Plugin Versions: ○ View the current version of each installed plugin. Adding Plugins to Jenkins Plugins are a crucial part of Jenkins, allowing you to extend its functionality to suit your project's specific needs. Whether you're integrating Jenkins with other tools, customizing your job configuration, or adding new build steps, plugins provide the flexibility you need. Here's a detailed guide on how to add plugins to Jenkins. 1. Why Plugins Are Important - Extending Jenkins Functionality: Plugins allow Jenkins to integrate with various tools and technologies, such as Git, Maven, Docker, and more. - Customizing Jobs: With plugins, you can add specific steps, triggers, and post-build actions to your jobs, making Jenkins adaptable to any workflow. - Automation: Plugins help automate processes like code quality checks, notifications, and deployments. 2. Accessing the Plugin Manager 1. Navigate to Jenkins Dashboard: Open your Jenkins dashboard in a web browser (e.g., http://localhost:8080). CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 175º Rank

--- Page 93 ---
Course Code/Title:CS3V15/Devops Unit: III And configure the Jenkins job as follows - General Description In this job we have used GitHub plugin Plain text Preview Discard old builds ? GitHub project This project is parameterized ? Throttle builds Execute concurrent builds if necessary Advanced V Source Code Management None Git ? Repositories ? Repository URL https://github.com/AnurdhaP/MyJavaPrograms.gil Please enter Git repository. Credentials ? - none - Add Advanced CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 18 N3 nirf 1750 Rank

--- Page 94 ---
Course Code/Title:CS3V15/Devops Unit: III Build Triggers Trigger builds remotely (e.g. from scripts) ? Build after other projects are built ? Build periodically ? GitHub hook trigger for GITSem polling ? Poll SCM ? Schedule ? A Do you really mean "every minute" when you say "" "" " ""? Perhaps you meant "H" "" "" to poll once per hour Would last have run at Wednesday, 30 August, 2023 at 4:11:02 pm india Standard Time; would'nest run at Wednesday, 30.A Ignore post-commit hooks ? Build Steps Execute Windows batch command ? Command See the list of available environment variables javas test.java java test. java ----- Advanced V Add build sten Save this configuration. Step 5 : Now if we build this job the output that we see is as follows on Console Output. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 19 N3 nirf 1750 Rank

--- Page 83 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Click "OK" to proceed. 4. Configure the Job General Settings 1. Project Description: ○ Enter a description for your job. 2. Discard Old Builds: ○ Set up a policy to manage the number of builds Jenkins keeps. Source Code Management (SCM) 1. Select SCM: ○ Choose your source code management system (e.g., Git). 2. Configure Repository: ○ Enter the repository URL and provide credentials if needed. 3. Branch Specification: ○ Specify the branch to build (e.g., main, develop). Build Triggers 1. Choose Build Triggers: ○ Poll SCM: Check for changes at intervals. ○ Build Periodically: Schedule builds. ○ GitHub hook trigger: Trigger builds on GitHub push. ○ Build after other projects: Trigger builds based on other jobs. Build Environment 1. Configure Build Environment: ○ Clean workspace: Delete workspace before build starts. ○ Manage secrets: Use secret texts or files. ○ Node selection: Specify where to run the job. Build Steps 1. Add Build Steps: ○ Click "Add build step". ○ Choose the build step type (e.g., Execute shell, Invoke Gradle script). ○ Enter the commands or scripts needed for the build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 nirf 8 1750 Rank

--- Page 84 ---
Course Code/Title:CS3V15/Devops Unit: III Post-build Actions 1. Add Post-build Actions: Click "Add post-build action". ○ Common actions include: Archive artifacts: Store build artifacts. Email notification: Send build status emails. I Publish test results: Aggregate test results. Trigger other jobs: Trigger other jobs based on results. 5. Save and Build 1. Save the Configuration: ○ Click "Save". 2. Build the Job: ○ Click "Build Now" to manually trigger a build. ○ View progress and logs by clicking on the build number. 6. Monitor and Manage Builds 1. Check Build History: ○ View past builds and their statuses. 2. Console Output: ○ Click on a build number to see detailed logs. 3. Build Artifacts: ○ Access any artifacts produced by the build. 7. Advanced Configuration (Optional) 1. Pipeline as Code: 0 For complex workflows, use Jenkins Pipelines. Create a Jenkinsfile in your repository. 2. Plugins: ○ Extend Jenkins functionality with plugins from the Jenkins Plugin Manager. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) 9 N32 nirf 1750 Rank

--- Page 79 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Plugins: ○ Role: Extend Jenkins functionality without modifying the core software. ○ Types: I Source Control Plugins: Git, SVN, Mercurial. Build Tools Plugins: Maven, Gradle, Ant. Notification Plugins: Email, Slack, HipChat. · Reporting Plugins: JUnit, Cobertura, Checkstyle. ○ Management: Plugins can be installed, updated, and configured via the Jenkins web interface. 5. Master-Slave Architecture: ○ Concept: The Jenkins master distributes build tasks to multiple agents, enabling distributed builds. ○ Benefits: Scalability: Handle more builds simultaneously. I Resource Management: Assign builds to agents with the necessary resources. Isolation: Run builds in isolated environments to avoid conflicts. Workflow and Data Flow 1. Job Trigger: ○ Jobs can be triggered by various events: Manual Trigger: Initiated by a user through the Jenkins interface. Scheduled Trigger: Using CRON-like syntax to schedule jobs. · SCM Trigger: Automatically triggered by changes in the source code repository. Upstream/Downstream Trigger: Triggered by the completion of other jobs. 2. Build Execution: ○ Job Assignment: The master assigns the job to an available agent based on labels, availability, and resource requirements. Build Environment Setup: The agent sets up the environment, including checking out the code, installing dependencies, and configuring the workspace. ○ Execution: The agent runs the build steps as defined in the job configuration. 3. Build Results: ○ Logs: Captures console output and logs from the build process. ○ Artifacts: Stores build artifacts like binaries, packages, and reports. ○ Test Results: Collects and displays test results and code coverage reports. ○ Build History: Maintains a history of all builds, including status, duration, and changes. 4. Notification and Reporting: ○ Notifications: Jenkins can send notifications through various channels (email, chat, etc.) upon build completion or failure. ○ Dashboards: Provides dashboards for visualizing build status, trends, and metrics. ○ Reports: Generates and displays reports on test results, code quality, and other metrics. INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 4 1750 Rank

--- Page 80 ---
Unit: III Course Code/Title:CS3V15/Devops Summary Jenkins' architecture is designed to be flexible and scalable. The master-agent model allows for efficient distribution of build tasks, while plugins provide extensive customization options. By leveraging Jenkins' comprehensive job configuration and robust notification and reporting capabilities, teams can streamline their CI/CD pipelines and improve software quality and delivery speed. Creating a Jenkins Job Creating a Jenkins job involves setting up a new project within Jenkins to automate various tasks such as building code, running tests, and deploying applications. Here is a step-by-step guide to creating a Jenkins job. Prerequisites · Jenkins installed and running. · Basic understanding of the Jenkins interface. . Access to the Jenkins dashboard. Step-by-Step Guide 1. Access Jenkins Dashboard: . Open your web browser and navigate to your Jenkins instance URL (e.g., http://localhost:8080). 2. Create a New Job: ○ On the Jenkins dashboard, click on the "New Item" link on the left-hand side menu. 3. Enter Job Name: ○ Enter a name for your new job in the "Enter an item name" field. o Choose the type of job you want to create. For most uses, "Freestyle project" is a good starting point. ○ Click "OK". 4. Configure the Job: ○ After clicking "OK", you will be directed to the job configuration page. 5. General Configuration: ○ Description: Provide a brief description of the job. ○ Discard Old Builds: Optionally set this to limit the number of old builds Jenkins keeps. ○ Select Version Control System: Choose the version control system (e.g., Git, Subversion). 6. Source Code Management: ○ Repository URL: Enter the repository URL. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 5 nirf 175 Rank N33

--- Page 87 ---
Course Code/Title:CS3V15/Devops Unit: III 2. Go to Plugin Manager: From the dashboard, click on 'Manage Jenkins' on the left-hand side menu. On the Manage Jenkins page, click on 'Manage Plugins'. This will take you to the Plugin Manager, where you can view, install, and update plugins. 3. Exploring Available Plugins 1. Available Tab: The 'Available' tab lists all plugins that are available for installation. This includes thousands of plugins categorized by functionality (e.g., SCM, build tools, user interface enhancements). 2. Search for Plugins: Use the search box to quickly find the plugin you need. For example, if you're looking to integrate Jenkins with Git, type 'Git' in the search box. 3. Popular Plugins: Jenkins highlights popular plugins at the top of the Available tab. These are commonly used plugins that most users find essential. 4. Installing Plugins 1. Selecting Plugins to Install: Check the box next to each plugin you want to install. You can select multiple plugins at once. 2. Install Without Restart: After selecting your plugins, scroll down and click 'Install without restart'. Jenkins will install the plugins immediately, and you can continue using Jenkins during the installation. 3. Install After Restart: If you prefer, you can choose to 'Download now and install after restart'. This will install the plugins after Jenkins is restarted, ensuring that all changes are applied without interference. 4. Installation Progress: The installation process will show a progress bar for each plugin. Once a plugin is installed, it will move to the 'Installed' tab. 5. Configuring Installed Plugins 1. Plugin Configuration: After installation, some plugins may require additional configuration. Navigate to 'Manage Jenkins' > 'Configure System' or 'Configure Global Security' to adjust settings for your new plugins. 2. Global Tool Configuration: For build tools like Maven or JDKs installed via plugins, you can configure them under 'Global Tool Configuration'. Here, you define the paths to the tools or specify installation options. 6. Updating and Managing Plugins 1. Updating Plugins: Regularly update your plugins to ensure compatibility with the latest version of Jenkins and to benefit from new features or security patches. Go to the 'Updates' tab in the Plugin CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N32 nirf 12 1750 Rank

--- Page 88 ---
Unit: III Course Code/Title:CS3V15/Devops Manager. Jenkins will show you all plugins with available updates. Click 'Update' to install the latest versions. 2. Uninstalling Plugins: If a plugin is no longer needed, you can uninstall it from the 'Installed' tab. Select the plugin and click 'Uninstall'. Uninstallation typically requires a Jenkins restart to complete. 7. Commonly Used Plugins Here are a few essential plugins that you might consider adding to your Jenkins setup: 1. Git Plugin: Integrates Jenkins with Git, allowing you to pull code from GitHub, Bitbucket, or other Git repositories. 2. Pipeline Plugin: Enables the creation and management of Jenkins pipelines, a powerful way to define complex build, test, and deployment processes. 3. Maven Integration Plugin: Adds support for Maven projects, allowing you to build and manage Maven-based projects directly within Jenkins. 4. Email Extension Plugin: Provides advanced email notification options for build statuses, including customizable email templates. 5. Blue Ocean: An alternative user interface for Jenkins that simplifies pipeline creation and offers a modern, user-friendly design. 8. Troubleshooting Plugin Issues 1. Compatibility Issues: Sometimes, plugins might conflict with each other or with the Jenkins version. In such cases, refer to the plugin's documentation or Jenkins logs to diagnose the issue. 2. Rollback: If a plugin update causes problems, you can rollback to a previous version from the 'Installed' tab by selecting the desired version. 3. Restart Jenkins: Some plugin changes require a Jenkins restart. Ensure all jobs are completed before restarting to avoid any interruptions. Commonly Used Jenkins Plugins 1. Git Plugin Overview: The Git Plugin integrates Jenkins with Git repositories, allowing Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nårf 1750 Rank

--- Page 65 ---
Course Code/Title:CS3V15/Devops Unit: II Output The output is as follows - java version "1.8.0 66" Java(TM) SE Runtime Environment (build 1.8.0_66-b18) Java HotSpot(TM) 64-Bit Server VM (build 25.66-b18, mixed mode) Step 2 - Download Gradle Build File Download the latest https://gradle.org/install/. version of Gradle from the link available at Step 3 - Set Up Environment for Gradle Setting up environment means, we have to extract the distribution file and copy the library files into proper location. Set up GRADLE_HOME and PATH environmental variables. This step is platform dependent. In Windows Extract the downloaded zip file named gradle-2.11-all.zip and copy the distribution files from Downloads\gradle-2.11\ to C:\gradlel location. After that, add the C: \gradle and C: \gradlelbin directories to the GRADLE_HOME and PATH system variables. Follow the given instructions - Right Click On My Computers -> Click On Properties -> Advanced System Settings -> Click On Environmental Variables. There you will find a dialog box for creating and editing system variables. Click on new button for creating GRADLE_HOME variable (follow the left side screenshot). Click on Edit for editing the existing Path system variable (follow the right side screenshot). Follow the below given screenshots. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 24 N33 nirf 1750 Rank

--- Page 66 ---
Course Code/Title:CS3V15/Devops System Properties X Computer Name Hardware Advanced System Protection Remote Environment Variables x Unit: II System Properties × Computer Name |Hardware Advanced System Protection Remote Environment Variables × New System Variable × Variable name: GRADLE_HOME| Variable value: C:\grade Cancel System variables Variable Value ComSpec C:\Windows\system32\and.exe FP_NO_HOST_C ... NO GRADLE_HOME E:\work\22- Orientdb \gradle-2.11 Variable name: Path Variable value: #C:\gradle bin - System variables Variable Value Edit System Variable × OK Cancel < ORIENTDB_HOME E:\work\22- Orientdblorientdb-communi ... " JAVA_HOME C:\Program Files\Java\jdk1.7.0_60 New ... Edt ... Delete OK Cancel Step 4 - Verify the Gradle installation In windows os Windows_NT Path C:\oraclexe \app \prade\product\11.2.0\, .. PATHEXT .COM ;. EXE ;. BAT ;. CMD ;. VBS ;. VBE ;. JS ;.... v New ... Edit ... Delete OK Cancel You can execute the following command in command prompt. C:\> gradle -v Output Here you will find the Gradle version. Gradle 2.11 Build time: 2016-02-08 07:59:16 UTC Build number: none Revision: 584db1c7c90bdd1de1d1c4c51271c665bfcba978 Groovy: 2.4.4 CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 25 N33 nirf 1750 Rank

--- Page 91 ---
Course Code/Title:CS3V15/Devops Unit: III Now we will create a repository on GitHub and then push the above committed Git repository on the GitHub. Open a web browser, and login to www.GitHub.com. Create a repository by the name MyJavaPrograms. Start a new repository A repository contains all of your project's files, revision history, and collaborator discussion. AnurdhaP / MyJavaPrograms Public Anyone on the internet can see this repository Private You choose who can see and commit to this repository Create a new repository 1 then copy the URL of this repository. HTTPS SSH https://github.com/AnurdhaP/MyJavaPrograms.git Now open the command prompt window and go to the MyJavaPrograms directory and issue the following command. git remote add origin https://github.com/AnurdhaP/MyJavaPrograms.git then push the Git to GitHub repository using the command push -u origin master CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 16

--- Page 92 ---
Course Code/Title:CS3V15/Devops Unit: III It is illustrated as follows - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git push -u origin master Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 719 bytes | 719.00 KiB/s, done. Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 To https://github.com/AnurdhaP/MyJavaPrograms.git * [new branch] master -> master branch 'master' set up to track 'origin/master'. E: \MyJavaPrograms> · Now it we refresh the repository on GitHub then we can see our test.java and test.class programs are present in that repository. MyJavaPrograms Public !" master - 1º 1 branch 0 tags Go to f AnurdhaP First Commit for Java program .4cb test.class First Commit for Java program test.java First Commit for Java program Help people interested in this repository understand your project by adding a README. Step 4 : Now create a job in Jenkins that makes use of GitHub plugin in order to access the GitHub repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 17 N3 nirf 1750 Rank

--- Page 101 ---
Course Code/Title:CS3V15/Devops Unit: III v => SeleniumDemo src/main/java > src/main/resources > src/test/java > 15 src/test/resources > al JRE System Library [J2SE-1.5] › al Maven Dependencies » drivers v src > @ main v fs test v java v 2> myPackage 10 myTestPage.java 23 resources › › target > > test-output 2 pom.xml > Servers myTestPage.java package myPackage; import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver; import org.testng.annotations.Test; public class myTestPage { @Test public void myFun() { System.setProperty("webdriver.chrome.driver","./drivers/chromedriver.exe"); WebDriver driver = new ChromeDriver(); driver.get("https://google.com"); String title = driver.getTitle(); System.out.println("Hey Anuradha, the tile of this page is: "+title); driver.quit(); } Program explanation : In above program, we have written one test case by the function myFun. In this function, using the Chromedriver which we have installed, we open the web page google.com Then we try to get the tile of this page and display it on the console using System.out statement. Finally we quit the driver. Step 7 : Right click on the Project name in Project Explorer window, Run As-> Maven Test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 26 N3 nirf 1750 Rank

--- Page 102 ---
Unit: III Course Code/Title:CS3V15/Devops Step 8 : Now, locate the Java file myTestPage.java and right click Run As->TestNG Test. It is as follows - 0 Run As # Debug As Profile As Web Services > > 1 TestNG Test > Run Configurations ... > Alt+Shift+X, N Step 9 : We will get the output as follows - Hey Anuradha, the tile of this page is: Google PASSED: myFun Default test Tests run: 1, Failures: 0, Skips: 0 Default suite Total tests run: 1, Failures: 0, Skips: 0 Step 10 : Now, we will open the Jenkin's page and create the HTML Publisher report for the above SeleniumDemo project. The prerequisite to this activity is that the HTML Publisher report plugin must be installed in Jenkins. · Click on New Item, Enter the Item Name and select as Freestyle project, I have given the name HTMLReportDemo. · Then click on configure. Write the suitable description in General section. · The in Build Steps and Add Build steps as Invoke top-level Maven targets provide the Maven version, and under goals write clean test. . Under Post Build Actions, click on Add Post Build Action and select Publish HTML Report. Now give the path of index.html file which is created by Selenium testNG framework in Maven. It is the path of test-output folder. Also give some suitable HTML report title. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 27 N3 nirf 1750 Rank

--- Page 75 ---
Course Code/Title:CS3V15/Devops Customization This tool is highly customizable as it supports a variety of IDE's . Languages supported Project Configuration Based on Goal of time to get used to it. available. It supports software development in Java, C, C++, and Groovy. For declaring the project configuration, it does not use the XML files. On the phases of the fixed and linear model. Graph of task dependencies that do the work. To add functionality in the project is the main goal of the Gradle. To finish the project in the given timeline is the main goal of the Maven. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY Unit: II This tool serves a limited number of developers and is not that customizable. It supports software development in Java, Scala, C# , and Ruby and it does not natively support C and C+ but can support through plugins like "maven-native- plugin" or we can integrate other build systems like CMake or Makefile. For declaring the project configuration, it uses the XML files. 34 N33 nirf 1750 Rank

--- Page 76 ---
Course Code/Title:CS3V15/Devops Unit: III Unit - III CONTINUOUS INTEGRATION USING JENKINS Install & Configure Jenkins, Jenkins Architecture Overview, creating a Jenkins Job, configuring a Jenkins job, Introduction to Plugins, Adding Plugins to Jenkins, commonly used plugins (Git Plugin, Parameter Plugin, HTML Publisher, Copy Artifact and Extended choice parameters). Configuring Jenkins to work with java, Git and Maven, creating a Jenkins Build and Jenkins workspace. Installing and Configuring Jenkins Overview Jenkins is a widely-used open-source automation server that helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration and continuous delivery (CI/CD). Below are detailed steps and considerations for installing and configuring Jenkins. Installation 1. System Requirements: ○ Operating System: Jenkins can run on any major operating system. ○ Java: Jenkins requires Java Runtime Environment (JRE) 8 or later. 2. Download Jenkins: ○ Download the latest version of Jenkins from the official Jenkins website. 3. Installation Steps: ○ Windows: I Run the .msi installer and follow the installation wizard. ■ After installation, Jenkins will start automatically as a Windows service. ○ Linux: Add Jenkins repository and import the GPG key. I Use package manager to install Jenkins (e.g., apt-get install jenkins). I Start Jenkins using systemctl (systemctl start jenkins). 4. Initial Setup: ○ After installation, Jenkins can be accessed through a web browser at http : //< your _server _ip_or_domain>: 8080. o Unlock Jenkins by entering the initial admin password, which can be found in the jenkins_home directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 1 N32 nirf 1750 Rank

--- Page 107 ---
Course Code/Title:CS3V15/Devops Unit: III Step 8 : Then configure the Build Steps by selecting the option Copy artifacts from another project as follows - . . First of all give the name of the project from which you want to copy the files. In my case it is App1. . Then select the option "copy from WORKSPACE of latest completed build" · Give the regular expression ** / *.*. Here's a breakdown of what ** / *.* means : o ** : The double asterisk ( ** ) is a wildcard that matches zero or more directories and subdirectories. It's often used to recursively search through all directories and subdirectories. o / : The forward slash (/) is used as a directory separator, typically on Unix-like systems. o *.* : This part of the regular expression is used to match files with an extension. In regular expressions, the * symbol represents "zero or more" of the preceding character or pattern, and the . character is used to match any character. So, *.* essentially matches any file with any extension. So, when you use the regular expression ** / *.* in a Jenkins build configuration, you are instructing Jenkins to include all files with any extension ( *.* ) located in any directory and subdirectory ( ** ) within the workspace or specified location. Build Steps E Copy artifacts from another project Project name ? App: Which build ? Copy from WORKSPACE of latest completed buld Limitation Note ? Artifacts to copy ? Artifacts not to copy ? : Toget directory CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 32

--- Page 108 ---
Course Code/Title:CS3V15/Devops Unit: III Add one more Build Step and select Execute Windows batch Command. Give the command dir so that the name of files and directories that are copied from App1 to App2 will be displayed on the Console. = Execute Windows batch command ? Command See the list of available environment variables ---- dir ----- Advanced v Add build step * Click Apply and Save. Step 8 : Now Build this project (App2). The output will be as follows - Started by user Anuradha P Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\.jenkins\workspace\App2 Copied 16 artifacts from "App1" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621109248227895.bat C:\ProgramData\Jenkins\,jenkins\workspace\App2>dir Volume in drive C Is OS Volume Serial Number is CGC2-BA9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023* 15:07 <DIR> 01-09-2023 15:07 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 474 test.class 144 test. java 2 File(s) 616 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 33 N32 narf 1750 Rank

--- Page 97 ---
Course Code/Title:CS3V15/Devops Unit: III Build Steps Execute Windows batch command ? Command See the list of available environment variables javac welcome. java java welcome Advanced = Execute Windows batch command 7 Command See the list of avalable chuironment variables acho User name is Nuernane% Advanced Step 5 : Click on Apply and then Save. Step 6 : Select Build with parameters option. Dashboard > welcome > Status Changes Workspace Build with Parameters Configure Delete Project 0 Rename CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 22 N3 nirf 1750 Rank

--- Page 98 ---
Course Code/Title:CS3V15/Devops Unit: III Project welcome This build requires parameters: Username Parth D Build Cancel We can change the username as well. Suppose I changed the Username as Anand then click the Build, we get the console output as follows - C: \ProgramData\Jenkins\.jenkins\workspace\welcome>javac welcome.java C:\ProgramData\Jenkins\.jenkins\workspace\welcomeyjava welcome Welcome to first Job of Jenkins [welcome] $ cmd /c call C:\WINDOWS\TEMP\jenkins16582464065687933173.bat C:\ProgramData\Jenkins\.jenkins\workspace\welcome>echo User name is Anand User name is Anand C:\ProgramData\Jenkins\.jenkins\workspace\welcome>exit 0 Finished: SUCCESS Thus we set variety of parameters to the Jenkins job. We can set the Boolean parameter, choice parameter, credentials, file parameter, password parameter and so on. 3. HTML Publisher Filter Plain Boolean Parameter Choice Parameter Credentials Parameter File Parameter Multi-line String Parameter Password Parameter Run Parameter String Parameter Add Parameter Overview: The HTML Publisher Plugin allows Jenkins to publish HTML reports and artifacts generated during the build process. Key Features: · Report Publishing: Publishes HTML reports, dashboards, or other HTML artifacts. · Report Directory: Allows specifying the directory containing HTML reports. · Index Page: Sets a default HTML file to be displayed when accessing the report. Configuration: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 23 nirf 1750 Rank N33

--- Page 103 ---
Course Code/Title:CS3V15/Devops Unit: III =Publish HTML reports ? Reports HTML directory to archive ? E:\Eclipse_workplace\SeleniumDemo\test-output Index page[s] ? index.html Index page title[s] (Optional) Report title ? HTML Report Publishing options Step 11 : Click on Apply and then Save. Now build this project. Step 12 : On successful build, it creates the HTML Report as follows - x + € > C @ localhost:0000/juby/HTML/\portDemo/HTML_20Report/ Back to HTMLBepsatDemo Test results All suites Default suite 11 D myPackage.myTestPage Info . Repetir output Results . Thus using HTML Publish Report Plugin we can successfully generate HTML report of some testing framework application. · Troubleshooting : Sometimes, the raw file is displayed as HTML report. To get the HTML formatted report we must click on Manage Jenkins section, scroll down and locate Script Console section. Click on it. Tools and Actions C Broad Configuration Sium Disk Discard all the loaded alla lo memory and reloud Jerking CLI Script Console Ewecubes arbitrary suyt lor and run below given command : System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", ") . Then restart the Jenkins Page and Build the project once again. The HTML report can then be displayed as given in above step. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 28 N32 nirf 1750 Rank

--- Page 104 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Copy Artifact Overview: The Copy Artifact Plugin allows Jenkins jobs to copy build artifacts from other jobs, facilitating artifact reuse across different jobs. Key Features: · Artifact Copying: Copies files from one build to another, which can be from a specific build or the latest successful build. · Triggering Builds: Can be used in downstream jobs to fetch artifacts from upstream jobs. Configuration: 1. Install the Plugin: Go to Manage Jenkins > Manage Plugins > Available tab, search for "Copy Artifact Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Build section. ○ Add a Copy artifacts from another project build step. o Specify the project name, build number or criteria, and the target directory for copied artifacts. Usage: · Commonly used in multi-job pipelines where artifacts need to be shared between different jobs or stages. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 29 N3 nirf 1750 Rank

--- Page 115 ---
Unit: IV Course Code/Title:CS3V15/Devops UNIT IV CONFIGURATION MANAGEMENT USING ANSIBLE Ansible Introduction, Installation, Ansible master/slave configuration, YAML basics, Ansible modules, Ansible Inventory files, Ansible playbooks, Ansible Roles, adhoc commands in ansible. Ansible A SSH ANSIBLE TARGET NODE HOSTS PLAYBOOK Ansible is a powerful DevOps tool designed for automating tasks on remote servers or nodes. In simple terms, it allows you to automate commands and functions on multiple remote machines from a central 'master' node. To illustrate its usefulness, consider a scenario where you need to reboot dozens or even hundreds of remote hosts. You could manually SSH into each one and initiate the reboot, or you can use Ansible to streamline the process, making it efficient and offering a wide range of additional functionalities. In essence, Ansible operates much like the second method mentioned, using SSH (Secure Shell), a secure communication protocol, to control remote nodes in a secure and optimized manner. Ansible Components Ansible Core Components Control node Ansible Inventory Managed node 1 Managed node 2 Managed node 3 1. Control Node - · The central or main node where Ansible is installed. · Used to trigger commands like ansible and ansible-inventory on other nodes. · Acts as the orchestrator for Ansible operations. • TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 1 N33 nirf 1750 Rank

--- Page 116 ---
Course Code/Title:CS3V15/Devops Unit: IV 2. Manage Node- · A remote or slave node where tasks are executed or controlled by Ansible. . These are the servers or devices you want to manage or automate. 3. Inventory - · A list of managed node IPs and configurations. · Logically organized, typically using file formats like YAML or INI. · Created on the control node to describe the deployment of hosts to Ansible. Ansible Additional Components Ansible Architecture CMDB Users Public / Private Cloud Hosts ... ... Inventory .. Modules 000 Ansible Playbook API Plugins Networking www.educba.com 1. Ad-Hoc Commands - . These are one-off commands that you can execute using the ansible command. · Useful for quick tasks or tests on remote nodes. 2. Plugins - · Plugins are pieces of code that extend Ansible's core functionality. · Ansible uses a plugin architecture for flexibility and expandability. · Examples include connectivity plugins for establishing connections and cache plugins. · Learn more about Ansible plugins. 3. Module- · Modules are built-in functions that can be used to perform various tasks. . They eliminate the need to write custom code for common operations. · Examples include modules for package management (apt, yum), service management, and more. CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) CHENNAI INSTITUTE . TECHNOLOGY 2 N33 nirf 1750 Rank

--- Page 111 ---
Course Code/Title:CS3V15/Devops Unit: III Steps: 1. Install the Git Plugin: 0 Go to Manage Jenkins > Manage Plugins. ○ Under the Available tab, search for Git Plugin and install it. 2. Configure Git in Jenkins: ○ Navigate to Manage Jenkins> Global Tool Configuration. ○ Scroll down to the Git section and click Add Git. ○ Specify the path to the Git executable or use the default path. ○ Optionally, configure additional settings such as Git installation locations. 3. Example Configuration: ○ Name: Git ○ Path to Git executable: /usr/bin/ git (or use the default if Git is installed in a standard location). 4. Configure Job to Use Git: ○ Create or edit a Jenkins job and go to Source Code Management. ○ Select Git. ○ Enter the repository URL (e.g., https://github.com/user/repository.git). ○ Configure credentials if the repository is private. ○ Specify the branch to build (e.g., main). 5. Example Repository URL: ○ Repository URL: https://github.com/example/repo.git o Branch Specifier: main 3. Configuring Jenkins to Work with Maven Overview: The Maven Plugin integrates Jenkins with Apache Maven, allowing Jenkins to use Maven to build projects. Steps: 1. Install Maven: ○ Ensure that Apache Maven is installed on your system. You can download it from the Apache Maven website. 2. Configure Maven in Jenkins: ○ Go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to the Maven section and click Add Maven. ○ Enter a name for the Maven installation (e.g., Maven 3.8.6). ○ Check Install automatically to let Jenkins download Maven, or specify the path to an existing Maven installation. ○ CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 36 1750 Rank

--- Page 112 ---
Unit: III Course Code/Title:CS3V15/Devops 3. Example Configuration: ○ Name: Maven 3.8.6 ○ MAVEN_HOME: / usr / share / maven 4. Configure Maven in a Jenkins Job: ○ Create or edit a Jenkins job and go to Build section. 0 Add a build step and select Invoke top-level Maven targets. o Choose the Maven version configured earlier. ○ Specify the goals to run (e.g., clean install). 5. Example Maven Goals: ○ Goals: clean install 6. Configure Build Environment: ○ Ensure that the job's build environment is set up to use Maven. This might involve setting environment variables or configuring build scripts. 7. Example Environment Variables: ○ MAVEN_OPTS: - Xms512m -Xmx2048m (if you need to set specific JVM options for Maven). By following these steps, Jenkins will be properly configured to work with Java, Git, and Maven, allowing you to set up and manage continuous integration and delivery pipelines effectively. If you need further customization or run into issues, consulting the documentation for each tool or plugin can provide additional guidance. Creating a Jenkins Build and Understanding Jenkins Workspace 1. Creating a Jenkins Build Overview: A Jenkins build is a process where Jenkins executes a series of steps defined in a job configuration. This typically involves compiling code, running tests, and generating artifacts. Steps to Create a Jenkins Build: 1. Access Jenkins Dashboard: ○ Open your Jenkins dashboard, usually found at http://localhost: 8080. 2. Create a New Job: ○ Click on New Item on the left sidebar. Enter a name for your job. ○ Choose a job type. Common types include: I Freestyle project: For simple build jobs with basic configuration. · Pipeline: For more complex build processes using a Jenkinsfile. ○ Click OK to proceed. ○ TECHNOLOGY N33 37 nirf 1750 Rank CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous)

--- Page 117 ---
Course Code/Title:CS3V15/Devops Unit: IV • Explore Ansible modules. 4. Playbook - · Playbooks are a sequence of plays that define the order of tasks executed by Ansible. . A play consists of a list of tasks that target managed nodes in an inventory. · Tasks, in turn, are composed of one or more modules that specify operations. · Playbooks provide a structured way to define and automate complex workflows. 5. Roles - · Roles provide an organized environment for managing complex tasks. · They include templates, playbooks, inventories, error handlers, vars, and meta information. · Ideal for handling larger, multi-step automation processes. 6. Collections - · Collections are distribution formats for Ansible content. · They encompass playbooks, roles, modules, and plugins. · Find Ansible collections. 7. Galaxy - · Ansible Galaxy is a platform for sharing and downloading collections. . It allows the Ansible community to collaborate and exchange automation content. Setting Up the Ansible Control Node Prerequisite: Before you begin setting up Ansible, it's essential to ensure you meet the following prerequisites: · SSH: Make sure SSH is installed on your Linux system. SSH is crucial for secure communication between the control node and managed nodes. Installation Steps: 1. Open your terminal. 2. Run the following command to install Ansible: 1python3 -m pip install -- user ansible 3. SSH Key: o For secure and direct connections to managed nodes, you'll need to provide a public SSH key. You can generate one using the ssh-keygen command. o Follow the on-screen instructions to create your SSH key pair. This key pair is a crucial element of Ansible's secure communications. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 118 ---
Course Code/Title:CS3V15/Devops Unit: IV 4. For specific installation instructions tailored to your Linux distribution, click here. 5. Try to connect ssh using once your mange node is set up. 1ssh <user>@<ip> Note: If you're using a different operating system, find the appropriate installation instructions here. Setting Up the Ansible Mange Node Steps: 1. Install OpenSSH-Server: o To enable remote management of the managed node, you'll need to install the OpenSSH server. Use the following command (assuming you're using a Debian-based Linux distribution like Ubuntu): 1sudo apt install openssh-server o This command installs the OpenSSH server, allowing secure remote access. 2. Create a User: o For convenience and consistency, it's helpful to create a user on the managed node with the same name as the user on the control node. This makes it easier to manage SSH keys and ensures a smoother experience. 3. Configure SSH Key: o To establish secure and direct connections to the managed node, you need to paste the public SSH key from the control node into the authorized_keys file located in the .ssh directory of the user's home folder on the managed node. o The path is typically /home/<user>/.ssh/authorized_keys, where <user> is the username you created or are using on the managed node. You can use the ssh-copy-id command to automate the process of copying your public key to the managed node. For example: 1ssh-copy-id <user>@<managed_node_ip> o This command securely copies your public key to the authorized_keys file, allowing passwordless SSH authentication. Managing Ansible Inventories In the world of Ansible, inventories are like the backbone of your automation infrastructure. They serve as a vital component that lists and organizes the managed nodes, making automation tasks seamless and organized. Inventories use a parent-child concept that allows you to create groupings, which come in handy during specific tasks or scenarios. Why Are Inventories Important? · Inventories provide a comprehensive list of managed nodes, making it easy to interact with and manage them. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 4 N33 nirf 1750 Rank

--- Page 71 ---
Unit: II Course Code/Title:CS3V15/Devops build.gradle &3 I 1 /* 2 * This file was generated by the Gradle 'init' task. 3 * 4 * This generated file contains a sample Java Library project to get you started. 5 * For more details take a look at the Java Libraries chapter in the Gradle 6 * User Manual available at https://docs.gradle.org/6.0.1/userguide/java_library_plugin.html 7 */ 8 9 plugins { 10 // Apply the java-library plugin to add support for Java Library 11 id 'java-library' 12 } 13 14 repositories { 15 // Use jcenter for resolving dependencies. 16 // You can declare any Maven/Ivy/file repository here. 17 jcenter() 18 } 19 20 dependencies { 21 // This dependency is exported to consumers, that is to say found on their compile classpath. 22 api 'org. apache. commons : commons-math3: 3.6.1' 23 24 // This dependency is used internally, and not exposed to consumers on their own compile classpath. 25 implementation 'com. google. guava: guava: 28.0-jre' 26 27 // Use JUnit test framework 30 28 testImplementation 'junit : junit: 4.12' 29 } 30 The build.gradle file contains three default sections. They are as follows: o plugins: In this section, we can apply the java-library plugin to add support for java library. o Repositories: In this section, we can declare internal and external repository for resolving dependencies. We can declare the different types of repository supported by Gradle like Maven, Ant, and Ivy. · Dependencies: In this section, we can declare dependencies that are necessary for a particular subject. Additionally, we can declare other project-related modules like a task in this file. Display the Information of the Gradle project To understand the structure, dependencies and debugging problems of a build, Gradle provides many built-in features that display information on a project. Following are some basic commands to display the information of the project: Listing projects In Gradle, all the sub-projects of a project in the workspace can be listed in a hierarchy. To do so, run the below command from the root directory of the project. 1. gradle -q projects CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 72 ---
Course Code/Title:CS3V15/Devops Unit: II To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> C:\Users \HiMaNshU\eclipse-workspace >gradle -q projects Root project Root project 'eclipse-workspace' No sub-projects To see a list of the tasks of a project, run gradle <project-path> : tasks For example, try running gradle :tasks C:\Users\HiMaNshU\eclipse-workspace> Listing Tasks Gradle allows us to list all the essential tasks of the project. To list the task, run the below command: 1. gradle -q tasks Output: C:\Users\HiMaNshUleclipse-workspace>gradle -q tasks Tasks runnable from root project Build Setup tasks CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 31 N33 nirf 1750 Rank

--- Page 105 ---
Course Code/Title:CS3V15/Devops Unit: III How to install copy artifact plugin ? Step 1 : Start the Jenkins tool by entering the login and password. Step 2 : Click on Manage Jenkins. Click on Plugins. Step 3 : In the search window, type the name copy Artifact. Step 4 : The plugin name will be displayed if it is not already installed. Select it. Name Adde a build atop to copy artifacts from another project. Thủa plugin le up for adoption! We are looking for naw maintainent. Velt our Adopt a Plugin intuitive for mort information. then click on Install. The plugin gets installed. Example Demo · Let us discuss how to use Copy Artifact plugin. Step 1 : Create a new Job by clicking on item New Item. Give the some name to your job. I have given the name App1 and selected the Freestyle Project. Step 2: Give suitable description in the description box. Step 3 : Under the source code management, click on Git option and specify the repository URL of GitHub. Source Code Management None Git ? Repositories 1 Repository URL + ·hong · CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 30 N3 nirf 1750 Rank

--- Page 106 ---
Unit: III Course Code/Title:CS3V15/Devops Step 4 : Under the Build Steps section, choose the Execute Windows batch command option. And type the javac and java commands. Build Steps = Execute Windows batch command Command See the list of available environment variables javac test.java java test Advanced Add bulld stop * Step 5 : Click in Apply and Save. Step 6 : Select the Build Now option. On successful built, the test, class file gets generated in the Jenkins workspace of the App1 job. C:\ProgramData\Jenkins\.jenkins\workspace\Appl>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\Appi>java test Good Morning Pathill Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! C:\ProgramData\Jenkins\.jenkins\workspace\Appl>exit @ Finished: SUCCESS Step 7 : Now we have to copy test.class file from the workspace of Appl to another job. For that' purpose we will create another Job named App2 by clicking New Item. Select the Freestyle Project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 31 N3 nirf 1750 Rank

--- Page 119 ---
Course Code/Title:CS3V15/Devops Unit: IV · Groupings in inventories enable you to organize your infrastructure, allowing you to target specific subsets of nodes based on your needs. For instance, when you need to perform tasks in a particular location, groupings become essential. Creating Inventories There are two primary methods to create an inventory on the control node: using a YAML file or an INI file. 1. INI Inventory Example (inventory.ini): 1[virtualmachines] 2host1 ansible_host:192.168.0.1 3192.168.0.2 2. In this INI file, it captures the IP addresses of managed nodes. The default username used for SSH connections is typically the working user of your control node. 3. ansible_host: This parameter is used to specify the IP address, although you can write the IP directly. 4. Inventory.yaml 1 --- 2virtualmachines: # Define a class name to represent a particular group of devices. 1 hosts: # Define the hosts. 2 vm01: # Define a parent name to indicate a class of parent type. 3 ansible_host: 192.168.0.1 # Specify the IP address. 4 http_port: 80 # (optional) Define the HTTP port. 5 ansible_user: auriga # (optional) Provide the username if it's different from the control node's user. o ansible_user: Use this to specify a different user if the managed node's user is not the same as the control node's user. o http_port: Specify a port if necessary. In real-world scenarios, inventory files can be more complex, reflecting the diverse and extensive infrastructure of organizations. You can explore more configurations here. Verifying Your Inventory After creating your inventory, it's a good practice to verify its correctness. This step ensures that the inventory is correctly structured and accessible to Ansible. To do this, use the following command: 1ansible-inventory -i inventory.yaml -list · - i: Indicates the inventory file you want to use. . - list: Requests the listing of the inventory content. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 5 N33 nirf 1750 Rank

--- Page 120 ---
Course Code/Title:CS3V15/Devops Unit:IV output: auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-inventory ·i inventory.yaml -- list "_meta": { "hostvars": { "vm01": "ansible_host": "192.168.0.208", "ansible_user"; "au" "children": [ "ungrouped" "ungrouped": { "hosts": "vn01 1 By following these steps, you have effectively set up and verified your inventory, ensuring a solid foundation for your Ansible automation. Using Ansible Ad-Hoc Commands Introduction: In Ansible, ad-hoc commands are your go-to solution for executing quick, one-off tasks on remote nodes. They provide a straightforward and efficient way to interact with managed nodes without the need for creating full-fledged playbooks. Ad-hoc commands are particularly useful when you need immediate results without the overhead of playbook development. When to Use Ad-Hoc Commands: Ad-hoc commands are best suited for scenarios where the task at hand is simple and doesn't require the complexity of a playbook. They are perfect for tasks like system health checks, package installation, service management, or any other single-operation job. Ad-Hoc Command Syntax: Ad-hoc commands follow a specific syntax that comprises various components, each serving a unique role: . Target Group: This is the group of hosts you intend to target with the ad-hoc command. · Module (-m): Specifies the module to execute. Modules are Ansible's building blocks for performing tasks, and they can range from basic operations like "ping" to more advanced tasks such as package management. · Inventory File (-i): Indicates the location of your inventory file, which defines the list of target devices. Example: Ping All Inventory Devices To illustrate the use of ad-hoc commands, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: 1ansible virtualmachines -m ping -i inventory.yaml virtualmachines is the target group. In this case, it could be any group, or you can use all to target all devices in your inventory. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 77 ---
Unit: III Course Code/Title:CS3V15/Devops 5. Customize Jenkins: ○ Install suggested plugins or select the required plugins manually. ○ Create the first admin user. Configuration 1. Global Configuration: ○ Navigate to Manage Jenkins > Configure System. ○ Set up global environment variables, email notifications, and other system-wide settings. 2. Security Configuration: ○ Configure security settings under Manage Jenkins > Configure Global Security. o Use the built-in user database or integrate with external authentication mechanisms like LDAP. 3. Node Configuration: ○ Jenkins can distribute build loads to multiple nodes. ○ Configure nodes by navigating to Manage Jenkins > Manage Nodes and Clouds. 4. Job Configuration: ○ Create new jobs by clicking New Item on the Jenkins dashboard. o Configure job-specific settings such as source code management, build triggers, and post-build actions. 5. Pipeline Configuration: ○ Jenkins supports pipeline as code using Jenkinsfile. ○ Define your pipeline stages in a Jenkinsfile placed in the root of your project repository. 6. Plugins: ○ Enhance Jenkins functionality by installing plugins from the Manage Jenkins > Manage Plugins section. 7. Backup and Restore: ○ Regularly back up the jenkins_home directory, which contains Jenkins configurations, job configurations, and build history. ○ Use plugins like ThinBackup for automated backups. Ref: https://youtube.com/playlist?list=PL6flErFppaj35spJjPy41-IruDjw2kRV-&si=Vzd9AAMv53xwWB S Ref: https://bit.ly/youtube_jenkins (click here) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 78 ---
Course Code/Title:CS3V15/Devops Unit: III Jenkins Architecture Overview Jenkins is a robust automation server widely used for continuous integration and continuous delivery (CI/CD) processes. Here's a detailed breakdown of the Jenkins architecture and its core components. Core Components of Jenkins Architecture 1. Jenkins Master: ○ Role: The Jenkins master is the central control unit. 0 Functions: Scheduling Jobs: Assigns build jobs to the appropriate agents. Monitoring Agents: Keeps track of the state and health of agents. I Job Execution: Executes build jobs directly if no agents are available. I User Interface: Provides a web-based interface for users to configure jobs, view build results, and manage the Jenkins environment. 2. Build Agents (Nodes): ○ Role: Agents are responsible for executing the build jobs assigned by the master. ○ Types: Agents can run on different operating systems and hardware configurations. ○ Configuration: Static Agents: Permanently assigned to the Jenkins master. . Dynamic Agents: Created and destroyed as needed, often used in cloud environments. ○ Communication: Agents communicate with the master using the Jenkins Remoting protocol. 3. Job Configuration: ○ Types of Jobs: I Freestyle Projects: Basic job type with simple configurations. Pipeline Jobs: Scripted or declarative pipelines that define the entire build process as code. I Multi-Configuration (Matrix) Projects: Allows testing across different environments and configurations. ○ Components: I Source Code Management: Integration with version control systems like Git, SVN, etc. . Build Triggers: Conditions that start the job, such as code commits, scheduled times, or manual triggers. Build Steps: Actions performed during the build, such as compiling code, running tests, and packaging artifacts. . Post-Build Actions: Steps executed after the build, like deploying artifacts, sending notifications, or archiving results. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 151 ---
Azure DevOps Taking you to your Azure DevOps organization ... Congratulations, you're an organization owner! Sign in to your organization at any time, https://dev.azure.com/{yourorganization}. With your organization, the following aspects are included in the free tier: First five users free (Basic license): Azure Pipelines: o One Microsoft-hosted CI/CD (one concurrent job, upto 30 hours per month) o One self-hosted CI/CD concurrent job Azure Boards: Work item tracking and Kanban boards Azure Repos: Unlimited private Git repos Azure Artifacts: Two GB free per organization Build applications with Azure: Azure DevOps enables you to build, test, and deploy anyapplication to any cloud or on premises To configure build pipelines that continuously build, test, and verify your applications. 11

--- Page 152 ---
Part 1: Get started with Azure DevOps Part 2: Build applications with Azure DevOps Part 3: Deploy applications with Azure DevOps Create a build pipeline with Azure Pipelines: Prerequisites: Familiarity with forking and cloning a GitHub repo Account requirements: An Azure DevOps organization o To use Microsoft-hosted agents, your Azure DevOps organization must have access to Microsoft- hosted parallel jobs. Check your parallel jobs and request a free grant. O You can use GitHub Code spaces to complete the module, even if your Azure DevOps organization doesn't have any parallel jobs. A GitHub account Software requirements: If using GitHub Code spaces to complete the module, there are no software requirements as all software is included in the Codespace If using a local development environment withMicrosoft-hosted agents, you must have the following software installed: o Visual Studio Code o .NET 6.0 SDK o Git 12

--- Page 89 ---
Course Code/Title:CS3V15/Devops Unit: III Key Features: · Source Code Management: Configures Jenkins to use Git repositories as the source code for jobs. Example Demo Step 1 : Create a simple Java program. I have created a folder named MyJavaPrograms and inside it created a simple Java program as follows - e.g., SSH test.java public class test { public static void main(String args[I) { for(int i=1;i <= 5;1++) System.out.printin("Welcome Anuradha"); } Plugin," Step 2 : Open the command prompt, switch to that folder and execute the above Java program. It is illustrated by following screenshot. 1 C:\Windows\System32\cmd.e X + Microsoft Windows [Version 10.0.22621.2134] (c) Microsoft Corporation. All rights reserved. 0 X before E:\MyJavaPrograms>javac test. java E: \MyJavaPrograms>java test Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha E: \MyJavaPrograms> Step 3 : Now we will create a Git repository and push this repository on GitHub. First of all we will initialise the Git repository my using git init command. CHENNAI INSTITUTE TECHNOLOGY 14 - N32 nirf 1750 Rank CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt)

--- Page 90 ---
Unit: III Course Code/Title:CS3V15/Devops C:\Windows\System32\cmd.e x X E:\MyJavaPrograms>git init Initialized empty Git repository in E:/MyJavaPrograms/.git/ E:\MyJavaPrograms>git status On branch master No commits yet Untracked files: (use "git add <file> ... " to include in what will be committed) test.class test. java nothing added to commit but untracked files present (use "git add" to track) E:\MyJavaPrograms> then add the java and class files to git repository - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git add . E:\MyJavaPrograms>git status On branch master No commits yet Changes to be committed: (use "git rm -- cached <file> ... " to unstage) new file: test.class new file: test. java E: \MyJavaPrograms> Now we will commit the changes - C:\Windows\System32\cmd.e X E:\MyJavaPrograms>git commit -m "First Commit for Java program" [master (root-commit) 4cb04db] First Commit for Java program 2 files changed, 6 insertions(+) create mode 100644 test.class create mode 100644 test. java E:\MyJavaPrograms> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) - INSTITUTE . TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 157 ---
Select a pipeline run to view information about that run. You can choose to Retain or Delete a run from the context menu. For more information on run retention, see Build and release retention policies. Stages La 51m ago > ... 6 42s Retain Delete Ld 2h ago 0 1m 13s - View pipeline details The details page for a pipeline allows you to view and manage that pipeline. Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. Azure DevOps · Search F 4 FabrikamFiber Edit Run pipeline : + Runs Branches Analytics Y Description Stages Ac #20191209.3 Set up CI with Azure Pipelines Manually triggered { main 2b4b23c #20191209.2 Set up CI with Azure Pipelines & Manually triggered & main 2b4b23c Là 54m ago Č 1m 13s Là 55m ago Č 1m 55s #20191209.1 Set up CI with Azure Pipelines Individual CI 1º main 2b4b23c Lô 56m ago 1m 6s 17

--- Page 158 ---
Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. View pipeline run details From the pipeline run summary you can view the status of your run, both while itis running and when it is complete. #20191210.2 Update azure-pipelines.yml for Azure Pipe ... on FabrikamFiber Summary Environments Triggered by Steve Danielson FabrikamFiber & main b2f795e Today at 12:56 PM Duration: 1m 9s Tests: Get started Changes: ¢ 2 commits Work items: ₾ 1 linked Artifacts: 1 published Stages Jobs Build 1 job completed 415 Run new : Deploy 1 job completed 13s 1 artifact From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, test results, and work Items From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, testresults, and work items Jobs and stages The jobs pane displays an overview of the status of your stages and jobs. This pane may have multiple tabs depending on whether your pipeline has stages and jobs, or just jobs. In this example, the pipeline has two stages named Build and Deploy. You can drill down into the pipeline steps by choosing the job from either the Stages or Jobs pane. 10

--- Page 127 ---
Course Code/Title:CS3V15/Devops Unit:IV Abbreviation You can also use abbreviation to represent lists. Example Countries: ['America', 'China', 'Canada', 'Iceland'] List inside Dictionaries We can use list inside dictionaries, i.e., value of key is list. Example james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english … List of Dictionaries We can also make list of dictionaries. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english - robert: name: robert richardson rollNo: 53 div: B sex: male CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 13 N32 nirf 1750 Rank

--- Page 128 ---
Course Code/Title:CS3V15/Devops Unit:IV likes: - biology - chemistry … YAML uses "|" to include newlines while showing multiple lines and ">" to suppress newlines while showing multiple lines. Due to this we can read and edit large lines. In both the cases intendentation will be ignored. We can also represent Boolean (True/false) values in YAML. where boolean values can be case insensitive. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english result: maths: 87 chemistry: 45 biology: 56 physics: 70 english: 80 passed: TRUE messageIncludeNewLines: | Congratulation !! You passed with 79% messageExcludeNewLines: > Congratulation !! You passed with 79% CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 14 N33 nirf 1750 Rank

--- Page 137 ---
Course Code/Title:CS3V15/Devops Unit:IV I Example 2: Get the Hostname and Version of remote servers with UNAME we have used the command module and executing an uname -a command as AD-HOC command $ ansible testservers -m command -a "uname -a" -i ansible hosts As Playbook - name: Check the remote host Hostname, Version, Distribution with UNAME hosts: testservers tasks: - name: Execute the UNAME command register: unameout command: "uname -a" - debug: var: unameout.stdout lines I Example 3: Check the Disk Usage of Remote server To get the disk usage, we are using df -h , here -h is human readable As Ad Hoc command $ ansible testservers -m command -a "df -h" -i ansible hosts As Playbook - name: Check the disk usage of all the file system in the remote servers hosts: testservers tasks: - name: Execute the df command register: dfout command: "df -h" - debug: var: dfout.stdout lines CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 138 ---
Course Code/Title:CS3V15/Devops Unit:IV Example 4: Restart Apache Server using Ansible Command Module So far, In all the sample we have seen we are using one application server and one web server. Totally two servers. But in this example, we need to limit our execution only to the web server as we are going to check the restart the apache web server. this is done using -- limit parameter As Ad hoc command $ ansible testservers -m command -a "httpd -k restart" -i ansible_hosts -b - limit As Playbook - name: restart apache web server hosts: testservers tasks: - name: restartapache register: httpdresout become: yes command: "httpd -k restart" when: ansible_hostname == "mwiweb02" - debug: var: httpdresout.stdout_lines I Example 5: Execute a command when a file exists or not exists There are two most useful parameters in ansible command module such as removes and creates removes - used to tell ansible to Execute the command only if the file exist creates - used to tell ansible to Execute the specified command only if the file does not exist Here we are going to do a very simple file creation and removal based on the file availability or existence. In General using the ansible command module, This method of creating and removing the file is not recommended as Ansible has a dedicated module named file to do the same effortlessly. As Ad hoc command Remove the file, if it does exist (or) present $ ansible testservers -a "rm -rf /tmp/testfile removes=/tmp/testfile" -i ansible_hosts CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 24 N33 nirf 1750 Rank

--- Page 143 ---
Unit: V Course Code/Title: CS3V15/Devops Email preferences Send me occasional product updates, announcements, and offers. Verify your account Pick one square that shows two identical objects. wiki How Step 4: Click the green Create account button. It's belowthe form, at the bottom of the page. This will take you to an email verification page. Verify your account Create account By creating an account, you agree to the Terms of Service. For more information about GitHub's privacy practices, see the GitHub Privacy Statement. We'll occasionally send you account-related wikiHow UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 3

--- Page 144 ---
Course Code/Title: CS3V15/Devops Unit: V Carefully review the Terms of Service at https://help.github.com/en/articles/github- terms-of-service and the Privacy Statement at https://help.github.com/en/articles/github- privacy-statement before you continue. You're almost done! We sent a launch code to wikihowneveconcepts@gmail.com + Enter code Didn't get your email? Resend the code or update your email address. wiki How Step 5: Verify your email by entering the code. After clicking Create account, you'll receive an email with a code.Enter this code on the verification page. Entering the code will automatically take you to the welcome page. Step 6: Select your preferences and click Continue. GitHub displays a quick survey that can help you tailor your experience to match what you're looking for. You'll be sent tothe plan selection page after completing the survey. How many team members will be working with you? This will help us guide you to the tools that are best suited for your projects. Just me 2 - 5 5 - 10 10 - 20 20 - 50 50+ Are you a student or teacher? Student Teacher Continue wikiHow CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 4 175° Rank ---

--- Page 141 ---
Course Code/Title: CS3V15/Devops Unit: V UNIT V - BUILDING DEVOPS PIPELINES USING AZURE Create Github Account, Create Repository, Create Azure Organization, Create a new pipeline, Build a sample code, Modify azure-pipelines.yaml file Github Account Creation What is Github? GitHub is a code hosting platform for collaboration and version control. GitHub lets you and others work together on projects fromanywhere. Github is owned by Microsoft, provides access to public(free) and private(paid) repositories. Stepsto create Github Account: Join GitHub · GitHub × + < C https://github.com/join f Facebook - Log In o ... Twitter G Google f Facebook YouTub Product v Solu wiki How Step 1:Go to https://github.com/join in a web browser. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 1 175" Rank

--- Page 142 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: Enter your personal details. In addition to creating a username and entering an email address, you'll also have to create a password. Your password must be at least 15 characters in length or at least 8 characters with at least one number and lowercase letter. Join GitHub First, let's create your user account Username * wikihowneveconcepts Email address * Password * .......... Make sure it's at least 15 characters OR at least 8 characters including a number and a lowercase letter. Learn more. Email preferences Send me occasional product updates, announcements, and offers. Verify your account wiki How Step 3: Click Verify to start the verification puzzle. The instructions vary by puzzle, so just follow the on-screen instructions to confirm that you are a human. A green checkmark will appear after completing the puzzle. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 2

--- Page 149 ---
Course Code/Title: CS3V15/Devops Unit: V Step 6: Now you will see that all of our files uploaded in our github. Namanbhatia7 Add initial resume website files Latest commit f47be5b 1 minute ago Display_Picture.png Add initial resume website files 1 minute ago My_Resume.html Add initial resume website files 1 minute ago README.md Initial commit 15 minutes ago Styles.css Add initial resume website files 1 minute ago Branch: · A GitHub branch is used to work with different versions of a repository at the same time. · By default a repository has a master branch (aproduction branch). · Any other branch is a copy of the master branch (as itwas at a point in time). · New Branches are for bug fixes and feature work separate from the master branch. When changes are ready, they can be merged into the master branch. If youmake changes to the master branch while working on a new branch, these updates can be pulled in. Commits: At GitHub, changes are called commits. Each commit (change) has a description explaining why achange was made. Pull Requests : . Pull Requests are the heart of GitHub collaboration. · With a pull request you are proposing that your changesshould be merged (pulled in) with the master. · Pull requests show content differences, changes, additions, and subtractions in colors (green and red). · As soon as you have a commit, you can open a pull request and start a discussion, even before the code isfinished. Git: · Git was created by Linus Torvalds in 2005 to develop Linux Kernel · Git is an open-source distributed version control system. It is designed to handle minor to major projectswith high speed and efficiency. . It is developed to co-ordinate the work among the developers. The version control allows us to track andwork together with our team members at the same workspace. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 9 175" Rank

--- Page 150 ---
CREATE AZURE ORGANIZATION AZURE: Microsoft Azure, often referred to as Azure cloud computing platform run by Microsoft. It offers access, management, and the development of applications and services through global data centers It also provides a range of capabilities, including software as a service (SaaS), platform as a service, and infrastructure as a service (IaaS). It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014 Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Prerequisites: To plan your organizational structure. Microsoft accounts or authenticate users with Microsoft Entra ID. For more information, see Choosing your organization administrator account type. Create an organization: 1. Sign in to Azure DevOps. 2. Select New organization 3. Confirm information, and then select Continue 10

--- Page 155 ---
10. When you're ready to make changes to your pipeline, select it in the Pipelines page, and then Edit the azure-pipelines.yml file. View and manage your pipelines You can view and manage your pipelines by choosing Pipelines from the left-hand menu to go to the pipelines landing page. Azure DevOps : FabrikamFiber 1 Pipelines 0 Search F FabrikamFiber + Pipelines New pipeline Overview Recent All Runs Y Filter pipelines Boards Repos Pipelines Pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... La 32m ago & Manually triggered 8º main & 42s Environments Releases FabrikamFiber #20191209.3 · Set up CI with Az ... La 1h ago & Manually triggered & main 1m 13s Library Task groups *** Deployment groups From the pipelines landing page you can view pipelines and pipeline runs, create and import pipelines, manage security, and drill down into pipeline and run details. Choose Recent to view recently run pipelines (the default view), or choose All to view all pipelines. Pipelines New pipeline : Recent All Runs Filter pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... & Manually triggered & main Là 36m ago ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Az ... Là 1h ago 1m 13s & Manually triggered & main 15

--- Page 156 ---
Select a pipeline to manage that pipeline and view the runs. Select the build number for the last run to view the results of that build, select the branch name toview the branch for that run, or select the context menu to run the pipeline and perform other management actions. Recently run pipelines Pipeline Last run #20191209.2 · Set up CI with Azure Pip ... > pipelines-dotnet-core 8 Manually triggered 12º main Là 1h ago * ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Azure Pip ... 試 2h 8 Manually triggered 8 main Edit in Run pipeline Manage security Rename/move Delete Select Runs to view all pipeline runs. You canoptionally filter the displayed runs. Pipelines Recent All Runs Filter by keywords State V Repository V New pipeline Requested forV Tags X All pipeline runs Description Stages Set up CI with Azure Pipelines & #20191209.2 on pipelines-dotnet-core & main d4964 .. Set up CI with Azure Pipelines #20191209.1 on pipelines-dotnet-core åº main d4964 ... Set up CI with Azure Pipelines & #20191209.3 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines & #20191209.2 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines #20191209.1 on FabrikamFiber & main 2b4b23c La 44m ago 42s La 53m ago ₾ 46s Là 1h ago 0 1m 13s Ld 1h ago 1m 555 Lo 1h ago 1m 6s 16

--- Page 113 ---
Course Code/Title:CS3V15/Devops Unit: III 3. Configure Job Details: ○ General: I Enter a description for your job. Configure options such as discarding old builds if needed. ○ Source Code Management: I Choose Git, Subversion, or another source control system. Enter the repository URL and credentials if the repository is private. Specify the branch or tag to build from. ○ Build Triggers: Set up triggers to start the build. Common options include: Poll SCM: Jenkins will periodically check for changes in the source code repository. . Build periodically: Schedule builds at specific intervals. I GitHub hook trigger for GITScm polling: Trigger builds based on GitHub webhooks. 0 Build Environment: ■ Configure the build environment, such as setting up environment variables or cleaning up before the build starts. ○ Build Steps: Add build steps by clicking Add build step. · Common build steps include: Execute shell: Run shell commands or scripts. . Invoke Gradle script: Use Gradle to build the project. I Invoke Ant: Use Apache Ant for building. ○ Post-build Actions: · Configure actions to perform after the build completes, such as: I Archive the artifacts: Save build artifacts for later use. I Publish JUnit test result report: Display test results in Jenkins. I Send build notifications: Notify stakeholders of build status. 4. Save and Build: ○ Click Save to store the job configuration. o To start a build, click Build Now in the job dashboard. Example Build Configuration: . Job Name: MyApp-Build · Source Code Management: Git o Repository URL: https://github.com/user/myapp.git ○ Branch: main · Build Step: Execute shell ○ Command : myn clean install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 38 nirf 1750 Rank N33

--- Page 114 ---
Course Code/Title:CS3V15/Devops Unit: III · Post-build Action: Archive artifacts ○ Files to archive: target/ *. jar 2. Understanding Jenkins Workspace Overview: The Jenkins workspace is a directory where Jenkins stores files and artifacts related to a particular build. Each job has its own workspace, which is used to perform build operations. Key Aspects of Jenkins Workspace: 1. Workspace Location: o By default, the workspace is located in the Jenkins home directory, typically at /var/lib/jenkins/workspace/ on Linux systems or C:\Program Files (x86) \Jenkins\workspace\ on Windows. 2. Workspace Structure: ○ Each job gets its own subdirectory within the workspace. For example, a job named MyApp-Build will have its workspace at /var/lib/jenkins/workspace/MyApp - Build/. ○ The workspace contains: Source Code: The code pulled from the repository. . Build Artifacts: Files generated during the build process. I Logs: Logs related to the build process. 3. Workspace Usage: ○ Building: During the build process, Jenkins checks out the code into the workspace, executes build steps, and generates artifacts. ○ Archiving Artifacts: After the build, files specified in the post-build actions are archived from the workspace. ○ Cleaning Up: Jenkins may clean up workspaces based on job configurations or policies to save disk space. 4. Customizing Workspace: ○ Configure Custom Workspace Location: In the job configuration, under the Advanced Project Options, you can specify a custom workspace directory. ○ Workspace Cleanup: I Use plugins like the Workspace Cleanup Plugin to manage and clean up workspaces automatically. Example of Workspace Structure: · Workspace Directory: /var/lib/jenkins/workspace/MyApp-Build/ o Source Code: /var/lib/jenkins/workspace/MyApp-Build/src/ o Build Artifacts: /var/lib/jenkins/workspace/MyApp-Build/target/ o Logs: /var/lib/jenkins/workspace/MyApp-Build/logs/ This guide should provide a comprehensive understanding of creating a Jenkins build and managing Jenkins workspaces. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY N32 39 nirf 175º Rank

--- Page 145 ---
Course Code/Title: CS3V15/Devops Unit: V Step 7:Note the types of plans offered by GitHub. There are a few different plans to choose from, varying in the amount of features provided. Learn to ship software like a pro. GitHub gives students free access to the best developer tools so they can learn by doing. Free Get additional student benefits GitHub Pro Unlimited public/private repositories 2,000 CI/CD minutes/month Free for public repositories 500MB of Packages storage Free for public repositories Protect your branches Ensure that collaborators on your repository cannot make irrevocable chang branches. O Draft pull requests O 120 core-hours of Codespaces compute O Pages and Wikis 15GB of Codespaces storage O Community support ® 3,000 CI/CD minutes/month Free for public repositories 2GB of Packages storage Free for public repositories O 180 core-hours of Codespaces compute 20GB of Codespaces storage Web-based support wikiHow Step 8: Select the free plan. On the plan selection page, scrolldown to click the button for choosing a free plan. This will immediately take you to your GitHub dashboard. Continue for free wiki How UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 5 N33 In 175º Rank ---

--- Page 146 ---
Course Code/Title: CS3V15/Devops Unit: V · If you choose a paid plan, you'll have to enter yourpayment information as requested before you can continue. . If you want to upgrade your Github account in the future, click the menu at the top- right corner, select Settings, andchoose Billing and plans to view your options. GitHub essentials are: · Repositories · Branches · Commits · Pull Requests · Git (the version control software GitHub is built on) Repository: A GitHub repository can be used to store a developmentproject. It can contain folders and any type of files (HTML, CSS,Javascript, Documents, Data, Images). A GitHub repository should also include a licence file and a README file about the project. A GitHub repository can also be used to store ideas, or any resources that you want to share. Github Repository Creation Step 1: Click on the new repository option Marketplace Explore New repository and GitHub without any code! Catch Universe Join a community Universe in San Fr ticket pricing-he Import repository × New gist bird New organization New project llo World guide, you'll create a repository, start a , write comments, and open a pull request. GitHub Sponsors Matching Fund × Ready to support open source? GitHub will match your contribution to developers during their first year in GitHub Sponsors. d the guide Start a project × Welcome to the new dashboard. Get closer to the stuff you care about most. Explore repositories atlas-engineer/next Next browser - Be productive. UnENVIVAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 175° Rank 6

--- Page 125 ---
Course Code/Title:CS3V15/Devops Unit: IV This command executes the specified playbook while prompting for the necessary privilege escalation password. Output and Result: Activities Terminal - Nov 2 13:00 . auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook-eg2.yaml PLAY [Install Nginx] TASK [Gathering Facts] Total: [vnet]: FAILEDT .. ['msg": "Missing sudo password') PLAY RECAP : ok=0 changed=0 unreachable=0 Fatled =! skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory. yaml playbook-eg2.yaml - - ask-becone-pass BECOME password: PLAY [Install Nginx] TASK [Gathering Facts] ok: [vri01] TASK [Install Nginx using apt] ok: [vri01] PLAY RECAP * VMD1 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ [] As shown in the example image above, the first command encountered an error due to the absence of the -ask-become-pass flag. This flag is essential when your task requires elevated privileges, as it prompts Ansible to request the sudo password for authentication. The corrected command includes the -ask-become-pass flag, ensuring that the necessary privileges are obtained before executing the task. This is particularly important when working with tasks that require administrative access, such as package installations or system configurations. Note: In this blog, we primarily focus on the core and fundamental components of Ansible, providing an introductory overview and understanding of its key concepts. While Ansible offers advanced features such as "Collections," "Ansible Galaxy," "Roles," "Dynamic Inventory," and "Custom Modules and Plugins," we recognize that these topics are extensive and may require separate dedicated discussions. For the purpose of this blog, we aim to establish a strong foundation of Ansible's core components. If you're interested in diving deeper into these advanced topics, we recommend exploring Ansible's official documentation and additional resources dedicated to each subject. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 11 N32 nirf 1750 Rank

--- Page 126 ---
Course Code/Title:CS3V15/Devops Unit:IV Understanding YAML Ansible uses YAML syntax for expressing Ansible playbooks. This chapter provides an overview of YAML. Ansible uses YAML because it is very easy for humans to understand, read and write when compared to other data formats like XML and JSON. Every YAML file optionally starts with " --- " and ends with " ... ". In this section, we will learn the different ways in which the YAML data is represented. key-value pair YAML uses simple key-value pair to represent the data. The dictionary is represented in key: value pair. Note - There should be space between : and value. Example: A student record --- #Optional YAML start syntax james: name: james john rollNo: 34 div: B sex: male ... #Optional YAML end syntax Abbreviation You can also use abbreviation to represent dictionaries. Example James: {name: james john, rollNo: 34, div: B, sex: male} Representing List We can also represent List in YAML. Every element(member) of list should be written in a new line with same indentation starting with "- " (- and space). Example countries: - America - China - Canada - Iceland CHENNAI INSTITUTE OF TECHNOLOGY CHENNAI INSTITUTE . TECHNOLOGY (Autonomous) 12 N33 nirf 1750 Rank

--- Page 139 ---
Course Code/Title:CS3V15/Devops Unit: IV Create the file, if it does NOT exist (or) absent $ ansible testservers -a "touch /tmp/testfile creates=/tmp/testfile" -i ansible_hosts As Playbook - name: "Validate if a file is present or not present using Ansible Command module" hosts: testservers tasks: - name: "Create a file if it does not exist" command: "touch /tmp/latestfile" args: creates: "/tmp/latestfile" register: createif - name: "Display the file to make sure its created" command: "ls -lrt /tmp/latestfile" register: displayif when: createif is changed - debug: var=displayif.stdout - name: "Remove the file if it exist" command: "rm -rf /tmp/latestfile" args: removes: "/tmp/latestfile" register: removeif In the playbook we execute three tasks, • First one is a create a file if it does not exist • Second to display if the creation is successful • Third one is to delete the file if it exists Example 5: Execute or Run the Script when a file exists or not exists Now for this example let us take something relatable to real world scenario. Like Start the Server instance (or) program if the PID file or LOCK file does not exist Here is the sample playbook for you to get started. I am giving a general playbook here as I do not want to pick some server/technology of my own which you cannot relate to. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 140 ---
Course Code/Title:CS3V15/Devops Unit:IV you can feel free to modify the script to your needs as this is just a template. - name: Start of Stop Server instance based on PID/LOCK file availability hosts: appservers tasks: - name: Start the instance when the PID file is not present become: yes become_user: appuser command: "startserver.sh" args: creates: "/path/to/pid/instance.pid" register: startinst - name: Stop the instance when the lock file is present become: yes become_user: appuser command: "stopserver.sh" args: removes: "/path/to/lockfile/instance.lck" register: stopinst CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 26 N33 nirf 1750 Rank

--- Page 163 ---
‹ FabrikamFiber Runs Branches Analytics Description Edit Run pipeline : Stages #20210609.3 Update azure-pipelines.yml for Azure Pipelines Manually triggered for main 9 61bba8c x2 La Wednesday Ø 215 #20210609.2 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for 1º main 9 bc6c105 x La Wednesday ₾ 305 #20210609.1 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for main 9 53eb92b x • #20210520.1 Updated README.md Individual CI for & new-branch 0 1ada387 ‹ azure-cli-example go main V azure-cli-example / azure-pipelines.yml 30 pool: default 31 32 steps: 33 # Specify python version and install if needed Settings 34 - task: UsePythonVersion@0 35 condition: false 36 inputs: 37 -versionSpec: '3.x' 38 architecture: 'x64' 39 40 # Update pip to latest 41 - bash: python -m pip install -- upgrade pip 42 condition: false 43 displayName: 'Upgrade pip' 44 45 container 46 &continueOnError 47 48 88988 name & parameters & resources & schedules 49 50 51 services 52 strategy 53 & variables 54 & workspace La Wednesday .455 Là May 20 ₾ 135 Variables Run : Tasks 7 Search tasks dotnet .NET Core Build, test, package, or publish a dotnet applicatio. Android signing Sign and align Android APK files Ant Build with Apache Ant 80 App Center distribute Distribute app builds to testers and users via Visu .. App Center test Test app packages with Visual Studio App Center Archive files Compress files into .7z, tar.gz on zip ARM template deployment Deploy an Azure Resource Manager (ARM) templ .. Azure App Service deploy Deploy to Azure App Service a web, mobile, or AP. · The YAML pipeline editor provides several keyboard shortcuts, which we show in the following examples. . Choose Ctrl+Space for Intellisense support while you're editing the YAML pipeline.

--- Page 164 ---
· The task assistant provides a method for adding tasksto your YAML pipeline. · To display the task assistant, edit your YAML pipeline and choose Show assistant. Understand the azure-pipelines.yml file · A pipeline is defined using a YAML file in your repo. Usually, this file is named azure-pipelines.yml and is located at the root of your repo. · Navigate to the Pipelines page in Azure Pipelines, select the pipeline you created, and choose Edit in the context menu of the pipeline to open the YAMLeditor for the pipeline. · This pipeline runs whenever your team pushes a change to the main branch of your repo or creates a pull request. It runs on a Microsoft-hosted Linux machine. · The pipeline process has a single step, which is to run the Maven task. YAML:Code trigger: - main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdk Version: "1.10" jdk11_windows: imageName: "windows-latest" jdk Version: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdk VersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: " ** /TEST -*. xml"goals: "package" Change the platform to build · Navigate to the editor for your pipeline by selectingEdit pipeline action on the build, or by selecting Edit from the pipeline's main page. · To choose a different platform like Windows or Mac, change thevmImage value: · pool: · vmImage: "windows-latest" Add steps . You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can usetasks for building, testing, publishing, or deploying your app. For Java, the

--- Page 129 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible ad-hoc Commands The Ansible ad-hoc command uses the /usr/bin/ansible command-line tool to automate a single task on one or more managed nodes. The Ad-hoc commands are quick and easy, but they are not re-usable. The Ad-hoc commands demonstrate the simplicity and power of Ansible. Syntax ansible <hosts> [-m <module_name>] -a <"arguments"> -u <username> [ -- become] Explanation Hosts: It can be an entry in the inventory file. For specifying all hosts in the inventory, use all or "*" . module_name: It is an optional parameter. There are hundreds of modules available in the Ansible, such as shell, yum, apt, file, and copy. By default, it is the command. Arguments: We should pass values that are required by the module. It can change according to the module used. Username: It specifies the user account in which Ansible can execute commands. Become: It's an optional parameter specified when we want to run operations that need sudo privilege. By default, it becomes false. 1. Parallelism and shell commands You can reboot your company server in 12 parallel forks at the same time. For this, you need to set up the SSHagent for connection. $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa To run reboot for all your company servers in the group, 'abc', in 12 parallel forks: $ ansible abc -a "/sbin/reboot" -f 12 By default, Ansible will run the above ad-hoc commands from the current user account. If you want to change then pass the username in ad-hoc command as follows: $ ansible abc -a "/sbin/reboot" -f 12 -u username 2. File Transfer You can use ad-hoc commands for doing SCP (secure copy protocol) which means lots of files in parallel on multiple machines or servers. CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 15 N32 nirf 175º Rank

--- Page 130 ---
Course Code/Title:CS3V15/Devops Unit:IV Transferring file on many machines or servers $ ansible abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf" Creating new directory $ ansible abc -m file - a "dest = /path/user1/new mode = 888 owner = user group = user1 state = directory" Deleting all directory and files $ ansible abc -m file -a "dest = /path/user1/new state = absent" 3. Managing Packages Ad-hoc commands are available for apt and yum module. Here are the following ad-hoc commands using yum. Below command checks, if the yum package is installed or not, but not update it. $ ansible abc -m yum -a "name = demo-tomcat-1 state = present" Below command checks the package is not installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = absent" And below command checks the latest version of package is installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = latest" 4. Managing Users and Groups You can manage, create, and remove a user account on your managed nodes with ad-hoc commands. $ ansible all -m user -a "name=foo password =< crypted password here>" $ ansible all -m user -a "name=foo state=absent" 5. Managing Services Ensure a service is started on all the webservers. $ ansible webservers -m service -a "name=httpd state=started" Alternatively, restart a service on all webservers: $ ansible webservers -m service -a "name=httpd state=restarted" CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 nirf 1750 Rank

--- Page 131 ---
Course Code/Title:CS3V15/Devops Unit: IV Ensure a service is stopped: $ ansible webservers -m service -a "name=httpd state=stopped" 6. Gathering Facts Fact represents the discovered variables about a system. You can use the facts to implement conditional execution of tasks, and also used to get ad-hoc information about your systems. To see all the facts: $ ansible all -m setup Ansible Playbooks Playbooks are the files where the Ansible code is written. Playbooks are written in YAML format. YAML means "Yet Another Markup Language," so there is not much syntax needed. Playbooks are one of the core features of Ansible and tell Ansible what to execute, and it is used in complex scenarios. They offer increased flexibility. Playbooks contain the steps which the user wants to execute on a particular machine. And playbooks are run sequentially. Playbooks are the building blocks for all the use cases of Ansible. Ansible playbooks tend to be more configuration language than a programming language. Through a playbook, you can designate specific roles to some of the hosts and other roles to other hosts. By doing this, you can orchestrate multiple servers in very different scenarios, all in one playbook. Playbook Structure Each playbook is a collection of one or more plays. Playbooks are structured by using Plays. There can be more than one play inside a playbook. Playbook Play Task Module Target Environment O Task Module Play Task Target Environment Play Module O Task CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY 17 N33 nirf 1750 Rank

--- Page 132 ---
Course Code/Title:CS3V15/Devops Unit:IV name: install and configure DB hosts: testServer become: yes vars: oracle_db_port_value : 1521 tasks: -name: Install the Oracle DB yum: < code to install the DB> -name: Ensure the installed service is enabled and running service: name: < your service name> The Different YAML Tags name This tag specifies the name of the Ansible playbook. As in what this playbook will be doing. Any logical name can be given to the playbook. hosts This tag specifies the lists of hosts or host group against which we want to run the task. The hosts field/tag is mandatory. It tells Ansible on which hosts to run the listed tasks. The tasks can be run on the same machine or on a remote machine. One can run the tasks on multiple machines and hence hosts tag can have a group of hosts' entry as well. vars Vars tag lets you define the variables which you can use in your playbook. Usage is similar to variables in any programming language. tasks All playbooks should contain tasks or a list of tasks to be executed. Tasks are a list of actions one needs to perform. A tasks field contains the name of the task. This works as the help text for the user. It is not mandatory but proves useful in debugging the playbook. Each task internally links to a piece of code called a module. A module that should be executed, and arguments that are required for the module you want to execute. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 18 N33 nirf 1750 Rank

--- Page 109 ---
Course Code/Title:CS3V15/Devops Unit: III Note that the git repository and the files from that repository are copied from the workspace of App1 to App2. Also note that test.class and test.java files are copied to the workspace of App2. Started by user Anuradha R Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\. jenkins\workspace\App2 Copied 16 artifacts from "Appl" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621189248227895.bat C:\ProgramData\Jenkins\.jenkins\workspace\App2>dir Volume in drive C is OS Volume Serial Number is C6C2-8A9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 .git 474 test. class 144 test.java 2 File(s) 618 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ Finished: SUCCESS 5. Extended Choice Parameter Overview: The Extended Choice Parameter Plugin provides advanced parameter types for Jenkins jobs, such as multi-select lists, checkboxes, and more complex formats. Key Features: · Parameter Types: Includes multi-select, checkboxes, and other advanced input options. . Dynamic Choices: Allows dynamic generation of choices from scripts or external sources. Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Extended Choice Parameter Plugin," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add an Extended Choice Parameter. ○ Configure the parameter type, choices, and other settings. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 110 ---
Unit: III Course Code/Title:CS3V15/Devops Usage: · Useful for scenarios where complex user input is needed or where multiple selection options are required. Configuring Jenkins to Work with Java, Git, and Maven 1. Configuring Jenkins to Work with Java Overview: Jenkins requires Java to run. Configuring Java in Jenkins involves specifying the Java Development Kit (JDK) installations Jenkins should use. Steps: 1. Install Java Development Kit (JDK): ○ Ensure that JDK is installed on your system. You can download it from the Oracle website or use OpenJDK. 2. Configure JDK in Jenkins: ○ Open Jenkins and go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to JDK section and click Add JDK. ○ Enter a name for the JDK installation (e.g., JDK 11). Check Install automatically to let Jenkins download and install the JDK, or specify the path to an existing JDK installation. ○ If specifying the path manually, provide the JAVA_HOME directory. 3. Example Configuration: ○ Name: JDK 11 ○ JAVA_HOME: /usr/lib/jvm/java-11-openjdk 4. Verify JDK Configuration: You can verify the JDK configuration by creating a simple Jenkins job and adding a build step that prints the Java version using: bash Copy code java -version 2. Configuring Jenkins to Work with Git Overview: The Git Plugin integrates Jenkins with Git repositories, enabling Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 35 1750 Rank

--- Page 123 ---
Course Code/Title:CS3V15/Devops Unit: IV Ad-Hoc Limitations: While ad-hoc commands are ideal for single, immediate tasks, they are not designed for complex, multi- step automation. Playbooks come to the rescue when tasks have interdependencies and need to be executed in a coordinated manner. Readability and Reusability: Playbooks are authored in YAML, a human-readable and straightforward format. This not only makes them easy to write but also facilitates sharing and collaboration. You can reuse playbooks across various scenarios, saving time and effort. Conditional and Looping Logic: Playbooks offer advanced features, including conditional statements and looping, that allow you to adapt automation to different situations. This flexibility makes playbooks versatile and capable of handling a wide range of automation needs. In essence, playbooks are your tool of choice when automation tasks become multi-faceted and require a structured and logical approach. They provide the power to streamline and automate complex workflows with precision and efficiency. Example: Ping All Inventory Devices To illustrate the use of a playbook, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: playbook.yaml 1 --- 2 -name : My First Play #Name of play 3 hosts: virtualmachines #defining host can be all 4 tasks: 5 -name:Ping My Hosts #Name of task 6 ansible.builtin.ping: # can also write ping To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 124 ---
Course Code/Title:CS3V15/Devops Unit: IV Output and Result: Activities Terminal - Nov 2 12:29 . O auriga@auriga-Latitude-E7470: - /Desktop/ansible X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook.eg1.yaml PLAY [My First Play] TASK [Gathering Facts] ok: [ vri01] TASK [Ping Devices] ok: [vm01] PLAY RECAP vn01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$[] rescued=0 ignored=0 X As you can see there is gathering facts task which we didn't create is running it is the default task which ping the connection. Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: playbook.yaml 1 --- 2- name: Install Nginx 3 hosts: all 4 become: yes 5 tasks: 6 - name: Install Nginx using apt 7 apt: 8 name: nginx 9 state: present To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml -- ask-become-pass CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 10 N33 nirf 1750 Rank

--- Page 135 ---
Course Code/Title:CS3V15/Devops Unit: IV Syntax for writing the module on command line While writing the module we use the options such as -m and -a. The -m is for module and -a is for argument, after the -a option we specify the executable command in double quotes. here 'all' stands for all the remote hosts, otherwise we can mention the group name of the hosts 1 $ ansible all -m command -a "echo 'Hello hosts'" The -m option followed by module name 'command' Command to be executed on remote hosts, given in double quotes Commonly used modules 1. Command : Following is a command module that executes a command on remote hosts $ ansible all -m command -a "echo 'Hello hosts'" 2. User : The user module is used to create, modify or delete users. For example - To create a user named 'new_user' on all the hosts in the 'dev' group using ansible, you can use the user module. Here's the ansible command to do that : $ ansible dev -m user -a "name=new_user state=present" Sometimes the above command fails as there is no privilege to create user on remote host. Then we can issue the command as follows $ ansible all -m user -a "name=new_user state=present" -- become -- ask-become-pass The above command asks for the password for the remote host to login. On supplying the password the user will be created. We can verify. if new user is getting created or not on remote host by issuing following command on each remote host $ less /etc/passwd If we want to create new user without asking for password on all the remote hosts then we can issue the command as - $ ansible all -m user -a "name=new_user1 state=present" -- become -b The -b option is used to become superuser. By this the sudo or su privileges can be granted. .3. Ping : The ping module is used to connectivity to remote hosts $ ansible all -m ping 4. Copy : The copy module is used to copy files from control machine to remote hosts $ ansible dev -m copy -a "src=/home/ansible_user/myfile.txt dest=/home/ansible_user/myfile.txt" The above command will copy the myfile.txt file from control node to all the managed nodes of dev group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 21 N32 nirf 1750 Rank

--- Page 136 ---
Course Code/Title:CS3V15/Devops Unit: IV 5. Package : The package module is used to install, remove or upgrade the packages, For example, the following command installs the nginx web server package : $ ansible package -a "name=nginx state=present" Here name indicates the name of the package to be installed, the state=present means install the package. Normally "present" is for installation, "remove" is for removing the package and "latest" is used to upgrade the package. 6. Shell : The shell module is used to run shell commands with more flexibility on remote hosts. For example - Following shell command executes on remote hosts. $ ansible dev -m shell -a "cmd=is" 7. Service : The service module is used to start, stop or restart a specific service on remote hosts. For example - Following service will start the apache web service on all the remote hosts. $ ansible dev -m service -a "name=apache2 state=started" Note that the state can started for starting the service, stopped for stopping the service. o started/stopped are idempotent actions that will not run commands unless necessary. o restarted will always bounce the service. o reloaded will always reload. I Example 1: Get the Uptime of remote servers We have used command module to run the uptime command and we have given both the ad hoc and the playbook form of execution. as AD-HOC Command $ ansible testservers -m command -a uptime -i ansible_hosts as Playbook - name: Check the remote host uptime hosts: testservers tasks: - name: Execute the Uptime command over Command module register: uptimeoutput command: "uptime" - debug: var: uptimeoutput.stdout_lines CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 22 N32 nirf 1750 Rank

--- Page 69 ---
Unit: II Course Code/Title:CS3V15/Devops 1 build.gradle gradle wrapper 2 gradle-wrapper.jar - gradle-wrapper.properties 3 gradlew 4 gradlew.bat 5 settings.gradle 6 It is the default structure of a Gradle project. Gradle will generate the following things for us: 1. The gradle file is build script for configuring the current project. 2. An executable JAR file is used as a Gradle wrapper. 3. Configuration properties for Gradle Wrapper. 4. The gradlew is a Gradle wrapper script for UNIX based OS. 5. The bat is the Gradle Wrapper script for Windows. 6. The settings script for configuring the Gradle build. Step3: Create a task Gradle supports APIs for creating and managing tasks through a Groovy-based DSL or Kotlin-based DSL. Every project contains a collection of tasks for some basic operation. Gradle supports a library of tasks that configure the project. For example, there is a Copy task, which copies files from one location to another. The Copy task is one of the most used tasks In Gradle. To use the Copy task in build script, follow the below process. Step1: Create a directory called src C: \Users \HiMaNshU\demo>mkdir src C: \Users \HiMaNshU\demo>cd src Step2: Add a file called myfile.txt in the src directory. Add the single line "Hello, World!" to it, also, we can leave it empty. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 28 N32 nirf 1750 Rank

--- Page 70 ---
Unit: II Course Code/Title:CS3V15/Devops C:\Users \HiMaNshU\demo\src>echo myfile.txt myfile.txt Define a task called Copy in build.gradle file. It will copy the src directory to a new directory called dest. We don't have to create the dest directory; the Copy task will do it for us. 1. task copy(type: Copy, group: "Custom", description: "The sources are copied to dest directory") { 2. from "src" 3. into "dest" 4. } We can provide anything in a group and description. Also, we can omit them, but doing so will also be omitted from the report of the task used later. Now execute our new copy task: C: \Users \HiMaNshU\demo>gradle copy BUILD SUCCESSFUL in 2s 1 actionable task: 1 executed C: \Users \HiMaNshU\demo>_ After the successful execution of the task, we will get BUILD SUCCESSFUL message. The build.gradle file The build.gradle file is build script of a Gradle project. All the tasks and plugins are defined in this file. When we run a gradle command, it looks for a file called build.gradle in the current directory. Although we have called it a build script, strictly, it is a build configuration script. The build script defines a project and its tasks. The default build.gradle file looks like as follows: CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 29 N33 nirf 1750 Rank

--- Page 133 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible Roles Roles provide a framework for fully independent or interdependent collections of files, tasks, templates, variables, and modules. The role is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks and makes them easier to reuse. The breaking of the playbook allows you to break the playbook into reusable components. Each role is limited to a particular functionality or desired output, with all the necessary steps to provide that result either within the same role itself or in other roles listed as dependencies. Roles are not playbooks. Roles are small functionality that can be used within the playbooks independently. Roles have no specific setting for which hosts the role will apply. Top-level playbooks are the bridge holding the hosts from your inventory file to roles that should be applied to those hosts. Creating a Role The directory structure for roles is essential to creating a new role, such as: Role Structure The roles have a structured layout on the file system. You can change the default structured of the roles as well. For example, let us stick to the default structure of the roles. Each role is a directory tree in itself. So the role name is the directory name within the /roles directory. $ ansible-galaxy -h Usage ansible-galaxy[delete|import|info|init|install|list|login|remove|search|setup][ -- help] [options] ... Options o -h: (help) it shows this help message and exit. o -v: (verbose) Verbose mode (-vvv for more, -vvvv to enable connection debugging). o -- version: it shows program version number and exit. Roles are stored in separate directories and have a particular directory structure CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 19 N33 nirf 1750 Rank

--- Page 134 ---
Course Code/Title:CS3V15/Devops Unit: IV Typical structure of ansible role myweb defaults L main.yml files · handlers L main.yml meta. L main.yml README.md tasks L_ main.yml templates tests inventory test.yml vars L main.yml · defaults : It stores the default variable for the role. For example the default port number is http is 8080, then it can be stored in defaults. · files : This folder contains the files required to transfer or deploy to the target machines. · handlers : It contains handlers, which may be used by some role. · meta : It defines some data or information about the role. · tasks : It contains the main list of tasks to be executed by the role. These tasks might be defined in separate files as per the functionalities. · templates : It contains the templates which can be deployed via a role. Ansible Module Ansible Command module is used to execute commands on a remote node. The Command module, is used mostly to run simple Linux commands on a remote node/server which is part of a host group or Stand alone server mentioned in the host group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 67 ---
Course Code/Title:CS3V15/Devops Unit: II Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013 JVM: 1.7.0_60 (Oracle Corporation 24.60-b09) OS: Windows 8.1 6.3 amd64 Understanding build using Gradle: The Gradle build is a process of creating a Gradle project. When we run a gradle command, it will look for a file called build.gradle in the current directory. This file is also called the Gradle build script. The build configuration, tasks, and plugins are described in this file. The build script describes a project and its tasks. Let's create a small Gradle project, run some of the basic Gradle commands, and understand how Gradle manages the project. Follow the steps below to create and test a Gradle project. Step1: Open the command line and create a directory First, Open the command line and create a directory for the project and change directory to it. Let's create a demo directory. C: \Users \HiMaNshU>mkdir demo C: \Users \HiMaNshU>cd demo Step2: Initialize a Gradle project To generate a Gradle project, run the gradle init command. It will generate a simple project. With this project, we will explore and understand everything that is generated. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 26 N33 nirf 1750 Rank

--- Page 68 ---
Course Code/Title:CS3V15/Devops Unit: II When we run the gradle init command, it will ask for some basic requirements. First, it will ask the type of project that we want to create. It will give four options: 1. 1: basic 2. 2: application 3. 3: library 4. 4: Gradle plugin Select our requirements. Hence, we are just making a demo project so that we will select the basic option. To select basic option, press 1 and Enter key. Consider the below output: C: \Users \HiMaNshU\demo>gradle init Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could ed, use -- status for details Select type of project to generate: 1: basic 2: application 3: library 4: Gradle plugin Enter selection (default: basic) [1 .. 4] 1 Next, it will ask for DSL. There are two options that are available for DSL: 1. 1: Groovy 2. 2: Kotlin Groovy is the default DSL provided by Gradle. Select build script DSL. Select build script DSL: 1: Groovy 2: Kotlin Enter selection (default: Groovy) [1 .. 2] 1 Next, it will ask for the project name. Type the project name and press Enter key. It will take a while to build a project. After the successful execution of the project, we will get a message BUILD SUCCESSFUL. Project name (default: demo): First_Gradle > Task : init Get more help with your project: https: //guides. gradle.org/creating- uilds BUILD SUCCESSFUL in 2m 56s 2 actionable tasks: 2 executed C: \Users\HiMaNshU\demo>mkdir src Now we have successfully created a Gradle project. Now, what will happen to our specified directory? Consider the below structure of the Gradle project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 27 N33 narf 1750 Rank

--- Page 165 ---
Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. Customize CI triggers Pipeline triggers cause a pipeline to run. You can use trigger: to cause a pipeline to run whenever you push an update to a branch. YAML pipelines are configured by default with a CI trigger on your default branch (which is usually main). You can set up triggers for specific branches or for pull request validation. For a pull request validation trigger, just replace the trigger: step with pr: as shown in the two examples below. By default, the pipeline runs for each pull request change. . If you'd like to set up triggers, add either of the following snippets at the beginning of your azure-pipelines.yml file. YAML I Copy trigger: - main - releases/* YAML Copy pr: - main - releases/* You can specify the full name of the branch (for example, main) or a prefix-matching · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. Build using multiple versions Add steps You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can use tasks for building, testing, publishing, or deploying your app. For Java, the Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. YAML Copy - task: PublishCodeCoverageResults@1 inputs: codeCoverageTool: "JaCoco" summaryFileLocation: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco/ reportDirectory: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco" failIfCoverageEmpty: true · Select Save and then confirm the changes. . You can view your test and code coverage results by selecting your build and going to the Test and Coverage tabs.

--- Page 166 ---
Build using multiple versions Rename/move pipeline Name ScheduledTriggerTest 1234 Select folder 1 .. . Cancel Save Pipeline settings X Processing of new run requests Enabled Paused Disabled YAML file path azure-pipelines.yml Automatically link work items included in this run Cancel Save YAML trigger: main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdkVersion: "1.10" jdk11_windows: imageName: "windows-latest" jdkversion: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdkVersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: **** /TEST -*. xml" goals: "package"

--- Page 147 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: After clicking new repository option, we will have to initialize some things like, naming our project, choosing the visibility etc. After performing these steps click Create Repository button. Create a new repository A repository contains all project files, including the revision history. Already have a project repository elsewhere? Import a repository. Owner Repository name * / Namanbhatia7 - Resume This is going to be name of our project Great repository names are short and memorable. Need inspiration? How about animated-memory? Description (optional) Public Anyone Keep this as public selected ository. You choose who can commit. see this rer Private You choose who can see and commit to this repository. We can add a project description if we want. Skip this step if you're importing an existing repository. Initialize this repository with a README This will let you immediately clone the repository to your comp Tick the README option Add .gitignore: None - Add a license: None ~ Create repository After performing above steps, Click this button Step 3: After clicking the button, we will be directed to below page. Right now the only file we have is a readme file. Namanbhatia7 / Resume o Unwatch - 1 * Star 0 Y Fork 0 <> Code Issues 0 (*) Pull requests 0 ||| Projects 0 BB Wiki Security the Insights # Settings No description, website, or topics provided. Edit Manage topics T 1 commit įº 1 branch > 0 releases 2 1 contributor Branch: master New pull request Create new file Upload files Find File Clone or download TT Namanbhatia7 Initial commit Latest commit 676ac98 now README.md Initial commit now DO README.md Resume UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 7 175" Rank

--- Page 148 ---
Course Code/Title: CS3V15/Devops Unit: V Step 4: Now click on the "Upload files" button. 4 > C https://github.com/avicnotes/html-css-projects ... Search or jump to ... Click to End Session Total: $80.83 Pull requests Issues Marketplace Explore Google" Custom Search @ avicnotes / html-css-projects Unwatch - 1 Star 0 Fork 0 <> Code Issues 1'] Pull requests Actions Projects Wiki O Security ~ Insights ... go main - ¿º 1 branch 0 tags Go to file Add file - { Code About avicnotes screenshots Create new file Upload files 2 24 commits No description, website, or topics provided. README.md Update README.md 2 days ago Readme Screenshot (10).png screenshots 2 days ago Screenshot (11).png screenshots 2 days ago Releases Screenshot (12).png screenshots 2 days ago No releases published Create a new release 19 Screenshot (6).png screenshots 2 days ago accordion.JPG Add files via upload 4 days ago Packages background-dots.png Add files via upload 2 days ago clone-project-1.html Updated navbar items to my original ones 2 days ago index.html Create index.html 's ago No packages published Publish your first package Activate Windows Go to Settings to activate Windows, Environments 1 O Type here to search O W. 11:32 AM 2/1/2021 Step 5:Follow the steps mentioned and click "commitchanges". Drag additional files here to add them to your repository Or choose your files Display_Picture.png Simply drag or add files to be uploaded × My_Resume.html X Styles.css × Commit changes Add initial resume website files Add an optional ended des Give this particular version a name -o- Commit directly to the master branch. Ensure this option is checked !] Create a new branch for this commit and start a pull request. Learn more about pull requests. Commit changes Cancel UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 8

--- Page 99 ---
Unit: III Course Code/Title:CS3V15/Devops 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "HTML Publisher Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Post-build Actions section. ○ Select Publish HTML reports. ○ Specify the directory containing HTML reports and the index page. Usage: . Ideal for displaying build reports such as test results or code coverage metrics in a user-friendly HTML format. DEMO: Step 1 : We will create a Maven project using Selenium and TestNG framework. Open Eclipse : IDE and Create a Maven Project. Select the Create Simple Project (Skip archetype selection) and enter the Group Id and Artifact Id as per your choice. My project name is SeleniumDemo. Step 2 : Add the Selenium dependency in your pom.xml file as follows - <dependency> <groupId>org.seleniumhq.selenium</groupId> <artifactId> selenium-java</artifactId> <version>3.141.59</version> </dependency > Save the pom.xml file. Step 3 : Now install testNG plugin. For that, from Menu select Help-> Eclipse Marketplace Help Welcome 7 Help Contents Search Show Context Help Show Active Keybindings ... Ctrl+Shift+L Tip of the Day Tips and Tricks ... Cheat Sheets ... @ Eclipse User Storage Open the Eclipse Marketpla ** Perform Setup Tasks ... % Check for Updates Install New Software ... Eclipse Marketplace ... = About Eclipse IDE · Donate Contribute Type TestNG in the search window. The testNG installation option will be available. click Install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 24 N32 nirf 1750 Rank ---

--- Page 100 ---
Course Code/Title:CS3V15/Devops Unit: III C Eclipse Meketplace × Eclipse Marketplace Select solutions to initiit, Prese leuralt Now to proceed with lutallation. Press the "more info" link to learn more about a solution Search Recent Popular Favorites itwled - Research at the Eclipse × All Categories Go TestNG for Eclipse . This plug-in late you run your TextNG wits from Erfiore. You can run sulter, groupe er individuel methods. Errors are reported in a pepwane tab that lets you. more info by Chat Bent Apache 20 tantog zielt testingunit integration functional selection * 737 ** * stal: 1,83M (25,021 last month) Install MoreUnit 3.3.0 Morelinit is an Solipoe plugin that should mult you in wilting mory unk with. It supports ali programming languages (witching between teds and clanset under. mone.info Install Infinitest 5.4.0.3 Infinitest is a continuont tout runner for tova,'and is valuable to developers valing a Marketplaces (7) Cancel Then confirm the selected features by clicking on Confirm button. x Confirá Selected Features These Confirm to gorthus with the induction. Cs go back te chocie bis sules it CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 25 N3 nirf 1750 Rank

--- Page 95 ---
Course Code/Title:CS3V15/Devops Unit: III C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha C: \ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>exit e Finished: SUCCESS Step 6 : Now go to GitHub repository and make some changes to your test.java file and commit these changes. BE AnurdhaP Changes in the test java Code Blame 6 lines (6 loc) · 158 Bytes 89 Code 55% faster with GitHub Copilot > public class test { 2 public static void main(String args[]) { for(int 1=1;i <= 5;1++) 3 4 System.out.println("Good Morning Path !!! "); 5 3 6 } Step 7 : Now, if you check on the Jenkins page, it automatically executes the above updated test.java file and creates a build. It is as follows .- C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test. java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Good Morning Path !!! Good Morning Pathi !! Good Morning Path !!! Good Morning Path !!! Good Morning Pathill 2. Parameter Plugin Overview: The Parameter Plugin allows Jenkins jobs to accept parameters at build time, enabling dynamic and flexible builds. Key Features: · Parameter Types: Supports various parameter types such as string, choice, boolean, and more. · Default Values: Provides default values for parameters. · Prompt for Parameters: Users are prompted to enter values for parameters when starting a build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 20

--- Page 96 ---
Course Code/Title:CS3V15/Devops Unit: III Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Parameterized Builds," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add parameters of different types (e.g., string, choice) and configure their options and default values. Usage: · Useful for creating jobs that require user input or need to be customized for different build scenarios. A Example Demo Step 1 : Click on New item and create a job. I have created a job by the name Welcome (Refer section 3.5) Step 2 : Click on the existing job and configure it with parameters. Step 3 : Check the option "This project is parameterized". Select the String Parameter. Create some string parameter. I have created a parameter named Username and default value to it. This project is parameterized ? =String Parameter ? Name ? Username Default Value ? Parth Description ? Plain text Preview Trim the string ? Step 4 : Under the Build Steps section, using echo command I tried to display the parameter value CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 21

--- Page 159 ---
Choose a job to see the steps for that job From the < Jobs in run #20191 ... FabrikamFiber Build O Build Build 40s Initialize job 1s 2 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 Duration: 40s 6 Pool: Azure Pipelines Image: Ubuntu-16.04 7 > Job preparation parameters Checkout 3s CmdLine 25 Component Detect 32s Post-job: Checkout <1s Finalize Job <1s Deploy DeployWeb 10s Finalize build Report build status <1s steps view, you can review the status and details of eachstep. From the Moreactions you can toggle timestamps or view a raw log of all steps in the pipeline. Build 9 Pool: Azure Pipelines 2 Image: Ubuntu-22.04 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 View job raw log Toggle timestamps 6 Duration: 40s Job preparation parameters 10

--- Page 160 ---
Cancel and re-run a pipeline If the pipeline is running, you can cancel it by choosing Cancel. If the run has completed, you can re-run the pipeline by choosing Run new. Pipeline run more actions menu: #20191210.3 Update azure-pipelines.yml for Azure Pip ... on FabrikamFiber Cancel Stages Jobs € Build C Deploy 0/1 compléted 48s Not started Build 48s Cancel From the More actions menu you can download logs, add tags, edit the pipeline, delete the run, and configure retention for the run. cure Pipe ... Run new Download logs Add tags Edit pipeline View retention leases Retain Delete 20

--- Page 161 ---
Add a status badge to your repository Many developers like to show that they're keeping their code quality high by displaying a status badge in their repo. Azure Pipelines succeeded To copy the status badge to your clipboard: 1. In Azure Pipelines, go to the Pipelines page to view the list of pipelines.Select the pipeline you created in the previous section. 2. Select , and then select Status badge. 3. Select Status badge. 4. Copy the sample Markdown from the Sample markdown section. Now with the badge Markdown in your clipboard, take the following steps in GitHub: 1. Go to the list of files and select Readme.md. Select the pencil icon to edit. 2. Paste the status badge Markdown at the beginning of the file. 3. Commit the change to the main branch. 4. Notice that the status badge appears in the description of your repository. To configure anonymous access to badges for private projects: 1. Navigate to Project Settings in the bottom left corner of the page 2. Open the Settings tab under Pipelines 3. Toggle the Disable anonymous access to badges slider under General 21

--- Page 162 ---
Azure-Pipeline • Azure Pipelineș provides a YAML pipeline editor that you can use to author and edit your pipelines. • The YAML editor is based on the Monaco Editor. • The editor provides tools like Intellisense support anda task assistant to provide guidance while you edit a pipeline. • You can also edit pipelines by modifying the azure pipelines.yml file directly in your pipeline's repository using a text editor of your choice Edit a YAML pipeline To access the YAML pipeline editor, do the following steps. • Sign in to your organization • (https://dev.azure.com/{yourorganization}). • Select your project, choose Pipelines, and then select the pipeline you want to edit. You can browse pipelines by Recent, All, and Runs. • Choose Edit. • Make edits to your pipeline using Intellisense and the task assistant forguidance. Azure DevOps fabrikam-tailspin FabrikamFiber F FabrikamFiber + Pipelines Overview Recent All Runs Boards Pipelines Recently run pipelines Repos 1 Pipeline Pipelines Pipelines FabrikamFiber 2 3 17

--- Page 153 ---
Create a pipeline in azure What is pipeline Azure Pipelines supports continuous integration (CI)and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline. The latest way to build pipelines is with the YAML pipeline editor. You can also use Classic pipelines with the Classic editor. code release build CI plan continuous testing CD deploy monitor operate Create your first pipeline This is a step-by-step guide to using Azure Pipelines to build a sample application from a Git repository. This guide uses YAML pipelines configured with the YAML pipeline editor. If you'd like to use Classic pipelines instead, see Define your Classic pipeline. For guidance on using TFVC, see Build TFVC repositories. Prerequisites - Azure DevOps A GitHub account where you can create are pository. An Azure DevOps organization. Create one for free. If your team already has one, then make sure you're an administrator of the Azure Devops. 12

--- Page 154 ---
Get the Java sample code To get started, fork the following repository into your GitHub account. https://github.com/MicrosoftDocs/pipelines-java Create your first Java pipeline 1. Sign-in to your Azure DevOps organization and go to your project. 2. Go to Pipelines, and then select New pipeline. 3. Do the steps of the wizard by first selecting GitHub as the location of your source code. 4. You might be redirected to GitHub to sign in. If so, enter your GitHub credentials. 5. When you see the list of repositories, select your repository. 6. You might be redirected to GitHub to install the Azure Pipelines app. If so,select Approve & install. 7. Azure Pipelines will analyze your repository and recommend the Maven pipeline template. 8. When your new pipeline appears, take a look at the YAML to see what it does. When you're ready, select Save and run. 9. You're prompted to commit a new azure pipelines.yml file to your repository. After you're happy with the message, select Save and run again. If you want to watch your pipeline in action, select the build job. You just created and ran a pipeline that we automatically created for you, because your code appeared to be a good match for the Maven template. You now have a working YAML pipeline (azure-pipelines.yml) in your repository that's ready for you to customize! 14

--- Page 121 ---
Course Code/Title:CS3V15/Devops Unit: IV · - m ping specifies the "ping" module, a basic module that checks the reachability of the managed nodes. . - i inventory.yaml points to the inventory file containing the list of devices. · The "ping" module sends a test command to the target devices and reports their status, confirming whether they are responsive. Output and Result: auriga@auriga-Latitude-E7470 :- /Desktop/ansibleŞ ansible all -[ inventory.yaml -m ping VOOS | UNREACHABLET => ( "changed's false. "mg's "Failed to connect to the host via s "unreachable": true auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ton denied (publickey_ password) In the above example the ssh host is not reachable. Activities Terminal Nov 2 12:13 . auriga@auriga-Latitude-E7470: - /Desktop/ansible C auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -i inventory.yaml -m ping vn81 | SUCCESS => 'ansible_facts": { "discovered_interpreter_python": "/usr/bin/python3" "changed": false, "ping": "pong" auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ In the above output image it is working fine Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: 1ansible all -i inventory.yaml -m apt -a "name=nginx state=present" -- become -- ask-become-pass · - m apt specifies the "apt" module, which is responsible for package management. · - a "name=nginx state=present" includes variables for the module to act upon, such as specifying that Nginx should be installed (state=present). · - become signifies that the command should run with elevated privileges (sudo). · - ask-become-pass prompts for the sudo password of the managed node to ensure the installation proceeds smoothly. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 7 N32 nirf 1750 Rank

--- Page 122 ---
Course Code/Title:CS3V15/Devops Unit:IV Output and Result: Activities Terminal * Nov 2 12:24 · auriga@auriga-Latitude-E7470: - /Desktop/ansible F X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -[ inventory.yaml -m apt -a "name=ansible state=present" -- become - - ask-become-pass BECOME password: vm01 | CHANGED => { "ansible_facts": { "discovered_interpreter_python": * /usr/bin/python3" "cache_update_time": 1698985056, "cache_updated": false, "changed": true, "stderr": "" 'stderr_Lines": []. stdout": "Reading package lists ... \nBuilding dependency tree ... \nReading state information ... \nThe following packages were automatically installed and are no longer required:\n gtr1.2-goa-1.0 hplip-data libcgt-fast-perl libcgt-pm-perl\n libdouble-conversion3 Libevent-core-2.1-7 libevent-pthreads-2.1-7\n \ibfcgt-perl libfprint-2-todi libfwupdplugin1 libht ml-template-perl\n Libpcre2-16-0 libqtScoresa libqtsdbuss libqtsguis libqt5network5\n libqtSpositionings libqt5printsupports libqt5qm15 Libqtsquick5\n libqtSsensors5 libqt5svg5 Li bqt5webchannel5 libqt5webkit5 libqt5widgets5\n Libxcb-xinerama@ Libxcb-xinpute Libxmlbi printer-driver-postscript-hp\n python3-renderpm python3-reportlab python3-reportlab-accel\n qt5-gtk-platformthene qttranslations5-lien shim\nUse 'sudo apt autoremove' to remove them. InThe following additional packages will be installed:\n teee-data python3-argcomplete pyt hon3-crypto python3-dnspython\n python3-jinja2 python3 . jmespath python3-kerberos python3-libcloud\n python3-netaddr python3-ntin-auth python3-requests-kerberos\n python3-requests- ntlm python3-selinux python3-winrm python3-xmltodict\nSuggested packages: \n cowsay sshpass python- jinja2-doc ipython3 python-netaddr-docs\nThe following NEW packages will be install ed:\n ansible leee-data python3-argcomplete python3-crypto python3-dnspython\n python3- jinja2 python3-jmespath python3-kerberos python3 . Libcloud\n python3-netaddr python3-ntlm-aut h python3-requests-kerberos\n python3-requests-ntIm python3-selinux python3-winrm python3-xmltodict\n0 upgraded, 16 newly installed, 0 to remove and 50 not upgraded. InNeed to get 97 25 kb of archives. \nAfter this operation, 98.6 MB of additional disk space will be used. \nGet:1 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-jinja2 all 2.10.1-2 [95.5 KB]\nGet:2 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-crypto amd64 2.6.1-13ubuntu2 [237 KB]\nGet:3 http://in.archive.ubuntu.com/ubuntu focal-updates/main amd64 pyt hon3-dnspython all 1.16.0-1ubuntu1 [89.2 KB]\nGet:4 http://in.archive.ubuntu.com/ubuntu focal/main amd64 ieee-data all 20180805.1 [1589 KB]\nGet: 5 http://in.archive. ubuntu.com/ubuntu focal-updates/main amd64 python3-netaddr all 0.7.19-3ubuntu1 [236 KB]\nGet:6 http://in.archive.ubuntu.com/ubuntu focal/universe and64 ansible all 2.9.6+dfsg-1 [5794 KB]\nGet:7 http: //\n.archive.ubuntu.com/ubuntu focal/universe amd64 python3-argcomplete all 1.8.1-1.3ubuntu1 [27.2 kB]\nGet:8 http://in.archive.ubuntu.com/ubuntu focal-updates/main and64 python3- jne spath all 0.9.4-2ubuntu1 [21.5 kB]\nGet:9 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-kerberos amd64 1.1.14-3.1build1 [22.6 k8]\nGet: 10 http://in.archive.ubuntu. con/ubuntu focal/universe amd64 python3-libcloud all 2.8.0-1 [1483 KB]\nGet:11 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-ntlm-auth all 1.1.0-1 [19.6 KB]\nGet: 1 2 http://in.archive.ubuntu.com/ubuntu focal/universe and64 python3-requests-kerberos all 0.12.0-2 [11.9 KB]\nGet: 13 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-r equests-ntlm all 1.1.0-1 [6004 B]\nGet: 14 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-selinux amd64 3.0-1build2 [139 KB]\nGet : 15 http://in.archive.ubuntu.com/ubu ntu focal/universe and64 python3-xmltodict all 0.12.0-1 [12.6 KB]\nGet:16 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-winrm all 0.3.0-2 [21.7 KB]\nFetched 9725 k B in 7s (1454 KB/s)\nSelecting previously unselected package python3-jinja2.\r\n(Reading database ... \r(Reading database ... 5*\r (Reading database .. . 10%\r(Reading database ... 15% \r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database . .. 45%\r (Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 68%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading da tabase ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 212542 files and directories currently installed.)\r\nPreparin g to unpack ... /00-python3-jinja2_2.10.1-2_all.deb ... \r\nUnpacking python3-jinja2 (2.10.1-2) ... \r\nSelecting previously unselected package python3-crypto. \r\nPreparing to unpack .. ./01-python3-crypto_2.6.1-13ubuntu2_amd64.deb ... \r\nUnpacking python3-crypto (2.6. 1-13ubuntu2) ... \r\nSelecting previously unselected package python3-dnspython. \r\nPreparing to unpa ck ... /02-python3-dnspython_1.16.0-lubuntu1_all.deb ... \r\nUnpacking python3-dnspython (1. 16.0-1ubuntu1) ... \r\nSelecting previously unselected package leee-data. \r\nPreparing to unp ack ... /03-ieee-data_20180805.1_all.deb ... \r\nUnpacking teee-data (20188805.1) ... \r\nSelecting previously unselected package python3-netaddr. \r\nPreparing to unpack ... /04-python3. netaddr_0.7.19-3ubuntu1_all.deb ... \r\nUnpacking python3-netaddr (0.7.19-3ubuntu1) ... \r\nSelecting previously unselected package ansible. \r\nPreparing to unpack .. . /05-ansible_2.9.6 +dfsg-1_all.deb ... \r\nUnpacking ansible (2.9.6+dfsg-1) ... \r\nSelecting previously unselected package python3-argcomplete. \r\nPreparing to unpack .. . /06-python3-argcomplete_1.8.1-1. 3ubuntu1_all.deb ... \r\nUnpacking python3-argcomplete (1.8.1-1.3ubuntu1) ... \r\nSelecting previously unselected package python3- jnespath. \r\nPreparing to unpack ... /07-python3- jnespa th_0.9.4-2ubuntul_all.deb ... \r\nUnpacking python3-jmespath (0.9.4-2ubuntu1) ... \r\nSelecting previously unselected package python3-kerberos. \r\nPreparing to unpack ... /08-python3-ke rberos_1.1.14-3.1build1_and64.deb ... \r\nUnpacking python3-kerberos (1.1.14-3.1build1) ... \r\nSelecting previously unselected package python3-libcloud. \r\nPreparing to unpack .. . /09- python3-libcloud_2.8.0-1_all.deb ... \r\nUnpacking python3-libcloud (2.8.0-1) ... \r\nSelecting previously unselected package python3-ntlm-auth. \r\nPreparing to unpack ... /10-python3-n tim-auth_1.1.0-1_all.deb .. . \r\nUnpacking python3-ntlm-auth (1.1.0-1) ... \r\nSelecting previously unselected package python3-requests-kerberos. \r\nPreparing to unpack ... /11-python3. requests-kerberos 0.12.8-2 all. deb .... \r\nUnpacking python3-requests-kerberos (0.12.0-2) ... \r\nSelecting previously unselected package python3-requests-ntln. \r\nPreparing to unpack By using these ad-hoc commands, you can perform quick, task-specific operations on your managed nodes, saving time and effort in your automation tasks. CommonModules: Ansible offers an extensive library of modules for various tasks, from system administration to application deployment. You can explore the full list of Ansible modules in the official documentation to find the most suitable module for your specific requirements. BestPractices: When working with ad-hoc commands, consider using SSH key-based authentication for secure, passwordless access. It streamlines the authentication process and enhances the security of your automation tasks. Using Ansible Playbooks Introduction: In the world of Ansible, playbooks are the cornerstone of structured and complex automation. While ad- hoc commands are perfect for quick and isolated tasks, playbooks step in when you need a well- orchestrated sequence of operations. They allow you to tackle multi-step automation scenarios where tasks depend on each other, and conditional actions are required. Let's delve deeper into the importance of playbooks: The Role of Playbooks: Playbooks serve as the framework for orchestrating automation tasks that involve a sequence of operations. Whether it's configuring servers, deploying applications, or managing infrastructure, playbooks provide a structured way to define how these tasks are executed. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N33 nirf 1750 Rank

--- Page 53 ---
Course Code/Title:CS3V15/Devops Unit: II <execution> <phase>test</phase> <goals> <goal>run</goal> </goals> <configuration> <tasks> <echo>Using env.test.properties</echo> <copy file="src/main/resources/env.test.properties" tofile="${project.build.outputDirectory}/env.properties"/> </tasks> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> Now open the command console, go to the folder containing pom.xml and execute the following mvn command. Pass the profile name as argument using -P option. C:\MVN\project>mvn test -Ptest Maven will start processing and displaying the result of test build profile. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 12 N32 nirf 1750 Rank

--- Page 54 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Maven Settings Open Maven settings.xml file available in %USER_HOME%/.m2 directory where %USER_HOME% represents the user home directory. If settings.xml file is not there, then create a new one. Add test profile as an active profile using active Profiles node as shown below in example. <settings xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"> <mirrors> <mirror> <id>maven.dev.snaponglobal.com</id> <name>Internal Artifactory Maven repository</name> <url>http://repo1.maven.org/maven2/</url> <mirrorOf> *< /mirrorOf> /mirror </mirrors> <activeProfiles> <activeProfile>test</activeProfile> </activeProfiles> </settings> Now open command console, go to the folder containing pom.xml and execute the following mvn command. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 13 N33 nirf 1750 Rank

--- Page 49 ---
Course Code/Title:CS3V15/Devops Unit: II Validate Deploy Compile Install Test Verify Package Integration Test · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. There are always pre and post phases to register goals, which must run prior to, or after a particular phase. When Maven starts building a project, it steps through a defined sequence of phases and executes goals, which are registered with each phase. Maven has the following three standard lifecycles - • default: This is the main lifecycle, as it's responsible for project deployment. CHENNAI CHENNAI INSTITUTE . TECHNOLOGY 8 clean: Handles project cleaning, ensuring that all artifacts generated by previous INSTITUTE OF TECHNOLOGY 175º Rank (Autonomous]

--- Page 50 ---
Course Code/Title:CS3V15/Devops Unit: II builds are removed. · site: Manages the creation of the project's site documentation. Default Lifecycle (default) Most Maven users will be familiar with the default lifecycle. It is a general model of a build process for a software application. The first phase is validate and the last phase is deploy. · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. Clean Lifecycle (clean) The first lifecycle in Maven. Running mvn clean invokes the clean lifecycle which consists of three lifecycle phases: · pre-clean : execute processes needed prior to the actual project cleaning · clean : remove all files generated by the previous build · post-clean : execute processes needed to finalize the project cleaning Site Lifecycle (site) Maven does more than build software artifacts from project, it can also generate project documentation and reports about the project, or a collection of projects. Project documentation and site generation have a dedicated lifecycle which contains four phases: 1. pre-site : execute processes needed prior to the actual project site generation 2. site : generate the project's site documentation 3. post-site : execute processes needed to finalize the site generation, and to prepare for site deployment 4. site-deploy: deploy the generated site documentation to the specified web server CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 13 ---
Course Code/Title:CS3V15/Devops Unit: I 2024. Terraform provides the agility and scalability required to keep up with the dynamic demands of modern applications. Terraform's importance lies in its ability to bring consistency, version control, and automation to infrastructure operations, thereby reducing manual errors, streamlining DevOps workflows, and facilitating applications' rapid and reliable deployment in an increasingly complex and cloud-centric environment. As organizations adopt cloud-native technologies, Terraform remains essential to ensure efficient and consistent infrastructure management. 25. Phantom Phantom enhances security automation and incident response capabilities. In today's rapidly evolving threat landscape, organizations face a constant barrage of cybersecurity incidents, and the ability to respond swiftly and effectively is necessary. It provides a platform for automating security workflows, from detecting and investigating potential threats to orchestrating responses and mitigating risks. Phantom's importance lies in its capacity to reduce response times, increase consistency in incident handling, and free up manual resources from repetitive tasks. With the growing complexity of cyber threats, Phantom empowers security teams to defend against attacks and safeguard critical assets proactively. 26. Nagios Nagios, an open-source monitoring and alerting system, remains vital due to its enduring significance in maintaining the reliability and performance of IT infrastructure and applications. Organizations increasingly rely on complex systems and services. Nagios plays a crucial role by providing real-time monitoring and alerting capabilities, allowing IT teams to detect and address issues before they impact users or cause system outages. Its versatility, extensibility, and support for both on-premises and cloud environments make Nagios a valuable tool for ensuring critical systems' availability, stability, and security, aligning perfectly with the demands of modern IT operations and DevOps practices. 27. Vagrant Vagrant continues to play a crucial role in software development and DevOps. It is a tool that simplifies creating and managing reproducible development environments. Its importance lies in its ability to provide developers and DevOps teams with a consistent and isolated environment for software development, testing, and deployment. With the ever-evolving complexity of software stacks, dependencies, and infrastructure configurations, Vagrant remains essential in ensuring these environments are easily shareable, scalable, and maintainable. It allows developers to work seamlessly across various operating systems and provides a standardized setup that minimizes compatibility issues. 28. Sentry Sentry plays a critical role in modern software development and DevOps practices. With software applications' increasing complexity and scale, identifying and addressing errors and issues has become crucial. Sentry is vital because it provides real-time error tracking and monitoring, allowing development teams to proactively detect and diagnose issues, whether they occur in production or during development. Its importance is minimizing downtime, improving user experience, and maintaining software systems' overall health and reliability. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nirf 1750 Rank

--- Page 14 ---
Course Code/Title:CS3V15/Devops Unit: I 29. Gradle Gradle continues to be a vital tool in software development and DevOps. Gradle is an advanced build automation system that plays a crucial role in managing dependencies, building projects, and orchestrating complex workflows efficiently. Its importance lies in its versatility and scalability, as it caters to various project sizes and types. Gradle's ability to easily handle multi-language, multi-project builds and its support for plugin-based customization make it indispensable in modern software development. As organizations increasingly adopt microservices architectures and cloud-native technologies, Gradle's capabilities are instrumental in managing the complexity of building, testing, and deploying applications across diverse environments. DevOps Architecture DevOps Architecture Plan Release Code Dep Deploy Development and operations both play essential roles in order to deliver applications. The deployment comprises analyzing the requirements, designing, developing, and testing of the software components or frameworks. The operation consists of the administrative processes, services, and support for the software. When both the development and operations cture is the solution to fix the gap between Test Monitor Build Operate ined with collaborating, then the DevOps al www deployment and operation terms; therefore, delivery can be faster. DevOps architecture is used for the applications hosted on the cloud platform and large distributed applications. Agile Development is used in the DevOps architecture so that integration and delivery can be contiguous. When the development and operations team works separately from each other, then it is time- consuming to design, test, and deploy. And if the terms are not in sync with each other, then it may cause a delay in the delivery. So DevOps enables the teams to change their shortcomings and increases productivity. Below are the various components that are used in the DevOps architecture: 1) Build: Without DevOps, the cost of the consumption of the resources was evaluated based on the pre-defined individual usage with fixed hardware allocation. And with DevOps, the usage of cloud, sharing of resources comes into the picture, and the build is dependent upon the user's need, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Build Code Test Plan DevOps Components Moniter Dev Ops Deploy Operate Release 14 N33 nirf 1750 Rank

--- Page 75 ---
Course Code/Title:CS3V15/Devops Customization This tool is highly customizable as it supports a variety of IDE's . Languages supported Project Configuration Based on Goal of time to get used to it. available. It supports software development in Java, C, C++, and Groovy. For declaring the project configuration, it does not use the XML files. On the phases of the fixed and linear model. Graph of task dependencies that do the work. To add functionality in the project is the main goal of the Gradle. To finish the project in the given timeline is the main goal of the Maven. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY Unit: II This tool serves a limited number of developers and is not that customizable. It supports software development in Java, Scala, C# , and Ruby and it does not natively support C and C+ but can support through plugins like "maven-native- plugin" or we can integrate other build systems like CMake or Makefile. For declaring the project configuration, it uses the XML files. 34 N33 nirf 1750 Rank

--- Page 76 ---
Course Code/Title:CS3V15/Devops Unit: III Unit - III CONTINUOUS INTEGRATION USING JENKINS Install & Configure Jenkins, Jenkins Architecture Overview, creating a Jenkins Job, configuring a Jenkins job, Introduction to Plugins, Adding Plugins to Jenkins, commonly used plugins (Git Plugin, Parameter Plugin, HTML Publisher, Copy Artifact and Extended choice parameters). Configuring Jenkins to work with java, Git and Maven, creating a Jenkins Build and Jenkins workspace. Installing and Configuring Jenkins Overview Jenkins is a widely-used open-source automation server that helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration and continuous delivery (CI/CD). Below are detailed steps and considerations for installing and configuring Jenkins. Installation 1. System Requirements: ○ Operating System: Jenkins can run on any major operating system. ○ Java: Jenkins requires Java Runtime Environment (JRE) 8 or later. 2. Download Jenkins: ○ Download the latest version of Jenkins from the official Jenkins website. 3. Installation Steps: ○ Windows: I Run the .msi installer and follow the installation wizard. ■ After installation, Jenkins will start automatically as a Windows service. ○ Linux: Add Jenkins repository and import the GPG key. I Use package manager to install Jenkins (e.g., apt-get install jenkins). I Start Jenkins using systemctl (systemctl start jenkins). 4. Initial Setup: ○ After installation, Jenkins can be accessed through a web browser at http : //< your _server _ip_or_domain>: 8080. o Unlock Jenkins by entering the initial admin password, which can be found in the jenkins_home directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 1 N32 nirf 1750 Rank

--- Page 97 ---
Course Code/Title:CS3V15/Devops Unit: III Build Steps Execute Windows batch command ? Command See the list of available environment variables javac welcome. java java welcome Advanced = Execute Windows batch command 7 Command See the list of avalable chuironment variables acho User name is Nuernane% Advanced Step 5 : Click on Apply and then Save. Step 6 : Select Build with parameters option. Dashboard > welcome > Status Changes Workspace Build with Parameters Configure Delete Project 0 Rename CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 22 N3 nirf 1750 Rank

--- Page 98 ---
Course Code/Title:CS3V15/Devops Unit: III Project welcome This build requires parameters: Username Parth D Build Cancel We can change the username as well. Suppose I changed the Username as Anand then click the Build, we get the console output as follows - C: \ProgramData\Jenkins\.jenkins\workspace\welcome>javac welcome.java C:\ProgramData\Jenkins\.jenkins\workspace\welcomeyjava welcome Welcome to first Job of Jenkins [welcome] $ cmd /c call C:\WINDOWS\TEMP\jenkins16582464065687933173.bat C:\ProgramData\Jenkins\.jenkins\workspace\welcome>echo User name is Anand User name is Anand C:\ProgramData\Jenkins\.jenkins\workspace\welcome>exit 0 Finished: SUCCESS Thus we set variety of parameters to the Jenkins job. We can set the Boolean parameter, choice parameter, credentials, file parameter, password parameter and so on. 3. HTML Publisher Filter Plain Boolean Parameter Choice Parameter Credentials Parameter File Parameter Multi-line String Parameter Password Parameter Run Parameter String Parameter Add Parameter Overview: The HTML Publisher Plugin allows Jenkins to publish HTML reports and artifacts generated during the build process. Key Features: · Report Publishing: Publishes HTML reports, dashboards, or other HTML artifacts. · Report Directory: Allows specifying the directory containing HTML reports. · Index Page: Sets a default HTML file to be displayed when accessing the report. Configuration: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 23 nirf 1750 Rank N33

--- Page 21 ---
Unit: I Course Code/Title:CS3V15/Devops · Gaming: AWS has been serving many gaming studios. Combining Amazon EC2 and S3 services with CloudFront enables gaming websites to deliver high-quality gaming experiences to their customers regardless of location. Use Cases of AWS · Netflix Netflix is an entertainment platform that started in the United States, but eventually, it expanded to many countries and soon became popular. However, once Netflix confronted the scalability problem because of the sudden increase in viewers. That made Netflix choose AWS services. Netflix reports that when it started using AWS services like DynamoDB and Cassandra for its distributed databases, it could handle the data easily. So, scalability is a great advantage of AWS. Netflix has adapted around 100,000 server instances from AWS for computing and storage databases, analytics, recommendation engines, and video transcoding as well. . McDonald's McDonald's is the world's largest fast-food company that serves around 64 million people per day. The growth of this company has gone to another level when it started home deliveries. By utilizing AWS services, McDonald's created a platform that integrates local restaurants with delivery partners such as Uber Eats. Scalability is also a reason for the company to choose AWS services. Moreover, with AWS Microservices Architecture, McDonald's platform can scale 20,000 orders per second and integrate with the global partners easily. · Airbnb Airbnb is an international online marketplace for rental homes. This platform connects people who are looking for rental accommodation with those who want to rent out their houses. Quite soon, Airbnb became unable to handle the constant streaming of data on the website from its customers. That is when it started using Amazon EC2 service and Elastic Load Balancing, which distributes incoming traffic to multiple Amazon EC2 instances. In this way, Airbnb could avoid traffic, and customers could use the online platform without any disruption. • Novartis Novartis is the best example for AWS use cases in healthcare. Novartis is one of the world's largest healthcare companies that provides solutions for patients' well-being. It adapted Amazon EC2 services and built a platform using other services such as Amazon Simple Storage Service, Amazon Elastic Block Store, and four availability zones. Data Analysts of Novartis are taking advantage of the AWS services and still implementing new solutions for the patients. · Expedia Expedia is a worldwide online travel agency that has always focused on the constant development and innovation of its platform to offer an extraordinary user experience for its clients. Since 2010, Expedia has been using AWS services to build a standard deployment model for better infrastructure as AWS offers the best data security through different availability zones. • Samsung If you are using Samsung mobile phones, then you may know about the Samsung app store. For setting up the apps stacked in its store, the company started using AWS services. Using AWS app development services, Samsung wanted to provide its customers with the facility to download the apps anywhere without any network traffic. • NASA NASA (National Aeronautics and Space Administration) has always wondered about creating a library to present people with all its achievements through pictures and videos of space. Later on, it created such platforms, but because it had 10 different NASA centers, it couldn't provide the best experience for viewers. So, all it wanted was to create an easy-access platform for TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 21 N32 nirf 1750 Rank

--- Page 22 ---
Unit: I Course Code/Title:CS3V15/Devops people to search for and view images and videos. Then, NASA started adopting many services from AWS to solve this problem, which included Amazon Elastic Compute Cloud, Elastic Load Balancing, Amazon Simple Storage Service, Amazon Simple Queue Service, etc. Among these, Amazon S3 helped the company store all the incoming data such as photos, videos, and audio files without any hassle. • Facebook Facebook, without a doubt, is a widespread social media platform. To build a scalable application, Facebook used services such as Amazon Elastic Compute Cloud, Amazon Simple Storage Service, Amazon Relational Database Service, Amazon SimpleDB, Amazon CloudFront, Amazon Simple Queue Service, etc. Amazon RDS helps the platform to make it easy to set up, operate, and scale the database in the cloud. Various Services offered by AWS · Amazon EC2 (Elastic Cloud computing) · Amazon RDS (Relational Database Services) · Bonus Service: Amazon Connect · Amazon S3 (Simple Storage Service) · Amazon Lambda · Amazon Cognito · Amazon Glacier · Amazon SNS (Simple Notification Service) · Bonus Service: Amazon Lex · Amazon Lightsail · Amazon VPC (Virtual Private Cloud) · Amazon Kinesis · Amazon Inspector · Amazon Auto-scaling · Amazon IAM (Identity and Access Management) · Dynamo DB · Amazon SQS (Simple Queue Service) · Amazon ElastiCache · Amazon Chime · AWS Athena · Code Catalyst · Web Application Firewall · AWS Amplify · AWS Rekognition · AWS QuickSight · AWS CloudFormation · AWS Management Console The Important Cloud Services according to various categories that are provided by AWS are given below : 1. Compute • Amazon EC2: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It allows organizations to obtain and configure virtual compute capacity in the cloud. You can select from a variety of operating systems and resource configurations like memory, CPU, and storage that are required for your TECHNOLOGY (Autonomous) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY 22 N33 narf 1750 Rank

--- Page 61 ---
Course Code/Title:CS3V15/Devops Unit: II Return to the previous screen and undo the deletion. The errors that occurred will disappear. *mavenproject/pom.xml & IApp.java AppTest.java 6 <artifactId>mavenproject</artifactId> 7 <version>0.0.1-SNAPSHOT</version> <packaging>jar</packaging> 8 9 16 <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 138 <properties> 14 15 16 17% <dependencies> 18 19 <groupId>junit</groupId> I 28 21 22 23 24 <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> cdependency> <artifactId>junit</artifactid> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> 25 </project> 26 9. Dependency Management: The dependencyManagement and dependencies are especially useful for multi-module projects. Dependency Management : This tag consists of a dependencies tag which itself might contain multiple dependency tags. Each dependency is supposed to have at least three maintags: groupId, artifactId, and version. For example: <dependencyManagement> <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 20 N33 nirf 1750 Rank -

--- Page 62 ---
Course Code/Title:CS3V15/Devops Unit: II The above code just declares the new artifact commons-lang3, but it doesn't really add it to the project dependency resource list. Dependencies: This tag contains a list of dependency tags. Each dependency is supposed to have at least two main tags, which are groupId and artifactId. For example: <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> The version and scope tags can be inherited implicitly if we have used the dependencyManagement tag before in the POM file. DependencyManagement is just a declaration, and it does not really add a dependency. Dependencies tag adds the actual dependency to the project. An example for adding the JUnit library dependency: <dependencyManagement> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.2</version> <scope>test</scope> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 21 N33 nirf 175º Rank

--- Page 115 ---
Unit: IV Course Code/Title:CS3V15/Devops UNIT IV CONFIGURATION MANAGEMENT USING ANSIBLE Ansible Introduction, Installation, Ansible master/slave configuration, YAML basics, Ansible modules, Ansible Inventory files, Ansible playbooks, Ansible Roles, adhoc commands in ansible. Ansible A SSH ANSIBLE TARGET NODE HOSTS PLAYBOOK Ansible is a powerful DevOps tool designed for automating tasks on remote servers or nodes. In simple terms, it allows you to automate commands and functions on multiple remote machines from a central 'master' node. To illustrate its usefulness, consider a scenario where you need to reboot dozens or even hundreds of remote hosts. You could manually SSH into each one and initiate the reboot, or you can use Ansible to streamline the process, making it efficient and offering a wide range of additional functionalities. In essence, Ansible operates much like the second method mentioned, using SSH (Secure Shell), a secure communication protocol, to control remote nodes in a secure and optimized manner. Ansible Components Ansible Core Components Control node Ansible Inventory Managed node 1 Managed node 2 Managed node 3 1. Control Node - · The central or main node where Ansible is installed. · Used to trigger commands like ansible and ansible-inventory on other nodes. · Acts as the orchestrator for Ansible operations. • TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 1 N33 nirf 1750 Rank

--- Page 116 ---
Course Code/Title:CS3V15/Devops Unit: IV 2. Manage Node- · A remote or slave node where tasks are executed or controlled by Ansible. . These are the servers or devices you want to manage or automate. 3. Inventory - · A list of managed node IPs and configurations. · Logically organized, typically using file formats like YAML or INI. · Created on the control node to describe the deployment of hosts to Ansible. Ansible Additional Components Ansible Architecture CMDB Users Public / Private Cloud Hosts ... ... Inventory .. Modules 000 Ansible Playbook API Plugins Networking www.educba.com 1. Ad-Hoc Commands - . These are one-off commands that you can execute using the ansible command. · Useful for quick tasks or tests on remote nodes. 2. Plugins - · Plugins are pieces of code that extend Ansible's core functionality. · Ansible uses a plugin architecture for flexibility and expandability. · Examples include connectivity plugins for establishing connections and cache plugins. · Learn more about Ansible plugins. 3. Module- · Modules are built-in functions that can be used to perform various tasks. . They eliminate the need to write custom code for common operations. · Examples include modules for package management (apt, yum), service management, and more. CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) CHENNAI INSTITUTE . TECHNOLOGY 2 N33 nirf 1750 Rank

--- Page 99 ---
Unit: III Course Code/Title:CS3V15/Devops 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "HTML Publisher Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Post-build Actions section. ○ Select Publish HTML reports. ○ Specify the directory containing HTML reports and the index page. Usage: . Ideal for displaying build reports such as test results or code coverage metrics in a user-friendly HTML format. DEMO: Step 1 : We will create a Maven project using Selenium and TestNG framework. Open Eclipse : IDE and Create a Maven Project. Select the Create Simple Project (Skip archetype selection) and enter the Group Id and Artifact Id as per your choice. My project name is SeleniumDemo. Step 2 : Add the Selenium dependency in your pom.xml file as follows - <dependency> <groupId>org.seleniumhq.selenium</groupId> <artifactId> selenium-java</artifactId> <version>3.141.59</version> </dependency > Save the pom.xml file. Step 3 : Now install testNG plugin. For that, from Menu select Help-> Eclipse Marketplace Help Welcome 7 Help Contents Search Show Context Help Show Active Keybindings ... Ctrl+Shift+L Tip of the Day Tips and Tricks ... Cheat Sheets ... @ Eclipse User Storage Open the Eclipse Marketpla ** Perform Setup Tasks ... % Check for Updates Install New Software ... Eclipse Marketplace ... = About Eclipse IDE · Donate Contribute Type TestNG in the search window. The testNG installation option will be available. click Install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 24 N32 nirf 1750 Rank ---

--- Page 100 ---
Course Code/Title:CS3V15/Devops Unit: III C Eclipse Meketplace × Eclipse Marketplace Select solutions to initiit, Prese leuralt Now to proceed with lutallation. Press the "more info" link to learn more about a solution Search Recent Popular Favorites itwled - Research at the Eclipse × All Categories Go TestNG for Eclipse . This plug-in late you run your TextNG wits from Erfiore. You can run sulter, groupe er individuel methods. Errors are reported in a pepwane tab that lets you. more info by Chat Bent Apache 20 tantog zielt testingunit integration functional selection * 737 ** * stal: 1,83M (25,021 last month) Install MoreUnit 3.3.0 Morelinit is an Solipoe plugin that should mult you in wilting mory unk with. It supports ali programming languages (witching between teds and clanset under. mone.info Install Infinitest 5.4.0.3 Infinitest is a continuont tout runner for tova,'and is valuable to developers valing a Marketplaces (7) Cancel Then confirm the selected features by clicking on Confirm button. x Confirá Selected Features These Confirm to gorthus with the induction. Cs go back te chocie bis sules it CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 25 N3 nirf 1750 Rank

--- Page 43 ---
Course Code/Title:CS3V15/Devops Unit: II o Generates source code (if auto-generated code is used) o Generates documentation from source code o Compiles source code o Packages compiled code into JAR of ZIP file o Installs the packaged code in local repository, server repository, or central repository 2. Installation of Maven: To install maven on windows, you need to perform following steps: 1. Download maven and extract it 2. Add JAVA_HOME and MAVEN_HOME in environment variable 3. Add maven path in environment variable 4. Verify Maven 1) Download Maven To install maven on windows, you need to download apache maven first. Download Maven latest Maven software from Download latest version of Maven For example: apache-maven-3.1.1-bin.zip 2) Add MAVEN_HOME in environment variable Right click on MyComputer -> properties -> Advanced > Environment variables -> click new button System Settings - Now add MAVEN_HOME in variable name and path of maven in variable value. It must be the home directory of maven i.e. outer directory of bin. For example: E:\apache-maven-3.1.1 3) Add Maven Path in environment variable Click on new tab if path is not set, then set the path of maven. If it is set, edit the path and append the path of maven.Here, we have installed JDK and its path is set by default, so we are going to append the path of maven. The path of maven should be %maven home%/bin. For example, E:\apache-maven- 3.1.1\bin . 4) Verify maven To verify whether maven is installed or not, open the command prompt and write: mvn -version Now it will display the version of maven and jdk including the maven home and java home. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 2 N32 nirf 175º Rank

--- Page 44 ---
Unit: II Course Code/Title:CS3V15/Devops 3. POM FILES: POM is an acronym for Project Object Model. The pom.xml file contains information of project and configuration information for the maven to build the project such as dependencies, build directory, source directory, test source directory, plugin, goals etc. Maven reads the pom.xml file, then executes the goal. Before maven 2, it was named as project.xml file. But, since maven 2 (also in maven 3), it is renamed as pom.xml. Elements of maven pom.xml file For creating the simple pom.xml file, you need to have following elements: Element Description project It is the root element of pom.xml file. modelVersion It is the sub element of project. It specifies the modelVersion. It should be set to 4.0.0. groupId It is the sub element of project. It specifies the id for the project group. artifactId It is the sub element of project. It specifies the id for the artifact (project). An artifact is something that is either produced or used by a project. Examples of artifacts produced by Maven for a project include: JARs, source and binary distributions, and WARs. version It is the sub element of project. It specifies the version of the artifact under given group. Maven pom.xml file with additional elements Here, we are going to add other elements in pom.xml file such as: Element Description packaging defines packaging type such as jar, war etc. name defines name of the maven project. url defines url of the project. dependencies defines dependencies for this project. dependency defines a dependency. It is used inside dependencies. scope defines scope for this maven project. It can be compile, provided, runtime, test and system. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 3 N32 narf 1750 Rank -

--- Page 37 ---
Course Code/Title:CS3V15/Devops Unit: I GIT INSTALLATION Git for Windows stand-alone installer · Download the latest Git for Windows installer. · When you've successfully started the installer, you should see the Git Setup wizard screen. Follow the Next and Finish prompts to complete the installation. The default options are pretty sensible for most users. · Open a Command Prompt (or Git Bash if during installation you elected not to use Git from the Windows Command Prompt). . Run the following commands to configure your Git username and email using the following commands, replacing Emma's name with your own. These details will be associated with any commits that you create: $ git config -- global user.name "CIT_CHENNAI" $ git config -- global user.email CITCHENNAI@atlassian.com • Optional: Install the Git credential helper on Windows Bitbucket supports pushing and pulling over HTTP to your remote Git repositories on Bitbucket. Every time you interact with the remote repository, you must supply a username/password combination. You can store these credentials, instead of supplying the combination every time, with the Git Credential Manager for Windows. BASIC COMMANDS OF GIT: Some basic Git commands along with their syntax and examples: · Initialize a Repository: Syntax: `git init' Example: `git init' · Clone a Repository: Syntax: `git clone <repository_url> Example: `git clone https://github.com/example/repository.git' · Check Repository Status: Syntax: `git status' Example: `git status' · Add Changes to Staging Area: Syntax: `git add <file(s)>' Example: `git add file.txt' · Commit Changes: Syntax:`git commit -m "Commit message" Example:`git commit -m "Add new feature" · Create a New Branch: Syntax: `git branch <branch_name>' Example: `git CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY (Autonomous] N33 37 nirf 1750 Rank

--- Page 38 ---
Course Code/Title:CS3V15/Devops branch feature-branch' Unit: I • Switch to a Branch: Syntax: `git checkout <branch_name>' Example:`git checkout feature-branch' OR Syntax: `git switch <branch_name>' (Git version 2.23 and later) Example: `git switch feature-branch' · Create and Switch to a New Branch: Syntax: `git checkout -b <new_branch name>' Example: `git checkout -b new-feature' OR Syntax: `git switch -c <new_branch_name>' (Git version 2.23 and later) Example: `git switch -c new-feature' . Merge Changes from One Branch to Another: Syntax: `git merge <branch_name>' Example: `git merge feature-branch' · View the Commit History: Syntax: `git log' Example: `git log' · Push Changes to a Remote Repository: Syntax: `git push <remote_name> <branch_name>' Example:`git push origin master' · Pull Changes from a Remote Repository: Syntax: `git pull <remote_name> <branch_name>' Example:`git pull origin master' . Show the Differences Between Working Directory and Staging Area: Syntax: `git diff Example: `git diff · Show the Differences Between Staging Area and Last Commit: Syntax: `git diff -- cached' Example: `git diff -- cached' . Show the Differences Between Working Directory and Last Commit: Syntax: `git diff HEAD' Example: `git diff HEAD' CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 38 N32 nirf 1750 Rank

--- Page 119 ---
Course Code/Title:CS3V15/Devops Unit: IV · Groupings in inventories enable you to organize your infrastructure, allowing you to target specific subsets of nodes based on your needs. For instance, when you need to perform tasks in a particular location, groupings become essential. Creating Inventories There are two primary methods to create an inventory on the control node: using a YAML file or an INI file. 1. INI Inventory Example (inventory.ini): 1[virtualmachines] 2host1 ansible_host:192.168.0.1 3192.168.0.2 2. In this INI file, it captures the IP addresses of managed nodes. The default username used for SSH connections is typically the working user of your control node. 3. ansible_host: This parameter is used to specify the IP address, although you can write the IP directly. 4. Inventory.yaml 1 --- 2virtualmachines: # Define a class name to represent a particular group of devices. 1 hosts: # Define the hosts. 2 vm01: # Define a parent name to indicate a class of parent type. 3 ansible_host: 192.168.0.1 # Specify the IP address. 4 http_port: 80 # (optional) Define the HTTP port. 5 ansible_user: auriga # (optional) Provide the username if it's different from the control node's user. o ansible_user: Use this to specify a different user if the managed node's user is not the same as the control node's user. o http_port: Specify a port if necessary. In real-world scenarios, inventory files can be more complex, reflecting the diverse and extensive infrastructure of organizations. You can explore more configurations here. Verifying Your Inventory After creating your inventory, it's a good practice to verify its correctness. This step ensures that the inventory is correctly structured and accessible to Ansible. To do this, use the following command: 1ansible-inventory -i inventory.yaml -list · - i: Indicates the inventory file you want to use. . - list: Requests the listing of the inventory content. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 5 N33 nirf 1750 Rank

--- Page 120 ---
Course Code/Title:CS3V15/Devops Unit:IV output: auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-inventory ·i inventory.yaml -- list "_meta": { "hostvars": { "vm01": "ansible_host": "192.168.0.208", "ansible_user"; "au" "children": [ "ungrouped" "ungrouped": { "hosts": "vn01 1 By following these steps, you have effectively set up and verified your inventory, ensuring a solid foundation for your Ansible automation. Using Ansible Ad-Hoc Commands Introduction: In Ansible, ad-hoc commands are your go-to solution for executing quick, one-off tasks on remote nodes. They provide a straightforward and efficient way to interact with managed nodes without the need for creating full-fledged playbooks. Ad-hoc commands are particularly useful when you need immediate results without the overhead of playbook development. When to Use Ad-Hoc Commands: Ad-hoc commands are best suited for scenarios where the task at hand is simple and doesn't require the complexity of a playbook. They are perfect for tasks like system health checks, package installation, service management, or any other single-operation job. Ad-Hoc Command Syntax: Ad-hoc commands follow a specific syntax that comprises various components, each serving a unique role: . Target Group: This is the group of hosts you intend to target with the ad-hoc command. · Module (-m): Specifies the module to execute. Modules are Ansible's building blocks for performing tasks, and they can range from basic operations like "ping" to more advanced tasks such as package management. · Inventory File (-i): Indicates the location of your inventory file, which defines the list of target devices. Example: Ping All Inventory Devices To illustrate the use of ad-hoc commands, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: 1ansible virtualmachines -m ping -i inventory.yaml virtualmachines is the target group. In this case, it could be any group, or you can use all to target all devices in your inventory. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 63 ---
Course Code/Title:CS3V15/Devops Unit: II 10. Introduction of Gradle: The list of features that Gradle provides, · Gradle is available with separate Domain Specific Language (DSL) based on Groovy language. · It provides the declarative language elements. Those elements also provide build-by- convention support for Java, Groovy, OSGI, Web and Scala. Language for dependency based programming The declarative language lies on a top of a general purpose task graph, which can be fully supported in the build. Structure your build Gradle allows you to apply common design principles to your build. It will give you a perfect structure for build, so that, you can design well-structured and easily maintained, comprehensible build. Deep API By using this API, you can monitor and customize its configuration and execution behavior to the core. Gradle scales Gradle can easily increase the productivity, from simple and single project builds to huge enterprise multi-project builds. Multi-project builds Gradle supports the multi-project builds and partial builds. If you build a subproject, Gradle takes care of building all the subprojects, that the subproject depends on. Different ways to manage your builds Gradle supports different strategies to manage your dependencies. Gradle is the first build integration tool Gradle is fully supported for your ANT tasks, Maven and lvy repository infrastructure for publishing and retrieving dependencies. It also provides a converter for turning a Maven pom.xml to Gradle script. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 22 N32 nirf 175º Rank -

--- Page 64 ---
Course Code/Title:CS3V15/Devops Unit: II Ease of migration Gradle can easily adapt to any structure. Therefore, you can always develop your Gradle build in the same branch, where you can build live script. Gradle Wrapper Gradle Wrapper allows you to execute the Gradle builds on machines, where Gradle is not installed. This is useful for continuous integration of servers. Free open source Gradle is an open source project, and licensed under the Apache Software License (ASL). Groovy Gradle's build script are written in Groovy programming language. The whole design of Gradle is oriented towards being used as a language and not as a rigid framework. Groovy allows you to write your own script with some abstractions. The whole Gradle API is fully designed in Groovy language. Installation of Gradle: Prerequisites to install Gradle JDK and Groovy are the prerequisites for Gradle installation. Gradle requires JDK version 6 or later to be installed in the system. It uses the JDK libraries which are installed, and sets to the JAVA_HOME environmental variable. Gradle carries its own Groovy library, therefore, we need not install Groovy explicitly. If it is installed, that is ignored by Gradle. The steps to install Gradle in your system are explained below. Step 1 - Verify JAVA Installation First of all, you need to have Java Software Development Kit (SDK) installed on your system. To verify this, execute Java -version command in any of the platform you are working on. In Windows Execute the following command to verify Java installation. I have installed JDK 1.8 in my system. C:\> java - version CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE . TECHNOLOGY 23 N32 narf 1750 Rank

--- Page 25 ---
Course Code/Title:CS3V15/Devops Unit: I · backups · automated multi-region replication in-memory caching · data export tools. Global Infrastructure of AWS · AWS is a cloud computing platform which is globally available. . Global infrastructure is a region around the world in which AWS is based. Global infrastructure is a bunch of high-level IT services which is shown below: AWS is available in 19 regions, and 57 availability zones in December 2018 and 5 more regions 15 more availability zones for 2019. The following are the components that make up the AWS infrastructure: o Availability Zones o Region o Edge locations o Regional Edge Caches Compnents of Global Infrastructure Availability zones Region Edge Locations Regional Edge Caches Availability zone as a Data Center An availability zone is a facility that can be somewhere in a country or in a city. Inside this facility, i.e., Data Centre, we can have multiple servers, switches, load balancing, firewalls. The things which interact with the cloud sits inside the data centers. o An availability zone can be a several data centers, but if they are close together, they are counted as 1 availability zone. Region o A region is a geographical area. Each region consists of 2 more availability zones. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 26 ---
Course Code/Title:CS3V15/Devops Unit: I o A region is a collection of data centers which are completely isolated from other regions. · A region consists of more than two availability zones connected to each other through links. AZ-a AZ-b AZ-c Availability zones are connected through redundant and isolated metro fibers. Edge Locations o Edge locations are the endpoints for AWS used for caching content. o Edge locations consist of CloudFront, Amazon's Content Delivery Network (CDN). o Edge locations are more than regions. Currently, there are over 150 edge locations. Edge location is not a region but a small location that AWS have. It is used for caching the content. o Edge locations are mainly located in most of the major cities to distribute the content to end users with reduced latency. o For example, some user accesses your website from Singapore; then this request would be redirected to the edge location closest to Singapore where cached data can be read. Regional Edge Cache o AWS announced a new type of edge location in November 2016, known as a Regional Edge Cache. o Regional Edge cache lies between CloudFront Origin servers and the edge locations. o A regional edge cache has a large cache than an individual edge location. o Data is removed from the cache at the edge location while the data is retained at the Regional Edge Caches. o When the user requests the data, then data is no longer available at the edge location. Therefore, the edge location retrieves the cached data from the Regional edge cache instead of the Origin servers that have high latency. What Is Cloud Computing? Cloud computing is the use of hardware and software components in an off-premises location to deliver a service to a network. Users can access files and applications from any device that can access the internet. Some features and capabilities include: · Cloud providers can pull the computing resources to provide services to multiple customers with the help of a multi-tenant model · Cloud computing proves an on-demand self-service, which helps administrators monitor performance CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 26 N32 nirf 1750 Rank -

--- Page 91 ---
Course Code/Title:CS3V15/Devops Unit: III Now we will create a repository on GitHub and then push the above committed Git repository on the GitHub. Open a web browser, and login to www.GitHub.com. Create a repository by the name MyJavaPrograms. Start a new repository A repository contains all of your project's files, revision history, and collaborator discussion. AnurdhaP / MyJavaPrograms Public Anyone on the internet can see this repository Private You choose who can see and commit to this repository Create a new repository 1 then copy the URL of this repository. HTTPS SSH https://github.com/AnurdhaP/MyJavaPrograms.git Now open the command prompt window and go to the MyJavaPrograms directory and issue the following command. git remote add origin https://github.com/AnurdhaP/MyJavaPrograms.git then push the Git to GitHub repository using the command push -u origin master CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 16

--- Page 92 ---
Course Code/Title:CS3V15/Devops Unit: III It is illustrated as follows - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git push -u origin master Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 719 bytes | 719.00 KiB/s, done. Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 To https://github.com/AnurdhaP/MyJavaPrograms.git * [new branch] master -> master branch 'master' set up to track 'origin/master'. E: \MyJavaPrograms> · Now it we refresh the repository on GitHub then we can see our test.java and test.class programs are present in that repository. MyJavaPrograms Public !" master - 1º 1 branch 0 tags Go to f AnurdhaP First Commit for Java program .4cb test.class First Commit for Java program test.java First Commit for Java program Help people interested in this repository understand your project by adding a README. Step 4 : Now create a job in Jenkins that makes use of GitHub plugin in order to access the GitHub repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 17 N3 nirf 1750 Rank

--- Page 5 ---
Unit: I Course Code/Title:CS3V15/Devops Example: Facebook's mobile app which is updated every two weeks effectively tells users you can have what you want and you can have it. Now ever wondered how Facebook was able to do social smoothing? It's the DevOps philosophy that helps facebook and sure that apps aren't outdated and that users get the best experience on Facebook. Facebook accomplishes this true code ownership model that makes its developers responsible that includes testing and supporting through production and delivery for each kernel of code. They write and update their true policies like this but Facebook has developed a DevOps culture and has successfully accelerated its development lifecycle. Difference between Agile and DevOps: S. No. Agile 1 It started in the year 2001. It started in the year 2007. 2 Invented by John Kern, and Martin Fowler. Invented by John Allspaw and Paul Hammond at Flickr, and the Phoenix Project by Gene Kim. 3 Agile is a method for creating software. It is not related to software development. Instead, the software that is used by DevOps is pre-built, dependable, and simple to deploy. 4 An advancement and administration approach. Typically a conclusion of administration related to designing. 5 The agile handle centers on consistent changes. DevOps centers on steady testing and conveyance. 6 A few of the finest steps embraced in Agile are recorded underneath - 1. Backlog Building 2.Sprint advancement DevOps to have a few best hones that ease the method - 1. Focus on specialized greatness. 2. Collaborate straightforwardly with clients and join their feedback 7 Agile relates generally to the way advancement is carried of, any division of the company can be spry in its hones. This may be accomplished through preparation. 8 All the group individuals working in a spry hone have a wide assortment of comparable ability sets. This is often one of the points of interest of having such a group since within the time of requirement any of the group individuals can loan help instead of holding up for the group leads or any pro impedances. 9 Spry accepts "smaller and concise". Littler the group superior it would be to convey with fewer complexities. 10 Since Agile groups are brief, a foreordained sum of time is there which are sprints. Tough, it happens that a sprint has endured longer than a month but regularly a week long. 11 A big team for your project is not required. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY DevOps DevOps centers more on program arrangement choosing the foremost dependable and most secure course. DevOps features a diverse approach and is very viable, most of the time it takes after "Divide and Conquer". Work partitioned among the improvement and operation groups. DevOps, on the other hand, accepts that "bigger is better". DevOps, on the other hand, prioritizes reliabilities. It is since of this behavior that they can center on a long-term plan that minimizes commerce's unsettling influences. It demands collaboration among different teams for the completion of work. 5 N33 nirf 1750 Rank

--- Page 6 ---
Unit: I Course Code/Title:CS3V15/Devops 12 Some of the Tools- · Bugzilla · JIRA · Kanboard and more. 13 It is suitable for managing complex projects in any department. It centers on the complete engineering process. 14 It does not focus on the automation. It focusses on automation. 15 Working system gets more significance in Agile than documentation. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Some of the Tools- · Puppet • Ansible • AWS · Chef · team City OpenStack and more. The process documentation is significant in DevOps. 6 N32 nirf 1750 Rank

--- Page 73 ---
Course Code/Title:CS3V15/Devops Unit: II init - Initializes a new Gradle build. wrapper - Generates Gradle wrapper files. Help tasks Build Environment - Displays all build script dependencies declared in root project 'eclipse-workspace'. components - Displays the components produced by root project 'eclipse-workspace '. [incubating] dependencies - Displays all dependencies declared in root project 'eclipse-works pace'. Dependency Insight - Displays the insight into a specific dependency in root project 'eclipse-workspace.' Dependent Components : It displays the dependent components of components in the root project 'eclipse-workspace.' [incubating] help - Displays a help message. model - Displays the configuration model of root project 'eclipse-workspace.' [incubating] projects - Displays the sub-projects of root project 'eclipse-workspace.' properties - Displays the properties of root project 'eclipse-workspace.' tasks - Displays the tasks runnable from root project 'eclipse-workspace.' To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> To list all the tasks of the project, run the below command: 1. gradle tasks -all CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 32 N33 nirf 1750 Rank

--- Page 74 ---
Course Code/Title:CS3V15/Devops Unit: II To display more details about a task, run the below command: 1. gradle help -- task Listing Dependencies In Gradle, we can list the dependencies which are broken down by the configuration. To list the dependencies, run the below command: 1. gradle -q dependencies Difference between Gradle and Maven Basis Based on Gradle is based on developing Maven is based on developing pure Java language-based software. Configuration It uses a Groovy-based Domain-specific language (DSL) for creating project structure. Focuses on Performance It performs better than maven as it optimized for tracking only current running task. It is necessary to compile. Java Compilation Usability It is a new tool , which requires users to spend a lot This tool is a known tool for many users and is easily Gradle domain-specific language projects. It uses Extensible Markup Language (XML) for creating project structure. Developing applications by adding new features to them . It avoids compilation. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] Maven Developing applications in a given time limit. It does not create local temporary files during software creation and is hence - slower . 33 N33 nirf 1750 Rank -

--- Page 39 ---
Course Code/Title:CS3V15/Devops Unit: I GITHUB GitHub is an increasingly popular programming resource used for code sharing. It's a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in October 2020, it is the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 56 million total developers. GitHub is a Git repository hosting service that provides a web-based graphical interface. It is the world's largest coding community. Putting a code or a project into GitHub brings it increased, widespread exposure. Programmers can find source codes in many different languages and use the command-line interface, Git, to make and keep track of any changes. GitHub helps every team member work together on a project from any location while facilitating collaboration. You can also review previous versions created at an earlier point in time. GitHub's Features? 1. Easy Project Management: GitHub is a place where project managers and developers come together to coordinate, track, and update their work so that projects are transparent and stay on schedule. 2. Increased Safety With Packages Packages can be published privately, within the team, or publicly to the open-source community. The packages can be used or reused by downloading them from GitHub. 3. Effective Team Management GitHub helps all the team members stay on the same page and organized. Moderation tools like Issue and Pull Request Locking help the team to focus on the code. 4. Improved Code Writing Pull requests help the organizations to review, develop, and propose new code. Team members can discuss any implementations and proposals through these before changing the source code. 5. Increased Code Safety GitHub uses dedicated tools to identify and analyze vulnerabilities to the code that other tools tend to miss. Development teams everywhere work together to secure the software supply chain, from start to finish. 6. Easy Code Hosting All the code and documentation are in one place. There are millions of repositories on GitHub, and each repository has its own tools to help you host and release code. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 39 N32 nirf 1750 Rank

--- Page 40 ---
Course Code/Title:CS3V15/Devops Unit: I HOSTING SERVICE FOR GIT REPOSITORY When it comes to hosting Git repositories, various platforms provide a robust infrastructure for collaborative development, version control, and project management. Choosing the right hosting service depends on factors like ease of use, collaboration features, and integration capabilities. Here are some popular Git hosting services widely utilized in the software development community: 1. GitHub: GitHub stands out as one of the most prevalent Git hosting platforms, offering a user- friendly interface, powerful collaboration features, and seamless integration with various tools. It serves as an ideal choice for open-source projects, private repositories, and team collaboration. 2. GitLab: GitLab is a comprehensive web-based Git repository manager that not only provides source code management but also includes features like continuous integration. It caters to both cloud-based and self-hosted solutions, giving users flexibility in hosting their repositories. 3. Bitbucket: Owned by Atlassian, Bitbucket is another popular Git repository hosting service. Supporting both Git and Mercurial repositories, it offers features like code collaboration, issue tracking, and continuous integration. Bitbucket is often preferred by teams using other Atlassian tools such as Jira and Confluence. 4. GitKraken Glo Boards: GitKraken Glo Boards is an integrated task and issue tracking service linked with GitKraken, a Git client. This platform allows teams to manage tasks directly associated with their Git repositories and provides a visual approach to monitoring project progress. 5. SourceForge: SourceForge, with a long history, hosts open-source software projects and offers version control, bug tracking, and project management tools. While not as prominent as some other options, it remains a viable choice for numerous projects. 6. AWS CodeCommit: As part of Amazon Web Services (AWS), AWS CodeCommit is a fully managed source control service. It seamlessly integrates with other AWS services and provides a secure and scalable environment for hosting Git repositories. Selecting the most suitable Git hosting service depends on your team's requirements, project size, and preferences for cloud-based or self-hosted solutions. Each platform has its strengths, catering to specific use cases within the software development landscape. Difference between GIT and GITHUB Git and GitHub are related concepts but serve different purposes in the context of version control and collaborative software development. Git: Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows. GitHub: GitHub is a web-based Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 40 N32 nirf 1750 Rank

--- Page 85 ---
Course Code/Title:CS3V15/Devops Unit: III By following these steps, you can set up a Jenkins job to automate builds, integrate with your source code management system, and perform various actions based on build results. Introduction to Jenkins Plugins What Are Jenkins Plugins? Jenkins plugins are extensions that add extra functionality to Jenkins. They allow you to customize and extend Jenkins to better fit your specific needs. Plugins can provide integrations with other tools, add new features, and improve existing functionalities. Why Use Plugins? . Extend Functionality: Add features not available in the core Jenkins. . Integrate Tools: Seamlessly integrate with other tools and platforms (e.g., GitHub, Docker). . Improve Productivity: Automate more tasks and streamline your workflow. · Customize UI: Tailor the Jenkins interface to better suit your preferences and needs. How to Manage Plugins 1. Accessing the Plugin Manager 1. Go to Jenkins Dashboard: ○ Open your Jenkins dashboard. 2. Navigate to Plugin Manager: ○ Click on "Manage Jenkins". ○ Select "Manage Plugins". 2. Installing Plugins 1. Available Plugins: ○ Go to the "Available" tab to see a list of plugins that can be installed. 2. Search for Plugins: ○ Use the search bar to find specific plugins. 3. Select Plugins: ○ Check the box next to the plugins you want to install. 4. Install Plugins: ○ Click "Install without restart" or "Download now and install after restart". 3. Updating Plugins CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 narf 10 1750 Rank

--- Page 86 ---
Course Code/Title:CS3V15/Devops Unit: III 1. Go to Updates: ○ Navigate to the "Updates" tab to see plugins with available updates. 2. Select Plugins: ○ Check the box next to the plugins you want to update. 3. Update Plugins: ○ Click "Download now and install after restart". 4. Managing Installed Plugins 1. Installed Plugins: ○ Go to the "Installed" tab to see all installed plugins. 2. Uninstall Plugins: ○ Click the "Uninstall" button next to the plugin you want to remove. 3. Check Plugin Versions: ○ View the current version of each installed plugin. Adding Plugins to Jenkins Plugins are a crucial part of Jenkins, allowing you to extend its functionality to suit your project's specific needs. Whether you're integrating Jenkins with other tools, customizing your job configuration, or adding new build steps, plugins provide the flexibility you need. Here's a detailed guide on how to add plugins to Jenkins. 1. Why Plugins Are Important - Extending Jenkins Functionality: Plugins allow Jenkins to integrate with various tools and technologies, such as Git, Maven, Docker, and more. - Customizing Jobs: With plugins, you can add specific steps, triggers, and post-build actions to your jobs, making Jenkins adaptable to any workflow. - Automation: Plugins help automate processes like code quality checks, notifications, and deployments. 2. Accessing the Plugin Manager 1. Navigate to Jenkins Dashboard: Open your Jenkins dashboard in a web browser (e.g., http://localhost:8080). CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 175º Rank

--- Page 83 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Click "OK" to proceed. 4. Configure the Job General Settings 1. Project Description: ○ Enter a description for your job. 2. Discard Old Builds: ○ Set up a policy to manage the number of builds Jenkins keeps. Source Code Management (SCM) 1. Select SCM: ○ Choose your source code management system (e.g., Git). 2. Configure Repository: ○ Enter the repository URL and provide credentials if needed. 3. Branch Specification: ○ Specify the branch to build (e.g., main, develop). Build Triggers 1. Choose Build Triggers: ○ Poll SCM: Check for changes at intervals. ○ Build Periodically: Schedule builds. ○ GitHub hook trigger: Trigger builds on GitHub push. ○ Build after other projects: Trigger builds based on other jobs. Build Environment 1. Configure Build Environment: ○ Clean workspace: Delete workspace before build starts. ○ Manage secrets: Use secret texts or files. ○ Node selection: Specify where to run the job. Build Steps 1. Add Build Steps: ○ Click "Add build step". ○ Choose the build step type (e.g., Execute shell, Invoke Gradle script). ○ Enter the commands or scripts needed for the build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 nirf 8 1750 Rank

--- Page 84 ---
Course Code/Title:CS3V15/Devops Unit: III Post-build Actions 1. Add Post-build Actions: Click "Add post-build action". ○ Common actions include: Archive artifacts: Store build artifacts. Email notification: Send build status emails. I Publish test results: Aggregate test results. Trigger other jobs: Trigger other jobs based on results. 5. Save and Build 1. Save the Configuration: ○ Click "Save". 2. Build the Job: ○ Click "Build Now" to manually trigger a build. ○ View progress and logs by clicking on the build number. 6. Monitor and Manage Builds 1. Check Build History: ○ View past builds and their statuses. 2. Console Output: ○ Click on a build number to see detailed logs. 3. Build Artifacts: ○ Access any artifacts produced by the build. 7. Advanced Configuration (Optional) 1. Pipeline as Code: 0 For complex workflows, use Jenkins Pipelines. Create a Jenkinsfile in your repository. 2. Plugins: ○ Extend Jenkins functionality with plugins from the Jenkins Plugin Manager. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) 9 N32 nirf 1750 Rank

--- Page 41 ---
Unit: I Course Code/Title:CS3V15/Devops S.No. Git 1 Git is a software. 2 Git is a command-line tool GitHub is a graphical user interface 3 Git is installed locally on the system GitHub is hosted on the web 4 Git is maintained by linux. GitHub is maintained by Microsoft. 5 Git is focused on version control and code sharing. GitHub is a hosting service for Git repositories. 6 Git is a version control system to manage source code history. 7 Git was first released in 2005. GHub was launched in 2008. 8 Git has no user management feature. 9 Git is open-source licensed. GitHub includes a free-tier and pay- for- use tier. 10 Git has minimal external tool configuration. 11 Git provides a Desktop interface named Git Gui. GitHub provides a Desktop interface named GitHub Desktop. 12 Git competes with CVS, Azure DevOps Server, Subversion, Mercurial, etc. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY GitHub GitHub is a service. GitHub is focused on centralized source code hosting. GitHub has a built-in user management feature. GitHub has an active marketplace for tool integration. GitHub competes with GitLab, Bit Bucket, AWS Code Commit, etc. 41 N32 nirf 1750 Rank

--- Page 42 ---
Course Code/Title:CS2V15/Devops Unit: II UNIT - II COMPILE AND BUILD USING MAVEN AND GRADLE Introduction, Installation of Maven, POM files, Maven Build lifecycle, Build phases(compile build, test, package) Maven Profiles, Maven repositories(local, central, global),Maven plugins, Maven create and build Artifacts, Dependency management, Installation of Gradle, Understand build using Gradle. 1. Introduction of Maven: Apache Maven is an automation tool. The tool is written in Java. It was initially released on 13 July 2004. It is developed by the Apache software foundation. It is part of the Jakarta Project. It is working on two aspects: how software is built, and its dependencies. It was created by Jason van Zyl. It is built by using a plugin-based architecture that allows it to make the use of any application controllable by standard input. It dynamically downloads Java libraries. Understanding the problem without Maven There are many problems that we face during the project development. They are discussed below: 1) Adding set of Jars in each project: In case of struts, spring, hibernate frameworks, we need to add set of jar files in each project. It must include all the dependencies of jars also. 2) Creating the right project structure: We must create the right project structure in servlet, struts etc, otherwise it will not be executed. 3) Building and Deploying the project: We must have to build and deploy the project so that it may work. Maven simplifies the above mentioned problems. It does mainly following tasks. 1. It makes a project easy to build 2. It provides uniform build process (maven project can be shared by all the maven projects) 3. It provides project information (log document, cross referenced sources, mailing list, dependency list, unit test reports etc.) 4. It is easy to migrate for new features of Maven Apache Maven helps to manage o Builds Documentation o Reporing o SCMs o Releases o Distribution What is Build Tool? A build tool is used for building a process. It does following: CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE TECHNOLOGY N32 nirf 1 1750 Rank

--- Page 105 ---
Course Code/Title:CS3V15/Devops Unit: III How to install copy artifact plugin ? Step 1 : Start the Jenkins tool by entering the login and password. Step 2 : Click on Manage Jenkins. Click on Plugins. Step 3 : In the search window, type the name copy Artifact. Step 4 : The plugin name will be displayed if it is not already installed. Select it. Name Adde a build atop to copy artifacts from another project. Thủa plugin le up for adoption! We are looking for naw maintainent. Velt our Adopt a Plugin intuitive for mort information. then click on Install. The plugin gets installed. Example Demo · Let us discuss how to use Copy Artifact plugin. Step 1 : Create a new Job by clicking on item New Item. Give the some name to your job. I have given the name App1 and selected the Freestyle Project. Step 2: Give suitable description in the description box. Step 3 : Under the source code management, click on Git option and specify the repository URL of GitHub. Source Code Management None Git ? Repositories 1 Repository URL + ·hong · CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 30 N3 nirf 1750 Rank

--- Page 106 ---
Unit: III Course Code/Title:CS3V15/Devops Step 4 : Under the Build Steps section, choose the Execute Windows batch command option. And type the javac and java commands. Build Steps = Execute Windows batch command Command See the list of available environment variables javac test.java java test Advanced Add bulld stop * Step 5 : Click in Apply and Save. Step 6 : Select the Build Now option. On successful built, the test, class file gets generated in the Jenkins workspace of the App1 job. C:\ProgramData\Jenkins\.jenkins\workspace\Appl>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\Appi>java test Good Morning Pathill Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! C:\ProgramData\Jenkins\.jenkins\workspace\Appl>exit @ Finished: SUCCESS Step 7 : Now we have to copy test.class file from the workspace of Appl to another job. For that' purpose we will create another Job named App2 by clicking New Item. Select the Freestyle Project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 31 N3 nirf 1750 Rank

--- Page 103 ---
Course Code/Title:CS3V15/Devops Unit: III =Publish HTML reports ? Reports HTML directory to archive ? E:\Eclipse_workplace\SeleniumDemo\test-output Index page[s] ? index.html Index page title[s] (Optional) Report title ? HTML Report Publishing options Step 11 : Click on Apply and then Save. Now build this project. Step 12 : On successful build, it creates the HTML Report as follows - x + € > C @ localhost:0000/juby/HTML/\portDemo/HTML_20Report/ Back to HTMLBepsatDemo Test results All suites Default suite 11 D myPackage.myTestPage Info . Repetir output Results . Thus using HTML Publish Report Plugin we can successfully generate HTML report of some testing framework application. · Troubleshooting : Sometimes, the raw file is displayed as HTML report. To get the HTML formatted report we must click on Manage Jenkins section, scroll down and locate Script Console section. Click on it. Tools and Actions C Broad Configuration Sium Disk Discard all the loaded alla lo memory and reloud Jerking CLI Script Console Ewecubes arbitrary suyt lor and run below given command : System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", ") . Then restart the Jenkins Page and Build the project once again. The HTML report can then be displayed as given in above step. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 28 N32 nirf 1750 Rank

--- Page 104 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Copy Artifact Overview: The Copy Artifact Plugin allows Jenkins jobs to copy build artifacts from other jobs, facilitating artifact reuse across different jobs. Key Features: · Artifact Copying: Copies files from one build to another, which can be from a specific build or the latest successful build. · Triggering Builds: Can be used in downstream jobs to fetch artifacts from upstream jobs. Configuration: 1. Install the Plugin: Go to Manage Jenkins > Manage Plugins > Available tab, search for "Copy Artifact Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Build section. ○ Add a Copy artifacts from another project build step. o Specify the project name, build number or criteria, and the target directory for copied artifacts. Usage: · Commonly used in multi-job pipelines where artifacts need to be shared between different jobs or stages. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 29 N3 nirf 1750 Rank

--- Page 101 ---
Course Code/Title:CS3V15/Devops Unit: III v => SeleniumDemo src/main/java > src/main/resources > src/test/java > 15 src/test/resources > al JRE System Library [J2SE-1.5] › al Maven Dependencies » drivers v src > @ main v fs test v java v 2> myPackage 10 myTestPage.java 23 resources › › target > > test-output 2 pom.xml > Servers myTestPage.java package myPackage; import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver; import org.testng.annotations.Test; public class myTestPage { @Test public void myFun() { System.setProperty("webdriver.chrome.driver","./drivers/chromedriver.exe"); WebDriver driver = new ChromeDriver(); driver.get("https://google.com"); String title = driver.getTitle(); System.out.println("Hey Anuradha, the tile of this page is: "+title); driver.quit(); } Program explanation : In above program, we have written one test case by the function myFun. In this function, using the Chromedriver which we have installed, we open the web page google.com Then we try to get the tile of this page and display it on the console using System.out statement. Finally we quit the driver. Step 7 : Right click on the Project name in Project Explorer window, Run As-> Maven Test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 26 N3 nirf 1750 Rank

--- Page 102 ---
Unit: III Course Code/Title:CS3V15/Devops Step 8 : Now, locate the Java file myTestPage.java and right click Run As->TestNG Test. It is as follows - 0 Run As # Debug As Profile As Web Services > > 1 TestNG Test > Run Configurations ... > Alt+Shift+X, N Step 9 : We will get the output as follows - Hey Anuradha, the tile of this page is: Google PASSED: myFun Default test Tests run: 1, Failures: 0, Skips: 0 Default suite Total tests run: 1, Failures: 0, Skips: 0 Step 10 : Now, we will open the Jenkin's page and create the HTML Publisher report for the above SeleniumDemo project. The prerequisite to this activity is that the HTML Publisher report plugin must be installed in Jenkins. · Click on New Item, Enter the Item Name and select as Freestyle project, I have given the name HTMLReportDemo. · Then click on configure. Write the suitable description in General section. · The in Build Steps and Add Build steps as Invoke top-level Maven targets provide the Maven version, and under goals write clean test. . Under Post Build Actions, click on Add Post Build Action and select Publish HTML Report. Now give the path of index.html file which is created by Selenium testNG framework in Maven. It is the path of test-output folder. Also give some suitable HTML report title. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 27 N3 nirf 1750 Rank

--- Page 107 ---
Course Code/Title:CS3V15/Devops Unit: III Step 8 : Then configure the Build Steps by selecting the option Copy artifacts from another project as follows - . . First of all give the name of the project from which you want to copy the files. In my case it is App1. . Then select the option "copy from WORKSPACE of latest completed build" · Give the regular expression ** / *.*. Here's a breakdown of what ** / *.* means : o ** : The double asterisk ( ** ) is a wildcard that matches zero or more directories and subdirectories. It's often used to recursively search through all directories and subdirectories. o / : The forward slash (/) is used as a directory separator, typically on Unix-like systems. o *.* : This part of the regular expression is used to match files with an extension. In regular expressions, the * symbol represents "zero or more" of the preceding character or pattern, and the . character is used to match any character. So, *.* essentially matches any file with any extension. So, when you use the regular expression ** / *.* in a Jenkins build configuration, you are instructing Jenkins to include all files with any extension ( *.* ) located in any directory and subdirectory ( ** ) within the workspace or specified location. Build Steps E Copy artifacts from another project Project name ? App: Which build ? Copy from WORKSPACE of latest completed buld Limitation Note ? Artifacts to copy ? Artifacts not to copy ? : Toget directory CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 32

--- Page 108 ---
Course Code/Title:CS3V15/Devops Unit: III Add one more Build Step and select Execute Windows batch Command. Give the command dir so that the name of files and directories that are copied from App1 to App2 will be displayed on the Console. = Execute Windows batch command ? Command See the list of available environment variables ---- dir ----- Advanced v Add build step * Click Apply and Save. Step 8 : Now Build this project (App2). The output will be as follows - Started by user Anuradha P Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\.jenkins\workspace\App2 Copied 16 artifacts from "App1" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621109248227895.bat C:\ProgramData\Jenkins\,jenkins\workspace\App2>dir Volume in drive C Is OS Volume Serial Number is CGC2-BA9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023* 15:07 <DIR> 01-09-2023 15:07 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 474 test.class 144 test. java 2 File(s) 616 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 33 N32 narf 1750 Rank

--- Page 29 ---
Unit: I Course Code/Title:CS3V15/Devops Enterprise services. AWS virtual private cloud. Outsourced cloud services. Salesforce User Controls Operating System, Runtime, Middleware, and Application data Others It is highly scalable and flexible. Microsoft Azure. Force.com, Gigaspaces. Data of the application It is highly scalable to suit the different businesses according to resources. Google cloud services Google offers a seven wide range of Services: · Compute · Networking · Storage and Databases · Big Data · Machine Learning · Identity & Security · Management and Developer Tools IBM cloud analysis. AWS, Terremark Nothing It is highly scalable to suit the small, mid and enterprise level business 1. Compute: GCP provides a scalable range of computing options you can tailor to match your needs. It provides highly customizable virtual machines. and the option to deploy your code directly or via containers. · Google Compute Engine · Google App Engine · Google Kubernetes Engine · Google Cloud Container Registry · Cloud Functions 2. Networking: The Storage domain includes services related to networking, it includes the following services • Google Virtual Private Cloud (VPC) . Google Cloud Load Balancing · Content Delivery Network · What is Google Cloud Connect · Google Cloud DNS · What is Google Cloud Web Hosting 3. Storage and Databases: The Storage domain includes services related to data storage, it includes the following services · Google Cloud Storage . Cloud SQL · Cloud Bigtable · Google Cloud Datastore • Persistent Disk TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 29 N32 nirf 1750 Rank

--- Page 30 ---
Unit: I Course Code/Title:CS3V15/Devops 4. Big Data: The Storage domain includes services related to big data, it includes the following services · Google BigQuery · Google Cloud Dataproc · Google Cloud Datalab · Google Cloud Pub/Sub 5. Cloud AI: The Storage domain includes services related to machine learning, it includes the following services · Cloud Machine Learning · Vision API · Speech API · Natural Language API · Translation API • Jobs API 6. Identity & Security: The Storage domain includes services related to security, it includes the following services · Cloud Resource Manager · Cloud IAM · Cloud Security Scanner · Cloud Platform Security 7. Management Tools: The Storage domain includes services related to monitoring and management, it includes the following services • Stackdriver · Monitoring · Logging · Error Reporting · Trace · Cloud Console 8. Developer Tools: The Storage domain includes services related to development, it includes the following services . Cloud SDK · Deployment Manager · Cloud Source Repositories · Cloud Test Lab AZURE Azure is Microsoft's cloud platform, just like Google has its Google Cloud and Amazon has its Amazon Web Service or AWS.000. Generally, it is a platform through which we can use Microsoft's resources. For example, to set up a huge server, we will require huge investment, effort, physical space, and so on. In such situations, Microsoft Azure comes to our rescue. It will provide us with virtual machines, fast processing of data, analytical and monitoring tools, and so on to make TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 30 N33 narf 1750 Rank

--- Page 51 ---
Course Code/Title:CS3V15/Devops Unit: II 7. Maven Profiles: A Build profile is a set of configuration values, which can be used to set or override default values of Maven build. Using a build profile, you can customize build for different environments such as Production v/s Development environments. Types of Build Profile Build profiles are majorly of three types. Type Where it is defined Per Project Defined in the project POM file, pom.xml Per User Defined in Maven settings xml file (%USER_HOME%/.m2/settings.xml) Global Defined in Maven global settings xml file (%M2_HOME%/conf/settings.xml) Profile Activation A Maven Build Profile can be activated in various ways. · Explicitly using command console input. · Through maven settings. · Based on environment variables (User/System variables). · OS Settings (for example, Windows family). · Present/missing files. Now, under src/main/resources, there are three environment specific files - Sl.No. File Name & Description env.properties 1 default configuration used if no profile is mentioned. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 10 N33 nirf 1750 Rank

--- Page 52 ---
Course Code/Title:CS3V15/Devops Unit: II env.test.properties 2 test configuration when test profile is used. 3 env.prod.properties production configuration when prod profile is used. Explicit Profile Activation In the following example, we will attach maven-antrun-plugin:run goal to test the phase. This will allow us to echo text messages for different profiles. We will be using pom.xml to define different profiles and will activate profile at command console using maven command. Assume, we've created the following pom.xml in C:\MVN\project folder. <project xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation = "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.companyname.projectgroup</groupId> <artifactId>project</artifactId> <version>1.0</version> <profiles> <profile> <id>test</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <version>1.1</version> <executions> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 11 N33 nirf 1750 Rank

--- Page 55 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Environment Variables Now remove active profile from maven settings.xml and update the test profile mentioned in pom.xml. Add activation element to profile element as shown below. The test profile will trigger when the system property "env" is specified with the value "test". Create an environment variable "env" and set its value as "test". <profile> <id>test</id> <activation> <property> <name>env</name> <value>test</value> </property> </activation> </profile> Let's open command console, go to the folder containing pom.xml and execute the following mvn command. C:\MVN\project>mvn test Profile Activation via Operating System Activation element to include os detail as shown below. This test profile will trigger when the system is windows XP. <profile> <id>test</id> <activation> <os> <name>Windows XP</name> <family>Windows</family> <arch>x86</arch> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 14 N32 narf 1750 Rank -

--- Page 56 ---
Course Code/Title:CS3V15/Devops Unit: II <version>5.1.2600</version> </os> </activation> </profile> Now open command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test Profile Activation via Present/Missing File Now activation element to include OS details as shown below. The test profile will triggerwhen target/generated-sources/axistools/wsdl2java/com/companyname/group is missing. <profile> <id>test</id> <activation> <file> <missing>target/generated-sources/axistools/wsdl2java/ com/companyname/group</missing> </file> </activation> </profile> Now open the command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test 8. Maven create and build artifacts: The first step is to open Eclipse, which comes with the integrated Maven environment. The Eclipse window opens on the screen. Complete the following steps: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 15 N32 nirf 175º Rank -

--- Page 89 ---
Course Code/Title:CS3V15/Devops Unit: III Key Features: · Source Code Management: Configures Jenkins to use Git repositories as the source code for jobs. Example Demo Step 1 : Create a simple Java program. I have created a folder named MyJavaPrograms and inside it created a simple Java program as follows - e.g., SSH test.java public class test { public static void main(String args[I) { for(int i=1;i <= 5;1++) System.out.printin("Welcome Anuradha"); } Plugin," Step 2 : Open the command prompt, switch to that folder and execute the above Java program. It is illustrated by following screenshot. 1 C:\Windows\System32\cmd.e X + Microsoft Windows [Version 10.0.22621.2134] (c) Microsoft Corporation. All rights reserved. 0 X before E:\MyJavaPrograms>javac test. java E: \MyJavaPrograms>java test Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha E: \MyJavaPrograms> Step 3 : Now we will create a Git repository and push this repository on GitHub. First of all we will initialise the Git repository my using git init command. CHENNAI INSTITUTE TECHNOLOGY 14 - N32 nirf 1750 Rank CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt)

--- Page 90 ---
Unit: III Course Code/Title:CS3V15/Devops C:\Windows\System32\cmd.e x X E:\MyJavaPrograms>git init Initialized empty Git repository in E:/MyJavaPrograms/.git/ E:\MyJavaPrograms>git status On branch master No commits yet Untracked files: (use "git add <file> ... " to include in what will be committed) test.class test. java nothing added to commit but untracked files present (use "git add" to track) E:\MyJavaPrograms> then add the java and class files to git repository - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git add . E:\MyJavaPrograms>git status On branch master No commits yet Changes to be committed: (use "git rm -- cached <file> ... " to unstage) new file: test.class new file: test. java E: \MyJavaPrograms> Now we will commit the changes - C:\Windows\System32\cmd.e X E:\MyJavaPrograms>git commit -m "First Commit for Java program" [master (root-commit) 4cb04db] First Commit for Java program 2 files changed, 6 insertions(+) create mode 100644 test.class create mode 100644 test. java E:\MyJavaPrograms> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) - INSTITUTE . TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 19 ---
Course Code/Title:CS3V15/Devops Unit: I 5. Compliance: By compliance, we mean that certain businesses are required to follow specific regulations. Financial services companies in the United States, for example, must comply with the Sarbanes-Oxley Act, while healthcare, education, and energy companies must comply with HIPAA and other regulations. AWS provides a number of compliance-related features and services, such as data encryption and identity and access management, to help businesses meet these requirements. 6. High-Performance: Interested in delivering your applications quickly and efficiently? Taking advantage of AWS features such as auto-scaling and load balancing will help ensure your applications are always available and running optimally. AWS can help businesses improve their performance by offering a variety of cloud-based services, including Amazon Elastic Compute Cloud (EC2), which provides high-performance computing resources, and Amazon CloudFront, which delivers content quickly and securely to users around the world. Others include machine learning (ML) and analytics services, such as Amazon SageMaker and Amazon Athena. These services provide the tools businesses need to quickly and easily analyze their data for insights. Fast networking in the cloud is also possible with AWS, thanks to its Elastic Load Balancing (ELB) and Amazon Virtual Private Cloud (VPC). With ELB, businesses can balance their workloads across multiple instances for increased performance, while VPC allows businesses to create isolated private networks in the cloud. 7. Developer Tools: Developer tools are designed to make it easier for developers to create, deploy, and manage applications - and AWS provides developers with what they need to build applications quickly and easily. By leveraging developer tools, developers can save time and money by automating tedious tasks. They also benefit from access to AWS's extensive library of pre-built applications that can help them get their projects off the ground quickly. Services such as Amazon Elastic Beanstalk and Amazon CloudFormation can help them automate the process of creating and deploying applications. Other ways developers can improve productivity with AWS include using AWS Code Commit to store and manage source code. 8. Integration: Thanks to its many integrations with other Amazon services, as well as third-party services, AWS makes it easy for businesses to get started with cloud computing. AWS provides a wide range of services that can be easily integrated into existing business infrastructure. This allows businesses to add new features and capabilities without having to make major changes or invest in new hardware or software. For instance, if a business wants to add mobile capabilities to its website, it can take advantage of Amazon's Mobile SDK and Web Services. These tools allow businesses to quickly develop and deploy mobile apps that connect directly with their existing infrastructure. 9. Management Console: The AWS management console is a web-based interface that provides users with a simple way to interact with and manage their AWS resources - essentially a place where you can access and manage everything on the cloud. It provides a graphical view of all the resources associated with an account, as well as tools for creating and configuring new resources. Compared to traditional command- line interfaces, the AWS management console saves time and makes it easier for users to get the most out of their AWS services. Not only that, but your business gets access to 350+ free digital training courses through the AWS Academy, covering topics such as cloud fundamentals, DevOps, security, and big data. This means you can train your employees on how to use AWS, and in turn, help them become more efficient at their jobs. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 19 N32 nirf 1750 Rank

--- Page 20 ---
Course Code/Title:CS3V15/Devops Unit: I 10. Scalability: With an on-demand service, businesses can quickly spin up new servers as needed with just a few clicks. This makes it much easier to scale resources up or down as demand changes, allowing businesses to save costs and maintain performance even during peak periods. For example, if a business is expecting a sudden surge in traffic due to an advertising campaign or seasonal event, they can easily add more capacity to their server infrastructure to handle the increased load. Bru Textiles, a specialty textile company in Belgium, was able to quickly scale its infrastructure by leveraging AWS. Bru Textiles went digital to grow and offer new services. Embracing technology, they brought in digital twin technology to give their customers an idea of the texture and essence of their physical fabrics. AWS Applications · Storage and Backup: Storage and backup are important for any Cloud Computing service. AWS provides you with reliable storage services like Amazon Simple Storage Service to store large- scale data and backup services like AWS Backup to take backups of this data, which is stored in other AWS services. AWS stores the data in three different availability zones so that if one fails, you can still access your data. This makes AWS storage reliable and easily accessible. Therefore, companies with huge application data to store and backup securely can use AWS. · Big Data: One of the biggest challenges faced by companies these days is Big Data. The companies are struggling to store their large amounts of data using traditional methods. With AWS Big Data storage services, they can manage to store their data even if the data limit increases unexpectedly as AWS provides virtually unlimited data storage with scale-in and scale- out options. AWS offers easy access and faster data retrieval as well. For data processing, it offers services like EMR, with which the companies can easily set up, operate, and scale their big data. Therefore, efficiently storing and managing Big Data is among the top AWS applications. · Enterprise IT: AWS is a one-stop solution for any IT business. Many features of it such as secure storage, scalability, flexibility, and elasticity support companies to innovate faster than ever before. Using AWS for IT enterprises makes them profitable in terms of both money and time. As AWS maintains its cloud architecture, it need not waste time and money on professionals to do the same. · Social Networking: Social networking is essential for businesses in the present-day scenario where Digital Marketing is key, and it is easier with AWS. Companies can connect with customers and stakeholders and communicate through social networking sites and develop their business. Services like AWS social networking engine, which is powered by Turn Key GNU/Linux (HVM) AMI stack, are used for performance and scalability to help companies build a suitable social networking site and gain profits. · Mobile Apps: Mobile applications are embedded with day-to-day life. With AWS, you have the facility to create an app in your desired programming language. You can also keep up the applications that are consistently accessible and solid with high compute, storage, database, and application services. You can take advantage of AWS auto-scaling and managed relational database service for the better performance of your apps. · Websites: AWS offers a wide range of website hosting options to create the best website for customers. Its services like Amazon Light sail have everything, such as a virtual machine, SSD- based storage, data transfer, DNS management, and a static IP, to launch a website in such a way that the user can manage the website easily. Amazon EC2, AWS Lambda, Elastic Load Balancing, AWS Amplify, Amazon S3, etc. also help users build reliable and scalable websites. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 95 ---
Course Code/Title:CS3V15/Devops Unit: III C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha C: \ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>exit e Finished: SUCCESS Step 6 : Now go to GitHub repository and make some changes to your test.java file and commit these changes. BE AnurdhaP Changes in the test java Code Blame 6 lines (6 loc) · 158 Bytes 89 Code 55% faster with GitHub Copilot > public class test { 2 public static void main(String args[]) { for(int 1=1;i <= 5;1++) 3 4 System.out.println("Good Morning Path !!! "); 5 3 6 } Step 7 : Now, if you check on the Jenkins page, it automatically executes the above updated test.java file and creates a build. It is as follows .- C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test. java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Good Morning Path !!! Good Morning Pathi !! Good Morning Path !!! Good Morning Path !!! Good Morning Pathill 2. Parameter Plugin Overview: The Parameter Plugin allows Jenkins jobs to accept parameters at build time, enabling dynamic and flexible builds. Key Features: · Parameter Types: Supports various parameter types such as string, choice, boolean, and more. · Default Values: Provides default values for parameters. · Prompt for Parameters: Users are prompted to enter values for parameters when starting a build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 20

--- Page 96 ---
Course Code/Title:CS3V15/Devops Unit: III Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Parameterized Builds," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add parameters of different types (e.g., string, choice) and configure their options and default values. Usage: · Useful for creating jobs that require user input or need to be customized for different build scenarios. A Example Demo Step 1 : Click on New item and create a job. I have created a job by the name Welcome (Refer section 3.5) Step 2 : Click on the existing job and configure it with parameters. Step 3 : Check the option "This project is parameterized". Select the String Parameter. Create some string parameter. I have created a parameter named Username and default value to it. This project is parameterized ? =String Parameter ? Name ? Username Default Value ? Parth Description ? Plain text Preview Trim the string ? Step 4 : Under the Build Steps section, using echo command I tried to display the parameter value CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 21

--- Page 57 ---
Course Code/Title:CS3V15/Devops · Go to the File option · In the drop-down menu, select New · Select the Project option Unit: II If you want to create a Java project, you can select the "Java Project" option. Since we are not creating a Java project specifically, we have chosen the "Project" option. eclipse-workspace - Eclipse File Edit Source Refactor Navigate Search Project Run Window Help New Open File ... G) Open Projects from File System ... Shift+Alt+N Close Close All Shift + Cb1+W Save Save As. Save All Shift+Ctrl+S Revert Move .:. Rename 2 Refresh ES Convert Line Delimiters To Print Import ... Export ... Ctrl+P de java Project ui Project ... El Package G Class Interface G Enum @ Annotation 69 Source Folder 18 Java Working Set Folder File Untitled Text File Task JUnit Test Case Example ... Other ... Ctrl+N The dialog box that appears on the screen will display different types of projects. · Select the Maven Project option . Click on Next New Project + Select a wizard Create a Maven Project Wizards: type filter text irroject Gradle Java 2º Java Project #Java Project from Existing Ant Buildfile - @Maven Lu Check out Maven Projects from SCM M& Maven Module Maven Project Examples ? < Back Next > X Cancel Finish A dialog box will appear. Select the default workspace. . Click on "Next" Several Group IDs, Artifact IDs, and Versions will then appear. · Select a plugin there and click on "Next" CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 16 N33 narf 1750 Rank

--- Page 58 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Select an Archetype M Catalog: All Catalogs Configure .. Biter: × Group Id Artifact id Version org.apache.maven.archetypes maven-archetype-archetype 1.0 org.apache.maven.archetypes maven-archetype-|2ee-simple 1.0 org.apache.maven archetypes maven-archetype-plugin 1.2 org.apache.maven.archetypes maven-archetype-plugin-site 1.1 org.apache.maven.archetypes maven-archetype-portlet 1.0.1 org.apache.maven.archetypes maven-archetype-profiles 1.0-alpha-4 org.apache.maven.archetypes maven-archetype-quickstart 1.1 org.apache.maven.archetypes maven-archetype-site 11 org anacha mauan archabenes masan.archchina.cito.cimnia 11 An archetype which contains a sample Maven project. Show the last version of Archetype only Include snapshot archetypes Add Archetype ... > Advanced ? < Back Next > Cancel Finish In the next dialog box that appears, you'll complete the following steps: · Enter the Group ID “com.xyz" · Enter the Artifact ID "mavenproject" · The version will appear on the screen These items can all be modified at a later time if needed. · Click on "Finish" CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 17 N33 nirf 1750 Rank

--- Page 59 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Specify Archetype parameters M Group Id: com.simplileam Artifact Id: mavenproject Version: 0.0.1-SNAPSHOT Package: com,simplilearn.mavenproject . Properties available from archetype: Name Value Add ... Remove Advanced ? < Back Next > Cancel Finish The project is now created. · Open the pom.xml file You can see all the basic information that you have entered on the screen, such as the Artifact ID, Group ID, etc. You can see the junit dependencies have been added. This process takes place by default in Eclipse. There will also be some by default test cases. File Edit Source Navigate Search Project Run Window Help # Package Explorer E Jo JUnit - mavenproject/pom.xml 28 mavenproject > ( src/main/java ( src/test/java > = JRE System Library (|2SE-1:5] i Maven Dependencies target 7 <artifactIdaavenproject</artifactId> <version>0. 0. 1-SNAPSHOT</version>: <packaging>jar</packaging> <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 136 sproperties> 14 <project. build.sourceEncoding>UTF-8</project. build.sourceEncoding> </properties> 15 16 «dependenciess 18- <dependency> 19 <groupId>junit</grouptasI <artifactId=junit</artifactle> sversion>3.8.1</version> <scope>tests/scope> 21 22 223 </dependency> 24 </dependencies> 25 </project> Overview Dependencies Dependency Hierarchy |Effective POM pom.xml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 18 N32 nirf 175º Rank -

--- Page 60 ---
Course Code/Title:CS3V15/Devops Unit: II There you can find AppTest.java to be a default test case. When you click on that, you can see the test cases written in JUnit on your Eclipse screen. # Package Explorer 13 Ju JUnit mavenproject/pom.xml Z App.java AppTest.java 12 1 package com.simplilearn.mavenproject; - fl mavenproject + ( src/main/java - il com.simplileam.mavenproject U App.java = ( src/test/java il com.simplileam.mavenproject , E AppTest.java > > JRE System Library ||2SE-1.5] al Maven Dependencies - target pom.xml 3w import jumit. framework. Test:[] 6 76 / ** . Unit test for simple App. 10 public class AppTest extends TestCase 12 1 14 . Create the test case 15 16 * @param testName name of the test cole 17 18% public AppTest( String testNane ) 19 1 super( testName ); 28 21 1 22 :23% 24 * @return the suite of tests being tested 75 When it comes to adding more test cases, it will depend on the user, but these test cases and commands can easily be added in the workspace. If we try to remove certain dependencies from our file, we will receive error messages. To troubleshoot this, complete the following steps: · Go to another tab: mavenproject/pom.xml · Delete any dependencies · Save the file Immediately, there will be several error messages in the AppTest.java. R mavenproject/pom.xml App.java AppTest.java 83 1 package com.simplilearn.mavenproject; 2 3@ import junit. framework. Test;[] 6 mavenproject/src/test/java/com/simplileam/mavenproject/AppTest java 8 * Unit test for simple App. 9 18 public class AppTest extends TestCase 011 12 { 136 14 . Create the test case 15 16 * @param testName name of the test case 17 188 19 € public AppTest( String testName ) 28 super( testName ); 21 22 - 230 24 * @return the suite of tests being tested Problems E @ Javadoc ) Declaration History 7 errors, 1 warning, 0 others Description + @ Errors (7 items) a Test cannot be resolved to a type Resource Path Location AppTest.java /mavenproject/src/te line 26 Java Problem Type CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 19 N32 nirf 1750 Rank -

--- Page 45 ---
Course Code/Title:CS3V15/Devops Unit: II Example for pom.xml file <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> 4. Maven Repository A maven repository is a directory of packaged JAR file with pom.xml file. Maven searches for dependencies in the repositories. There are 3 types of maven repository: 1. Local Repository 2. Central Repository 3. Remote Repository Maven searches for the dependencies in the following order: Local repository then Central repository then Remote repository. Local Central Remote Repository Repository Repository CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 4 N32 nirf 1750 Rank -

--- Page 46 ---
Course Code/Title:CS3V15/Devops Unit: II If dependency is not found in these repositories, maven stops processing and throws an error. 1) Maven Local Repository Maven local repository is located in your local system. It is created by the maven when you run any maven command. By default, maven local repository is %USER_HOME%/.m2 directory. For example: C:\Users\SSS IT\.m2. Update location of Local Repository We can change the location of maven local repository by changing the settings.xml file. It is located in MAVEN_HOME/conf/settings.xml, for example: E:\apache-maven- 3.1.1\conf\settings.xml. 2) Maven Central Repository Maven central repository is located on the web. It has been created by the apache maven community itself. The path of central repository is: http://repo1.maven.org/maven2/. The central repository contains a lot of common libraries that can be viewed by this url http://search.maven.org/#browse. 3) Maven Remote Repository Maven remote repository is located on the web. Most of libraries can be missing from the central repository such as JBoss library etc, so we need to define remote repository in pom.xml file. Let's see the code to add the jUnit library in pom.xml file. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 5 N32 narf 1750 Rank -

--- Page 33 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between AWS (Amazon Web Services), Google Cloud, and Azure Technology EC2 (Elastic Compute Cloud) AWS Databases Supported Pricing Models Difficulties Many enterprises find it difficult to understand the company cost AWS fully relational and supports NoSQL databases and Big Data. Per hour - rounded up. On demand, reserved spot. structure. Google Cloud Google Engine(GCE) Compute Technologies pioneered by Google, like Big Query, Big Table, and Hadoop, are databases, and Big Data,naturally fully supported. Per minute - rounded up On demand sustained use. — Fewer services. features Azure VHD Disk) (Virtual Hard Azure supports relational and both NoSQL Windows and through AzureTable HDInsight. Per minute - rounded up. Per minute- rounded up commitments(Pre-paid or monthly) and Less "Enterprise-ready. Storage Services Simple Storage Service(S3) Elastic Block Storage. Elastic File storage. Machine Learing Sage maker. Lex. polly.And many more Blob Storage Queue Storage. File Storage Disk Storage. Data Lake Store Machine learning Azure Bot service Cognitive service GIT Cloud storage. Persistent Disk Transfer appliance. Cloud speech AI Cloud Video Intelligence. Cloud Machine learning engine Git is a distributed version control system (DVCS) that helps manage and track changes in source code during software development. It was created by Linus Torvalds in 2005 and has become one of the most widely used version control systems in the software development industry. Some key concepts and features of Git: · Version Control: Git allows developers to keep track of changes made to their code over time. This includes modifications, additions, and deletions of files. · Distributed System: Git is a distributed version control system, meaning that each developer has a complete copy of the entire repository, including its full history. This allows developers to work independently and merge their changes when necessary. · Branching: Git enables developers to create branches, which are essentially separate lines of development. This allows for the parallel development of features or bug fixes without affecting the main codebase. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) LEN 33 nirf 1750 Rank

--- Page 34 ---
Course Code/Title:CS3V15/Devops Unit: I · Merging: Git provides tools for merging changes from one branch into another. This is essential when multiple developers are working on different branches and need to bring their changes together. · History Tracking: Git maintains a complete history of changes made to the codebase. Developers can view, revert, or analyze changes made over time. · Remote Repositories: Git supports remote repositories, allowing developers to collaborate with others by pushing and pulling changes to and from a shared repository. Platforms like GitHub, GitLab, and Bitbucket provide hosting services for Git repositories. · Staging Area: Git uses a staging area (also known as the index) to prepare and review changes before committing them to the repository. This allows developers to selectively include or exclude specific changes. · Open Source: Git is an open-source tool, and its source code is freely available for modification and distribution. Various GIT Components: Git is composed of several key components that work together to enable version control and collaborative development. Here are the main components of Git: · Repository (Repo): A repository is a directory or storage space where your project and its version history are stored. It contains all the files and directories associated with your project, along with the metadata and configuration information. · Working Directory: The working directory is the directory on your local machine where you manipulate files and make changes to your project. It is essentially your local copy of the repository. · Index (Staging Area): The index, also known as the staging area, is a middle ground where changes are prepared before being committed to the repository. It allows you to selectively stage changes, which means you can choose which modifications to include in the next commit. · Commit: A commit is a snapshot of the changes made to the files in the repository. It represents a specific point in the project's history and is accompanied by a commit message that describes the changes. · Branch: A branch is a parallel line of development within a repository. It allows developers to work on different features or bug fixes simultaneously without affecting the main codebase. Branches can be merged to incorporate changes into other branches. . Head: HEAD is a reference to the latest commit in the currently checked-out branch. It essentially points to the tip of the branch you are currently on. · Remote: A remote is a version of the repository stored on a different server. Git supports collaboration by allowing developers to push and pull changes between their local repository and remote repositories. Platforms like GitHub, GitLab, and Bitbucket are examples of remote repositories. · Clone: Cloning is the process of creating a copy of a remote repository on your local machine. This allows you to start working on your own copy of a project. · Fetch: The fetch operation retrieves changes from a remote repository but does not automatically merge them into your working directory. It is useful for reviewing changes before deciding to merge. · Pull: Pull is a combination of fetch and merge. It retrieves changes from a remote repository and automatically merges them into your working directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 65 ---
Course Code/Title:CS3V15/Devops Unit: II Output The output is as follows - java version "1.8.0 66" Java(TM) SE Runtime Environment (build 1.8.0_66-b18) Java HotSpot(TM) 64-Bit Server VM (build 25.66-b18, mixed mode) Step 2 - Download Gradle Build File Download the latest https://gradle.org/install/. version of Gradle from the link available at Step 3 - Set Up Environment for Gradle Setting up environment means, we have to extract the distribution file and copy the library files into proper location. Set up GRADLE_HOME and PATH environmental variables. This step is platform dependent. In Windows Extract the downloaded zip file named gradle-2.11-all.zip and copy the distribution files from Downloads\gradle-2.11\ to C:\gradlel location. After that, add the C: \gradle and C: \gradlelbin directories to the GRADLE_HOME and PATH system variables. Follow the given instructions - Right Click On My Computers -> Click On Properties -> Advanced System Settings -> Click On Environmental Variables. There you will find a dialog box for creating and editing system variables. Click on new button for creating GRADLE_HOME variable (follow the left side screenshot). Click on Edit for editing the existing Path system variable (follow the right side screenshot). Follow the below given screenshots. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 24 N33 nirf 1750 Rank

--- Page 66 ---
Course Code/Title:CS3V15/Devops System Properties X Computer Name Hardware Advanced System Protection Remote Environment Variables x Unit: II System Properties × Computer Name |Hardware Advanced System Protection Remote Environment Variables × New System Variable × Variable name: GRADLE_HOME| Variable value: C:\grade Cancel System variables Variable Value ComSpec C:\Windows\system32\and.exe FP_NO_HOST_C ... NO GRADLE_HOME E:\work\22- Orientdb \gradle-2.11 Variable name: Path Variable value: #C:\gradle bin - System variables Variable Value Edit System Variable × OK Cancel < ORIENTDB_HOME E:\work\22- Orientdblorientdb-communi ... " JAVA_HOME C:\Program Files\Java\jdk1.7.0_60 New ... Edt ... Delete OK Cancel Step 4 - Verify the Gradle installation In windows os Windows_NT Path C:\oraclexe \app \prade\product\11.2.0\, .. PATHEXT .COM ;. EXE ;. BAT ;. CMD ;. VBS ;. VBE ;. JS ;.... v New ... Edit ... Delete OK Cancel You can execute the following command in command prompt. C:\> gradle -v Output Here you will find the Gradle version. Gradle 2.11 Build time: 2016-02-08 07:59:16 UTC Build number: none Revision: 584db1c7c90bdd1de1d1c4c51271c665bfcba978 Groovy: 2.4.4 CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 25 N33 nirf 1750 Rank

--- Page 111 ---
Course Code/Title:CS3V15/Devops Unit: III Steps: 1. Install the Git Plugin: 0 Go to Manage Jenkins > Manage Plugins. ○ Under the Available tab, search for Git Plugin and install it. 2. Configure Git in Jenkins: ○ Navigate to Manage Jenkins> Global Tool Configuration. ○ Scroll down to the Git section and click Add Git. ○ Specify the path to the Git executable or use the default path. ○ Optionally, configure additional settings such as Git installation locations. 3. Example Configuration: ○ Name: Git ○ Path to Git executable: /usr/bin/ git (or use the default if Git is installed in a standard location). 4. Configure Job to Use Git: ○ Create or edit a Jenkins job and go to Source Code Management. ○ Select Git. ○ Enter the repository URL (e.g., https://github.com/user/repository.git). ○ Configure credentials if the repository is private. ○ Specify the branch to build (e.g., main). 5. Example Repository URL: ○ Repository URL: https://github.com/example/repo.git o Branch Specifier: main 3. Configuring Jenkins to Work with Maven Overview: The Maven Plugin integrates Jenkins with Apache Maven, allowing Jenkins to use Maven to build projects. Steps: 1. Install Maven: ○ Ensure that Apache Maven is installed on your system. You can download it from the Apache Maven website. 2. Configure Maven in Jenkins: ○ Go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to the Maven section and click Add Maven. ○ Enter a name for the Maven installation (e.g., Maven 3.8.6). ○ Check Install automatically to let Jenkins download Maven, or specify the path to an existing Maven installation. ○ CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 36 1750 Rank

--- Page 112 ---
Unit: III Course Code/Title:CS3V15/Devops 3. Example Configuration: ○ Name: Maven 3.8.6 ○ MAVEN_HOME: / usr / share / maven 4. Configure Maven in a Jenkins Job: ○ Create or edit a Jenkins job and go to Build section. 0 Add a build step and select Invoke top-level Maven targets. o Choose the Maven version configured earlier. ○ Specify the goals to run (e.g., clean install). 5. Example Maven Goals: ○ Goals: clean install 6. Configure Build Environment: ○ Ensure that the job's build environment is set up to use Maven. This might involve setting environment variables or configuring build scripts. 7. Example Environment Variables: ○ MAVEN_OPTS: - Xms512m -Xmx2048m (if you need to set specific JVM options for Maven). By following these steps, Jenkins will be properly configured to work with Java, Git, and Maven, allowing you to set up and manage continuous integration and delivery pipelines effectively. If you need further customization or run into issues, consulting the documentation for each tool or plugin can provide additional guidance. Creating a Jenkins Build and Understanding Jenkins Workspace 1. Creating a Jenkins Build Overview: A Jenkins build is a process where Jenkins executes a series of steps defined in a job configuration. This typically involves compiling code, running tests, and generating artifacts. Steps to Create a Jenkins Build: 1. Access Jenkins Dashboard: ○ Open your Jenkins dashboard, usually found at http://localhost: 8080. 2. Create a New Job: ○ Click on New Item on the left sidebar. Enter a name for your job. ○ Choose a job type. Common types include: I Freestyle project: For simple build jobs with basic configuration. · Pipeline: For more complex build processes using a Jenkinsfile. ○ Click OK to proceed. ○ TECHNOLOGY N33 37 nirf 1750 Rank CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous)

--- Page 7 ---
Course Code/Title:CS3V15/Devops Unit: I DevOps Tools 1. Git (GitLab, GitHub, Bitbucket) Git remains indispensable in software development and DevOps due to its pivotal role in version control, collaborative coding, and efficient project management. As technology has accelerated, the need for streamlined and organized code management has never been greater. Git empowers developers to collaborate on codebases, effortlessly creating and merging branches for new features and bug fixes. Its distributed nature ensures developers can work seamlessly offline, an increasingly valuable feature in today's remote and distributed work environments. Additionally, Git facilitates the tracking of code modifications, making it easier to identify when and why specific changes were made, a critical aspect of maintaining code quality and security. Software development is essential in driving innovation and advancing progress, and Git maintains its prominent position as the bedrock of efficient, cooperative, and secure coding methodologies. 2. Maven Due to its enduring significance in managing project dependencies, building, and project lifecycle management, Maven remains a pivotal tool in SD and DevOps. As a robust build automation and project management tool, Maven simplifies the complexities of Java-based project development by streamlining the compilation, testing, packaging, and distribution processes. It ensures consistent and reproducible builds, making it easier for development teams to collaborate efficiently and deliver high-quality software. Maven's role in managing dependencies and facilitating continuous integration and deployment remains crucial. Its ability to handle complex build scenarios and integrate seamlessly with modern DevOps practices makes it indispensable for ensuring software projects' reliability, maintainability, and scalability in 2024 and beyond. 3. Jenkins Its importance lies in its role as a powerful automation server that enables continuous integration and continuous delivery (CI/CD) pipelines. Jenkins streamlines software development by automating tasks such as building, testing, and deploying code changes, ensuring that software is delivered quickly and highly. With the growing complexity of modern applications, the need for efficient CI/CD processes has become even more paramount. Jenkins provides flexibility, extensibility, and a vast library of plugins that cater to a wide range of technologies and tools, making it adaptable to diverse development environments. As organizations prioritize speed, reliability, and collaboration in their software development practices, Jenkins stands as a cornerstone tool, enabling teams to achieve seamless automation and efficient delivery of software solutions. 4. Chef Chef, a powerful automation platform, is crucial in managing infrastructure as code. Chef empowers organizations to achieve scalability, reliability, and speed seamlessly. By allowing the automation of server provisioning, configuration, and maintenance, Chef enhances efficiency and consistency across the entire infrastructure, reducing manual errors and ensuring that infrastructure remains desired. Moreover, Chef integrates smoothly with various cloud providers, containerization technologies, and other DevOps tools, making it adaptable to the ever-evolving tech TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 7 N32 nirf 1750 Rank

--- Page 8 ---
Course Code/Title:CS3V15/Devops Unit: I landscape. As organizations prioritize agility and scalability, Chef remains a vital tool in automating complex infrastructure tasks and enabling DevOps teams to focus on innovation and delivery. 5. Puppet Puppet is essential because it simplifies the management and orchestration of complex IT infrastructures by allowing administrators to define infrastructure as code. It ensures consistency and repeatability in configuration across servers, cloud instances, and containers. Businesses increasingly rely on diverse, dynamic, and hybrid infrastructures. Puppet's importance lies in its ability to streamline provisioning, configuration, and continuous compliance, thus reducing operational complexity, minimizing errors, and accelerating software delivery. Puppet continues to empower organizations to efficiently manage and scale their infrastructure while maintaining high levels of security and compliance, making it a crucial tool for DevOps teams. 6. Ansible Ansible is a powerful and widely adopted automation and configuration management tool important in 2024 for several reasons. This tool stands out for its simplicity and versatility. It empowers organizations to automate repetitive tasks, provisioning of infrastructure, and configuration management across diverse environments, making it an invaluable asset for DevOps and IT teams. Furthermore, Ansible's agentless architecture, declarative language, and a vast library of pre- built modules make it accessible to both beginners and seasoned professionals. As organizations prioritize efficiency, scalability, and the rapid deployment of applications and services, Ansible remains an indispensable DevOps toolkit, helping teams streamline operations, enhance security, and maintain infrastructure at scale, all while reducing manual errors and increasing agility in a fast-paced technological landscape. 7. Docker Docker is crucial in modern software development and DevOps practices. It can simplify and streamline the management of applications across various environments. Docker containers encapsulate an app and its dependencies, ensuring consistent and reproducible deployments from development to production. This technology enhances portability and scalability, accelerates development cycles, and reduces the "it works on my machine" problem. In a rapidly evolving software landscape, Docker's containerization approach remains crucial for achieving efficient, isolated, and highly flexible application deployment, making it an essential component of DevOps and continuous delivery pipelines. 8. Kubernetes Kubernetes, often abbreviated as K8s, play a central role in modern software development and operations. Its importance lies in its ability to orchestrate, manage, and automate containerized applications at scale. As organizations increasingly embrace microservices architectures and containerization for their applications, Kubernetes provides the essential infrastructure for deploying, scaling, and maintaining these containers efficiently. The tool's resilience, self-healing capabilities, and support for hybrid and multi-cloud environments make it vital for achieving agility, reliability, and cost-effectiveness in application deployment. It serves as the backbone of cloud-native ecosystems, enabling organizations to accelerate software delivery, improve resource utilization, and respond effectively to the evolving demands of the digital landscape. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N32 nirf 1750 Rank

--- Page 11 ---
Course Code/Title:CS3V15/Devops Unit: I 17. Selenium It remains a vital tool in software testing and automation due to its enduring relevance in ensuring the quality of web applications. As technology evolves, web applications become increasingly complex, requiring thorough testing across various browsers and platforms. With its robust automation capabilities and extensive browser compatibility, Selenium allows developers and QA teams to automate repetitive testing tasks efficiently, conduct cross- browser testing, and ensure that web applications function flawlessly across diverse environments. Its open-source nature, active community support, and integration with other DevOps tools make Selenium a go-to choice for organizations striving for continuous delivery and the rapid deployment of high-quality software, a cornerstone of modern software development practices. 18. Gremlin Gremlin is an essential tool in chaos engineering, which has become increasingly critical for ensuring the resilience and reliability of modern software systems. As technology advances and complex distributed systems become the norm, the potential for unexpected failures and outages also rises. Gremlin allows organizations to proactively identify weaknesses and vulnerabilities in their infrastructure and applications by simulating controlled failures, such as network disruptions, service outages, and resource constraints. By intentionally inducing chaos and monitoring the system's response, teams can uncover weaknesses before they lead to costly downtime or security breaches. Gremlin facilitates organizations to build more robust, fault-tolerant systems that can withstand real-world challenges and deliver uninterrupted services to users. 19. ServiceNow ServiceNow is a vital platform for organizations seeking to streamline their IT service management and beyond. Its significance lies in its ability to provide a unified, cloud-based solution for automating and optimizing various business processes, including ITSM, ITOM, HR, customer service, and more. Due to the rapid digitization of services, remote work, and the growing complexity of technology infrastructures, ServiceNow offers a comprehensive approach to managing workflows, resolving issues, and delivering services efficiently. Its intelligent automation capabilities, analytics, and AI-driven insights empower organizations to enhance productivity, agility, and customer satisfaction while reducing operational costs. ServiceNow's role in orchestrating and integrating diverse systems and processes makes it an indispensable tool for driving digital transformation and ensuring smooth operations in the ever-evolving business landscape of 2024. 20. Status Service Updates: The Status Page "Status Service Updates: The Status Page" is a critical tool for organizations and businesses of all sizes. In today's world, where online services and applications are integral to operations, ensuring their availability and reliability is essential. It provides real-time information to users and stakeholders about the operational status of services, applications, and infrastructure. The Status Page plays a crucial role in transparency, trust-building, and customer satisfaction by promptly communicating service disruptions, planned maintenance, and incident resolutions. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 1750 Rank

--- Page 12 ---
Course Code/Title:CS3V15/Devops Unit: I Downtime can often lead to significant financial losses and damage to a company's reputation, so having a practical Status Page becomes not just a convenience but a necessity. It allows organizations to showcase their commitment to transparency and responsiveness in addressing service-related issues, ultimately fostering stronger customer relationships and trust. 21. ELK (Elasticsearch, Logstash and Kibana) ELK, which stands for Elasticsearch, Logstash, and Kibana, continues to shine in DevOps and IT operations. This powerful trio of tools remains essential for organizations seeking effective log management, monitoring, and data visualization. Elasticsearch is a highly scalable and fast search engine that enables real-time data indexing and search. Logstash facilitates the collection, processing, and transformation of log data from various sources, making it compatible with Elasticsearch. Kibana, on the other hand, provides a user- friendly interface for visualizing and analyzing data, offering customizable dashboards and powerful data exploration capabilities. ELK's significance in 2024 lies in its ability to empower organizations with comprehensive insights into their systems, applications, and infrastructure. It ultimately facilitates quick problem resolution, proactive monitoring, and data-driven decision-making in an increasingly complex and fast-paced technological landscape. 22. GitLab CI/CD GitLab CI/CD's significance lies in its ability to automate the complete software delivery pipeline, from code changes to deployment, in a single integrated environment. GitLab CI/CD ensures rapid and reliable delivery of software updates. It enables continuous integration (CI) by automatically building and testing code changes, allowing teams to catch issues early in the development cycle. Furthermore, the continuous deployment (CD) aspect automates the release and deployment process, reducing the risk of human errors and enabling organizations to deliver features and updates to users swiftly and confidently. GitLab CI/CD's importance is further accentuated as businesses seek to accelerate digital transformation efforts, respond rapidly to changing market demands, and maintain a competitive edge through efficient and automated software delivery practices. 23. Scripting Scripting remains vital due to its pivotal role in automating and streamlining various aspects of software development, system administration, and DevOps practices. Scripting languages like Python, Bash, and PowerShell empower tech professionals to write code that can execute repetitive tasks, manipulate data, and orchestrate complex processes efficiently. Scripting facilitates rapid prototyping, configuration management, and the creation of automated deployment pipelines. It enhances productivity, ensures consistency and reduces human error in tasks ranging from software testing and deployment to infrastructure provisioning and monitoring. As organizations increasingly embrace DevOps and cloud- native technologies, scripting stays competitive and adaptive in the tech landscape. 24. Terraform Terraform plays a crucial role in modern infrastructure provisioning and management. It allows organizations to define and deploy infrastructure as code, enabling the automated creation and configuration of cloud resources, containers, and other infrastructure components. Cloud computing, microservices, and containerization have become the norm in CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 12 N32 nirf 1750 Rank

--- Page 71 ---
Unit: II Course Code/Title:CS3V15/Devops build.gradle &3 I 1 /* 2 * This file was generated by the Gradle 'init' task. 3 * 4 * This generated file contains a sample Java Library project to get you started. 5 * For more details take a look at the Java Libraries chapter in the Gradle 6 * User Manual available at https://docs.gradle.org/6.0.1/userguide/java_library_plugin.html 7 */ 8 9 plugins { 10 // Apply the java-library plugin to add support for Java Library 11 id 'java-library' 12 } 13 14 repositories { 15 // Use jcenter for resolving dependencies. 16 // You can declare any Maven/Ivy/file repository here. 17 jcenter() 18 } 19 20 dependencies { 21 // This dependency is exported to consumers, that is to say found on their compile classpath. 22 api 'org. apache. commons : commons-math3: 3.6.1' 23 24 // This dependency is used internally, and not exposed to consumers on their own compile classpath. 25 implementation 'com. google. guava: guava: 28.0-jre' 26 27 // Use JUnit test framework 30 28 testImplementation 'junit : junit: 4.12' 29 } 30 The build.gradle file contains three default sections. They are as follows: o plugins: In this section, we can apply the java-library plugin to add support for java library. o Repositories: In this section, we can declare internal and external repository for resolving dependencies. We can declare the different types of repository supported by Gradle like Maven, Ant, and Ivy. · Dependencies: In this section, we can declare dependencies that are necessary for a particular subject. Additionally, we can declare other project-related modules like a task in this file. Display the Information of the Gradle project To understand the structure, dependencies and debugging problems of a build, Gradle provides many built-in features that display information on a project. Following are some basic commands to display the information of the project: Listing projects In Gradle, all the sub-projects of a project in the workspace can be listed in a hierarchy. To do so, run the below command from the root directory of the project. 1. gradle -q projects CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 72 ---
Course Code/Title:CS3V15/Devops Unit: II To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> C:\Users \HiMaNshU\eclipse-workspace >gradle -q projects Root project Root project 'eclipse-workspace' No sub-projects To see a list of the tasks of a project, run gradle <project-path> : tasks For example, try running gradle :tasks C:\Users\HiMaNshU\eclipse-workspace> Listing Tasks Gradle allows us to list all the essential tasks of the project. To list the task, run the below command: 1. gradle -q tasks Output: C:\Users\HiMaNshUleclipse-workspace>gradle -q tasks Tasks runnable from root project Build Setup tasks CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 31 N33 nirf 1750 Rank

--- Page 81 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Credentials: Add credentials if required. ○ Branch Specifier: Specify the branch (e.g., */main). 7. Build Triggers: ○ Build Periodically: Use CRON syntax to schedule builds. ○ Poll SCM: Jenkins will check the repository for changes at specified intervals. ○ Other Triggers: Configure other triggers such as GitHub hooks, upstream projects, etc. 8. Build Environment: o Configure the build environment settings like setting environment variables, running scripts before the build, etc. 9. Build Steps: ○ Click on "Add Build Step" and choose the appropriate build step (e.g., "Execute Shell", "Invoke Ant", "Invoke Gradle script"). ○ Example: For executing a shell script, add the necessary shell commands. 10. Post-build Actions: ○ Click on "Add post-build action" and choose the appropriate action (e.g., "Archive the artifacts", "Publish JUnit test result report"). ○ Configure the settings for each post-build action. 11. Save the Configuration: ○ Once you have configured all the necessary settings, click "Save" at the bottom of the page. 12. Run the Job: ○ On the job's main page, click "Build Now" to run the job immediately. 13. Monitor the Job: ○ Click on the build number in the "Build History" to view the detailed output and logs of the build process. ○ Check the console output for any errors or warnings. Practical Example Setting up a Freestyle Project for a Maven Build: 1. Create New Item: ○ Name: MyMavenProject o Type: Freestyle project 2. Source Code Management: ○ Git: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 82 ---
Unit: III Course Code/Title:CS3V15/Devops I Repository URL: https://github.com/example/my-maven-project.git · Branch Specifier: * /main 3. Build Triggers: ○ Poll SCM: H/15 * (poll every 15 minutes) 4. Build Steps: ○ Invoke Top-Level Maven Targets: Goals: clean install 5. Post-build Actions: ○ Publish JUnit test result report: I Test report XMLs: * /target/surefire-reports/ *. xml 6. Save and Build: ○ Click "Save". ○ Click "Build Now" on the job's main page. By following these steps, you can set up a Jenkins job to automate your build process for various types of projects. Jenkins provides extensive customization options, allowing you to tailor the job to meet your specific requirements. Configuring a Jenkins Job 1. Install Jenkins First, make sure Jenkins is installed. You can download it from the official Jenkins website. 2. Access the Jenkins Dashboard Open your web browser and go to http://your-server-ip: 8080 to access the Jenkins dashboard. 3. Create a New Job 1. Start a New Job: ○ Click on "New Item" on the left-hand menu. 2. Name the Job: ○ Enter a name for your job in the "Enter an item name" field. 3. Select Job Type: ○ Choose the type of job (e.g., Freestyle project, Pipeline). CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) - TECHNOLOGY N32 nirf 7 1750 Rank

--- Page 27 ---
Course Code/Title:CS3V15/Devops Unit: I · Servers are maintained easily and there is nearly zero downtime · Users can access cloud data and upload it on the cloud from any device with a solid internet connection · Cloud environments can be modified according to the user's requirements and is easily accessible · Clouds are highly secure, making data breaches more unlikely · Migrating to the cloud eliminates the need to buy on-premises infrastructure . It offers pay-as-you-go pricing, meaning you only pay for the resources you use 1. Infrastructure as a Service: IaaS delivers virtualized computing resources over the Internet. Users can rent virtual machines, storage, and networking infrastructure, allowing for easy scalability without investing in physical hardware. Examples include AWS EC2 and Azure Virtual Machines. 2. Platform as a Service: PaaS offers a robust platform for developers to build, deploy, and manage apps without worrying about the underlying infrastructure. It simplifies application development and deployment, with services like Google App Engine and Heroku leading the way. 3. Software as a Service: SaaS offers software applications on a subscription basis, accessible via a web browser. Users don't need to install or maintain software locally, making it ideal for collaboration tools (e.g., Microsoft 365, Google Workspace) and CRM systems (e.g., Salesforce). 4. Function as a Service: FaaS allows developers to execute code responding to events without managing servers. It's highly scalable and cost-efficient, exemplified by AWS Lambda and Azure Functions. FaaS is also known as serverless computing. 5. Container as a Service: CaaS enables the deployment and management of containerized applications using orchestration tools like Kubernetes. It provides portability and scalability for applications across different cloud environments. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 27 N33 nif 1750 Rank

--- Page 28 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between main cloud computing services Terms Stands for Uses Access Model Technical understanding. Popularity Percentage rise Usage Cloud services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY IAAS Infrastructure as a service. IAAS is used by network architects. IAAS gives access to the resources like virtual machines and virtual storage. It is a service model that provides virtualized computing resources over the internet. It requires technical knowledge. It is popular among developers and researchers. It has around a 12% increment. Used by the skilled developer to develop unique applications. Amazon Web Services, sun, vCloud Express. PAAS Platform as a service. PAAS is used by developers. PAAS gives access to run time environment to deployment and development tools for application. It is a cloud computing model that delivers tools that are used for the development of applications. Some knowledge is required for the basic setup. It is popular among developers who focus on the development of apps and scripts. It has around 32% increment. Used by mid-level developers to build applications. Facebook, and Google search engine. SAAS Software as a service. SAAS is used by the end user. SAAS gives access to the end user. It is a service model in cloud computing that hosts software to make it available to clients. There is no requirement about technicalities company handles everything. It is popular among consumers and companies, such as file sharing, email, and networking. It has about a 27 % rise in the cloud computing model. Used among the users of entertainment. MS Office web, Facebook and Google Apps. 28 N32 nirf 1750 Rank

--- Page 1 ---
Course Code/Title:CS3V15/Devops CHENNAI INSTITUTE OF TECHNOLOGY Transforming Lives CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) NAME OF THE PROGRAMME B. E (CSE) YEAR III SEMESTER V REGULATIONS 2022R COURSE CODE CS3V15 COURSE NAME DEVOPS FACULTY NAME (Prepared by) NAME OF SUBJECT EXPERT (Verified by) M.SUNDHARI CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Unit: I N33 INATIONAL BOARD . ACCREDITATION 100% Accreditation All eligible UG & PG Programs NIRF 151 - 200 Band Engineering 2023 A+ DE NAAC Contact 8681927167 Contact 1 N32 nirf 1750 Rank

--- Page 2 ---
Course Code/Title:CS3V15/Devops Unit: I UNIT - I Introduction to Devops Devops Essentials - Introduction To AWS, GCP, Azure - Version control systems: Git and Github. The word "DevOps" was coined in 2009 by Patrick Debois, who became one of its gurus. The term was formed by combining "development" and "operations," which provides a starting point for understanding exactly what people typically mean when they say "DevOps." Notably, DevOps isn't a process or a technology or a standard. Many devotees refer to DevOps as a "culture"-a viewpoint that New Relic favors. We also use the term "DevOps movement" when talking about topics such as adoption rates and trends for the future, and "DevOps environment" to refer to an IT organization that has adopted a DevOps culture. "DevOps represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile, lean practices in the context of a system-oriented approach. DevOps emphasizes people (and culture), and seeks to improve collaboration between operations and development teams. DevOps implementations utilize technology- especially automation tools that can leverage an increasingly programmable and dynamic infrastructure from a life cycle perspective." DevOps is defined as a combination of processes and tools created to facilitate organizations in delivering services and applications much faster than they can through conventional software development processes. It helps increase customers' confidence in the applications that an organization offers, thereby allowing the company to flourish and achieve its business goals faster. Development 3 3 DevOps 13 51 Operations Quality Assurance (QA) How DevOps Works? A DevOps process can be summarized as an infinite loop that comprises the following stages - build, test, and release through the delivery pipeline and plan and monitor through feedback, which resets the loop again. With such an amazing combination, teams use tech stack and tooling that assists them in reliably developing apps. Moreover, going away from the Under the DevOps model, development and operations teams work in constant cohesion throughout the entire project lifecycle, starting right from development to deployment. When security is the main focus, the quality assurance team is tightly knitted with the DevOps team throughout the app lifecycle. In this situation, some DevOps teams are also referred to as DevSecOps. Close coordination with the QA team ensures that no loopholes are left unchecked in the provided service/app. COMPANY HOW DEVOPS WORKS DELIVERY PIPELINE BUILD TEST RELEASE PLAN MONITOR FEEDBACK LOOP TOOLBOX" CUSTOMER norm, teams use automated processes here. DevOps tools also allow engineers to complete different tasks independently. Be it provisioning infrastructure or deploying code, they can accomplish these tasks without being dependent on one another. As such, the DevOps model accelerates the overall application development process. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 17 ---
Course Code/Title:CS3V15/Devops Unit: I that failed in a test suite gets simpler. Also, we can schedule the execution of the test cases at predefined times. After testing, the code is continuously integrated with the existing code. 4) Continuous Monitoring Monitoring is a phase that involves all the operational factors of the entire DevOps process, where important information about the use of the software is recorded and carefully processed to find out trends and identify problem areas. Usually, the monitoring is integrated within the operational capabilities of the software application. 5) Continuous Feedback The application development is consistently improved by analyzing the results from the operations of the software. This is carried out by placing the critical phase of constant feedback between the operations and the development of the next version of the current software application. The continuity is the essential factor in the DevOps as it removes the unnecessary steps which are required to take a software application from development, using it to find out its issues and then producing a better version. It kills the efficiency that may be possible with the app and reduce the number of interested customers. 6) Continuous Deployment In this phase, the code is deployed to the production servers. Also, it is essential to ensure that the code is correctly used on all the servers. The new code is deployed continuously, and configuration management tools play an essential role in executing tasks frequently and quickly. Here are some popular tools which are used in this phase, such as Chef, Puppet, Ansible, and SaltStack. Containerization tools are also playing an essential role in the deployment phase. Vagrant and Docker are popular tools that are used for this purpose. These tools help to produce consistency across development, staging, testing, and production environment. They also help in scaling up and scaling down instances softly. Containerization tools help to maintain consistency across the environments where the application is tested, developed, and deployed. There is no chance of errors or failure in the production environment as they package and replicate the same dependencies and packages used in the testing, development, and staging environment. It makes the application easy to run on different computers. 7) Continuous Operations All DevOps operations are based on the continuity with complete automation of the release process and allow the organization to accelerate the overall time to market continuingly. It is clear from the discussion that continuity is the critical factor in the DevOps in removing steps that often distract the development, take it longer to detect issues and produce a better version of the product after several months. With DevOps, we can make any software product more efficient and increase the overall count of interested customers in your product. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 17 N32 nirf 1750 Rank

--- Page 18 ---
Course Code/Title:CS3V15/Devops Unit: I AWS AWS stands for Amazon Web Services, It is an expanded cloud computing platform provided by Amazon Company. AWS provides a wide range of services with a pay-as-per-use pricing model over the Internet such as Storage, Computing power, Databases, Machine Learning services, and much more. AWS facilitates for both businesses and individual users with effectively hosting the applications, storing the data securely, and making use of a wide variety of tools and services improving management flexibility for IT resources. Advantages & Features of AWS: 1. Cost savings: One of the biggest benefits of AWS is that it can help businesses save money. As mentioned previously, businesses can avoid the high upfront costs of traditional infrastructure with AWS and pay only for the resources they use. Traditionally, businesses had to invest in hardware and software upfront, which often led to overspending. Let's look at this for example - if a business needs to run a website that gets 1000 visitors per day, they would need to purchase and maintain enough servers to support this traffic. With AWS, the business only pays for the compute resources they use when someone visits their website. This can result in significant cost savings. 2. Flexibility: Another key benefit of AWS is its flexibility. Businesses are able to customize their virtual environment - whether the operating system, database, programming language, or something else - to meet their specific needs. Especially in today's climate, the migration process to the cloud should be as frictionless as possible - and AWS makes that possible. Regardless of your use case or industry, AWS can be tailored to fit your needs, whether you're looking for a single cloud- hosted application or an entire suite of integrated solutions. 3. Reliability: AWS is known for being reliable, with an uptime of 99.9%. This makes it a great platform for mission-critical applications that need to be available 24/7. AWS also offers the ability to deploy resources across multiple availability zones for even greater reliability. The cloud platform also has a number of features that make it easier to ensure reliability, such as autoscaling and auto-healing. Autoscaling allows businesses to automatically scale their resources up or down based on demand, while auto-healing enables them to quickly identify and replace any faulty components. 4. Security: Businesses can take advantage of advanced security features, such as identity and access management, to help protect their data. Their tough infrastructure with an end-to-end approach is designed to withstand attacks and AWS provides customers with tools to help them monitor and respond to threats. When it comes to storage, Amazon S3 provides customers with a secure and reliable way to store and access data. The service is designed to be highly scalable and resilient, with built-in redundancy. Fine-grain identity and access controls can be applied to S3 buckets and objects, giving customers control over who has access to their data. Security tasks can be automated with AWS CloudFormation, making it easier for businesses to manage their security policies. And, you can rest easy knowing that AWS takes privacy seriously, with comprehensive customer data protection and compliance measures. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 18 N32 nirf 1750 Rank

--- Page 69 ---
Unit: II Course Code/Title:CS3V15/Devops 1 build.gradle gradle wrapper 2 gradle-wrapper.jar - gradle-wrapper.properties 3 gradlew 4 gradlew.bat 5 settings.gradle 6 It is the default structure of a Gradle project. Gradle will generate the following things for us: 1. The gradle file is build script for configuring the current project. 2. An executable JAR file is used as a Gradle wrapper. 3. Configuration properties for Gradle Wrapper. 4. The gradlew is a Gradle wrapper script for UNIX based OS. 5. The bat is the Gradle Wrapper script for Windows. 6. The settings script for configuring the Gradle build. Step3: Create a task Gradle supports APIs for creating and managing tasks through a Groovy-based DSL or Kotlin-based DSL. Every project contains a collection of tasks for some basic operation. Gradle supports a library of tasks that configure the project. For example, there is a Copy task, which copies files from one location to another. The Copy task is one of the most used tasks In Gradle. To use the Copy task in build script, follow the below process. Step1: Create a directory called src C: \Users \HiMaNshU\demo>mkdir src C: \Users \HiMaNshU\demo>cd src Step2: Add a file called myfile.txt in the src directory. Add the single line "Hello, World!" to it, also, we can leave it empty. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 28 N32 nirf 1750 Rank

--- Page 70 ---
Unit: II Course Code/Title:CS3V15/Devops C:\Users \HiMaNshU\demo\src>echo myfile.txt myfile.txt Define a task called Copy in build.gradle file. It will copy the src directory to a new directory called dest. We don't have to create the dest directory; the Copy task will do it for us. 1. task copy(type: Copy, group: "Custom", description: "The sources are copied to dest directory") { 2. from "src" 3. into "dest" 4. } We can provide anything in a group and description. Also, we can omit them, but doing so will also be omitted from the report of the task used later. Now execute our new copy task: C: \Users \HiMaNshU\demo>gradle copy BUILD SUCCESSFUL in 2s 1 actionable task: 1 executed C: \Users \HiMaNshU\demo>_ After the successful execution of the task, we will get BUILD SUCCESSFUL message. The build.gradle file The build.gradle file is build script of a Gradle project. All the tasks and plugins are defined in this file. When we run a gradle command, it looks for a file called build.gradle in the current directory. Although we have called it a build script, strictly, it is a build configuration script. The build script defines a project and its tasks. The default build.gradle file looks like as follows: CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 29 N33 nirf 1750 Rank

--- Page 113 ---
Course Code/Title:CS3V15/Devops Unit: III 3. Configure Job Details: ○ General: I Enter a description for your job. Configure options such as discarding old builds if needed. ○ Source Code Management: I Choose Git, Subversion, or another source control system. Enter the repository URL and credentials if the repository is private. Specify the branch or tag to build from. ○ Build Triggers: Set up triggers to start the build. Common options include: Poll SCM: Jenkins will periodically check for changes in the source code repository. . Build periodically: Schedule builds at specific intervals. I GitHub hook trigger for GITScm polling: Trigger builds based on GitHub webhooks. 0 Build Environment: ■ Configure the build environment, such as setting up environment variables or cleaning up before the build starts. ○ Build Steps: Add build steps by clicking Add build step. · Common build steps include: Execute shell: Run shell commands or scripts. . Invoke Gradle script: Use Gradle to build the project. I Invoke Ant: Use Apache Ant for building. ○ Post-build Actions: · Configure actions to perform after the build completes, such as: I Archive the artifacts: Save build artifacts for later use. I Publish JUnit test result report: Display test results in Jenkins. I Send build notifications: Notify stakeholders of build status. 4. Save and Build: ○ Click Save to store the job configuration. o To start a build, click Build Now in the job dashboard. Example Build Configuration: . Job Name: MyApp-Build · Source Code Management: Git o Repository URL: https://github.com/user/myapp.git ○ Branch: main · Build Step: Execute shell ○ Command : myn clean install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 38 nirf 1750 Rank N33

--- Page 114 ---
Course Code/Title:CS3V15/Devops Unit: III · Post-build Action: Archive artifacts ○ Files to archive: target/ *. jar 2. Understanding Jenkins Workspace Overview: The Jenkins workspace is a directory where Jenkins stores files and artifacts related to a particular build. Each job has its own workspace, which is used to perform build operations. Key Aspects of Jenkins Workspace: 1. Workspace Location: o By default, the workspace is located in the Jenkins home directory, typically at /var/lib/jenkins/workspace/ on Linux systems or C:\Program Files (x86) \Jenkins\workspace\ on Windows. 2. Workspace Structure: ○ Each job gets its own subdirectory within the workspace. For example, a job named MyApp-Build will have its workspace at /var/lib/jenkins/workspace/MyApp - Build/. ○ The workspace contains: Source Code: The code pulled from the repository. . Build Artifacts: Files generated during the build process. I Logs: Logs related to the build process. 3. Workspace Usage: ○ Building: During the build process, Jenkins checks out the code into the workspace, executes build steps, and generates artifacts. ○ Archiving Artifacts: After the build, files specified in the post-build actions are archived from the workspace. ○ Cleaning Up: Jenkins may clean up workspaces based on job configurations or policies to save disk space. 4. Customizing Workspace: ○ Configure Custom Workspace Location: In the job configuration, under the Advanced Project Options, you can specify a custom workspace directory. ○ Workspace Cleanup: I Use plugins like the Workspace Cleanup Plugin to manage and clean up workspaces automatically. Example of Workspace Structure: · Workspace Directory: /var/lib/jenkins/workspace/MyApp-Build/ o Source Code: /var/lib/jenkins/workspace/MyApp-Build/src/ o Build Artifacts: /var/lib/jenkins/workspace/MyApp-Build/target/ o Logs: /var/lib/jenkins/workspace/MyApp-Build/logs/ This guide should provide a comprehensive understanding of creating a Jenkins build and managing Jenkins workspaces. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY N32 39 nirf 175º Rank

--- Page 77 ---
Unit: III Course Code/Title:CS3V15/Devops 5. Customize Jenkins: ○ Install suggested plugins or select the required plugins manually. ○ Create the first admin user. Configuration 1. Global Configuration: ○ Navigate to Manage Jenkins > Configure System. ○ Set up global environment variables, email notifications, and other system-wide settings. 2. Security Configuration: ○ Configure security settings under Manage Jenkins > Configure Global Security. o Use the built-in user database or integrate with external authentication mechanisms like LDAP. 3. Node Configuration: ○ Jenkins can distribute build loads to multiple nodes. ○ Configure nodes by navigating to Manage Jenkins > Manage Nodes and Clouds. 4. Job Configuration: ○ Create new jobs by clicking New Item on the Jenkins dashboard. o Configure job-specific settings such as source code management, build triggers, and post-build actions. 5. Pipeline Configuration: ○ Jenkins supports pipeline as code using Jenkinsfile. ○ Define your pipeline stages in a Jenkinsfile placed in the root of your project repository. 6. Plugins: ○ Enhance Jenkins functionality by installing plugins from the Manage Jenkins > Manage Plugins section. 7. Backup and Restore: ○ Regularly back up the jenkins_home directory, which contains Jenkins configurations, job configurations, and build history. ○ Use plugins like ThinBackup for automated backups. Ref: https://youtube.com/playlist?list=PL6flErFppaj35spJjPy41-IruDjw2kRV-&si=Vzd9AAMv53xwWB S Ref: https://bit.ly/youtube_jenkins (click here) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 78 ---
Course Code/Title:CS3V15/Devops Unit: III Jenkins Architecture Overview Jenkins is a robust automation server widely used for continuous integration and continuous delivery (CI/CD) processes. Here's a detailed breakdown of the Jenkins architecture and its core components. Core Components of Jenkins Architecture 1. Jenkins Master: ○ Role: The Jenkins master is the central control unit. 0 Functions: Scheduling Jobs: Assigns build jobs to the appropriate agents. Monitoring Agents: Keeps track of the state and health of agents. I Job Execution: Executes build jobs directly if no agents are available. I User Interface: Provides a web-based interface for users to configure jobs, view build results, and manage the Jenkins environment. 2. Build Agents (Nodes): ○ Role: Agents are responsible for executing the build jobs assigned by the master. ○ Types: Agents can run on different operating systems and hardware configurations. ○ Configuration: Static Agents: Permanently assigned to the Jenkins master. . Dynamic Agents: Created and destroyed as needed, often used in cloud environments. ○ Communication: Agents communicate with the master using the Jenkins Remoting protocol. 3. Job Configuration: ○ Types of Jobs: I Freestyle Projects: Basic job type with simple configurations. Pipeline Jobs: Scripted or declarative pipelines that define the entire build process as code. I Multi-Configuration (Matrix) Projects: Allows testing across different environments and configurations. ○ Components: I Source Code Management: Integration with version control systems like Git, SVN, etc. . Build Triggers: Conditions that start the job, such as code commits, scheduled times, or manual triggers. Build Steps: Actions performed during the build, such as compiling code, running tests, and packaging artifacts. . Post-Build Actions: Steps executed after the build, like deploying artifacts, sending notifications, or archiving results. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 79 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Plugins: ○ Role: Extend Jenkins functionality without modifying the core software. ○ Types: I Source Control Plugins: Git, SVN, Mercurial. Build Tools Plugins: Maven, Gradle, Ant. Notification Plugins: Email, Slack, HipChat. · Reporting Plugins: JUnit, Cobertura, Checkstyle. ○ Management: Plugins can be installed, updated, and configured via the Jenkins web interface. 5. Master-Slave Architecture: ○ Concept: The Jenkins master distributes build tasks to multiple agents, enabling distributed builds. ○ Benefits: Scalability: Handle more builds simultaneously. I Resource Management: Assign builds to agents with the necessary resources. Isolation: Run builds in isolated environments to avoid conflicts. Workflow and Data Flow 1. Job Trigger: ○ Jobs can be triggered by various events: Manual Trigger: Initiated by a user through the Jenkins interface. Scheduled Trigger: Using CRON-like syntax to schedule jobs. · SCM Trigger: Automatically triggered by changes in the source code repository. Upstream/Downstream Trigger: Triggered by the completion of other jobs. 2. Build Execution: ○ Job Assignment: The master assigns the job to an available agent based on labels, availability, and resource requirements. Build Environment Setup: The agent sets up the environment, including checking out the code, installing dependencies, and configuring the workspace. ○ Execution: The agent runs the build steps as defined in the job configuration. 3. Build Results: ○ Logs: Captures console output and logs from the build process. ○ Artifacts: Stores build artifacts like binaries, packages, and reports. ○ Test Results: Collects and displays test results and code coverage reports. ○ Build History: Maintains a history of all builds, including status, duration, and changes. 4. Notification and Reporting: ○ Notifications: Jenkins can send notifications through various channels (email, chat, etc.) upon build completion or failure. ○ Dashboards: Provides dashboards for visualizing build status, trends, and metrics. ○ Reports: Generates and displays reports on test results, code quality, and other metrics. INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 4 1750 Rank

--- Page 80 ---
Unit: III Course Code/Title:CS3V15/Devops Summary Jenkins' architecture is designed to be flexible and scalable. The master-agent model allows for efficient distribution of build tasks, while plugins provide extensive customization options. By leveraging Jenkins' comprehensive job configuration and robust notification and reporting capabilities, teams can streamline their CI/CD pipelines and improve software quality and delivery speed. Creating a Jenkins Job Creating a Jenkins job involves setting up a new project within Jenkins to automate various tasks such as building code, running tests, and deploying applications. Here is a step-by-step guide to creating a Jenkins job. Prerequisites · Jenkins installed and running. · Basic understanding of the Jenkins interface. . Access to the Jenkins dashboard. Step-by-Step Guide 1. Access Jenkins Dashboard: . Open your web browser and navigate to your Jenkins instance URL (e.g., http://localhost:8080). 2. Create a New Job: ○ On the Jenkins dashboard, click on the "New Item" link on the left-hand side menu. 3. Enter Job Name: ○ Enter a name for your new job in the "Enter an item name" field. o Choose the type of job you want to create. For most uses, "Freestyle project" is a good starting point. ○ Click "OK". 4. Configure the Job: ○ After clicking "OK", you will be directed to the job configuration page. 5. General Configuration: ○ Description: Provide a brief description of the job. ○ Discard Old Builds: Optionally set this to limit the number of old builds Jenkins keeps. ○ Select Version Control System: Choose the version control system (e.g., Git, Subversion). 6. Source Code Management: ○ Repository URL: Enter the repository URL. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 5 nirf 175 Rank N33

--- Page 67 ---
Course Code/Title:CS3V15/Devops Unit: II Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013 JVM: 1.7.0_60 (Oracle Corporation 24.60-b09) OS: Windows 8.1 6.3 amd64 Understanding build using Gradle: The Gradle build is a process of creating a Gradle project. When we run a gradle command, it will look for a file called build.gradle in the current directory. This file is also called the Gradle build script. The build configuration, tasks, and plugins are described in this file. The build script describes a project and its tasks. Let's create a small Gradle project, run some of the basic Gradle commands, and understand how Gradle manages the project. Follow the steps below to create and test a Gradle project. Step1: Open the command line and create a directory First, Open the command line and create a directory for the project and change directory to it. Let's create a demo directory. C: \Users \HiMaNshU>mkdir demo C: \Users \HiMaNshU>cd demo Step2: Initialize a Gradle project To generate a Gradle project, run the gradle init command. It will generate a simple project. With this project, we will explore and understand everything that is generated. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 26 N33 nirf 1750 Rank

--- Page 68 ---
Course Code/Title:CS3V15/Devops Unit: II When we run the gradle init command, it will ask for some basic requirements. First, it will ask the type of project that we want to create. It will give four options: 1. 1: basic 2. 2: application 3. 3: library 4. 4: Gradle plugin Select our requirements. Hence, we are just making a demo project so that we will select the basic option. To select basic option, press 1 and Enter key. Consider the below output: C: \Users \HiMaNshU\demo>gradle init Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could ed, use -- status for details Select type of project to generate: 1: basic 2: application 3: library 4: Gradle plugin Enter selection (default: basic) [1 .. 4] 1 Next, it will ask for DSL. There are two options that are available for DSL: 1. 1: Groovy 2. 2: Kotlin Groovy is the default DSL provided by Gradle. Select build script DSL. Select build script DSL: 1: Groovy 2: Kotlin Enter selection (default: Groovy) [1 .. 2] 1 Next, it will ask for the project name. Type the project name and press Enter key. It will take a while to build a project. After the successful execution of the project, we will get a message BUILD SUCCESSFUL. Project name (default: demo): First_Gradle > Task : init Get more help with your project: https: //guides. gradle.org/creating- uilds BUILD SUCCESSFUL in 2m 56s 2 actionable tasks: 2 executed C: \Users\HiMaNshU\demo>mkdir src Now we have successfully created a Gradle project. Now, what will happen to our specified directory? Consider the below structure of the Gradle project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 27 N33 narf 1750 Rank

--- Page 87 ---
Course Code/Title:CS3V15/Devops Unit: III 2. Go to Plugin Manager: From the dashboard, click on 'Manage Jenkins' on the left-hand side menu. On the Manage Jenkins page, click on 'Manage Plugins'. This will take you to the Plugin Manager, where you can view, install, and update plugins. 3. Exploring Available Plugins 1. Available Tab: The 'Available' tab lists all plugins that are available for installation. This includes thousands of plugins categorized by functionality (e.g., SCM, build tools, user interface enhancements). 2. Search for Plugins: Use the search box to quickly find the plugin you need. For example, if you're looking to integrate Jenkins with Git, type 'Git' in the search box. 3. Popular Plugins: Jenkins highlights popular plugins at the top of the Available tab. These are commonly used plugins that most users find essential. 4. Installing Plugins 1. Selecting Plugins to Install: Check the box next to each plugin you want to install. You can select multiple plugins at once. 2. Install Without Restart: After selecting your plugins, scroll down and click 'Install without restart'. Jenkins will install the plugins immediately, and you can continue using Jenkins during the installation. 3. Install After Restart: If you prefer, you can choose to 'Download now and install after restart'. This will install the plugins after Jenkins is restarted, ensuring that all changes are applied without interference. 4. Installation Progress: The installation process will show a progress bar for each plugin. Once a plugin is installed, it will move to the 'Installed' tab. 5. Configuring Installed Plugins 1. Plugin Configuration: After installation, some plugins may require additional configuration. Navigate to 'Manage Jenkins' > 'Configure System' or 'Configure Global Security' to adjust settings for your new plugins. 2. Global Tool Configuration: For build tools like Maven or JDKs installed via plugins, you can configure them under 'Global Tool Configuration'. Here, you define the paths to the tools or specify installation options. 6. Updating and Managing Plugins 1. Updating Plugins: Regularly update your plugins to ensure compatibility with the latest version of Jenkins and to benefit from new features or security patches. Go to the 'Updates' tab in the Plugin CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N32 nirf 12 1750 Rank

--- Page 88 ---
Unit: III Course Code/Title:CS3V15/Devops Manager. Jenkins will show you all plugins with available updates. Click 'Update' to install the latest versions. 2. Uninstalling Plugins: If a plugin is no longer needed, you can uninstall it from the 'Installed' tab. Select the plugin and click 'Uninstall'. Uninstallation typically requires a Jenkins restart to complete. 7. Commonly Used Plugins Here are a few essential plugins that you might consider adding to your Jenkins setup: 1. Git Plugin: Integrates Jenkins with Git, allowing you to pull code from GitHub, Bitbucket, or other Git repositories. 2. Pipeline Plugin: Enables the creation and management of Jenkins pipelines, a powerful way to define complex build, test, and deployment processes. 3. Maven Integration Plugin: Adds support for Maven projects, allowing you to build and manage Maven-based projects directly within Jenkins. 4. Email Extension Plugin: Provides advanced email notification options for build statuses, including customizable email templates. 5. Blue Ocean: An alternative user interface for Jenkins that simplifies pipeline creation and offers a modern, user-friendly design. 8. Troubleshooting Plugin Issues 1. Compatibility Issues: Sometimes, plugins might conflict with each other or with the Jenkins version. In such cases, refer to the plugin's documentation or Jenkins logs to diagnose the issue. 2. Rollback: If a plugin update causes problems, you can rollback to a previous version from the 'Installed' tab by selecting the desired version. 3. Restart Jenkins: Some plugin changes require a Jenkins restart. Ensure all jobs are completed before restarting to avoid any interruptions. Commonly Used Jenkins Plugins 1. Git Plugin Overview: The Git Plugin integrates Jenkins with Git repositories, allowing Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nårf 1750 Rank

--- Page 117 ---
Course Code/Title:CS3V15/Devops Unit: IV • Explore Ansible modules. 4. Playbook - · Playbooks are a sequence of plays that define the order of tasks executed by Ansible. . A play consists of a list of tasks that target managed nodes in an inventory. · Tasks, in turn, are composed of one or more modules that specify operations. · Playbooks provide a structured way to define and automate complex workflows. 5. Roles - · Roles provide an organized environment for managing complex tasks. · They include templates, playbooks, inventories, error handlers, vars, and meta information. · Ideal for handling larger, multi-step automation processes. 6. Collections - · Collections are distribution formats for Ansible content. · They encompass playbooks, roles, modules, and plugins. · Find Ansible collections. 7. Galaxy - · Ansible Galaxy is a platform for sharing and downloading collections. . It allows the Ansible community to collaborate and exchange automation content. Setting Up the Ansible Control Node Prerequisite: Before you begin setting up Ansible, it's essential to ensure you meet the following prerequisites: · SSH: Make sure SSH is installed on your Linux system. SSH is crucial for secure communication between the control node and managed nodes. Installation Steps: 1. Open your terminal. 2. Run the following command to install Ansible: 1python3 -m pip install -- user ansible 3. SSH Key: o For secure and direct connections to managed nodes, you'll need to provide a public SSH key. You can generate one using the ssh-keygen command. o Follow the on-screen instructions to create your SSH key pair. This key pair is a crucial element of Ansible's secure communications. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 118 ---
Course Code/Title:CS3V15/Devops Unit: IV 4. For specific installation instructions tailored to your Linux distribution, click here. 5. Try to connect ssh using once your mange node is set up. 1ssh <user>@<ip> Note: If you're using a different operating system, find the appropriate installation instructions here. Setting Up the Ansible Mange Node Steps: 1. Install OpenSSH-Server: o To enable remote management of the managed node, you'll need to install the OpenSSH server. Use the following command (assuming you're using a Debian-based Linux distribution like Ubuntu): 1sudo apt install openssh-server o This command installs the OpenSSH server, allowing secure remote access. 2. Create a User: o For convenience and consistency, it's helpful to create a user on the managed node with the same name as the user on the control node. This makes it easier to manage SSH keys and ensures a smoother experience. 3. Configure SSH Key: o To establish secure and direct connections to the managed node, you need to paste the public SSH key from the control node into the authorized_keys file located in the .ssh directory of the user's home folder on the managed node. o The path is typically /home/<user>/.ssh/authorized_keys, where <user> is the username you created or are using on the managed node. You can use the ssh-copy-id command to automate the process of copying your public key to the managed node. For example: 1ssh-copy-id <user>@<managed_node_ip> o This command securely copies your public key to the authorized_keys file, allowing passwordless SSH authentication. Managing Ansible Inventories In the world of Ansible, inventories are like the backbone of your automation infrastructure. They serve as a vital component that lists and organizes the managed nodes, making automation tasks seamless and organized. Inventories use a parent-child concept that allows you to create groupings, which come in handy during specific tasks or scenarios. Why Are Inventories Important? · Inventories provide a comprehensive list of managed nodes, making it easy to interact with and manage them. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 4 N33 nirf 1750 Rank

--- Page 15 ---
Course Code/Title:CS3V15/Devops Unit: I which is a mechanism to control the usage of resources or capacity. 2) Code: Many good practices such as Git enables the code to be used, which ensures writing the code for business, helps to track changes, getting notified about the reason behind the difference in the actual and the expected output, and if necessary reverting to the original code developed. The code can be appropriately arranged in files, folders, etc. And they can be reused. 3) Test: The application will be ready for production after testing. In the case of manual testing, it consumes more time in testing and moving the code to the output. The testing can be automated, which decreases the time for testing so that the time to deploy the code to production can be reduced as automating the running of the scripts will remove many manual steps. 4) Plan: DevOps use Agile methodology to plan the development. With the operations and development team in sync, it helps in organizing the work to plan accordingly to increase productivity. 5) Monitor: Continuous monitoring is used to identify any risk of failure. Also, it helps in tracking the system accurately so that the health of the application can be checked. The monitoring becomes more comfortable with services where the log data may get monitored through many third-party tools such as Splunk. 6) Deploy: Many systems can support the scheduler for automated deployment. The cloud management platform enables users to capture accurate insights and view the optimization scenario, analytics on trends by the deployment of dashboards. 7) Operate: DevOps changes the way traditional approach of developing and testing separately. The teams operate in a collaborative way where both the teams actively participate throughout the service lifecycle. The operation team interacts with developers, and they come up with a monitoring plan which serves the IT and business requirements. 8) Release: Deployment to an environment can be done by automation. But when the deployment is made to the production environment, it is done by manual triggering. Many processes involved in release management commonly used to do the deployment in the production environment manually to lessen the impact on the customers. DevOps Lifecycle DevOps defines an agile relationship between operations and Development. It is a process that is practiced by the development team and operational engineers Learning DevOps is not complete without understanding the DevOps lifecycle phases. The DevOps lifecycle includes seven phases as given below: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 16 ---
Unit: I Course Code/Title:CS3V15/Devops together from beginning to the final stage of the product. Development Operations Integration Deployment DevOps Lidecycle Feedback 2) Continuous Integration Testing Monitering 1) Continuous Development This phase involves the planning and coding of the software. The vision of the project is decided during the planning phase. And the developers begin developing the code for the application. There are no DevOps tools that are required for planning, but there are several tools for maintaining the code. This stage is the heart of the entire DevOps lifecycle. It is a software development practice in which the developers require to commit changes to the source code more frequently. This may be on a daily or weekly basis. Then every commit is built, and this allows early detection of problems if they are present. Building code is not only involved compilation, but it also includes unit testing, integration testing, code review, and packaging. The code supporting new functionality is continuously integrated with the existing code. Therefore, there is continuous development of software. The updated code needs to be integrated continuously and smoothly with the systems to reflect changes to the end-users. Commit Build O O Test O Stage Deploy Dev/QA O O > Continuous Integration/Delivery Development </> git Code Commit = production server. 3) Continuous Testing Production Jenkins is a popular tool used in this phase. Whenever there is a change in the Git repository, then Jenkins fetches the updated code and prepares a build of that code, which is an executable file in the form of war or jar. Then this build is forwarded to the test server or the This phase, where the developed software is continuously testing for bugs. For constant testing, automation testing tools such as TestNG, JUnit, Selenium, etc are used. These tools allow QAs to test multiple code- bases thoroughly in parallel to ensure that there is no flaw in the functionality. In this phase, Docker Containers can be used for simulating the test environment. It may occur in the form of documentation files or maybe produce large-scale data about the application parameters when it is in a continuous use position. The system errors such as server not reachable, low memory, etc are resolved in this phase. It maintains the security and availability of the service. Automation testing saves a lot of time and effort for executing the tests instead of doing this manually. Apart from that, report generation is a big plus. The task of evaluating the test cases INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 narf 1750 Rank

--- Page 151 ---
Azure DevOps Taking you to your Azure DevOps organization ... Congratulations, you're an organization owner! Sign in to your organization at any time, https://dev.azure.com/{yourorganization}. With your organization, the following aspects are included in the free tier: First five users free (Basic license): Azure Pipelines: o One Microsoft-hosted CI/CD (one concurrent job, upto 30 hours per month) o One self-hosted CI/CD concurrent job Azure Boards: Work item tracking and Kanban boards Azure Repos: Unlimited private Git repos Azure Artifacts: Two GB free per organization Build applications with Azure: Azure DevOps enables you to build, test, and deploy anyapplication to any cloud or on premises To configure build pipelines that continuously build, test, and verify your applications. 11

--- Page 152 ---
Part 1: Get started with Azure DevOps Part 2: Build applications with Azure DevOps Part 3: Deploy applications with Azure DevOps Create a build pipeline with Azure Pipelines: Prerequisites: Familiarity with forking and cloning a GitHub repo Account requirements: An Azure DevOps organization o To use Microsoft-hosted agents, your Azure DevOps organization must have access to Microsoft- hosted parallel jobs. Check your parallel jobs and request a free grant. O You can use GitHub Code spaces to complete the module, even if your Azure DevOps organization doesn't have any parallel jobs. A GitHub account Software requirements: If using GitHub Code spaces to complete the module, there are no software requirements as all software is included in the Codespace If using a local development environment withMicrosoft-hosted agents, you must have the following software installed: o Visual Studio Code o .NET 6.0 SDK o Git 12

--- Page 155 ---
10. When you're ready to make changes to your pipeline, select it in the Pipelines page, and then Edit the azure-pipelines.yml file. View and manage your pipelines You can view and manage your pipelines by choosing Pipelines from the left-hand menu to go to the pipelines landing page. Azure DevOps : FabrikamFiber 1 Pipelines 0 Search F FabrikamFiber + Pipelines New pipeline Overview Recent All Runs Y Filter pipelines Boards Repos Pipelines Pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... La 32m ago & Manually triggered 8º main & 42s Environments Releases FabrikamFiber #20191209.3 · Set up CI with Az ... La 1h ago & Manually triggered & main 1m 13s Library Task groups *** Deployment groups From the pipelines landing page you can view pipelines and pipeline runs, create and import pipelines, manage security, and drill down into pipeline and run details. Choose Recent to view recently run pipelines (the default view), or choose All to view all pipelines. Pipelines New pipeline : Recent All Runs Filter pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... & Manually triggered & main Là 36m ago ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Az ... Là 1h ago 1m 13s & Manually triggered & main 15

--- Page 156 ---
Select a pipeline to manage that pipeline and view the runs. Select the build number for the last run to view the results of that build, select the branch name toview the branch for that run, or select the context menu to run the pipeline and perform other management actions. Recently run pipelines Pipeline Last run #20191209.2 · Set up CI with Azure Pip ... > pipelines-dotnet-core 8 Manually triggered 12º main Là 1h ago * ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Azure Pip ... 試 2h 8 Manually triggered 8 main Edit in Run pipeline Manage security Rename/move Delete Select Runs to view all pipeline runs. You canoptionally filter the displayed runs. Pipelines Recent All Runs Filter by keywords State V Repository V New pipeline Requested forV Tags X All pipeline runs Description Stages Set up CI with Azure Pipelines & #20191209.2 on pipelines-dotnet-core & main d4964 .. Set up CI with Azure Pipelines #20191209.1 on pipelines-dotnet-core åº main d4964 ... Set up CI with Azure Pipelines & #20191209.3 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines & #20191209.2 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines #20191209.1 on FabrikamFiber & main 2b4b23c La 44m ago 42s La 53m ago ₾ 46s Là 1h ago 0 1m 13s Ld 1h ago 1m 555 Lo 1h ago 1m 6s 16

--- Page 127 ---
Course Code/Title:CS3V15/Devops Unit:IV Abbreviation You can also use abbreviation to represent lists. Example Countries: ['America', 'China', 'Canada', 'Iceland'] List inside Dictionaries We can use list inside dictionaries, i.e., value of key is list. Example james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english … List of Dictionaries We can also make list of dictionaries. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english - robert: name: robert richardson rollNo: 53 div: B sex: male CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 13 N32 nirf 1750 Rank

--- Page 128 ---
Course Code/Title:CS3V15/Devops Unit:IV likes: - biology - chemistry … YAML uses "|" to include newlines while showing multiple lines and ">" to suppress newlines while showing multiple lines. Due to this we can read and edit large lines. In both the cases intendentation will be ignored. We can also represent Boolean (True/false) values in YAML. where boolean values can be case insensitive. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english result: maths: 87 chemistry: 45 biology: 56 physics: 70 english: 80 passed: TRUE messageIncludeNewLines: | Congratulation !! You passed with 79% messageExcludeNewLines: > Congratulation !! You passed with 79% CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 14 N33 nirf 1750 Rank

--- Page 109 ---
Course Code/Title:CS3V15/Devops Unit: III Note that the git repository and the files from that repository are copied from the workspace of App1 to App2. Also note that test.class and test.java files are copied to the workspace of App2. Started by user Anuradha R Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\. jenkins\workspace\App2 Copied 16 artifacts from "Appl" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621189248227895.bat C:\ProgramData\Jenkins\.jenkins\workspace\App2>dir Volume in drive C is OS Volume Serial Number is C6C2-8A9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 .git 474 test. class 144 test.java 2 File(s) 618 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ Finished: SUCCESS 5. Extended Choice Parameter Overview: The Extended Choice Parameter Plugin provides advanced parameter types for Jenkins jobs, such as multi-select lists, checkboxes, and more complex formats. Key Features: · Parameter Types: Includes multi-select, checkboxes, and other advanced input options. . Dynamic Choices: Allows dynamic generation of choices from scripts or external sources. Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Extended Choice Parameter Plugin," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add an Extended Choice Parameter. ○ Configure the parameter type, choices, and other settings. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 110 ---
Unit: III Course Code/Title:CS3V15/Devops Usage: · Useful for scenarios where complex user input is needed or where multiple selection options are required. Configuring Jenkins to Work with Java, Git, and Maven 1. Configuring Jenkins to Work with Java Overview: Jenkins requires Java to run. Configuring Java in Jenkins involves specifying the Java Development Kit (JDK) installations Jenkins should use. Steps: 1. Install Java Development Kit (JDK): ○ Ensure that JDK is installed on your system. You can download it from the Oracle website or use OpenJDK. 2. Configure JDK in Jenkins: ○ Open Jenkins and go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to JDK section and click Add JDK. ○ Enter a name for the JDK installation (e.g., JDK 11). Check Install automatically to let Jenkins download and install the JDK, or specify the path to an existing JDK installation. ○ If specifying the path manually, provide the JAVA_HOME directory. 3. Example Configuration: ○ Name: JDK 11 ○ JAVA_HOME: /usr/lib/jvm/java-11-openjdk 4. Verify JDK Configuration: You can verify the JDK configuration by creating a simple Jenkins job and adding a build step that prints the Java version using: bash Copy code java -version 2. Configuring Jenkins to Work with Git Overview: The Git Plugin integrates Jenkins with Git repositories, enabling Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 35 1750 Rank

--- Page 31 ---
Course Code/Title:CS3V15/Devops Unit: I our work simpler. The pricing of Azure is also simpler and cost-effective. Popularly termed as "Pay As You Go", which means how much you use, pay only for that. Microsoft Azure Used for · Deployment Of applications: You can develop and deploy the application in the azure cloud by using the service called Azure App Service and Azure Functions after deploying the applications end users can access it. · Identity and Access Managment: The application and data which is deployed and stored in the Microsoft Azure can be secured with the help of Identity and Access Managment. It's commonly used for single sign-on, multi-factor authentication, and identity governance. · Data Storage and Databases: You can store the data in Microsoft azure in service like blob storage for unstructured data, table storage for NoSQL data, file storage, and Azure SQL Database for relational databases. The service can be scaled depending on the amount of data we are getting. · DevOps and Continuous Integration/Continuous Deployment (CI/CD): Azure DevOps will provide some tools like ncluding version control, build automation, release management, and application monitoring Following are some of the services Microsoft Azure offers: 1. Compute: Includes Virtual Machines, Virtual Machine Scale Sets, Functions for serverless computing, Batch for containerized batch workloads, Service Fabric for microservices and container orchestration, and Cloud Services for building cloud-based apps and APIs. 2. Networking: With Azure, you can use a variety of networking tools, like the Virtual Network, which can connect to on-premise data centers; Load Balancer; Application Gateway; VPN Gateway; Azure DNS for domain hosting, Content Delivery Network, Traffic Manager, ExpressRoute dedicated private network fiber connections; and Network Watcher monitoring and diagnostics 3. Storage: Includes Blob, Queue, File, and Disk Storage, as well as a Data Lake Store, Backup, and Site Recovery, among others. 4. Web + Mobile: Creating Web + Mobile applications is very easy as it includes several services for building and deploying applications. 5. Containers: Azure has a property that includes Container Service, which supports Kubernetes, DC/OS or Docker Swarm, and Container Registry, as well as tools for microservices. 6. Databases: Azure also included several SQL-based databases and related tools. 7. Data + Analytics: Azure has some big data tools like HDInsight for Hadoop Spark, R Server, HBase, and Storm clusters 8. AI + Cognitive Services: With Azure developing applications with artificial intelligence capabilities, like the Computer Vision API, Face API, Bing Web Search, Video Indexer, and Language Understanding Intelligent. 9. Internet of Things: Includes IoT Hub and IoT Edge services that can be combined with a variety of machine learning, analytics, and communications services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 31 N32 nirf 1750 Rank

--- Page 32 ---
Unit: I Course Code/Title:CS3V15/Devops 10. Security + Identity: Includes Security Center, Azure Active Directory, Key Vault, and Multi-Factor Authentication Services. 11. Developer Tools: Includes cloud development services like Visual Studio Team Services, Azure DevTest Labs, HockeyApp mobile app deployment and monitoring, Xamarin cross- platform mobile development, and more. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 32 N32 nirf 1750 Rank

--- Page 47 ---
Course Code/Title:CS3V15/Devops Unit: II <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> Any repository of maven is available in mvnrepository.com 5. Maven Plugins: The maven plugins are central part of maven framework, it is used to perform specific goal. According to Apache Maven, there are 2 types of maven plugins. 1. Build Plugins 2. Reporting Plugins Build Plugins These plugins are executed at the time of build. These plugins should be declared inside the <build> element. Reporting Plugins These plugins are executed at the time of site generation. These plugins should be declared inside the <reporting> element. Maven Core Plugins A list of maven core plugins are given below: Plugin Description clean clean up after build. compiler compiles java source code. deploy deploys the artifact to the remote repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY N32 nirf 6 175º Rank -

--- Page 48 ---
Course Code/Title:CS3V15/Devops Unit: II failsafe runs the JUnit integration tests in an isolated classloader. install installs the built artifact into the local repository. resources copies the resources to the output directory for including in the JAR. site generates a site for the current project. surefire runs the JUnit unit tests in an isolated classloader. verifier verifies the existence of certain conditions. It is useful for integration tests. Example for maven plugin: The compiler plugin is used to compile the source code of a Maven project. This plugin has two goals, which are already bound to specific phases of the default lifecycle: · compile - compile main source files · testCompile - compile test source files Here's the compiler plugin in the POM: <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.12.1</version> <configuration> <source>1.8</source> <target>1.8</target> </configuration> </plugin> 6. Maven Build Lifecycle: A Build Lifecycle is a well-defined sequence of phases, which define the order in which the goals are to be executed. Here phase represents a stage in life cycle. As an example, a typical Maven Build Lifecycle consists of the following sequence of phases. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 7 N33 nirf 175º Rank -

--- Page 153 ---
Create a pipeline in azure What is pipeline Azure Pipelines supports continuous integration (CI)and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline. The latest way to build pipelines is with the YAML pipeline editor. You can also use Classic pipelines with the Classic editor. code release build CI plan continuous testing CD deploy monitor operate Create your first pipeline This is a step-by-step guide to using Azure Pipelines to build a sample application from a Git repository. This guide uses YAML pipelines configured with the YAML pipeline editor. If you'd like to use Classic pipelines instead, see Define your Classic pipeline. For guidance on using TFVC, see Build TFVC repositories. Prerequisites - Azure DevOps A GitHub account where you can create are pository. An Azure DevOps organization. Create one for free. If your team already has one, then make sure you're an administrator of the Azure Devops. 12

--- Page 154 ---
Get the Java sample code To get started, fork the following repository into your GitHub account. https://github.com/MicrosoftDocs/pipelines-java Create your first Java pipeline 1. Sign-in to your Azure DevOps organization and go to your project. 2. Go to Pipelines, and then select New pipeline. 3. Do the steps of the wizard by first selecting GitHub as the location of your source code. 4. You might be redirected to GitHub to sign in. If so, enter your GitHub credentials. 5. When you see the list of repositories, select your repository. 6. You might be redirected to GitHub to install the Azure Pipelines app. If so,select Approve & install. 7. Azure Pipelines will analyze your repository and recommend the Maven pipeline template. 8. When your new pipeline appears, take a look at the YAML to see what it does. When you're ready, select Save and run. 9. You're prompted to commit a new azure pipelines.yml file to your repository. After you're happy with the message, select Save and run again. If you want to watch your pipeline in action, select the build job. You just created and ran a pipeline that we automatically created for you, because your code appeared to be a good match for the Maven template. You now have a working YAML pipeline (azure-pipelines.yml) in your repository that's ready for you to customize! 14

--- Page 141 ---
Course Code/Title: CS3V15/Devops Unit: V UNIT V - BUILDING DEVOPS PIPELINES USING AZURE Create Github Account, Create Repository, Create Azure Organization, Create a new pipeline, Build a sample code, Modify azure-pipelines.yaml file Github Account Creation What is Github? GitHub is a code hosting platform for collaboration and version control. GitHub lets you and others work together on projects fromanywhere. Github is owned by Microsoft, provides access to public(free) and private(paid) repositories. Stepsto create Github Account: Join GitHub · GitHub × + < C https://github.com/join f Facebook - Log In o ... Twitter G Google f Facebook YouTub Product v Solu wiki How Step 1:Go to https://github.com/join in a web browser. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 1 175" Rank

--- Page 142 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: Enter your personal details. In addition to creating a username and entering an email address, you'll also have to create a password. Your password must be at least 15 characters in length or at least 8 characters with at least one number and lowercase letter. Join GitHub First, let's create your user account Username * wikihowneveconcepts Email address * Password * .......... Make sure it's at least 15 characters OR at least 8 characters including a number and a lowercase letter. Learn more. Email preferences Send me occasional product updates, announcements, and offers. Verify your account wiki How Step 3: Click Verify to start the verification puzzle. The instructions vary by puzzle, so just follow the on-screen instructions to confirm that you are a human. A green checkmark will appear after completing the puzzle. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 2

--- Page 3 ---
Course Code/Title:CS3V15/Devops Unit: I Key Goals and Benefits of DevOps Goals of DevOps The fast-paced growth of the IT industry and continuous advancements in technology make it critical to set DevOps goals that are experimental and challenging for companies to compete and thrive in the market. Here are the key goals and principles that every successful DevOps program has in common. 1. Ensures effective collaboration between teams: Effective collaboration in any process relies on shared ownership. During the development process, all those involved should embrace the fact that everyone is equally responsible for the entire development process. Whether it is development, testing, or deployment, each team member should be involved. They should understand that they have an equal stake in the final outcome. In the DevOps paradigm, passing of work from one team to another is completely defined and broken down. This accelerates the entire process of development since collaboration between all the teams involved is streamlined. 2. Creates scalable infrastructure platforms: The primary focus of DevOps is to create a sustainable infrastructure for applications that make them highly scalable. According to the demands of the modern-day business world, scalable apps have become an absolute necessity. In an ideal situation, the process of scaling should be reliable and fully automated. As a result, the app will have the ability to adapt to any situation when a marketing effort goes viral. With the app being scalable, it can adjust itself to large traffic volumes and provide an immaculate user experience. 3. Builds on-demand release capabilities: Companies must focus on keeping their software in a 'releasable' state. Continuous delivery will allow the software to add new features and go live at any stage. DevOps aims to automate the process of release management because it has a plethora of advantages. Automated release management is predictable, fast, and very consistent. Moreover, through automation, companies can release new versions as per their requirements. Automated release management also has complete and thorough audit trials, as these are essential for compliance purposes. 4. Provides faster feedback: Automating monotonous tasks such as testing and reporting will accelerate the process of rapid feedback. Since the development team will know what has to change, it can roll out the updated version faster. In addition, the team can better understand the impact of the changes that it has done in the software lifecycle. A concrete understanding of changes will assist team members in working efficiently in tandem. With rapid feedback, the operations team and developers can make better decisions collectively and enhance the app's performance. Benefits of DevOps DevOps helps organizations deliver added value to their customers. Here are some compelling benefits of DevOps. 1. Smarter work and faster release: With DevOps, your development team can release the required deliverables quickly. Faster release of deliverables will keep you miles ahead of your competitors, which is very important in today's cut-throat business realm. Businesses should understand that if their review cycle is not automated, it will slow down the release process. Moreover, the inclusion of disparate tools will lead to context switching and higher costs. Thus, DevOps can help rectify this worrisome business situation. 2. Quick resolution of issues: In a business world where speed and accuracy are paramount, a fast feedback loop will help you thrive. With DevOps, the communication process becomes seamless, and, as such, it minimizes the time required to solve issues. Without open communication, key issues can slip out of mind, which will have serious repercussions in the long run. DevOps fosters open communication that helps resolve issues, thus unblocking the release pipeline faster. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 3 N32 nirf 1750 Rank

--- Page 4 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Better collaboration between teams: DevOps paves the way for more dynamic and round-the- clock communication between teams. It renders an environment for mutual collaboration and integration among teams that are distributed globally. Eliminating the traditional departmental barriers between teams forms a new sense of ownership, wherein each team member feels equally responsible for meeting delivery timelines. This collaboration contributes to happier and more engaged employees. 4. Fostering innovative mindsets: With DevOps, deployment phases of the application are more relaxed as compared to traditional methods. This is because it streamlines the entire process, ensures that there are no lapses in quality, and allows on-time and efficient release. Thus, as everything is in order, the development team is more at peace. This allows it to think out of the box and provide additional value to the user. Having a development team with an innovative mindset is a boon for any business organization. An innovative approach, in itself, has immense scope and leads to better quality and resolution of issues at hand. Thus, through DevOps, the process of expanding the horizon of an app becomes much easier. 5. Faster threat detection: Automated and continuous testing of the code will make the process of threat detection faster. As developers can locate problem areas at an early stage, they can then resolve them faster. Thus, DevOps is a vital cog in maintaining and enhancing the quality and performance of an app. As the overall build of the app is in capable hands, teams working together are empowered to share feedback as and when necessary. 6. Increased customer satisfaction: Customer satisfaction is paramount in any day and age, irrespective of the business one is involved in. DevOps is known for enhancing customer experience, which ultimately increases the level of customer satisfaction. Dissatisfied customers are never a good sign for any business. Feedback loops are an important component of DevOps. These loops empower end users to track the progress of app development at various stages. 7. In addition, they can suggest changes (if any) or give their inputs to make the app more customer- centric. Due to their dynamic nature, feedback loops help developers and customers remain on the same page. Moreover, DevOps accelerates the process of app development, which eventually lessens the delivery timer. This has a positive impact on the customer satisfaction ratio. 8. Providing the much-needed edge: Along with staying true to their development process, companies need to ensure that they sustain themselves in the cut-throat competition. Implementing DevOps can be your trump card to provide your organization with that much- needed edge. Competitive advantage is necessary, as it can become the deciding factor in the popularity of an application in many cases. Some factors set expert businesses apart from mediocre ones: · Top-quality features · Quicker and timely software releases · Maximizing return on investments · Listening to constructive feedback Difference between Agile and DevOps Agile: Agile program advancement comprises different approaches to computer program improvement beneath which prerequisites and arrangements advance through the collaborative exertion of self- organizing and cross-functional groups and their customer/end client. DevOps: DevOps could be a set of hones that combines program improvement and information- technology operations which points to abbreviating the framework's advancement life cycle and giving nonstop conveyance with tall program quality. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 4 N32 nirf 1750 Rank

--- Page 93 ---
Course Code/Title:CS3V15/Devops Unit: III And configure the Jenkins job as follows - General Description In this job we have used GitHub plugin Plain text Preview Discard old builds ? GitHub project This project is parameterized ? Throttle builds Execute concurrent builds if necessary Advanced V Source Code Management None Git ? Repositories ? Repository URL https://github.com/AnurdhaP/MyJavaPrograms.gil Please enter Git repository. Credentials ? - none - Add Advanced CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 18 N3 nirf 1750 Rank

--- Page 94 ---
Course Code/Title:CS3V15/Devops Unit: III Build Triggers Trigger builds remotely (e.g. from scripts) ? Build after other projects are built ? Build periodically ? GitHub hook trigger for GITSem polling ? Poll SCM ? Schedule ? A Do you really mean "every minute" when you say "" "" " ""? Perhaps you meant "H" "" "" to poll once per hour Would last have run at Wednesday, 30 August, 2023 at 4:11:02 pm india Standard Time; would'nest run at Wednesday, 30.A Ignore post-commit hooks ? Build Steps Execute Windows batch command ? Command See the list of available environment variables javas test.java java test. java ----- Advanced V Add build sten Save this configuration. Step 5 : Now if we build this job the output that we see is as follows on Console Output. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 19 N3 nirf 1750 Rank

--- Page 137 ---
Course Code/Title:CS3V15/Devops Unit:IV I Example 2: Get the Hostname and Version of remote servers with UNAME we have used the command module and executing an uname -a command as AD-HOC command $ ansible testservers -m command -a "uname -a" -i ansible hosts As Playbook - name: Check the remote host Hostname, Version, Distribution with UNAME hosts: testservers tasks: - name: Execute the UNAME command register: unameout command: "uname -a" - debug: var: unameout.stdout lines I Example 3: Check the Disk Usage of Remote server To get the disk usage, we are using df -h , here -h is human readable As Ad Hoc command $ ansible testservers -m command -a "df -h" -i ansible hosts As Playbook - name: Check the disk usage of all the file system in the remote servers hosts: testservers tasks: - name: Execute the df command register: dfout command: "df -h" - debug: var: dfout.stdout lines CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 138 ---
Course Code/Title:CS3V15/Devops Unit:IV Example 4: Restart Apache Server using Ansible Command Module So far, In all the sample we have seen we are using one application server and one web server. Totally two servers. But in this example, we need to limit our execution only to the web server as we are going to check the restart the apache web server. this is done using -- limit parameter As Ad hoc command $ ansible testservers -m command -a "httpd -k restart" -i ansible_hosts -b - limit As Playbook - name: restart apache web server hosts: testservers tasks: - name: restartapache register: httpdresout become: yes command: "httpd -k restart" when: ansible_hostname == "mwiweb02" - debug: var: httpdresout.stdout_lines I Example 5: Execute a command when a file exists or not exists There are two most useful parameters in ansible command module such as removes and creates removes - used to tell ansible to Execute the command only if the file exist creates - used to tell ansible to Execute the specified command only if the file does not exist Here we are going to do a very simple file creation and removal based on the file availability or existence. In General using the ansible command module, This method of creating and removing the file is not recommended as Ansible has a dedicated module named file to do the same effortlessly. As Ad hoc command Remove the file, if it does exist (or) present $ ansible testservers -a "rm -rf /tmp/testfile removes=/tmp/testfile" -i ansible_hosts CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 24 N33 nirf 1750 Rank

--- Page 23 ---
Course Code/Title:CS3V15/Devops Unit: I application. Amazon EC2 enables you to increase or decrease capacity within minutes. You can use one or hundreds or even thousands of server instances simultaneously. Because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs. Amazon EC2 is integrated with most AWS services, such as Amazon Simple Storage Service (Amazon S3), Amazon Relational Database Service (Amazon RDS), and Amazon Virtual Private Cloud (Amazon VPC) to provide a complete, secure solution for computing applications. Amazon EC2 is an example of Infrastructure as a Service(IaaS). EC2 delivers secure, reliable, cost-effective compute and high-performance compute infrastructure so as to meet the needs of demanding businesses. Amazon EC2 is one of the easiest ways of providing servers on AWS Cloud and also the access to Operating system. · AWS Lambda: AWS Lambda is a serverless, event-driven compute service that allows you to run code without managing servers. You pay only for the compute time you consume and there is no charge when your code is not running. With AWS Lambda, you can run code for any type of application with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services, or you can call it directly from any web or mobile app. But triggering Lambda is possible with over 200 AWS services. You can only pay for what you have used. The compute time that you consume, you are needed to pay for it. You just only need to upload your code and everything required to run will take care of by Lambda and it automatically scales your code with high availability. · AWS Elastic Beanstalk: AWS Elastic Beanstalk is a Platform as a Service that facilitates quick deployment of your applications by providing all the application services that you need for your application. Beanstalk is a plug- and-play platform that allows working with multiple programming languages and environments. Elastic Beanstalk supports a large range of platforms like Node js, Java, PHP, Python, and Ruby. So, you can develop your application to meet your requirements and simply deploy it on Elastic Beanstalk. The main aim to use AWS Elastic Beanstalk is to allow you to focus on the deployment and management of your applications. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. 2. Networking · Amazon VPC: Amazon VPC is your network environment in the cloud. It allows you to create a private network within the AWS cloud that uses many of the same concepts and constructs as an on-premises network. Amazon VPC also gives you complete control of the network configuration. Customers can define normal networking configuration items such as IP address ranges, subnet creation, route table creation, network gateways, and security settings. Amazon VPC is an AWS foundational service and integrates with numerous AWS services. For instance, Amazon EC2 instances are deployed into your CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 24 ---
Course Code/Title:CS3V15/Devops Unit: I Amazon VPC. Similarly, Amazon Relational Database Service (Amazon RDS) database instances deploy into your Amazon VPC, where the database is protected by the structure of the network just like your on-premises network. You can easily launch AWS resources into a virtual network by Amazon Virtual Private Cloud. An isolated virtual network environment in the AWS cloud is created by Amazon VPC. . Amazon Route 53: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating human-readable names, such as www.geeksforgeeks.com, into the numeric IP addresses that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. 2. Storage · Amazon S3 (Simple Storage Service): Amazon Simple Storage Service (Amazon S3) is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the web. It is designed to provide an infinite amount of storage and it is delivered with 99.999999999% durability. You can use Amazon S3 as primary storage for cloud-native applications as a target for backup and recovery and disaster recovery. It offers industry-leading scalability, data availability, security, and performance. It's simple to move large volumes of data into or out of Amazon S3 with Amazon's cloud data migration options. Once data is stored in Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like Amazon S3 Standard - Infrequent Access and Amazon Glacier for archiving. · Amazon Glacier: Amazon Glacier is a secure, durable, and extremely low- cost storage service for data archiving and long-term backup. Data stored in Amazon Glacier takes several hours to retrieve, which is why it's ideal for archiving. The fastest access to your archive data is via Amazon Glacier. 3. Databases · Amazon RDS (Relational Database Service): Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost- efficient and resizable capacity while managing time-consuming database administration tasks, freeing you up to focus on your applications and business. You can find Amazon RDS is also available on several database instance types - optimized for memory, performance, or I/O. Amazon RDS provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. · Amazon DynamoDB (Non-Relational Database): Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. When you create a database table that can store and retrieve any amount of data you can simply use Amazon DynamoDB that will serve any level of requested traffic. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, Internet of Things (IoT), and many other applications. DynamoDB provides many features like · built-in security CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 24 N32 nirf 1750 Rank

--- Page 133 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible Roles Roles provide a framework for fully independent or interdependent collections of files, tasks, templates, variables, and modules. The role is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks and makes them easier to reuse. The breaking of the playbook allows you to break the playbook into reusable components. Each role is limited to a particular functionality or desired output, with all the necessary steps to provide that result either within the same role itself or in other roles listed as dependencies. Roles are not playbooks. Roles are small functionality that can be used within the playbooks independently. Roles have no specific setting for which hosts the role will apply. Top-level playbooks are the bridge holding the hosts from your inventory file to roles that should be applied to those hosts. Creating a Role The directory structure for roles is essential to creating a new role, such as: Role Structure The roles have a structured layout on the file system. You can change the default structured of the roles as well. For example, let us stick to the default structure of the roles. Each role is a directory tree in itself. So the role name is the directory name within the /roles directory. $ ansible-galaxy -h Usage ansible-galaxy[delete|import|info|init|install|list|login|remove|search|setup][ -- help] [options] ... Options o -h: (help) it shows this help message and exit. o -v: (verbose) Verbose mode (-vvv for more, -vvvv to enable connection debugging). o -- version: it shows program version number and exit. Roles are stored in separate directories and have a particular directory structure CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 19 N33 nirf 1750 Rank

--- Page 134 ---
Course Code/Title:CS3V15/Devops Unit: IV Typical structure of ansible role myweb defaults L main.yml files · handlers L main.yml meta. L main.yml README.md tasks L_ main.yml templates tests inventory test.yml vars L main.yml · defaults : It stores the default variable for the role. For example the default port number is http is 8080, then it can be stored in defaults. · files : This folder contains the files required to transfer or deploy to the target machines. · handlers : It contains handlers, which may be used by some role. · meta : It defines some data or information about the role. · tasks : It contains the main list of tasks to be executed by the role. These tasks might be defined in separate files as per the functionalities. · templates : It contains the templates which can be deployed via a role. Ansible Module Ansible Command module is used to execute commands on a remote node. The Command module, is used mostly to run simple Linux commands on a remote node/server which is part of a host group or Stand alone server mentioned in the host group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 125 ---
Course Code/Title:CS3V15/Devops Unit: IV This command executes the specified playbook while prompting for the necessary privilege escalation password. Output and Result: Activities Terminal - Nov 2 13:00 . auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook-eg2.yaml PLAY [Install Nginx] TASK [Gathering Facts] Total: [vnet]: FAILEDT .. ['msg": "Missing sudo password') PLAY RECAP : ok=0 changed=0 unreachable=0 Fatled =! skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory. yaml playbook-eg2.yaml - - ask-becone-pass BECOME password: PLAY [Install Nginx] TASK [Gathering Facts] ok: [vri01] TASK [Install Nginx using apt] ok: [vri01] PLAY RECAP * VMD1 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ [] As shown in the example image above, the first command encountered an error due to the absence of the -ask-become-pass flag. This flag is essential when your task requires elevated privileges, as it prompts Ansible to request the sudo password for authentication. The corrected command includes the -ask-become-pass flag, ensuring that the necessary privileges are obtained before executing the task. This is particularly important when working with tasks that require administrative access, such as package installations or system configurations. Note: In this blog, we primarily focus on the core and fundamental components of Ansible, providing an introductory overview and understanding of its key concepts. While Ansible offers advanced features such as "Collections," "Ansible Galaxy," "Roles," "Dynamic Inventory," and "Custom Modules and Plugins," we recognize that these topics are extensive and may require separate dedicated discussions. For the purpose of this blog, we aim to establish a strong foundation of Ansible's core components. If you're interested in diving deeper into these advanced topics, we recommend exploring Ansible's official documentation and additional resources dedicated to each subject. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 11 N32 nirf 1750 Rank

--- Page 126 ---
Course Code/Title:CS3V15/Devops Unit:IV Understanding YAML Ansible uses YAML syntax for expressing Ansible playbooks. This chapter provides an overview of YAML. Ansible uses YAML because it is very easy for humans to understand, read and write when compared to other data formats like XML and JSON. Every YAML file optionally starts with " --- " and ends with " ... ". In this section, we will learn the different ways in which the YAML data is represented. key-value pair YAML uses simple key-value pair to represent the data. The dictionary is represented in key: value pair. Note - There should be space between : and value. Example: A student record --- #Optional YAML start syntax james: name: james john rollNo: 34 div: B sex: male ... #Optional YAML end syntax Abbreviation You can also use abbreviation to represent dictionaries. Example James: {name: james john, rollNo: 34, div: B, sex: male} Representing List We can also represent List in YAML. Every element(member) of list should be written in a new line with same indentation starting with "- " (- and space). Example countries: - America - China - Canada - Iceland CHENNAI INSTITUTE OF TECHNOLOGY CHENNAI INSTITUTE . TECHNOLOGY (Autonomous) 12 N33 nirf 1750 Rank

--- Page 159 ---
Choose a job to see the steps for that job From the < Jobs in run #20191 ... FabrikamFiber Build O Build Build 40s Initialize job 1s 2 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 Duration: 40s 6 Pool: Azure Pipelines Image: Ubuntu-16.04 7 > Job preparation parameters Checkout 3s CmdLine 25 Component Detect 32s Post-job: Checkout <1s Finalize Job <1s Deploy DeployWeb 10s Finalize build Report build status <1s steps view, you can review the status and details of eachstep. From the Moreactions you can toggle timestamps or view a raw log of all steps in the pipeline. Build 9 Pool: Azure Pipelines 2 Image: Ubuntu-22.04 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 View job raw log Toggle timestamps 6 Duration: 40s Job preparation parameters 10

--- Page 160 ---
Cancel and re-run a pipeline If the pipeline is running, you can cancel it by choosing Cancel. If the run has completed, you can re-run the pipeline by choosing Run new. Pipeline run more actions menu: #20191210.3 Update azure-pipelines.yml for Azure Pip ... on FabrikamFiber Cancel Stages Jobs € Build C Deploy 0/1 compléted 48s Not started Build 48s Cancel From the More actions menu you can download logs, add tags, edit the pipeline, delete the run, and configure retention for the run. cure Pipe ... Run new Download logs Add tags Edit pipeline View retention leases Retain Delete 20

--- Page 123 ---
Course Code/Title:CS3V15/Devops Unit: IV Ad-Hoc Limitations: While ad-hoc commands are ideal for single, immediate tasks, they are not designed for complex, multi- step automation. Playbooks come to the rescue when tasks have interdependencies and need to be executed in a coordinated manner. Readability and Reusability: Playbooks are authored in YAML, a human-readable and straightforward format. This not only makes them easy to write but also facilitates sharing and collaboration. You can reuse playbooks across various scenarios, saving time and effort. Conditional and Looping Logic: Playbooks offer advanced features, including conditional statements and looping, that allow you to adapt automation to different situations. This flexibility makes playbooks versatile and capable of handling a wide range of automation needs. In essence, playbooks are your tool of choice when automation tasks become multi-faceted and require a structured and logical approach. They provide the power to streamline and automate complex workflows with precision and efficiency. Example: Ping All Inventory Devices To illustrate the use of a playbook, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: playbook.yaml 1 --- 2 -name : My First Play #Name of play 3 hosts: virtualmachines #defining host can be all 4 tasks: 5 -name:Ping My Hosts #Name of task 6 ansible.builtin.ping: # can also write ping To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 124 ---
Course Code/Title:CS3V15/Devops Unit: IV Output and Result: Activities Terminal - Nov 2 12:29 . O auriga@auriga-Latitude-E7470: - /Desktop/ansible X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook.eg1.yaml PLAY [My First Play] TASK [Gathering Facts] ok: [ vri01] TASK [Ping Devices] ok: [vm01] PLAY RECAP vn01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$[] rescued=0 ignored=0 X As you can see there is gathering facts task which we didn't create is running it is the default task which ping the connection. Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: playbook.yaml 1 --- 2- name: Install Nginx 3 hosts: all 4 become: yes 5 tasks: 6 - name: Install Nginx using apt 7 apt: 8 name: nginx 9 state: present To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml -- ask-become-pass CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 10 N33 nirf 1750 Rank

--- Page 35 ---
Unit: I Course Code/Title:CS3V15/Devops Push: Push is the operation that sends your committed changes to a remote repository, making them accessible to others. Git workflow v0.1 v0.2 v1.0 Master Hotfix Release Develop Feature Feature Git Flow is a structured branching model designed for projects with well- defined release cycles and a need for strict quality control. Branches: The branching model described is commonly known as the Gitflow Workflow. It's a branching strategy that defines a strict branching model designed to facilitate collaboration and streamline the release process. Let's go into detail about each branch: clone operation Working Copy edit, add, move files update operation Modify working copy status and diff operation Review changes commit and push operation push operation Commit changes push operation amend and push operation Review changes 1. Master Branch: The `master'branch represents the main codebase and contains production- ready code. This branch is typically stable and should only include thoroughly tested and approved changes. Each commit on the 'master' branch represents a new version or release of the software. 2. Develop Branch: The 'develop' branch is an integration branch where various feature branches are merged. It serves as a staging area for testing new features and ensuring they work well together before merging into the 'master' branch. This branch may have ongoing development work and is not necessarily always in a production- ready state. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 35 nirf 1750 Rank N33

--- Page 36 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Feature Branches: Feature branches are created for developing new features or implementing changes. These branches are typically based on the 'develop'branch. Once a feature is complete, the branch is merged back into the 'develop' branch. Feature branches allow developers to work on specific tasks without affecting the main codebase. 4. Release Branch: The 'release' branch is created when the 'develop' branch reaches a point where it is ready for a production release. This branch is used for final testing, bug fixes, and preparing the code for deployment. No new features should be added to the release branch. Once the release is deemed stable, it is merged into both the 'master' branch and the 'develop' branch. 5. Hotfix Branch: The 'hotfix' branch is used to quickly address critical issues or bugs in the production code. It is created directly from the 'master' branch. Hotfixes are intended to be small and focused on resolving the specific issue at hand. Once the hotfix is complete, it is merged into both the 'master'branch and the `develop'branch to ensure that the fix is applied to future releases. Here is the typical flow: · Developers work on feature branches based on the 'develop' branch. · Completed features are merged into the 'develop' branch. · When ready for a release, a 'release' branch is created from 'develop". · The release branch undergoes testing and bug fixes. · The release branch is merged into both 'master' and 'develop' once it's stable. · If a critical issue arises in production, a 'hotfix' branch is created from 'master'. · The hotfix is merged into both 'master' and 'develop to keep both branches in sync. · This Gitflow Workflow helps maintain a structured development process, ensuring that features are developed, tested, and released in a controlled manner. Example Scenario: · Imagine you're working on a large software project with a team of 10 developers. You have a major release planned for every six months. · You create a "feature/Ticket-Id" branch to develop a new login system. · Once the feature is complete, it's merged into the "develop" branch for integration and testing. · As the release date approaches, you create a "release/v1.0" branch to freeze code for the upcoming release. · Any critical issues discovered in the production environment are fixed in "hotfix" branch and merged into "master" and "develop." CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 36 N33 nirf 1750 Rank

--- Page 163 ---
‹ FabrikamFiber Runs Branches Analytics Description Edit Run pipeline : Stages #20210609.3 Update azure-pipelines.yml for Azure Pipelines Manually triggered for main 9 61bba8c x2 La Wednesday Ø 215 #20210609.2 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for 1º main 9 bc6c105 x La Wednesday ₾ 305 #20210609.1 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for main 9 53eb92b x • #20210520.1 Updated README.md Individual CI for & new-branch 0 1ada387 ‹ azure-cli-example go main V azure-cli-example / azure-pipelines.yml 30 pool: default 31 32 steps: 33 # Specify python version and install if needed Settings 34 - task: UsePythonVersion@0 35 condition: false 36 inputs: 37 -versionSpec: '3.x' 38 architecture: 'x64' 39 40 # Update pip to latest 41 - bash: python -m pip install -- upgrade pip 42 condition: false 43 displayName: 'Upgrade pip' 44 45 container 46 &continueOnError 47 48 88988 name & parameters & resources & schedules 49 50 51 services 52 strategy 53 & variables 54 & workspace La Wednesday .455 Là May 20 ₾ 135 Variables Run : Tasks 7 Search tasks dotnet .NET Core Build, test, package, or publish a dotnet applicatio. Android signing Sign and align Android APK files Ant Build with Apache Ant 80 App Center distribute Distribute app builds to testers and users via Visu .. App Center test Test app packages with Visual Studio App Center Archive files Compress files into .7z, tar.gz on zip ARM template deployment Deploy an Azure Resource Manager (ARM) templ .. Azure App Service deploy Deploy to Azure App Service a web, mobile, or AP. · The YAML pipeline editor provides several keyboard shortcuts, which we show in the following examples. . Choose Ctrl+Space for Intellisense support while you're editing the YAML pipeline.

--- Page 164 ---
· The task assistant provides a method for adding tasksto your YAML pipeline. · To display the task assistant, edit your YAML pipeline and choose Show assistant. Understand the azure-pipelines.yml file · A pipeline is defined using a YAML file in your repo. Usually, this file is named azure-pipelines.yml and is located at the root of your repo. · Navigate to the Pipelines page in Azure Pipelines, select the pipeline you created, and choose Edit in the context menu of the pipeline to open the YAMLeditor for the pipeline. · This pipeline runs whenever your team pushes a change to the main branch of your repo or creates a pull request. It runs on a Microsoft-hosted Linux machine. · The pipeline process has a single step, which is to run the Maven task. YAML:Code trigger: - main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdk Version: "1.10" jdk11_windows: imageName: "windows-latest" jdk Version: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdk VersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: " ** /TEST -*. xml"goals: "package" Change the platform to build · Navigate to the editor for your pipeline by selectingEdit pipeline action on the build, or by selecting Edit from the pipeline's main page. · To choose a different platform like Windows or Mac, change thevmImage value: · pool: · vmImage: "windows-latest" Add steps . You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can usetasks for building, testing, publishing, or deploying your app. For Java, the

--- Page 149 ---
Course Code/Title: CS3V15/Devops Unit: V Step 6: Now you will see that all of our files uploaded in our github. Namanbhatia7 Add initial resume website files Latest commit f47be5b 1 minute ago Display_Picture.png Add initial resume website files 1 minute ago My_Resume.html Add initial resume website files 1 minute ago README.md Initial commit 15 minutes ago Styles.css Add initial resume website files 1 minute ago Branch: · A GitHub branch is used to work with different versions of a repository at the same time. · By default a repository has a master branch (aproduction branch). · Any other branch is a copy of the master branch (as itwas at a point in time). · New Branches are for bug fixes and feature work separate from the master branch. When changes are ready, they can be merged into the master branch. If youmake changes to the master branch while working on a new branch, these updates can be pulled in. Commits: At GitHub, changes are called commits. Each commit (change) has a description explaining why achange was made. Pull Requests : . Pull Requests are the heart of GitHub collaboration. · With a pull request you are proposing that your changesshould be merged (pulled in) with the master. · Pull requests show content differences, changes, additions, and subtractions in colors (green and red). · As soon as you have a commit, you can open a pull request and start a discussion, even before the code isfinished. Git: · Git was created by Linus Torvalds in 2005 to develop Linux Kernel · Git is an open-source distributed version control system. It is designed to handle minor to major projectswith high speed and efficiency. . It is developed to co-ordinate the work among the developers. The version control allows us to track andwork together with our team members at the same workspace. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 9 175" Rank

--- Page 150 ---
CREATE AZURE ORGANIZATION AZURE: Microsoft Azure, often referred to as Azure cloud computing platform run by Microsoft. It offers access, management, and the development of applications and services through global data centers It also provides a range of capabilities, including software as a service (SaaS), platform as a service, and infrastructure as a service (IaaS). It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014 Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Prerequisites: To plan your organizational structure. Microsoft accounts or authenticate users with Microsoft Entra ID. For more information, see Choosing your organization administrator account type. Create an organization: 1. Sign in to Azure DevOps. 2. Select New organization 3. Confirm information, and then select Continue 10

--- Page 9 ---
Course Code/Title:CS3V15/Devops Unit: I 9. Slack Slack is a crucial tool for businesses and organizations worldwide. Its significance lies in facilitating seamless communication and collaboration among teams, whether working in the same office or remotely. Slack's real-time messaging, file sharing, and integration capabilities streamline workflow, enhance productivity and keep teams connected across different time zones and locations. As the work landscape evolves, with more companies embracing hybrid and remote work models, Slack is a vital hub for quick decision-making, project coordination, and knowledge sharing. With an ever- expanding ecosystem of integrations and features, Slack remains at the forefront of modern workplace communication, making it essential for businesses to stay agile, efficient, and competitive. 10. AWS Cloud Computing and Storage in DevOps AWS (Amazon Web Services) Cloud Computing and Storage are crucial in DevOps because they provide scalable, flexible, and cost-effective infrastructure for DevOps practices. AWS offers many services, including compute resources, databases, container orchestration, and serverless computing, which align perfectly with modern software development and deployment demands. Organizations adopt DevOps to accelerate software delivery. AWS provides the foundation for rapidly deploying and scaling applications, supporting continuous integration and continuous delivery (CI/CD) pipelines, and automating infrastructure provisioning through tools like AWS CloudFormation. Furthermore, AWS's storage solutions enable efficient data management, backup, and recovery, ensuring the resilience and reliability required for DevOps operations. As cloud technology evolves, AWS remains at the forefront, enabling DevOps teams to focus on innovation and efficiency. 11. Azure Cloud Computing and Storage in DevOps Azure Cloud Computing and Storage will be pivotal in DevOps practices in 2024 and beyond. Azure offers a comprehensive cloud ecosystem that enables organizations to scale their infrastructure, deploy applications, and store data efficiently. Azure provides essential services for continuous integration and continuous deployment (CI/CD), automation, monitoring, and security. Its cloud computing capabilities facilitate the provisioning of resources on demand, ensuring that development and testing environments are readily available. Azure's storage solutions, including Azure Blob Storage, Azure Files, and Azure SQL Database, enable secure data storage and retrieval, supporting the data-driven aspects of DevOps. Besides, Azure's integration with DevOps tools like Azure DevOps Services streamlines the software development lifecycle, enhancing collaboration and automation. 12. GCP Cloud Computing and Storage in DevOps Google Cloud Platform (GCP) offers robust cloud computing and storage solutions. GCP provides a scalable, reliable, and highly available infrastructure essential for modern DevOps practices. With its comprehensive set of services, including Google Compute Engine, Google Kubernetes Engine, Cloud Storage, and BigQuery, GCP empowers DevOps teams to build, deploy, and manage applications easily. Its emphasis on automation, infrastructure as code, and container orchestration aligns seamlessly with DevOps principles. Moreover, GCP's cutting-edge technologies, such as AI and machine learning capabilities, provide DevOps practitioners with advanced tools for monitoring, analytics, and automation, making it a powerful choice for organizations seeking to optimize their software development and delivery processes. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 10 ---
Course Code/Title:CS3V15/Devops Unit: I 13. Monitoring, Alerting, and Incident Response Tools: SignalFx Monitoring, alerting, and incident response tools like SignalFx are pivotal in DevOps and software development. As software systems become complex and distributed, the need for real-time visibility into performance and the ability to respond swiftly to incidents is significant. SignalFx excels in this regard by providing advanced monitoring and observability solutions that enable organizations to detect anomalies, trace issues across microservices proactively, and set up intelligent alerts. As applications scale, cloud-native architectures become the norm, and user expectations for reliability grow, SignalFx's capabilities are crucial. It empowers DevOps teams to ensure high availability, optimize resource utilization, and maintain a seamless user experience by identifying and addressing performance issues before they impact end-users. It is one of the most essential tools for modern software operations. 14. Appdynamics AppDynamics, a leading application performance management and monitoring platform, remains critically important as it ensures the optimal performance of modern digital businesses. As organizations rely on complex and distributed software systems, proactively monitoring, troubleshooting, and optimizing these applications becomes essential. AppDynamics provides real-time visibility into application performance, allowing businesses to swiftly identify bottlenecks, latency issues, and errors. With the ever-growing complexity of applications, the importance of AppDynamics lies in its ability to empower organizations to deliver exceptional user experiences, maintain application reliability, and swiftly respond to performance issues, thereby ensuring the continued success and competitiveness of digital businesses. 15. Raygun It is a crucial tool in software development and DevOps because it ensures application reliability and performance. Raygun is an application monitoring and error-tracking platform that empowers development teams to identify, diagnose, and resolve real-time issues. With software systems growing in complexity and the increased demand for seamless user experiences, Raygun's importance lies in providing actionable insights into application errors and performance bottlenecks. It enables organizations to proactively address issues, reduce downtime, and enhance user satisfaction, leading to higher software quality and improved customer experiences. Software is central to businesses across industries. Raygun's role in maintaining application health and facilitating rapid issue resolution makes it a fundamental tool for DevOps professionals and software developers. 16. Splunk Cloud Splunk Cloud helps organizations gain critical insights from the ever-expanding volume of data generated in today's digital landscape. As businesses increasingly rely on data- driven decision-making, Splunk Cloud stands out as a robust and scalable platform for monitoring, searching, analyzing, and visualizing machine-generated data. Its importance lies in providing real-time visibility into the health and performance of complex systems, applications, and infrastructures, enabling rapid incident detection and response. As cybersecurity threats evolve, Splunk Cloud's advanced security analytics and threat detection capabilities remain indispensable for safeguarding against cyberattacks and ensuring data integrity. In a world where data is a strategic asset, Splunk Cloud's role in harnessing the power of data for operational excellence and security cannot be overstated. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 10 N32 nirf 1750 Rank

--- Page 139 ---
Course Code/Title:CS3V15/Devops Unit: IV Create the file, if it does NOT exist (or) absent $ ansible testservers -a "touch /tmp/testfile creates=/tmp/testfile" -i ansible_hosts As Playbook - name: "Validate if a file is present or not present using Ansible Command module" hosts: testservers tasks: - name: "Create a file if it does not exist" command: "touch /tmp/latestfile" args: creates: "/tmp/latestfile" register: createif - name: "Display the file to make sure its created" command: "ls -lrt /tmp/latestfile" register: displayif when: createif is changed - debug: var=displayif.stdout - name: "Remove the file if it exist" command: "rm -rf /tmp/latestfile" args: removes: "/tmp/latestfile" register: removeif In the playbook we execute three tasks, • First one is a create a file if it does not exist • Second to display if the creation is successful • Third one is to delete the file if it exists Example 5: Execute or Run the Script when a file exists or not exists Now for this example let us take something relatable to real world scenario. Like Start the Server instance (or) program if the PID file or LOCK file does not exist Here is the sample playbook for you to get started. I am giving a general playbook here as I do not want to pick some server/technology of my own which you cannot relate to. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 140 ---
Course Code/Title:CS3V15/Devops Unit:IV you can feel free to modify the script to your needs as this is just a template. - name: Start of Stop Server instance based on PID/LOCK file availability hosts: appservers tasks: - name: Start the instance when the PID file is not present become: yes become_user: appuser command: "startserver.sh" args: creates: "/path/to/pid/instance.pid" register: startinst - name: Stop the instance when the lock file is present become: yes become_user: appuser command: "stopserver.sh" args: removes: "/path/to/lockfile/instance.lck" register: stopinst CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 26 N33 nirf 1750 Rank

--- Page 129 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible ad-hoc Commands The Ansible ad-hoc command uses the /usr/bin/ansible command-line tool to automate a single task on one or more managed nodes. The Ad-hoc commands are quick and easy, but they are not re-usable. The Ad-hoc commands demonstrate the simplicity and power of Ansible. Syntax ansible <hosts> [-m <module_name>] -a <"arguments"> -u <username> [ -- become] Explanation Hosts: It can be an entry in the inventory file. For specifying all hosts in the inventory, use all or "*" . module_name: It is an optional parameter. There are hundreds of modules available in the Ansible, such as shell, yum, apt, file, and copy. By default, it is the command. Arguments: We should pass values that are required by the module. It can change according to the module used. Username: It specifies the user account in which Ansible can execute commands. Become: It's an optional parameter specified when we want to run operations that need sudo privilege. By default, it becomes false. 1. Parallelism and shell commands You can reboot your company server in 12 parallel forks at the same time. For this, you need to set up the SSHagent for connection. $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa To run reboot for all your company servers in the group, 'abc', in 12 parallel forks: $ ansible abc -a "/sbin/reboot" -f 12 By default, Ansible will run the above ad-hoc commands from the current user account. If you want to change then pass the username in ad-hoc command as follows: $ ansible abc -a "/sbin/reboot" -f 12 -u username 2. File Transfer You can use ad-hoc commands for doing SCP (secure copy protocol) which means lots of files in parallel on multiple machines or servers. CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 15 N32 nirf 175º Rank

--- Page 130 ---
Course Code/Title:CS3V15/Devops Unit:IV Transferring file on many machines or servers $ ansible abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf" Creating new directory $ ansible abc -m file - a "dest = /path/user1/new mode = 888 owner = user group = user1 state = directory" Deleting all directory and files $ ansible abc -m file -a "dest = /path/user1/new state = absent" 3. Managing Packages Ad-hoc commands are available for apt and yum module. Here are the following ad-hoc commands using yum. Below command checks, if the yum package is installed or not, but not update it. $ ansible abc -m yum -a "name = demo-tomcat-1 state = present" Below command checks the package is not installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = absent" And below command checks the latest version of package is installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = latest" 4. Managing Users and Groups You can manage, create, and remove a user account on your managed nodes with ad-hoc commands. $ ansible all -m user -a "name=foo password =< crypted password here>" $ ansible all -m user -a "name=foo state=absent" 5. Managing Services Ensure a service is started on all the webservers. $ ansible webservers -m service -a "name=httpd state=started" Alternatively, restart a service on all webservers: $ ansible webservers -m service -a "name=httpd state=restarted" CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 nirf 1750 Rank

--- Page 143 ---
Unit: V Course Code/Title: CS3V15/Devops Email preferences Send me occasional product updates, announcements, and offers. Verify your account Pick one square that shows two identical objects. wiki How Step 4: Click the green Create account button. It's belowthe form, at the bottom of the page. This will take you to an email verification page. Verify your account Create account By creating an account, you agree to the Terms of Service. For more information about GitHub's privacy practices, see the GitHub Privacy Statement. We'll occasionally send you account-related wikiHow UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 3

--- Page 144 ---
Course Code/Title: CS3V15/Devops Unit: V Carefully review the Terms of Service at https://help.github.com/en/articles/github- terms-of-service and the Privacy Statement at https://help.github.com/en/articles/github- privacy-statement before you continue. You're almost done! We sent a launch code to wikihowneveconcepts@gmail.com + Enter code Didn't get your email? Resend the code or update your email address. wiki How Step 5: Verify your email by entering the code. After clicking Create account, you'll receive an email with a code.Enter this code on the verification page. Entering the code will automatically take you to the welcome page. Step 6: Select your preferences and click Continue. GitHub displays a quick survey that can help you tailor your experience to match what you're looking for. You'll be sent tothe plan selection page after completing the survey. How many team members will be working with you? This will help us guide you to the tools that are best suited for your projects. Just me 2 - 5 5 - 10 10 - 20 20 - 50 50+ Are you a student or teacher? Student Teacher Continue wikiHow CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 4 175° Rank ---

--- Page 161 ---
Add a status badge to your repository Many developers like to show that they're keeping their code quality high by displaying a status badge in their repo. Azure Pipelines succeeded To copy the status badge to your clipboard: 1. In Azure Pipelines, go to the Pipelines page to view the list of pipelines.Select the pipeline you created in the previous section. 2. Select , and then select Status badge. 3. Select Status badge. 4. Copy the sample Markdown from the Sample markdown section. Now with the badge Markdown in your clipboard, take the following steps in GitHub: 1. Go to the list of files and select Readme.md. Select the pencil icon to edit. 2. Paste the status badge Markdown at the beginning of the file. 3. Commit the change to the main branch. 4. Notice that the status badge appears in the description of your repository. To configure anonymous access to badges for private projects: 1. Navigate to Project Settings in the bottom left corner of the page 2. Open the Settings tab under Pipelines 3. Toggle the Disable anonymous access to badges slider under General 21

--- Page 162 ---
Azure-Pipeline • Azure Pipelineș provides a YAML pipeline editor that you can use to author and edit your pipelines. • The YAML editor is based on the Monaco Editor. • The editor provides tools like Intellisense support anda task assistant to provide guidance while you edit a pipeline. • You can also edit pipelines by modifying the azure pipelines.yml file directly in your pipeline's repository using a text editor of your choice Edit a YAML pipeline To access the YAML pipeline editor, do the following steps. • Sign in to your organization • (https://dev.azure.com/{yourorganization}). • Select your project, choose Pipelines, and then select the pipeline you want to edit. You can browse pipelines by Recent, All, and Runs. • Choose Edit. • Make edits to your pipeline using Intellisense and the task assistant forguidance. Azure DevOps fabrikam-tailspin FabrikamFiber F FabrikamFiber + Pipelines Overview Recent All Runs Boards Pipelines Recently run pipelines Repos 1 Pipeline Pipelines Pipelines FabrikamFiber 2 3 17

--- Page 131 ---
Course Code/Title:CS3V15/Devops Unit: IV Ensure a service is stopped: $ ansible webservers -m service -a "name=httpd state=stopped" 6. Gathering Facts Fact represents the discovered variables about a system. You can use the facts to implement conditional execution of tasks, and also used to get ad-hoc information about your systems. To see all the facts: $ ansible all -m setup Ansible Playbooks Playbooks are the files where the Ansible code is written. Playbooks are written in YAML format. YAML means "Yet Another Markup Language," so there is not much syntax needed. Playbooks are one of the core features of Ansible and tell Ansible what to execute, and it is used in complex scenarios. They offer increased flexibility. Playbooks contain the steps which the user wants to execute on a particular machine. And playbooks are run sequentially. Playbooks are the building blocks for all the use cases of Ansible. Ansible playbooks tend to be more configuration language than a programming language. Through a playbook, you can designate specific roles to some of the hosts and other roles to other hosts. By doing this, you can orchestrate multiple servers in very different scenarios, all in one playbook. Playbook Structure Each playbook is a collection of one or more plays. Playbooks are structured by using Plays. There can be more than one play inside a playbook. Playbook Play Task Module Target Environment O Task Module Play Task Target Environment Play Module O Task CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY 17 N33 nirf 1750 Rank

--- Page 132 ---
Course Code/Title:CS3V15/Devops Unit:IV name: install and configure DB hosts: testServer become: yes vars: oracle_db_port_value : 1521 tasks: -name: Install the Oracle DB yum: < code to install the DB> -name: Ensure the installed service is enabled and running service: name: < your service name> The Different YAML Tags name This tag specifies the name of the Ansible playbook. As in what this playbook will be doing. Any logical name can be given to the playbook. hosts This tag specifies the lists of hosts or host group against which we want to run the task. The hosts field/tag is mandatory. It tells Ansible on which hosts to run the listed tasks. The tasks can be run on the same machine or on a remote machine. One can run the tasks on multiple machines and hence hosts tag can have a group of hosts' entry as well. vars Vars tag lets you define the variables which you can use in your playbook. Usage is similar to variables in any programming language. tasks All playbooks should contain tasks or a list of tasks to be executed. Tasks are a list of actions one needs to perform. A tasks field contains the name of the task. This works as the help text for the user. It is not mandatory but proves useful in debugging the playbook. Each task internally links to a piece of code called a module. A module that should be executed, and arguments that are required for the module you want to execute. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 18 N33 nirf 1750 Rank

--- Page 145 ---
Course Code/Title: CS3V15/Devops Unit: V Step 7:Note the types of plans offered by GitHub. There are a few different plans to choose from, varying in the amount of features provided. Learn to ship software like a pro. GitHub gives students free access to the best developer tools so they can learn by doing. Free Get additional student benefits GitHub Pro Unlimited public/private repositories 2,000 CI/CD minutes/month Free for public repositories 500MB of Packages storage Free for public repositories Protect your branches Ensure that collaborators on your repository cannot make irrevocable chang branches. O Draft pull requests O 120 core-hours of Codespaces compute O Pages and Wikis 15GB of Codespaces storage O Community support ® 3,000 CI/CD minutes/month Free for public repositories 2GB of Packages storage Free for public repositories O 180 core-hours of Codespaces compute 20GB of Codespaces storage Web-based support wikiHow Step 8: Select the free plan. On the plan selection page, scrolldown to click the button for choosing a free plan. This will immediately take you to your GitHub dashboard. Continue for free wiki How UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 5 N33 In 175º Rank ---

--- Page 146 ---
Course Code/Title: CS3V15/Devops Unit: V · If you choose a paid plan, you'll have to enter yourpayment information as requested before you can continue. . If you want to upgrade your Github account in the future, click the menu at the top- right corner, select Settings, andchoose Billing and plans to view your options. GitHub essentials are: · Repositories · Branches · Commits · Pull Requests · Git (the version control software GitHub is built on) Repository: A GitHub repository can be used to store a developmentproject. It can contain folders and any type of files (HTML, CSS,Javascript, Documents, Data, Images). A GitHub repository should also include a licence file and a README file about the project. A GitHub repository can also be used to store ideas, or any resources that you want to share. Github Repository Creation Step 1: Click on the new repository option Marketplace Explore New repository and GitHub without any code! Catch Universe Join a community Universe in San Fr ticket pricing-he Import repository × New gist bird New organization New project llo World guide, you'll create a repository, start a , write comments, and open a pull request. GitHub Sponsors Matching Fund × Ready to support open source? GitHub will match your contribution to developers during their first year in GitHub Sponsors. d the guide Start a project × Welcome to the new dashboard. Get closer to the stuff you care about most. Explore repositories atlas-engineer/next Next browser - Be productive. UnENVIVAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 175° Rank 6

--- Page 157 ---
Select a pipeline run to view information about that run. You can choose to Retain or Delete a run from the context menu. For more information on run retention, see Build and release retention policies. Stages La 51m ago > ... 6 42s Retain Delete Ld 2h ago 0 1m 13s - View pipeline details The details page for a pipeline allows you to view and manage that pipeline. Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. Azure DevOps · Search F 4 FabrikamFiber Edit Run pipeline : + Runs Branches Analytics Y Description Stages Ac #20191209.3 Set up CI with Azure Pipelines Manually triggered { main 2b4b23c #20191209.2 Set up CI with Azure Pipelines & Manually triggered & main 2b4b23c Là 54m ago Č 1m 13s Là 55m ago Č 1m 55s #20191209.1 Set up CI with Azure Pipelines Individual CI 1º main 2b4b23c Lô 56m ago 1m 6s 17

--- Page 158 ---
Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. View pipeline run details From the pipeline run summary you can view the status of your run, both while itis running and when it is complete. #20191210.2 Update azure-pipelines.yml for Azure Pipe ... on FabrikamFiber Summary Environments Triggered by Steve Danielson FabrikamFiber & main b2f795e Today at 12:56 PM Duration: 1m 9s Tests: Get started Changes: ¢ 2 commits Work items: ₾ 1 linked Artifacts: 1 published Stages Jobs Build 1 job completed 415 Run new : Deploy 1 job completed 13s 1 artifact From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, test results, and work Items From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, testresults, and work items Jobs and stages The jobs pane displays an overview of the status of your stages and jobs. This pane may have multiple tabs depending on whether your pipeline has stages and jobs, or just jobs. In this example, the pipeline has two stages named Build and Deploy. You can drill down into the pipeline steps by choosing the job from either the Stages or Jobs pane. 10

--- Page 121 ---
Course Code/Title:CS3V15/Devops Unit: IV · - m ping specifies the "ping" module, a basic module that checks the reachability of the managed nodes. . - i inventory.yaml points to the inventory file containing the list of devices. · The "ping" module sends a test command to the target devices and reports their status, confirming whether they are responsive. Output and Result: auriga@auriga-Latitude-E7470 :- /Desktop/ansibleŞ ansible all -[ inventory.yaml -m ping VOOS | UNREACHABLET => ( "changed's false. "mg's "Failed to connect to the host via s "unreachable": true auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ton denied (publickey_ password) In the above example the ssh host is not reachable. Activities Terminal Nov 2 12:13 . auriga@auriga-Latitude-E7470: - /Desktop/ansible C auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -i inventory.yaml -m ping vn81 | SUCCESS => 'ansible_facts": { "discovered_interpreter_python": "/usr/bin/python3" "changed": false, "ping": "pong" auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ In the above output image it is working fine Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: 1ansible all -i inventory.yaml -m apt -a "name=nginx state=present" -- become -- ask-become-pass · - m apt specifies the "apt" module, which is responsible for package management. · - a "name=nginx state=present" includes variables for the module to act upon, such as specifying that Nginx should be installed (state=present). · - become signifies that the command should run with elevated privileges (sudo). · - ask-become-pass prompts for the sudo password of the managed node to ensure the installation proceeds smoothly. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 7 N32 nirf 1750 Rank

--- Page 122 ---
Course Code/Title:CS3V15/Devops Unit:IV Output and Result: Activities Terminal * Nov 2 12:24 · auriga@auriga-Latitude-E7470: - /Desktop/ansible F X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -[ inventory.yaml -m apt -a "name=ansible state=present" -- become - - ask-become-pass BECOME password: vm01 | CHANGED => { "ansible_facts": { "discovered_interpreter_python": * /usr/bin/python3" "cache_update_time": 1698985056, "cache_updated": false, "changed": true, "stderr": "" 'stderr_Lines": []. stdout": "Reading package lists ... \nBuilding dependency tree ... \nReading state information ... \nThe following packages were automatically installed and are no longer required:\n gtr1.2-goa-1.0 hplip-data libcgt-fast-perl libcgt-pm-perl\n libdouble-conversion3 Libevent-core-2.1-7 libevent-pthreads-2.1-7\n \ibfcgt-perl libfprint-2-todi libfwupdplugin1 libht ml-template-perl\n Libpcre2-16-0 libqtScoresa libqtsdbuss libqtsguis libqt5network5\n libqtSpositionings libqt5printsupports libqt5qm15 Libqtsquick5\n libqtSsensors5 libqt5svg5 Li bqt5webchannel5 libqt5webkit5 libqt5widgets5\n Libxcb-xinerama@ Libxcb-xinpute Libxmlbi printer-driver-postscript-hp\n python3-renderpm python3-reportlab python3-reportlab-accel\n qt5-gtk-platformthene qttranslations5-lien shim\nUse 'sudo apt autoremove' to remove them. InThe following additional packages will be installed:\n teee-data python3-argcomplete pyt hon3-crypto python3-dnspython\n python3-jinja2 python3 . jmespath python3-kerberos python3-libcloud\n python3-netaddr python3-ntin-auth python3-requests-kerberos\n python3-requests- ntlm python3-selinux python3-winrm python3-xmltodict\nSuggested packages: \n cowsay sshpass python- jinja2-doc ipython3 python-netaddr-docs\nThe following NEW packages will be install ed:\n ansible leee-data python3-argcomplete python3-crypto python3-dnspython\n python3- jinja2 python3-jmespath python3-kerberos python3 . Libcloud\n python3-netaddr python3-ntlm-aut h python3-requests-kerberos\n python3-requests-ntIm python3-selinux python3-winrm python3-xmltodict\n0 upgraded, 16 newly installed, 0 to remove and 50 not upgraded. InNeed to get 97 25 kb of archives. \nAfter this operation, 98.6 MB of additional disk space will be used. \nGet:1 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-jinja2 all 2.10.1-2 [95.5 KB]\nGet:2 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-crypto amd64 2.6.1-13ubuntu2 [237 KB]\nGet:3 http://in.archive.ubuntu.com/ubuntu focal-updates/main amd64 pyt hon3-dnspython all 1.16.0-1ubuntu1 [89.2 KB]\nGet:4 http://in.archive.ubuntu.com/ubuntu focal/main amd64 ieee-data all 20180805.1 [1589 KB]\nGet: 5 http://in.archive. ubuntu.com/ubuntu focal-updates/main amd64 python3-netaddr all 0.7.19-3ubuntu1 [236 KB]\nGet:6 http://in.archive.ubuntu.com/ubuntu focal/universe and64 ansible all 2.9.6+dfsg-1 [5794 KB]\nGet:7 http: //\n.archive.ubuntu.com/ubuntu focal/universe amd64 python3-argcomplete all 1.8.1-1.3ubuntu1 [27.2 kB]\nGet:8 http://in.archive.ubuntu.com/ubuntu focal-updates/main and64 python3- jne spath all 0.9.4-2ubuntu1 [21.5 kB]\nGet:9 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-kerberos amd64 1.1.14-3.1build1 [22.6 k8]\nGet: 10 http://in.archive.ubuntu. con/ubuntu focal/universe amd64 python3-libcloud all 2.8.0-1 [1483 KB]\nGet:11 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-ntlm-auth all 1.1.0-1 [19.6 KB]\nGet: 1 2 http://in.archive.ubuntu.com/ubuntu focal/universe and64 python3-requests-kerberos all 0.12.0-2 [11.9 KB]\nGet: 13 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-r equests-ntlm all 1.1.0-1 [6004 B]\nGet: 14 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-selinux amd64 3.0-1build2 [139 KB]\nGet : 15 http://in.archive.ubuntu.com/ubu ntu focal/universe and64 python3-xmltodict all 0.12.0-1 [12.6 KB]\nGet:16 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-winrm all 0.3.0-2 [21.7 KB]\nFetched 9725 k B in 7s (1454 KB/s)\nSelecting previously unselected package python3-jinja2.\r\n(Reading database ... \r(Reading database ... 5*\r (Reading database .. . 10%\r(Reading database ... 15% \r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database . .. 45%\r (Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 68%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading da tabase ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 212542 files and directories currently installed.)\r\nPreparin g to unpack ... /00-python3-jinja2_2.10.1-2_all.deb ... \r\nUnpacking python3-jinja2 (2.10.1-2) ... \r\nSelecting previously unselected package python3-crypto. \r\nPreparing to unpack .. ./01-python3-crypto_2.6.1-13ubuntu2_amd64.deb ... \r\nUnpacking python3-crypto (2.6. 1-13ubuntu2) ... \r\nSelecting previously unselected package python3-dnspython. \r\nPreparing to unpa ck ... /02-python3-dnspython_1.16.0-lubuntu1_all.deb ... \r\nUnpacking python3-dnspython (1. 16.0-1ubuntu1) ... \r\nSelecting previously unselected package leee-data. \r\nPreparing to unp ack ... /03-ieee-data_20180805.1_all.deb ... \r\nUnpacking teee-data (20188805.1) ... \r\nSelecting previously unselected package python3-netaddr. \r\nPreparing to unpack ... /04-python3. netaddr_0.7.19-3ubuntu1_all.deb ... \r\nUnpacking python3-netaddr (0.7.19-3ubuntu1) ... \r\nSelecting previously unselected package ansible. \r\nPreparing to unpack .. . /05-ansible_2.9.6 +dfsg-1_all.deb ... \r\nUnpacking ansible (2.9.6+dfsg-1) ... \r\nSelecting previously unselected package python3-argcomplete. \r\nPreparing to unpack .. . /06-python3-argcomplete_1.8.1-1. 3ubuntu1_all.deb ... \r\nUnpacking python3-argcomplete (1.8.1-1.3ubuntu1) ... \r\nSelecting previously unselected package python3- jnespath. \r\nPreparing to unpack ... /07-python3- jnespa th_0.9.4-2ubuntul_all.deb ... \r\nUnpacking python3-jmespath (0.9.4-2ubuntu1) ... \r\nSelecting previously unselected package python3-kerberos. \r\nPreparing to unpack ... /08-python3-ke rberos_1.1.14-3.1build1_and64.deb ... \r\nUnpacking python3-kerberos (1.1.14-3.1build1) ... \r\nSelecting previously unselected package python3-libcloud. \r\nPreparing to unpack .. . /09- python3-libcloud_2.8.0-1_all.deb ... \r\nUnpacking python3-libcloud (2.8.0-1) ... \r\nSelecting previously unselected package python3-ntlm-auth. \r\nPreparing to unpack ... /10-python3-n tim-auth_1.1.0-1_all.deb .. . \r\nUnpacking python3-ntlm-auth (1.1.0-1) ... \r\nSelecting previously unselected package python3-requests-kerberos. \r\nPreparing to unpack ... /11-python3. requests-kerberos 0.12.8-2 all. deb .... \r\nUnpacking python3-requests-kerberos (0.12.0-2) ... \r\nSelecting previously unselected package python3-requests-ntln. \r\nPreparing to unpack By using these ad-hoc commands, you can perform quick, task-specific operations on your managed nodes, saving time and effort in your automation tasks. CommonModules: Ansible offers an extensive library of modules for various tasks, from system administration to application deployment. You can explore the full list of Ansible modules in the official documentation to find the most suitable module for your specific requirements. BestPractices: When working with ad-hoc commands, consider using SSH key-based authentication for secure, passwordless access. It streamlines the authentication process and enhances the security of your automation tasks. Using Ansible Playbooks Introduction: In the world of Ansible, playbooks are the cornerstone of structured and complex automation. While ad- hoc commands are perfect for quick and isolated tasks, playbooks step in when you need a well- orchestrated sequence of operations. They allow you to tackle multi-step automation scenarios where tasks depend on each other, and conditional actions are required. Let's delve deeper into the importance of playbooks: The Role of Playbooks: Playbooks serve as the framework for orchestrating automation tasks that involve a sequence of operations. Whether it's configuring servers, deploying applications, or managing infrastructure, playbooks provide a structured way to define how these tasks are executed. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N33 nirf 1750 Rank

--- Page 147 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: After clicking new repository option, we will have to initialize some things like, naming our project, choosing the visibility etc. After performing these steps click Create Repository button. Create a new repository A repository contains all project files, including the revision history. Already have a project repository elsewhere? Import a repository. Owner Repository name * / Namanbhatia7 - Resume This is going to be name of our project Great repository names are short and memorable. Need inspiration? How about animated-memory? Description (optional) Public Anyone Keep this as public selected ository. You choose who can commit. see this rer Private You choose who can see and commit to this repository. We can add a project description if we want. Skip this step if you're importing an existing repository. Initialize this repository with a README This will let you immediately clone the repository to your comp Tick the README option Add .gitignore: None - Add a license: None ~ Create repository After performing above steps, Click this button Step 3: After clicking the button, we will be directed to below page. Right now the only file we have is a readme file. Namanbhatia7 / Resume o Unwatch - 1 * Star 0 Y Fork 0 <> Code Issues 0 (*) Pull requests 0 ||| Projects 0 BB Wiki Security the Insights # Settings No description, website, or topics provided. Edit Manage topics T 1 commit įº 1 branch > 0 releases 2 1 contributor Branch: master New pull request Create new file Upload files Find File Clone or download TT Namanbhatia7 Initial commit Latest commit 676ac98 now README.md Initial commit now DO README.md Resume UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 7 175" Rank

--- Page 148 ---
Course Code/Title: CS3V15/Devops Unit: V Step 4: Now click on the "Upload files" button. 4 > C https://github.com/avicnotes/html-css-projects ... Search or jump to ... Click to End Session Total: $80.83 Pull requests Issues Marketplace Explore Google" Custom Search @ avicnotes / html-css-projects Unwatch - 1 Star 0 Fork 0 <> Code Issues 1'] Pull requests Actions Projects Wiki O Security ~ Insights ... go main - ¿º 1 branch 0 tags Go to file Add file - { Code About avicnotes screenshots Create new file Upload files 2 24 commits No description, website, or topics provided. README.md Update README.md 2 days ago Readme Screenshot (10).png screenshots 2 days ago Screenshot (11).png screenshots 2 days ago Releases Screenshot (12).png screenshots 2 days ago No releases published Create a new release 19 Screenshot (6).png screenshots 2 days ago accordion.JPG Add files via upload 4 days ago Packages background-dots.png Add files via upload 2 days ago clone-project-1.html Updated navbar items to my original ones 2 days ago index.html Create index.html 's ago No packages published Publish your first package Activate Windows Go to Settings to activate Windows, Environments 1 O Type here to search O W. 11:32 AM 2/1/2021 Step 5:Follow the steps mentioned and click "commitchanges". Drag additional files here to add them to your repository Or choose your files Display_Picture.png Simply drag or add files to be uploaded × My_Resume.html X Styles.css × Commit changes Add initial resume website files Add an optional ended des Give this particular version a name -o- Commit directly to the master branch. Ensure this option is checked !] Create a new branch for this commit and start a pull request. Learn more about pull requests. Commit changes Cancel UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 8

--- Page 135 ---
Course Code/Title:CS3V15/Devops Unit: IV Syntax for writing the module on command line While writing the module we use the options such as -m and -a. The -m is for module and -a is for argument, after the -a option we specify the executable command in double quotes. here 'all' stands for all the remote hosts, otherwise we can mention the group name of the hosts 1 $ ansible all -m command -a "echo 'Hello hosts'" The -m option followed by module name 'command' Command to be executed on remote hosts, given in double quotes Commonly used modules 1. Command : Following is a command module that executes a command on remote hosts $ ansible all -m command -a "echo 'Hello hosts'" 2. User : The user module is used to create, modify or delete users. For example - To create a user named 'new_user' on all the hosts in the 'dev' group using ansible, you can use the user module. Here's the ansible command to do that : $ ansible dev -m user -a "name=new_user state=present" Sometimes the above command fails as there is no privilege to create user on remote host. Then we can issue the command as follows $ ansible all -m user -a "name=new_user state=present" -- become -- ask-become-pass The above command asks for the password for the remote host to login. On supplying the password the user will be created. We can verify. if new user is getting created or not on remote host by issuing following command on each remote host $ less /etc/passwd If we want to create new user without asking for password on all the remote hosts then we can issue the command as - $ ansible all -m user -a "name=new_user1 state=present" -- become -b The -b option is used to become superuser. By this the sudo or su privileges can be granted. .3. Ping : The ping module is used to connectivity to remote hosts $ ansible all -m ping 4. Copy : The copy module is used to copy files from control machine to remote hosts $ ansible dev -m copy -a "src=/home/ansible_user/myfile.txt dest=/home/ansible_user/myfile.txt" The above command will copy the myfile.txt file from control node to all the managed nodes of dev group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 21 N32 nirf 1750 Rank

--- Page 136 ---
Course Code/Title:CS3V15/Devops Unit: IV 5. Package : The package module is used to install, remove or upgrade the packages, For example, the following command installs the nginx web server package : $ ansible package -a "name=nginx state=present" Here name indicates the name of the package to be installed, the state=present means install the package. Normally "present" is for installation, "remove" is for removing the package and "latest" is used to upgrade the package. 6. Shell : The shell module is used to run shell commands with more flexibility on remote hosts. For example - Following shell command executes on remote hosts. $ ansible dev -m shell -a "cmd=is" 7. Service : The service module is used to start, stop or restart a specific service on remote hosts. For example - Following service will start the apache web service on all the remote hosts. $ ansible dev -m service -a "name=apache2 state=started" Note that the state can started for starting the service, stopped for stopping the service. o started/stopped are idempotent actions that will not run commands unless necessary. o restarted will always bounce the service. o reloaded will always reload. I Example 1: Get the Uptime of remote servers We have used command module to run the uptime command and we have given both the ad hoc and the playbook form of execution. as AD-HOC Command $ ansible testservers -m command -a uptime -i ansible_hosts as Playbook - name: Check the remote host uptime hosts: testservers tasks: - name: Execute the Uptime command over Command module register: uptimeoutput command: "uptime" - debug: var: uptimeoutput.stdout_lines CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 22 N32 nirf 1750 Rank

--- Page 165 ---
Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. Customize CI triggers Pipeline triggers cause a pipeline to run. You can use trigger: to cause a pipeline to run whenever you push an update to a branch. YAML pipelines are configured by default with a CI trigger on your default branch (which is usually main). You can set up triggers for specific branches or for pull request validation. For a pull request validation trigger, just replace the trigger: step with pr: as shown in the two examples below. By default, the pipeline runs for each pull request change. . If you'd like to set up triggers, add either of the following snippets at the beginning of your azure-pipelines.yml file. YAML I Copy trigger: - main - releases/* YAML Copy pr: - main - releases/* You can specify the full name of the branch (for example, main) or a prefix-matching · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. Build using multiple versions Add steps You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can use tasks for building, testing, publishing, or deploying your app. For Java, the Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. YAML Copy - task: PublishCodeCoverageResults@1 inputs: codeCoverageTool: "JaCoco" summaryFileLocation: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco/ reportDirectory: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco" failIfCoverageEmpty: true · Select Save and then confirm the changes. . You can view your test and code coverage results by selecting your build and going to the Test and Coverage tabs.

--- Page 166 ---
Build using multiple versions Rename/move pipeline Name ScheduledTriggerTest 1234 Select folder 1 .. . Cancel Save Pipeline settings X Processing of new run requests Enabled Paused Disabled YAML file path azure-pipelines.yml Automatically link work items included in this run Cancel Save YAML trigger: main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdkVersion: "1.10" jdk11_windows: imageName: "windows-latest" jdkversion: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdkVersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: **** /TEST -*. xml" goals: "package"

--- Page 53 ---
Course Code/Title:CS3V15/Devops Unit: II <execution> <phase>test</phase> <goals> <goal>run</goal> </goals> <configuration> <tasks> <echo>Using env.test.properties</echo> <copy file="src/main/resources/env.test.properties" tofile="${project.build.outputDirectory}/env.properties"/> </tasks> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> Now open the command console, go to the folder containing pom.xml and execute the following mvn command. Pass the profile name as argument using -P option. C:\MVN\project>mvn test -Ptest Maven will start processing and displaying the result of test build profile. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 12 N32 nirf 1750 Rank

--- Page 54 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Maven Settings Open Maven settings.xml file available in %USER_HOME%/.m2 directory where %USER_HOME% represents the user home directory. If settings.xml file is not there, then create a new one. Add test profile as an active profile using active Profiles node as shown below in example. <settings xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"> <mirrors> <mirror> <id>maven.dev.snaponglobal.com</id> <name>Internal Artifactory Maven repository</name> <url>http://repo1.maven.org/maven2/</url> <mirrorOf> *< /mirrorOf> /mirror </mirrors> <activeProfiles> <activeProfile>test</activeProfile> </activeProfiles> </settings> Now open command console, go to the folder containing pom.xml and execute the following mvn command. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 13 N33 nirf 1750 Rank

--- Page 75 ---
Course Code/Title:CS3V15/Devops Customization This tool is highly customizable as it supports a variety of IDE's . Languages supported Project Configuration Based on Goal of time to get used to it. available. It supports software development in Java, C, C++, and Groovy. For declaring the project configuration, it does not use the XML files. On the phases of the fixed and linear model. Graph of task dependencies that do the work. To add functionality in the project is the main goal of the Gradle. To finish the project in the given timeline is the main goal of the Maven. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY Unit: II This tool serves a limited number of developers and is not that customizable. It supports software development in Java, Scala, C# , and Ruby and it does not natively support C and C+ but can support through plugins like "maven-native- plugin" or we can integrate other build systems like CMake or Makefile. For declaring the project configuration, it uses the XML files. 34 N33 nirf 1750 Rank

--- Page 76 ---
Course Code/Title:CS3V15/Devops Unit: III Unit - III CONTINUOUS INTEGRATION USING JENKINS Install & Configure Jenkins, Jenkins Architecture Overview, creating a Jenkins Job, configuring a Jenkins job, Introduction to Plugins, Adding Plugins to Jenkins, commonly used plugins (Git Plugin, Parameter Plugin, HTML Publisher, Copy Artifact and Extended choice parameters). Configuring Jenkins to work with java, Git and Maven, creating a Jenkins Build and Jenkins workspace. Installing and Configuring Jenkins Overview Jenkins is a widely-used open-source automation server that helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration and continuous delivery (CI/CD). Below are detailed steps and considerations for installing and configuring Jenkins. Installation 1. System Requirements: ○ Operating System: Jenkins can run on any major operating system. ○ Java: Jenkins requires Java Runtime Environment (JRE) 8 or later. 2. Download Jenkins: ○ Download the latest version of Jenkins from the official Jenkins website. 3. Installation Steps: ○ Windows: I Run the .msi installer and follow the installation wizard. ■ After installation, Jenkins will start automatically as a Windows service. ○ Linux: Add Jenkins repository and import the GPG key. I Use package manager to install Jenkins (e.g., apt-get install jenkins). I Start Jenkins using systemctl (systemctl start jenkins). 4. Initial Setup: ○ After installation, Jenkins can be accessed through a web browser at http : //< your _server _ip_or_domain>: 8080. o Unlock Jenkins by entering the initial admin password, which can be found in the jenkins_home directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 1 N32 nirf 1750 Rank

--- Page 85 ---
Course Code/Title:CS3V15/Devops Unit: III By following these steps, you can set up a Jenkins job to automate builds, integrate with your source code management system, and perform various actions based on build results. Introduction to Jenkins Plugins What Are Jenkins Plugins? Jenkins plugins are extensions that add extra functionality to Jenkins. They allow you to customize and extend Jenkins to better fit your specific needs. Plugins can provide integrations with other tools, add new features, and improve existing functionalities. Why Use Plugins? . Extend Functionality: Add features not available in the core Jenkins. . Integrate Tools: Seamlessly integrate with other tools and platforms (e.g., GitHub, Docker). . Improve Productivity: Automate more tasks and streamline your workflow. · Customize UI: Tailor the Jenkins interface to better suit your preferences and needs. How to Manage Plugins 1. Accessing the Plugin Manager 1. Go to Jenkins Dashboard: ○ Open your Jenkins dashboard. 2. Navigate to Plugin Manager: ○ Click on "Manage Jenkins". ○ Select "Manage Plugins". 2. Installing Plugins 1. Available Plugins: ○ Go to the "Available" tab to see a list of plugins that can be installed. 2. Search for Plugins: ○ Use the search bar to find specific plugins. 3. Select Plugins: ○ Check the box next to the plugins you want to install. 4. Install Plugins: ○ Click "Install without restart" or "Download now and install after restart". 3. Updating Plugins CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 narf 10 1750 Rank

--- Page 86 ---
Course Code/Title:CS3V15/Devops Unit: III 1. Go to Updates: ○ Navigate to the "Updates" tab to see plugins with available updates. 2. Select Plugins: ○ Check the box next to the plugins you want to update. 3. Update Plugins: ○ Click "Download now and install after restart". 4. Managing Installed Plugins 1. Installed Plugins: ○ Go to the "Installed" tab to see all installed plugins. 2. Uninstall Plugins: ○ Click the "Uninstall" button next to the plugin you want to remove. 3. Check Plugin Versions: ○ View the current version of each installed plugin. Adding Plugins to Jenkins Plugins are a crucial part of Jenkins, allowing you to extend its functionality to suit your project's specific needs. Whether you're integrating Jenkins with other tools, customizing your job configuration, or adding new build steps, plugins provide the flexibility you need. Here's a detailed guide on how to add plugins to Jenkins. 1. Why Plugins Are Important - Extending Jenkins Functionality: Plugins allow Jenkins to integrate with various tools and technologies, such as Git, Maven, Docker, and more. - Customizing Jobs: With plugins, you can add specific steps, triggers, and post-build actions to your jobs, making Jenkins adaptable to any workflow. - Automation: Plugins help automate processes like code quality checks, notifications, and deployments. 2. Accessing the Plugin Manager 1. Navigate to Jenkins Dashboard: Open your Jenkins dashboard in a web browser (e.g., http://localhost:8080). CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 175º Rank

--- Page 77 ---
Unit: III Course Code/Title:CS3V15/Devops 5. Customize Jenkins: ○ Install suggested plugins or select the required plugins manually. ○ Create the first admin user. Configuration 1. Global Configuration: ○ Navigate to Manage Jenkins > Configure System. ○ Set up global environment variables, email notifications, and other system-wide settings. 2. Security Configuration: ○ Configure security settings under Manage Jenkins > Configure Global Security. o Use the built-in user database or integrate with external authentication mechanisms like LDAP. 3. Node Configuration: ○ Jenkins can distribute build loads to multiple nodes. ○ Configure nodes by navigating to Manage Jenkins > Manage Nodes and Clouds. 4. Job Configuration: ○ Create new jobs by clicking New Item on the Jenkins dashboard. o Configure job-specific settings such as source code management, build triggers, and post-build actions. 5. Pipeline Configuration: ○ Jenkins supports pipeline as code using Jenkinsfile. ○ Define your pipeline stages in a Jenkinsfile placed in the root of your project repository. 6. Plugins: ○ Enhance Jenkins functionality by installing plugins from the Manage Jenkins > Manage Plugins section. 7. Backup and Restore: ○ Regularly back up the jenkins_home directory, which contains Jenkins configurations, job configurations, and build history. ○ Use plugins like ThinBackup for automated backups. Ref: https://youtube.com/playlist?list=PL6flErFppaj35spJjPy41-IruDjw2kRV-&si=Vzd9AAMv53xwWB S Ref: https://bit.ly/youtube_jenkins (click here) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 78 ---
Course Code/Title:CS3V15/Devops Unit: III Jenkins Architecture Overview Jenkins is a robust automation server widely used for continuous integration and continuous delivery (CI/CD) processes. Here's a detailed breakdown of the Jenkins architecture and its core components. Core Components of Jenkins Architecture 1. Jenkins Master: ○ Role: The Jenkins master is the central control unit. 0 Functions: Scheduling Jobs: Assigns build jobs to the appropriate agents. Monitoring Agents: Keeps track of the state and health of agents. I Job Execution: Executes build jobs directly if no agents are available. I User Interface: Provides a web-based interface for users to configure jobs, view build results, and manage the Jenkins environment. 2. Build Agents (Nodes): ○ Role: Agents are responsible for executing the build jobs assigned by the master. ○ Types: Agents can run on different operating systems and hardware configurations. ○ Configuration: Static Agents: Permanently assigned to the Jenkins master. . Dynamic Agents: Created and destroyed as needed, often used in cloud environments. ○ Communication: Agents communicate with the master using the Jenkins Remoting protocol. 3. Job Configuration: ○ Types of Jobs: I Freestyle Projects: Basic job type with simple configurations. Pipeline Jobs: Scripted or declarative pipelines that define the entire build process as code. I Multi-Configuration (Matrix) Projects: Allows testing across different environments and configurations. ○ Components: I Source Code Management: Integration with version control systems like Git, SVN, etc. . Build Triggers: Conditions that start the job, such as code commits, scheduled times, or manual triggers. Build Steps: Actions performed during the build, such as compiling code, running tests, and packaging artifacts. . Post-Build Actions: Steps executed after the build, like deploying artifacts, sending notifications, or archiving results. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 11 ---
Course Code/Title:CS3V15/Devops Unit: I 17. Selenium It remains a vital tool in software testing and automation due to its enduring relevance in ensuring the quality of web applications. As technology evolves, web applications become increasingly complex, requiring thorough testing across various browsers and platforms. With its robust automation capabilities and extensive browser compatibility, Selenium allows developers and QA teams to automate repetitive testing tasks efficiently, conduct cross- browser testing, and ensure that web applications function flawlessly across diverse environments. Its open-source nature, active community support, and integration with other DevOps tools make Selenium a go-to choice for organizations striving for continuous delivery and the rapid deployment of high-quality software, a cornerstone of modern software development practices. 18. Gremlin Gremlin is an essential tool in chaos engineering, which has become increasingly critical for ensuring the resilience and reliability of modern software systems. As technology advances and complex distributed systems become the norm, the potential for unexpected failures and outages also rises. Gremlin allows organizations to proactively identify weaknesses and vulnerabilities in their infrastructure and applications by simulating controlled failures, such as network disruptions, service outages, and resource constraints. By intentionally inducing chaos and monitoring the system's response, teams can uncover weaknesses before they lead to costly downtime or security breaches. Gremlin facilitates organizations to build more robust, fault-tolerant systems that can withstand real-world challenges and deliver uninterrupted services to users. 19. ServiceNow ServiceNow is a vital platform for organizations seeking to streamline their IT service management and beyond. Its significance lies in its ability to provide a unified, cloud-based solution for automating and optimizing various business processes, including ITSM, ITOM, HR, customer service, and more. Due to the rapid digitization of services, remote work, and the growing complexity of technology infrastructures, ServiceNow offers a comprehensive approach to managing workflows, resolving issues, and delivering services efficiently. Its intelligent automation capabilities, analytics, and AI-driven insights empower organizations to enhance productivity, agility, and customer satisfaction while reducing operational costs. ServiceNow's role in orchestrating and integrating diverse systems and processes makes it an indispensable tool for driving digital transformation and ensuring smooth operations in the ever-evolving business landscape of 2024. 20. Status Service Updates: The Status Page "Status Service Updates: The Status Page" is a critical tool for organizations and businesses of all sizes. In today's world, where online services and applications are integral to operations, ensuring their availability and reliability is essential. It provides real-time information to users and stakeholders about the operational status of services, applications, and infrastructure. The Status Page plays a crucial role in transparency, trust-building, and customer satisfaction by promptly communicating service disruptions, planned maintenance, and incident resolutions. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 1750 Rank

--- Page 12 ---
Course Code/Title:CS3V15/Devops Unit: I Downtime can often lead to significant financial losses and damage to a company's reputation, so having a practical Status Page becomes not just a convenience but a necessity. It allows organizations to showcase their commitment to transparency and responsiveness in addressing service-related issues, ultimately fostering stronger customer relationships and trust. 21. ELK (Elasticsearch, Logstash and Kibana) ELK, which stands for Elasticsearch, Logstash, and Kibana, continues to shine in DevOps and IT operations. This powerful trio of tools remains essential for organizations seeking effective log management, monitoring, and data visualization. Elasticsearch is a highly scalable and fast search engine that enables real-time data indexing and search. Logstash facilitates the collection, processing, and transformation of log data from various sources, making it compatible with Elasticsearch. Kibana, on the other hand, provides a user- friendly interface for visualizing and analyzing data, offering customizable dashboards and powerful data exploration capabilities. ELK's significance in 2024 lies in its ability to empower organizations with comprehensive insights into their systems, applications, and infrastructure. It ultimately facilitates quick problem resolution, proactive monitoring, and data-driven decision-making in an increasingly complex and fast-paced technological landscape. 22. GitLab CI/CD GitLab CI/CD's significance lies in its ability to automate the complete software delivery pipeline, from code changes to deployment, in a single integrated environment. GitLab CI/CD ensures rapid and reliable delivery of software updates. It enables continuous integration (CI) by automatically building and testing code changes, allowing teams to catch issues early in the development cycle. Furthermore, the continuous deployment (CD) aspect automates the release and deployment process, reducing the risk of human errors and enabling organizations to deliver features and updates to users swiftly and confidently. GitLab CI/CD's importance is further accentuated as businesses seek to accelerate digital transformation efforts, respond rapidly to changing market demands, and maintain a competitive edge through efficient and automated software delivery practices. 23. Scripting Scripting remains vital due to its pivotal role in automating and streamlining various aspects of software development, system administration, and DevOps practices. Scripting languages like Python, Bash, and PowerShell empower tech professionals to write code that can execute repetitive tasks, manipulate data, and orchestrate complex processes efficiently. Scripting facilitates rapid prototyping, configuration management, and the creation of automated deployment pipelines. It enhances productivity, ensures consistency and reduces human error in tasks ranging from software testing and deployment to infrastructure provisioning and monitoring. As organizations increasingly embrace DevOps and cloud- native technologies, scripting stays competitive and adaptive in the tech landscape. 24. Terraform Terraform plays a crucial role in modern infrastructure provisioning and management. It allows organizations to define and deploy infrastructure as code, enabling the automated creation and configuration of cloud resources, containers, and other infrastructure components. Cloud computing, microservices, and containerization have become the norm in CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 12 N32 nirf 1750 Rank

--- Page 27 ---
Course Code/Title:CS3V15/Devops Unit: I · Servers are maintained easily and there is nearly zero downtime · Users can access cloud data and upload it on the cloud from any device with a solid internet connection · Cloud environments can be modified according to the user's requirements and is easily accessible · Clouds are highly secure, making data breaches more unlikely · Migrating to the cloud eliminates the need to buy on-premises infrastructure . It offers pay-as-you-go pricing, meaning you only pay for the resources you use 1. Infrastructure as a Service: IaaS delivers virtualized computing resources over the Internet. Users can rent virtual machines, storage, and networking infrastructure, allowing for easy scalability without investing in physical hardware. Examples include AWS EC2 and Azure Virtual Machines. 2. Platform as a Service: PaaS offers a robust platform for developers to build, deploy, and manage apps without worrying about the underlying infrastructure. It simplifies application development and deployment, with services like Google App Engine and Heroku leading the way. 3. Software as a Service: SaaS offers software applications on a subscription basis, accessible via a web browser. Users don't need to install or maintain software locally, making it ideal for collaboration tools (e.g., Microsoft 365, Google Workspace) and CRM systems (e.g., Salesforce). 4. Function as a Service: FaaS allows developers to execute code responding to events without managing servers. It's highly scalable and cost-efficient, exemplified by AWS Lambda and Azure Functions. FaaS is also known as serverless computing. 5. Container as a Service: CaaS enables the deployment and management of containerized applications using orchestration tools like Kubernetes. It provides portability and scalability for applications across different cloud environments. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 27 N33 nif 1750 Rank

--- Page 28 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between main cloud computing services Terms Stands for Uses Access Model Technical understanding. Popularity Percentage rise Usage Cloud services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY IAAS Infrastructure as a service. IAAS is used by network architects. IAAS gives access to the resources like virtual machines and virtual storage. It is a service model that provides virtualized computing resources over the internet. It requires technical knowledge. It is popular among developers and researchers. It has around a 12% increment. Used by the skilled developer to develop unique applications. Amazon Web Services, sun, vCloud Express. PAAS Platform as a service. PAAS is used by developers. PAAS gives access to run time environment to deployment and development tools for application. It is a cloud computing model that delivers tools that are used for the development of applications. Some knowledge is required for the basic setup. It is popular among developers who focus on the development of apps and scripts. It has around 32% increment. Used by mid-level developers to build applications. Facebook, and Google search engine. SAAS Software as a service. SAAS is used by the end user. SAAS gives access to the end user. It is a service model in cloud computing that hosts software to make it available to clients. There is no requirement about technicalities company handles everything. It is popular among consumers and companies, such as file sharing, email, and networking. It has about a 27 % rise in the cloud computing model. Used among the users of entertainment. MS Office web, Facebook and Google Apps. 28 N32 nirf 1750 Rank

--- Page 119 ---
Course Code/Title:CS3V15/Devops Unit: IV · Groupings in inventories enable you to organize your infrastructure, allowing you to target specific subsets of nodes based on your needs. For instance, when you need to perform tasks in a particular location, groupings become essential. Creating Inventories There are two primary methods to create an inventory on the control node: using a YAML file or an INI file. 1. INI Inventory Example (inventory.ini): 1[virtualmachines] 2host1 ansible_host:192.168.0.1 3192.168.0.2 2. In this INI file, it captures the IP addresses of managed nodes. The default username used for SSH connections is typically the working user of your control node. 3. ansible_host: This parameter is used to specify the IP address, although you can write the IP directly. 4. Inventory.yaml 1 --- 2virtualmachines: # Define a class name to represent a particular group of devices. 1 hosts: # Define the hosts. 2 vm01: # Define a parent name to indicate a class of parent type. 3 ansible_host: 192.168.0.1 # Specify the IP address. 4 http_port: 80 # (optional) Define the HTTP port. 5 ansible_user: auriga # (optional) Provide the username if it's different from the control node's user. o ansible_user: Use this to specify a different user if the managed node's user is not the same as the control node's user. o http_port: Specify a port if necessary. In real-world scenarios, inventory files can be more complex, reflecting the diverse and extensive infrastructure of organizations. You can explore more configurations here. Verifying Your Inventory After creating your inventory, it's a good practice to verify its correctness. This step ensures that the inventory is correctly structured and accessible to Ansible. To do this, use the following command: 1ansible-inventory -i inventory.yaml -list · - i: Indicates the inventory file you want to use. . - list: Requests the listing of the inventory content. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 5 N33 nirf 1750 Rank

--- Page 120 ---
Course Code/Title:CS3V15/Devops Unit:IV output: auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-inventory ·i inventory.yaml -- list "_meta": { "hostvars": { "vm01": "ansible_host": "192.168.0.208", "ansible_user"; "au" "children": [ "ungrouped" "ungrouped": { "hosts": "vn01 1 By following these steps, you have effectively set up and verified your inventory, ensuring a solid foundation for your Ansible automation. Using Ansible Ad-Hoc Commands Introduction: In Ansible, ad-hoc commands are your go-to solution for executing quick, one-off tasks on remote nodes. They provide a straightforward and efficient way to interact with managed nodes without the need for creating full-fledged playbooks. Ad-hoc commands are particularly useful when you need immediate results without the overhead of playbook development. When to Use Ad-Hoc Commands: Ad-hoc commands are best suited for scenarios where the task at hand is simple and doesn't require the complexity of a playbook. They are perfect for tasks like system health checks, package installation, service management, or any other single-operation job. Ad-Hoc Command Syntax: Ad-hoc commands follow a specific syntax that comprises various components, each serving a unique role: . Target Group: This is the group of hosts you intend to target with the ad-hoc command. · Module (-m): Specifies the module to execute. Modules are Ansible's building blocks for performing tasks, and they can range from basic operations like "ping" to more advanced tasks such as package management. · Inventory File (-i): Indicates the location of your inventory file, which defines the list of target devices. Example: Ping All Inventory Devices To illustrate the use of ad-hoc commands, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: 1ansible virtualmachines -m ping -i inventory.yaml virtualmachines is the target group. In this case, it could be any group, or you can use all to target all devices in your inventory. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 93 ---
Course Code/Title:CS3V15/Devops Unit: III And configure the Jenkins job as follows - General Description In this job we have used GitHub plugin Plain text Preview Discard old builds ? GitHub project This project is parameterized ? Throttle builds Execute concurrent builds if necessary Advanced V Source Code Management None Git ? Repositories ? Repository URL https://github.com/AnurdhaP/MyJavaPrograms.gil Please enter Git repository. Credentials ? - none - Add Advanced CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 18 N3 nirf 1750 Rank

--- Page 94 ---
Course Code/Title:CS3V15/Devops Unit: III Build Triggers Trigger builds remotely (e.g. from scripts) ? Build after other projects are built ? Build periodically ? GitHub hook trigger for GITSem polling ? Poll SCM ? Schedule ? A Do you really mean "every minute" when you say "" "" " ""? Perhaps you meant "H" "" "" to poll once per hour Would last have run at Wednesday, 30 August, 2023 at 4:11:02 pm india Standard Time; would'nest run at Wednesday, 30.A Ignore post-commit hooks ? Build Steps Execute Windows batch command ? Command See the list of available environment variables javas test.java java test. java ----- Advanced V Add build sten Save this configuration. Step 5 : Now if we build this job the output that we see is as follows on Console Output. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 19 N3 nirf 1750 Rank

--- Page 51 ---
Course Code/Title:CS3V15/Devops Unit: II 7. Maven Profiles: A Build profile is a set of configuration values, which can be used to set or override default values of Maven build. Using a build profile, you can customize build for different environments such as Production v/s Development environments. Types of Build Profile Build profiles are majorly of three types. Type Where it is defined Per Project Defined in the project POM file, pom.xml Per User Defined in Maven settings xml file (%USER_HOME%/.m2/settings.xml) Global Defined in Maven global settings xml file (%M2_HOME%/conf/settings.xml) Profile Activation A Maven Build Profile can be activated in various ways. · Explicitly using command console input. · Through maven settings. · Based on environment variables (User/System variables). · OS Settings (for example, Windows family). · Present/missing files. Now, under src/main/resources, there are three environment specific files - Sl.No. File Name & Description env.properties 1 default configuration used if no profile is mentioned. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 10 N33 nirf 1750 Rank

--- Page 52 ---
Course Code/Title:CS3V15/Devops Unit: II env.test.properties 2 test configuration when test profile is used. 3 env.prod.properties production configuration when prod profile is used. Explicit Profile Activation In the following example, we will attach maven-antrun-plugin:run goal to test the phase. This will allow us to echo text messages for different profiles. We will be using pom.xml to define different profiles and will activate profile at command console using maven command. Assume, we've created the following pom.xml in C:\MVN\project folder. <project xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation = "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.companyname.projectgroup</groupId> <artifactId>project</artifactId> <version>1.0</version> <profiles> <profile> <id>test</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <version>1.1</version> <executions> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 11 N33 nirf 1750 Rank

--- Page 57 ---
Course Code/Title:CS3V15/Devops · Go to the File option · In the drop-down menu, select New · Select the Project option Unit: II If you want to create a Java project, you can select the "Java Project" option. Since we are not creating a Java project specifically, we have chosen the "Project" option. eclipse-workspace - Eclipse File Edit Source Refactor Navigate Search Project Run Window Help New Open File ... G) Open Projects from File System ... Shift+Alt+N Close Close All Shift + Cb1+W Save Save As. Save All Shift+Ctrl+S Revert Move .:. Rename 2 Refresh ES Convert Line Delimiters To Print Import ... Export ... Ctrl+P de java Project ui Project ... El Package G Class Interface G Enum @ Annotation 69 Source Folder 18 Java Working Set Folder File Untitled Text File Task JUnit Test Case Example ... Other ... Ctrl+N The dialog box that appears on the screen will display different types of projects. · Select the Maven Project option . Click on Next New Project + Select a wizard Create a Maven Project Wizards: type filter text irroject Gradle Java 2º Java Project #Java Project from Existing Ant Buildfile - @Maven Lu Check out Maven Projects from SCM M& Maven Module Maven Project Examples ? < Back Next > X Cancel Finish A dialog box will appear. Select the default workspace. . Click on "Next" Several Group IDs, Artifact IDs, and Versions will then appear. · Select a plugin there and click on "Next" CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 16 N33 narf 1750 Rank

--- Page 58 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Select an Archetype M Catalog: All Catalogs Configure .. Biter: × Group Id Artifact id Version org.apache.maven.archetypes maven-archetype-archetype 1.0 org.apache.maven.archetypes maven-archetype-|2ee-simple 1.0 org.apache.maven archetypes maven-archetype-plugin 1.2 org.apache.maven.archetypes maven-archetype-plugin-site 1.1 org.apache.maven.archetypes maven-archetype-portlet 1.0.1 org.apache.maven.archetypes maven-archetype-profiles 1.0-alpha-4 org.apache.maven.archetypes maven-archetype-quickstart 1.1 org.apache.maven.archetypes maven-archetype-site 11 org anacha mauan archabenes masan.archchina.cito.cimnia 11 An archetype which contains a sample Maven project. Show the last version of Archetype only Include snapshot archetypes Add Archetype ... > Advanced ? < Back Next > Cancel Finish In the next dialog box that appears, you'll complete the following steps: · Enter the Group ID “com.xyz" · Enter the Artifact ID "mavenproject" · The version will appear on the screen These items can all be modified at a later time if needed. · Click on "Finish" CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 17 N33 nirf 1750 Rank

--- Page 49 ---
Course Code/Title:CS3V15/Devops Unit: II Validate Deploy Compile Install Test Verify Package Integration Test · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. There are always pre and post phases to register goals, which must run prior to, or after a particular phase. When Maven starts building a project, it steps through a defined sequence of phases and executes goals, which are registered with each phase. Maven has the following three standard lifecycles - • default: This is the main lifecycle, as it's responsible for project deployment. CHENNAI CHENNAI INSTITUTE . TECHNOLOGY 8 clean: Handles project cleaning, ensuring that all artifacts generated by previous INSTITUTE OF TECHNOLOGY 175º Rank (Autonomous]

--- Page 50 ---
Course Code/Title:CS3V15/Devops Unit: II builds are removed. · site: Manages the creation of the project's site documentation. Default Lifecycle (default) Most Maven users will be familiar with the default lifecycle. It is a general model of a build process for a software application. The first phase is validate and the last phase is deploy. · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. Clean Lifecycle (clean) The first lifecycle in Maven. Running mvn clean invokes the clean lifecycle which consists of three lifecycle phases: · pre-clean : execute processes needed prior to the actual project cleaning · clean : remove all files generated by the previous build · post-clean : execute processes needed to finalize the project cleaning Site Lifecycle (site) Maven does more than build software artifacts from project, it can also generate project documentation and reports about the project, or a collection of projects. Project documentation and site generation have a dedicated lifecycle which contains four phases: 1. pre-site : execute processes needed prior to the actual project site generation 2. site : generate the project's site documentation 3. post-site : execute processes needed to finalize the site generation, and to prepare for site deployment 4. site-deploy: deploy the generated site documentation to the specified web server CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 37 ---
Course Code/Title:CS3V15/Devops Unit: I GIT INSTALLATION Git for Windows stand-alone installer · Download the latest Git for Windows installer. · When you've successfully started the installer, you should see the Git Setup wizard screen. Follow the Next and Finish prompts to complete the installation. The default options are pretty sensible for most users. · Open a Command Prompt (or Git Bash if during installation you elected not to use Git from the Windows Command Prompt). . Run the following commands to configure your Git username and email using the following commands, replacing Emma's name with your own. These details will be associated with any commits that you create: $ git config -- global user.name "CIT_CHENNAI" $ git config -- global user.email CITCHENNAI@atlassian.com • Optional: Install the Git credential helper on Windows Bitbucket supports pushing and pulling over HTTP to your remote Git repositories on Bitbucket. Every time you interact with the remote repository, you must supply a username/password combination. You can store these credentials, instead of supplying the combination every time, with the Git Credential Manager for Windows. BASIC COMMANDS OF GIT: Some basic Git commands along with their syntax and examples: · Initialize a Repository: Syntax: `git init' Example: `git init' · Clone a Repository: Syntax: `git clone <repository_url> Example: `git clone https://github.com/example/repository.git' · Check Repository Status: Syntax: `git status' Example: `git status' · Add Changes to Staging Area: Syntax: `git add <file(s)>' Example: `git add file.txt' · Commit Changes: Syntax:`git commit -m "Commit message" Example:`git commit -m "Add new feature" · Create a New Branch: Syntax: `git branch <branch_name>' Example: `git CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY (Autonomous] N33 37 nirf 1750 Rank

--- Page 38 ---
Course Code/Title:CS3V15/Devops branch feature-branch' Unit: I • Switch to a Branch: Syntax: `git checkout <branch_name>' Example:`git checkout feature-branch' OR Syntax: `git switch <branch_name>' (Git version 2.23 and later) Example: `git switch feature-branch' · Create and Switch to a New Branch: Syntax: `git checkout -b <new_branch name>' Example: `git checkout -b new-feature' OR Syntax: `git switch -c <new_branch_name>' (Git version 2.23 and later) Example: `git switch -c new-feature' . Merge Changes from One Branch to Another: Syntax: `git merge <branch_name>' Example: `git merge feature-branch' · View the Commit History: Syntax: `git log' Example: `git log' · Push Changes to a Remote Repository: Syntax: `git push <remote_name> <branch_name>' Example:`git push origin master' · Pull Changes from a Remote Repository: Syntax: `git pull <remote_name> <branch_name>' Example:`git pull origin master' . Show the Differences Between Working Directory and Staging Area: Syntax: `git diff Example: `git diff · Show the Differences Between Staging Area and Last Commit: Syntax: `git diff -- cached' Example: `git diff -- cached' . Show the Differences Between Working Directory and Last Commit: Syntax: `git diff HEAD' Example: `git diff HEAD' CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 38 N32 nirf 1750 Rank

--- Page 45 ---
Course Code/Title:CS3V15/Devops Unit: II Example for pom.xml file <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> 4. Maven Repository A maven repository is a directory of packaged JAR file with pom.xml file. Maven searches for dependencies in the repositories. There are 3 types of maven repository: 1. Local Repository 2. Central Repository 3. Remote Repository Maven searches for the dependencies in the following order: Local repository then Central repository then Remote repository. Local Central Remote Repository Repository Repository CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 4 N32 nirf 1750 Rank -

--- Page 46 ---
Course Code/Title:CS3V15/Devops Unit: II If dependency is not found in these repositories, maven stops processing and throws an error. 1) Maven Local Repository Maven local repository is located in your local system. It is created by the maven when you run any maven command. By default, maven local repository is %USER_HOME%/.m2 directory. For example: C:\Users\SSS IT\.m2. Update location of Local Repository We can change the location of maven local repository by changing the settings.xml file. It is located in MAVEN_HOME/conf/settings.xml, for example: E:\apache-maven- 3.1.1\conf\settings.xml. 2) Maven Central Repository Maven central repository is located on the web. It has been created by the apache maven community itself. The path of central repository is: http://repo1.maven.org/maven2/. The central repository contains a lot of common libraries that can be viewed by this url http://search.maven.org/#browse. 3) Maven Remote Repository Maven remote repository is located on the web. Most of libraries can be missing from the central repository such as JBoss library etc, so we need to define remote repository in pom.xml file. Let's see the code to add the jUnit library in pom.xml file. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 5 N32 narf 1750 Rank -

--- Page 115 ---
Unit: IV Course Code/Title:CS3V15/Devops UNIT IV CONFIGURATION MANAGEMENT USING ANSIBLE Ansible Introduction, Installation, Ansible master/slave configuration, YAML basics, Ansible modules, Ansible Inventory files, Ansible playbooks, Ansible Roles, adhoc commands in ansible. Ansible A SSH ANSIBLE TARGET NODE HOSTS PLAYBOOK Ansible is a powerful DevOps tool designed for automating tasks on remote servers or nodes. In simple terms, it allows you to automate commands and functions on multiple remote machines from a central 'master' node. To illustrate its usefulness, consider a scenario where you need to reboot dozens or even hundreds of remote hosts. You could manually SSH into each one and initiate the reboot, or you can use Ansible to streamline the process, making it efficient and offering a wide range of additional functionalities. In essence, Ansible operates much like the second method mentioned, using SSH (Secure Shell), a secure communication protocol, to control remote nodes in a secure and optimized manner. Ansible Components Ansible Core Components Control node Ansible Inventory Managed node 1 Managed node 2 Managed node 3 1. Control Node - · The central or main node where Ansible is installed. · Used to trigger commands like ansible and ansible-inventory on other nodes. · Acts as the orchestrator for Ansible operations. • TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 1 N33 nirf 1750 Rank

--- Page 116 ---
Course Code/Title:CS3V15/Devops Unit: IV 2. Manage Node- · A remote or slave node where tasks are executed or controlled by Ansible. . These are the servers or devices you want to manage or automate. 3. Inventory - · A list of managed node IPs and configurations. · Logically organized, typically using file formats like YAML or INI. · Created on the control node to describe the deployment of hosts to Ansible. Ansible Additional Components Ansible Architecture CMDB Users Public / Private Cloud Hosts ... ... Inventory .. Modules 000 Ansible Playbook API Plugins Networking www.educba.com 1. Ad-Hoc Commands - . These are one-off commands that you can execute using the ansible command. · Useful for quick tasks or tests on remote nodes. 2. Plugins - · Plugins are pieces of code that extend Ansible's core functionality. · Ansible uses a plugin architecture for flexibility and expandability. · Examples include connectivity plugins for establishing connections and cache plugins. · Learn more about Ansible plugins. 3. Module- · Modules are built-in functions that can be used to perform various tasks. . They eliminate the need to write custom code for common operations. · Examples include modules for package management (apt, yum), service management, and more. CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) CHENNAI INSTITUTE . TECHNOLOGY 2 N33 nirf 1750 Rank

--- Page 47 ---
Course Code/Title:CS3V15/Devops Unit: II <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> Any repository of maven is available in mvnrepository.com 5. Maven Plugins: The maven plugins are central part of maven framework, it is used to perform specific goal. According to Apache Maven, there are 2 types of maven plugins. 1. Build Plugins 2. Reporting Plugins Build Plugins These plugins are executed at the time of build. These plugins should be declared inside the <build> element. Reporting Plugins These plugins are executed at the time of site generation. These plugins should be declared inside the <reporting> element. Maven Core Plugins A list of maven core plugins are given below: Plugin Description clean clean up after build. compiler compiles java source code. deploy deploys the artifact to the remote repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY N32 nirf 6 175º Rank -

--- Page 48 ---
Course Code/Title:CS3V15/Devops Unit: II failsafe runs the JUnit integration tests in an isolated classloader. install installs the built artifact into the local repository. resources copies the resources to the output directory for including in the JAR. site generates a site for the current project. surefire runs the JUnit unit tests in an isolated classloader. verifier verifies the existence of certain conditions. It is useful for integration tests. Example for maven plugin: The compiler plugin is used to compile the source code of a Maven project. This plugin has two goals, which are already bound to specific phases of the default lifecycle: · compile - compile main source files · testCompile - compile test source files Here's the compiler plugin in the POM: <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.12.1</version> <configuration> <source>1.8</source> <target>1.8</target> </configuration> </plugin> 6. Maven Build Lifecycle: A Build Lifecycle is a well-defined sequence of phases, which define the order in which the goals are to be executed. Here phase represents a stage in life cycle. As an example, a typical Maven Build Lifecycle consists of the following sequence of phases. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 7 N33 nirf 175º Rank -

--- Page 25 ---
Course Code/Title:CS3V15/Devops Unit: I · backups · automated multi-region replication in-memory caching · data export tools. Global Infrastructure of AWS · AWS is a cloud computing platform which is globally available. . Global infrastructure is a region around the world in which AWS is based. Global infrastructure is a bunch of high-level IT services which is shown below: AWS is available in 19 regions, and 57 availability zones in December 2018 and 5 more regions 15 more availability zones for 2019. The following are the components that make up the AWS infrastructure: o Availability Zones o Region o Edge locations o Regional Edge Caches Compnents of Global Infrastructure Availability zones Region Edge Locations Regional Edge Caches Availability zone as a Data Center An availability zone is a facility that can be somewhere in a country or in a city. Inside this facility, i.e., Data Centre, we can have multiple servers, switches, load balancing, firewalls. The things which interact with the cloud sits inside the data centers. o An availability zone can be a several data centers, but if they are close together, they are counted as 1 availability zone. Region o A region is a geographical area. Each region consists of 2 more availability zones. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 26 ---
Course Code/Title:CS3V15/Devops Unit: I o A region is a collection of data centers which are completely isolated from other regions. · A region consists of more than two availability zones connected to each other through links. AZ-a AZ-b AZ-c Availability zones are connected through redundant and isolated metro fibers. Edge Locations o Edge locations are the endpoints for AWS used for caching content. o Edge locations consist of CloudFront, Amazon's Content Delivery Network (CDN). o Edge locations are more than regions. Currently, there are over 150 edge locations. Edge location is not a region but a small location that AWS have. It is used for caching the content. o Edge locations are mainly located in most of the major cities to distribute the content to end users with reduced latency. o For example, some user accesses your website from Singapore; then this request would be redirected to the edge location closest to Singapore where cached data can be read. Regional Edge Cache o AWS announced a new type of edge location in November 2016, known as a Regional Edge Cache. o Regional Edge cache lies between CloudFront Origin servers and the edge locations. o A regional edge cache has a large cache than an individual edge location. o Data is removed from the cache at the edge location while the data is retained at the Regional Edge Caches. o When the user requests the data, then data is no longer available at the edge location. Therefore, the edge location retrieves the cached data from the Regional edge cache instead of the Origin servers that have high latency. What Is Cloud Computing? Cloud computing is the use of hardware and software components in an off-premises location to deliver a service to a network. Users can access files and applications from any device that can access the internet. Some features and capabilities include: · Cloud providers can pull the computing resources to provide services to multiple customers with the help of a multi-tenant model · Cloud computing proves an on-demand self-service, which helps administrators monitor performance CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 26 N32 nirf 1750 Rank -

--- Page 41 ---
Unit: I Course Code/Title:CS3V15/Devops S.No. Git 1 Git is a software. 2 Git is a command-line tool GitHub is a graphical user interface 3 Git is installed locally on the system GitHub is hosted on the web 4 Git is maintained by linux. GitHub is maintained by Microsoft. 5 Git is focused on version control and code sharing. GitHub is a hosting service for Git repositories. 6 Git is a version control system to manage source code history. 7 Git was first released in 2005. GHub was launched in 2008. 8 Git has no user management feature. 9 Git is open-source licensed. GitHub includes a free-tier and pay- for- use tier. 10 Git has minimal external tool configuration. 11 Git provides a Desktop interface named Git Gui. GitHub provides a Desktop interface named GitHub Desktop. 12 Git competes with CVS, Azure DevOps Server, Subversion, Mercurial, etc. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY GitHub GitHub is a service. GitHub is focused on centralized source code hosting. GitHub has a built-in user management feature. GitHub has an active marketplace for tool integration. GitHub competes with GitLab, Bit Bucket, AWS Code Commit, etc. 41 N32 nirf 1750 Rank

--- Page 42 ---
Course Code/Title:CS2V15/Devops Unit: II UNIT - II COMPILE AND BUILD USING MAVEN AND GRADLE Introduction, Installation of Maven, POM files, Maven Build lifecycle, Build phases(compile build, test, package) Maven Profiles, Maven repositories(local, central, global),Maven plugins, Maven create and build Artifacts, Dependency management, Installation of Gradle, Understand build using Gradle. 1. Introduction of Maven: Apache Maven is an automation tool. The tool is written in Java. It was initially released on 13 July 2004. It is developed by the Apache software foundation. It is part of the Jakarta Project. It is working on two aspects: how software is built, and its dependencies. It was created by Jason van Zyl. It is built by using a plugin-based architecture that allows it to make the use of any application controllable by standard input. It dynamically downloads Java libraries. Understanding the problem without Maven There are many problems that we face during the project development. They are discussed below: 1) Adding set of Jars in each project: In case of struts, spring, hibernate frameworks, we need to add set of jar files in each project. It must include all the dependencies of jars also. 2) Creating the right project structure: We must create the right project structure in servlet, struts etc, otherwise it will not be executed. 3) Building and Deploying the project: We must have to build and deploy the project so that it may work. Maven simplifies the above mentioned problems. It does mainly following tasks. 1. It makes a project easy to build 2. It provides uniform build process (maven project can be shared by all the maven projects) 3. It provides project information (log document, cross referenced sources, mailing list, dependency list, unit test reports etc.) 4. It is easy to migrate for new features of Maven Apache Maven helps to manage o Builds Documentation o Reporing o SCMs o Releases o Distribution What is Build Tool? A build tool is used for building a process. It does following: CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE TECHNOLOGY N32 nirf 1 1750 Rank

--- Page 81 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Credentials: Add credentials if required. ○ Branch Specifier: Specify the branch (e.g., */main). 7. Build Triggers: ○ Build Periodically: Use CRON syntax to schedule builds. ○ Poll SCM: Jenkins will check the repository for changes at specified intervals. ○ Other Triggers: Configure other triggers such as GitHub hooks, upstream projects, etc. 8. Build Environment: o Configure the build environment settings like setting environment variables, running scripts before the build, etc. 9. Build Steps: ○ Click on "Add Build Step" and choose the appropriate build step (e.g., "Execute Shell", "Invoke Ant", "Invoke Gradle script"). ○ Example: For executing a shell script, add the necessary shell commands. 10. Post-build Actions: ○ Click on "Add post-build action" and choose the appropriate action (e.g., "Archive the artifacts", "Publish JUnit test result report"). ○ Configure the settings for each post-build action. 11. Save the Configuration: ○ Once you have configured all the necessary settings, click "Save" at the bottom of the page. 12. Run the Job: ○ On the job's main page, click "Build Now" to run the job immediately. 13. Monitor the Job: ○ Click on the build number in the "Build History" to view the detailed output and logs of the build process. ○ Check the console output for any errors or warnings. Practical Example Setting up a Freestyle Project for a Maven Build: 1. Create New Item: ○ Name: MyMavenProject o Type: Freestyle project 2. Source Code Management: ○ Git: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 82 ---
Unit: III Course Code/Title:CS3V15/Devops I Repository URL: https://github.com/example/my-maven-project.git · Branch Specifier: * /main 3. Build Triggers: ○ Poll SCM: H/15 * (poll every 15 minutes) 4. Build Steps: ○ Invoke Top-Level Maven Targets: Goals: clean install 5. Post-build Actions: ○ Publish JUnit test result report: I Test report XMLs: * /target/surefire-reports/ *. xml 6. Save and Build: ○ Click "Save". ○ Click "Build Now" on the job's main page. By following these steps, you can set up a Jenkins job to automate your build process for various types of projects. Jenkins provides extensive customization options, allowing you to tailor the job to meet your specific requirements. Configuring a Jenkins Job 1. Install Jenkins First, make sure Jenkins is installed. You can download it from the official Jenkins website. 2. Access the Jenkins Dashboard Open your web browser and go to http://your-server-ip: 8080 to access the Jenkins dashboard. 3. Create a New Job 1. Start a New Job: ○ Click on "New Item" on the left-hand menu. 2. Name the Job: ○ Enter a name for your job in the "Enter an item name" field. 3. Select Job Type: ○ Choose the type of job (e.g., Freestyle project, Pipeline). CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) - TECHNOLOGY N32 nirf 7 1750 Rank

--- Page 111 ---
Course Code/Title:CS3V15/Devops Unit: III Steps: 1. Install the Git Plugin: 0 Go to Manage Jenkins > Manage Plugins. ○ Under the Available tab, search for Git Plugin and install it. 2. Configure Git in Jenkins: ○ Navigate to Manage Jenkins> Global Tool Configuration. ○ Scroll down to the Git section and click Add Git. ○ Specify the path to the Git executable or use the default path. ○ Optionally, configure additional settings such as Git installation locations. 3. Example Configuration: ○ Name: Git ○ Path to Git executable: /usr/bin/ git (or use the default if Git is installed in a standard location). 4. Configure Job to Use Git: ○ Create or edit a Jenkins job and go to Source Code Management. ○ Select Git. ○ Enter the repository URL (e.g., https://github.com/user/repository.git). ○ Configure credentials if the repository is private. ○ Specify the branch to build (e.g., main). 5. Example Repository URL: ○ Repository URL: https://github.com/example/repo.git o Branch Specifier: main 3. Configuring Jenkins to Work with Maven Overview: The Maven Plugin integrates Jenkins with Apache Maven, allowing Jenkins to use Maven to build projects. Steps: 1. Install Maven: ○ Ensure that Apache Maven is installed on your system. You can download it from the Apache Maven website. 2. Configure Maven in Jenkins: ○ Go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to the Maven section and click Add Maven. ○ Enter a name for the Maven installation (e.g., Maven 3.8.6). ○ Check Install automatically to let Jenkins download Maven, or specify the path to an existing Maven installation. ○ CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 36 1750 Rank

--- Page 112 ---
Unit: III Course Code/Title:CS3V15/Devops 3. Example Configuration: ○ Name: Maven 3.8.6 ○ MAVEN_HOME: / usr / share / maven 4. Configure Maven in a Jenkins Job: ○ Create or edit a Jenkins job and go to Build section. 0 Add a build step and select Invoke top-level Maven targets. o Choose the Maven version configured earlier. ○ Specify the goals to run (e.g., clean install). 5. Example Maven Goals: ○ Goals: clean install 6. Configure Build Environment: ○ Ensure that the job's build environment is set up to use Maven. This might involve setting environment variables or configuring build scripts. 7. Example Environment Variables: ○ MAVEN_OPTS: - Xms512m -Xmx2048m (if you need to set specific JVM options for Maven). By following these steps, Jenkins will be properly configured to work with Java, Git, and Maven, allowing you to set up and manage continuous integration and delivery pipelines effectively. If you need further customization or run into issues, consulting the documentation for each tool or plugin can provide additional guidance. Creating a Jenkins Build and Understanding Jenkins Workspace 1. Creating a Jenkins Build Overview: A Jenkins build is a process where Jenkins executes a series of steps defined in a job configuration. This typically involves compiling code, running tests, and generating artifacts. Steps to Create a Jenkins Build: 1. Access Jenkins Dashboard: ○ Open your Jenkins dashboard, usually found at http://localhost: 8080. 2. Create a New Job: ○ Click on New Item on the left sidebar. Enter a name for your job. ○ Choose a job type. Common types include: I Freestyle project: For simple build jobs with basic configuration. · Pipeline: For more complex build processes using a Jenkinsfile. ○ Click OK to proceed. ○ TECHNOLOGY N33 37 nirf 1750 Rank CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous)

--- Page 15 ---
Course Code/Title:CS3V15/Devops Unit: I which is a mechanism to control the usage of resources or capacity. 2) Code: Many good practices such as Git enables the code to be used, which ensures writing the code for business, helps to track changes, getting notified about the reason behind the difference in the actual and the expected output, and if necessary reverting to the original code developed. The code can be appropriately arranged in files, folders, etc. And they can be reused. 3) Test: The application will be ready for production after testing. In the case of manual testing, it consumes more time in testing and moving the code to the output. The testing can be automated, which decreases the time for testing so that the time to deploy the code to production can be reduced as automating the running of the scripts will remove many manual steps. 4) Plan: DevOps use Agile methodology to plan the development. With the operations and development team in sync, it helps in organizing the work to plan accordingly to increase productivity. 5) Monitor: Continuous monitoring is used to identify any risk of failure. Also, it helps in tracking the system accurately so that the health of the application can be checked. The monitoring becomes more comfortable with services where the log data may get monitored through many third-party tools such as Splunk. 6) Deploy: Many systems can support the scheduler for automated deployment. The cloud management platform enables users to capture accurate insights and view the optimization scenario, analytics on trends by the deployment of dashboards. 7) Operate: DevOps changes the way traditional approach of developing and testing separately. The teams operate in a collaborative way where both the teams actively participate throughout the service lifecycle. The operation team interacts with developers, and they come up with a monitoring plan which serves the IT and business requirements. 8) Release: Deployment to an environment can be done by automation. But when the deployment is made to the production environment, it is done by manual triggering. Many processes involved in release management commonly used to do the deployment in the production environment manually to lessen the impact on the customers. DevOps Lifecycle DevOps defines an agile relationship between operations and Development. It is a process that is practiced by the development team and operational engineers Learning DevOps is not complete without understanding the DevOps lifecycle phases. The DevOps lifecycle includes seven phases as given below: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 16 ---
Unit: I Course Code/Title:CS3V15/Devops together from beginning to the final stage of the product. Development Operations Integration Deployment DevOps Lidecycle Feedback 2) Continuous Integration Testing Monitering 1) Continuous Development This phase involves the planning and coding of the software. The vision of the project is decided during the planning phase. And the developers begin developing the code for the application. There are no DevOps tools that are required for planning, but there are several tools for maintaining the code. This stage is the heart of the entire DevOps lifecycle. It is a software development practice in which the developers require to commit changes to the source code more frequently. This may be on a daily or weekly basis. Then every commit is built, and this allows early detection of problems if they are present. Building code is not only involved compilation, but it also includes unit testing, integration testing, code review, and packaging. The code supporting new functionality is continuously integrated with the existing code. Therefore, there is continuous development of software. The updated code needs to be integrated continuously and smoothly with the systems to reflect changes to the end-users. Commit Build O O Test O Stage Deploy Dev/QA O O > Continuous Integration/Delivery Development </> git Code Commit = production server. 3) Continuous Testing Production Jenkins is a popular tool used in this phase. Whenever there is a change in the Git repository, then Jenkins fetches the updated code and prepares a build of that code, which is an executable file in the form of war or jar. Then this build is forwarded to the test server or the This phase, where the developed software is continuously testing for bugs. For constant testing, automation testing tools such as TestNG, JUnit, Selenium, etc are used. These tools allow QAs to test multiple code- bases thoroughly in parallel to ensure that there is no flaw in the functionality. In this phase, Docker Containers can be used for simulating the test environment. It may occur in the form of documentation files or maybe produce large-scale data about the application parameters when it is in a continuous use position. The system errors such as server not reachable, low memory, etc are resolved in this phase. It maintains the security and availability of the service. Automation testing saves a lot of time and effort for executing the tests instead of doing this manually. Apart from that, report generation is a big plus. The task of evaluating the test cases INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 narf 1750 Rank

--- Page 117 ---
Course Code/Title:CS3V15/Devops Unit: IV • Explore Ansible modules. 4. Playbook - · Playbooks are a sequence of plays that define the order of tasks executed by Ansible. . A play consists of a list of tasks that target managed nodes in an inventory. · Tasks, in turn, are composed of one or more modules that specify operations. · Playbooks provide a structured way to define and automate complex workflows. 5. Roles - · Roles provide an organized environment for managing complex tasks. · They include templates, playbooks, inventories, error handlers, vars, and meta information. · Ideal for handling larger, multi-step automation processes. 6. Collections - · Collections are distribution formats for Ansible content. · They encompass playbooks, roles, modules, and plugins. · Find Ansible collections. 7. Galaxy - · Ansible Galaxy is a platform for sharing and downloading collections. . It allows the Ansible community to collaborate and exchange automation content. Setting Up the Ansible Control Node Prerequisite: Before you begin setting up Ansible, it's essential to ensure you meet the following prerequisites: · SSH: Make sure SSH is installed on your Linux system. SSH is crucial for secure communication between the control node and managed nodes. Installation Steps: 1. Open your terminal. 2. Run the following command to install Ansible: 1python3 -m pip install -- user ansible 3. SSH Key: o For secure and direct connections to managed nodes, you'll need to provide a public SSH key. You can generate one using the ssh-keygen command. o Follow the on-screen instructions to create your SSH key pair. This key pair is a crucial element of Ansible's secure communications. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 118 ---
Course Code/Title:CS3V15/Devops Unit: IV 4. For specific installation instructions tailored to your Linux distribution, click here. 5. Try to connect ssh using once your mange node is set up. 1ssh <user>@<ip> Note: If you're using a different operating system, find the appropriate installation instructions here. Setting Up the Ansible Mange Node Steps: 1. Install OpenSSH-Server: o To enable remote management of the managed node, you'll need to install the OpenSSH server. Use the following command (assuming you're using a Debian-based Linux distribution like Ubuntu): 1sudo apt install openssh-server o This command installs the OpenSSH server, allowing secure remote access. 2. Create a User: o For convenience and consistency, it's helpful to create a user on the managed node with the same name as the user on the control node. This makes it easier to manage SSH keys and ensures a smoother experience. 3. Configure SSH Key: o To establish secure and direct connections to the managed node, you need to paste the public SSH key from the control node into the authorized_keys file located in the .ssh directory of the user's home folder on the managed node. o The path is typically /home/<user>/.ssh/authorized_keys, where <user> is the username you created or are using on the managed node. You can use the ssh-copy-id command to automate the process of copying your public key to the managed node. For example: 1ssh-copy-id <user>@<managed_node_ip> o This command securely copies your public key to the authorized_keys file, allowing passwordless SSH authentication. Managing Ansible Inventories In the world of Ansible, inventories are like the backbone of your automation infrastructure. They serve as a vital component that lists and organizes the managed nodes, making automation tasks seamless and organized. Inventories use a parent-child concept that allows you to create groupings, which come in handy during specific tasks or scenarios. Why Are Inventories Important? · Inventories provide a comprehensive list of managed nodes, making it easy to interact with and manage them. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 4 N33 nirf 1750 Rank

--- Page 61 ---
Course Code/Title:CS3V15/Devops Unit: II Return to the previous screen and undo the deletion. The errors that occurred will disappear. *mavenproject/pom.xml & IApp.java AppTest.java 6 <artifactId>mavenproject</artifactId> 7 <version>0.0.1-SNAPSHOT</version> <packaging>jar</packaging> 8 9 16 <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 138 <properties> 14 15 16 17% <dependencies> 18 19 <groupId>junit</groupId> I 28 21 22 23 24 <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> cdependency> <artifactId>junit</artifactid> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> 25 </project> 26 9. Dependency Management: The dependencyManagement and dependencies are especially useful for multi-module projects. Dependency Management : This tag consists of a dependencies tag which itself might contain multiple dependency tags. Each dependency is supposed to have at least three maintags: groupId, artifactId, and version. For example: <dependencyManagement> <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 20 N33 nirf 1750 Rank -

--- Page 62 ---
Course Code/Title:CS3V15/Devops Unit: II The above code just declares the new artifact commons-lang3, but it doesn't really add it to the project dependency resource list. Dependencies: This tag contains a list of dependency tags. Each dependency is supposed to have at least two main tags, which are groupId and artifactId. For example: <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> The version and scope tags can be inherited implicitly if we have used the dependencyManagement tag before in the POM file. DependencyManagement is just a declaration, and it does not really add a dependency. Dependencies tag adds the actual dependency to the project. An example for adding the JUnit library dependency: <dependencyManagement> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.2</version> <scope>test</scope> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 21 N33 nirf 175º Rank

--- Page 83 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Click "OK" to proceed. 4. Configure the Job General Settings 1. Project Description: ○ Enter a description for your job. 2. Discard Old Builds: ○ Set up a policy to manage the number of builds Jenkins keeps. Source Code Management (SCM) 1. Select SCM: ○ Choose your source code management system (e.g., Git). 2. Configure Repository: ○ Enter the repository URL and provide credentials if needed. 3. Branch Specification: ○ Specify the branch to build (e.g., main, develop). Build Triggers 1. Choose Build Triggers: ○ Poll SCM: Check for changes at intervals. ○ Build Periodically: Schedule builds. ○ GitHub hook trigger: Trigger builds on GitHub push. ○ Build after other projects: Trigger builds based on other jobs. Build Environment 1. Configure Build Environment: ○ Clean workspace: Delete workspace before build starts. ○ Manage secrets: Use secret texts or files. ○ Node selection: Specify where to run the job. Build Steps 1. Add Build Steps: ○ Click "Add build step". ○ Choose the build step type (e.g., Execute shell, Invoke Gradle script). ○ Enter the commands or scripts needed for the build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 nirf 8 1750 Rank

--- Page 84 ---
Course Code/Title:CS3V15/Devops Unit: III Post-build Actions 1. Add Post-build Actions: Click "Add post-build action". ○ Common actions include: Archive artifacts: Store build artifacts. Email notification: Send build status emails. I Publish test results: Aggregate test results. Trigger other jobs: Trigger other jobs based on results. 5. Save and Build 1. Save the Configuration: ○ Click "Save". 2. Build the Job: ○ Click "Build Now" to manually trigger a build. ○ View progress and logs by clicking on the build number. 6. Monitor and Manage Builds 1. Check Build History: ○ View past builds and their statuses. 2. Console Output: ○ Click on a build number to see detailed logs. 3. Build Artifacts: ○ Access any artifacts produced by the build. 7. Advanced Configuration (Optional) 1. Pipeline as Code: 0 For complex workflows, use Jenkins Pipelines. Create a Jenkinsfile in your repository. 2. Plugins: ○ Extend Jenkins functionality with plugins from the Jenkins Plugin Manager. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) 9 N32 nirf 1750 Rank

--- Page 67 ---
Course Code/Title:CS3V15/Devops Unit: II Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013 JVM: 1.7.0_60 (Oracle Corporation 24.60-b09) OS: Windows 8.1 6.3 amd64 Understanding build using Gradle: The Gradle build is a process of creating a Gradle project. When we run a gradle command, it will look for a file called build.gradle in the current directory. This file is also called the Gradle build script. The build configuration, tasks, and plugins are described in this file. The build script describes a project and its tasks. Let's create a small Gradle project, run some of the basic Gradle commands, and understand how Gradle manages the project. Follow the steps below to create and test a Gradle project. Step1: Open the command line and create a directory First, Open the command line and create a directory for the project and change directory to it. Let's create a demo directory. C: \Users \HiMaNshU>mkdir demo C: \Users \HiMaNshU>cd demo Step2: Initialize a Gradle project To generate a Gradle project, run the gradle init command. It will generate a simple project. With this project, we will explore and understand everything that is generated. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 26 N33 nirf 1750 Rank

--- Page 68 ---
Course Code/Title:CS3V15/Devops Unit: II When we run the gradle init command, it will ask for some basic requirements. First, it will ask the type of project that we want to create. It will give four options: 1. 1: basic 2. 2: application 3. 3: library 4. 4: Gradle plugin Select our requirements. Hence, we are just making a demo project so that we will select the basic option. To select basic option, press 1 and Enter key. Consider the below output: C: \Users \HiMaNshU\demo>gradle init Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could ed, use -- status for details Select type of project to generate: 1: basic 2: application 3: library 4: Gradle plugin Enter selection (default: basic) [1 .. 4] 1 Next, it will ask for DSL. There are two options that are available for DSL: 1. 1: Groovy 2. 2: Kotlin Groovy is the default DSL provided by Gradle. Select build script DSL. Select build script DSL: 1: Groovy 2: Kotlin Enter selection (default: Groovy) [1 .. 2] 1 Next, it will ask for the project name. Type the project name and press Enter key. It will take a while to build a project. After the successful execution of the project, we will get a message BUILD SUCCESSFUL. Project name (default: demo): First_Gradle > Task : init Get more help with your project: https: //guides. gradle.org/creating- uilds BUILD SUCCESSFUL in 2m 56s 2 actionable tasks: 2 executed C: \Users\HiMaNshU\demo>mkdir src Now we have successfully created a Gradle project. Now, what will happen to our specified directory? Consider the below structure of the Gradle project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 27 N33 narf 1750 Rank

--- Page 29 ---
Unit: I Course Code/Title:CS3V15/Devops Enterprise services. AWS virtual private cloud. Outsourced cloud services. Salesforce User Controls Operating System, Runtime, Middleware, and Application data Others It is highly scalable and flexible. Microsoft Azure. Force.com, Gigaspaces. Data of the application It is highly scalable to suit the different businesses according to resources. Google cloud services Google offers a seven wide range of Services: · Compute · Networking · Storage and Databases · Big Data · Machine Learning · Identity & Security · Management and Developer Tools IBM cloud analysis. AWS, Terremark Nothing It is highly scalable to suit the small, mid and enterprise level business 1. Compute: GCP provides a scalable range of computing options you can tailor to match your needs. It provides highly customizable virtual machines. and the option to deploy your code directly or via containers. · Google Compute Engine · Google App Engine · Google Kubernetes Engine · Google Cloud Container Registry · Cloud Functions 2. Networking: The Storage domain includes services related to networking, it includes the following services • Google Virtual Private Cloud (VPC) . Google Cloud Load Balancing · Content Delivery Network · What is Google Cloud Connect · Google Cloud DNS · What is Google Cloud Web Hosting 3. Storage and Databases: The Storage domain includes services related to data storage, it includes the following services · Google Cloud Storage . Cloud SQL · Cloud Bigtable · Google Cloud Datastore • Persistent Disk TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 29 N32 nirf 1750 Rank

--- Page 30 ---
Unit: I Course Code/Title:CS3V15/Devops 4. Big Data: The Storage domain includes services related to big data, it includes the following services · Google BigQuery · Google Cloud Dataproc · Google Cloud Datalab · Google Cloud Pub/Sub 5. Cloud AI: The Storage domain includes services related to machine learning, it includes the following services · Cloud Machine Learning · Vision API · Speech API · Natural Language API · Translation API • Jobs API 6. Identity & Security: The Storage domain includes services related to security, it includes the following services · Cloud Resource Manager · Cloud IAM · Cloud Security Scanner · Cloud Platform Security 7. Management Tools: The Storage domain includes services related to monitoring and management, it includes the following services • Stackdriver · Monitoring · Logging · Error Reporting · Trace · Cloud Console 8. Developer Tools: The Storage domain includes services related to development, it includes the following services . Cloud SDK · Deployment Manager · Cloud Source Repositories · Cloud Test Lab AZURE Azure is Microsoft's cloud platform, just like Google has its Google Cloud and Amazon has its Amazon Web Service or AWS.000. Generally, it is a platform through which we can use Microsoft's resources. For example, to set up a huge server, we will require huge investment, effort, physical space, and so on. In such situations, Microsoft Azure comes to our rescue. It will provide us with virtual machines, fast processing of data, analytical and monitoring tools, and so on to make TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 30 N33 narf 1750 Rank

--- Page 97 ---
Course Code/Title:CS3V15/Devops Unit: III Build Steps Execute Windows batch command ? Command See the list of available environment variables javac welcome. java java welcome Advanced = Execute Windows batch command 7 Command See the list of avalable chuironment variables acho User name is Nuernane% Advanced Step 5 : Click on Apply and then Save. Step 6 : Select Build with parameters option. Dashboard > welcome > Status Changes Workspace Build with Parameters Configure Delete Project 0 Rename CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 22 N3 nirf 1750 Rank

--- Page 98 ---
Course Code/Title:CS3V15/Devops Unit: III Project welcome This build requires parameters: Username Parth D Build Cancel We can change the username as well. Suppose I changed the Username as Anand then click the Build, we get the console output as follows - C: \ProgramData\Jenkins\.jenkins\workspace\welcome>javac welcome.java C:\ProgramData\Jenkins\.jenkins\workspace\welcomeyjava welcome Welcome to first Job of Jenkins [welcome] $ cmd /c call C:\WINDOWS\TEMP\jenkins16582464065687933173.bat C:\ProgramData\Jenkins\.jenkins\workspace\welcome>echo User name is Anand User name is Anand C:\ProgramData\Jenkins\.jenkins\workspace\welcome>exit 0 Finished: SUCCESS Thus we set variety of parameters to the Jenkins job. We can set the Boolean parameter, choice parameter, credentials, file parameter, password parameter and so on. 3. HTML Publisher Filter Plain Boolean Parameter Choice Parameter Credentials Parameter File Parameter Multi-line String Parameter Password Parameter Run Parameter String Parameter Add Parameter Overview: The HTML Publisher Plugin allows Jenkins to publish HTML reports and artifacts generated during the build process. Key Features: · Report Publishing: Publishes HTML reports, dashboards, or other HTML artifacts. · Report Directory: Allows specifying the directory containing HTML reports. · Index Page: Sets a default HTML file to be displayed when accessing the report. Configuration: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 23 nirf 1750 Rank N33

--- Page 101 ---
Course Code/Title:CS3V15/Devops Unit: III v => SeleniumDemo src/main/java > src/main/resources > src/test/java > 15 src/test/resources > al JRE System Library [J2SE-1.5] › al Maven Dependencies » drivers v src > @ main v fs test v java v 2> myPackage 10 myTestPage.java 23 resources › › target > > test-output 2 pom.xml > Servers myTestPage.java package myPackage; import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver; import org.testng.annotations.Test; public class myTestPage { @Test public void myFun() { System.setProperty("webdriver.chrome.driver","./drivers/chromedriver.exe"); WebDriver driver = new ChromeDriver(); driver.get("https://google.com"); String title = driver.getTitle(); System.out.println("Hey Anuradha, the tile of this page is: "+title); driver.quit(); } Program explanation : In above program, we have written one test case by the function myFun. In this function, using the Chromedriver which we have installed, we open the web page google.com Then we try to get the tile of this page and display it on the console using System.out statement. Finally we quit the driver. Step 7 : Right click on the Project name in Project Explorer window, Run As-> Maven Test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 26 N3 nirf 1750 Rank

--- Page 102 ---
Unit: III Course Code/Title:CS3V15/Devops Step 8 : Now, locate the Java file myTestPage.java and right click Run As->TestNG Test. It is as follows - 0 Run As # Debug As Profile As Web Services > > 1 TestNG Test > Run Configurations ... > Alt+Shift+X, N Step 9 : We will get the output as follows - Hey Anuradha, the tile of this page is: Google PASSED: myFun Default test Tests run: 1, Failures: 0, Skips: 0 Default suite Total tests run: 1, Failures: 0, Skips: 0 Step 10 : Now, we will open the Jenkin's page and create the HTML Publisher report for the above SeleniumDemo project. The prerequisite to this activity is that the HTML Publisher report plugin must be installed in Jenkins. · Click on New Item, Enter the Item Name and select as Freestyle project, I have given the name HTMLReportDemo. · Then click on configure. Write the suitable description in General section. · The in Build Steps and Add Build steps as Invoke top-level Maven targets provide the Maven version, and under goals write clean test. . Under Post Build Actions, click on Add Post Build Action and select Publish HTML Report. Now give the path of index.html file which is created by Selenium testNG framework in Maven. It is the path of test-output folder. Also give some suitable HTML report title. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 27 N3 nirf 1750 Rank

--- Page 91 ---
Course Code/Title:CS3V15/Devops Unit: III Now we will create a repository on GitHub and then push the above committed Git repository on the GitHub. Open a web browser, and login to www.GitHub.com. Create a repository by the name MyJavaPrograms. Start a new repository A repository contains all of your project's files, revision history, and collaborator discussion. AnurdhaP / MyJavaPrograms Public Anyone on the internet can see this repository Private You choose who can see and commit to this repository Create a new repository 1 then copy the URL of this repository. HTTPS SSH https://github.com/AnurdhaP/MyJavaPrograms.git Now open the command prompt window and go to the MyJavaPrograms directory and issue the following command. git remote add origin https://github.com/AnurdhaP/MyJavaPrograms.git then push the Git to GitHub repository using the command push -u origin master CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 16

--- Page 92 ---
Course Code/Title:CS3V15/Devops Unit: III It is illustrated as follows - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git push -u origin master Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 719 bytes | 719.00 KiB/s, done. Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 To https://github.com/AnurdhaP/MyJavaPrograms.git * [new branch] master -> master branch 'master' set up to track 'origin/master'. E: \MyJavaPrograms> · Now it we refresh the repository on GitHub then we can see our test.java and test.class programs are present in that repository. MyJavaPrograms Public !" master - 1º 1 branch 0 tags Go to f AnurdhaP First Commit for Java program .4cb test.class First Commit for Java program test.java First Commit for Java program Help people interested in this repository understand your project by adding a README. Step 4 : Now create a job in Jenkins that makes use of GitHub plugin in order to access the GitHub repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 17 N3 nirf 1750 Rank

--- Page 99 ---
Unit: III Course Code/Title:CS3V15/Devops 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "HTML Publisher Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Post-build Actions section. ○ Select Publish HTML reports. ○ Specify the directory containing HTML reports and the index page. Usage: . Ideal for displaying build reports such as test results or code coverage metrics in a user-friendly HTML format. DEMO: Step 1 : We will create a Maven project using Selenium and TestNG framework. Open Eclipse : IDE and Create a Maven Project. Select the Create Simple Project (Skip archetype selection) and enter the Group Id and Artifact Id as per your choice. My project name is SeleniumDemo. Step 2 : Add the Selenium dependency in your pom.xml file as follows - <dependency> <groupId>org.seleniumhq.selenium</groupId> <artifactId> selenium-java</artifactId> <version>3.141.59</version> </dependency > Save the pom.xml file. Step 3 : Now install testNG plugin. For that, from Menu select Help-> Eclipse Marketplace Help Welcome 7 Help Contents Search Show Context Help Show Active Keybindings ... Ctrl+Shift+L Tip of the Day Tips and Tricks ... Cheat Sheets ... @ Eclipse User Storage Open the Eclipse Marketpla ** Perform Setup Tasks ... % Check for Updates Install New Software ... Eclipse Marketplace ... = About Eclipse IDE · Donate Contribute Type TestNG in the search window. The testNG installation option will be available. click Install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 24 N32 nirf 1750 Rank ---

--- Page 100 ---
Course Code/Title:CS3V15/Devops Unit: III C Eclipse Meketplace × Eclipse Marketplace Select solutions to initiit, Prese leuralt Now to proceed with lutallation. Press the "more info" link to learn more about a solution Search Recent Popular Favorites itwled - Research at the Eclipse × All Categories Go TestNG for Eclipse . This plug-in late you run your TextNG wits from Erfiore. You can run sulter, groupe er individuel methods. Errors are reported in a pepwane tab that lets you. more info by Chat Bent Apache 20 tantog zielt testingunit integration functional selection * 737 ** * stal: 1,83M (25,021 last month) Install MoreUnit 3.3.0 Morelinit is an Solipoe plugin that should mult you in wilting mory unk with. It supports ali programming languages (witching between teds and clanset under. mone.info Install Infinitest 5.4.0.3 Infinitest is a continuont tout runner for tova,'and is valuable to developers valing a Marketplaces (7) Cancel Then confirm the selected features by clicking on Confirm button. x Confirá Selected Features These Confirm to gorthus with the induction. Cs go back te chocie bis sules it CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 25 N3 nirf 1750 Rank

--- Page 69 ---
Unit: II Course Code/Title:CS3V15/Devops 1 build.gradle gradle wrapper 2 gradle-wrapper.jar - gradle-wrapper.properties 3 gradlew 4 gradlew.bat 5 settings.gradle 6 It is the default structure of a Gradle project. Gradle will generate the following things for us: 1. The gradle file is build script for configuring the current project. 2. An executable JAR file is used as a Gradle wrapper. 3. Configuration properties for Gradle Wrapper. 4. The gradlew is a Gradle wrapper script for UNIX based OS. 5. The bat is the Gradle Wrapper script for Windows. 6. The settings script for configuring the Gradle build. Step3: Create a task Gradle supports APIs for creating and managing tasks through a Groovy-based DSL or Kotlin-based DSL. Every project contains a collection of tasks for some basic operation. Gradle supports a library of tasks that configure the project. For example, there is a Copy task, which copies files from one location to another. The Copy task is one of the most used tasks In Gradle. To use the Copy task in build script, follow the below process. Step1: Create a directory called src C: \Users \HiMaNshU\demo>mkdir src C: \Users \HiMaNshU\demo>cd src Step2: Add a file called myfile.txt in the src directory. Add the single line "Hello, World!" to it, also, we can leave it empty. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 28 N32 nirf 1750 Rank

--- Page 70 ---
Unit: II Course Code/Title:CS3V15/Devops C:\Users \HiMaNshU\demo\src>echo myfile.txt myfile.txt Define a task called Copy in build.gradle file. It will copy the src directory to a new directory called dest. We don't have to create the dest directory; the Copy task will do it for us. 1. task copy(type: Copy, group: "Custom", description: "The sources are copied to dest directory") { 2. from "src" 3. into "dest" 4. } We can provide anything in a group and description. Also, we can omit them, but doing so will also be omitted from the report of the task used later. Now execute our new copy task: C: \Users \HiMaNshU\demo>gradle copy BUILD SUCCESSFUL in 2s 1 actionable task: 1 executed C: \Users \HiMaNshU\demo>_ After the successful execution of the task, we will get BUILD SUCCESSFUL message. The build.gradle file The build.gradle file is build script of a Gradle project. All the tasks and plugins are defined in this file. When we run a gradle command, it looks for a file called build.gradle in the current directory. Although we have called it a build script, strictly, it is a build configuration script. The build script defines a project and its tasks. The default build.gradle file looks like as follows: CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 29 N33 nirf 1750 Rank

--- Page 105 ---
Course Code/Title:CS3V15/Devops Unit: III How to install copy artifact plugin ? Step 1 : Start the Jenkins tool by entering the login and password. Step 2 : Click on Manage Jenkins. Click on Plugins. Step 3 : In the search window, type the name copy Artifact. Step 4 : The plugin name will be displayed if it is not already installed. Select it. Name Adde a build atop to copy artifacts from another project. Thủa plugin le up for adoption! We are looking for naw maintainent. Velt our Adopt a Plugin intuitive for mort information. then click on Install. The plugin gets installed. Example Demo · Let us discuss how to use Copy Artifact plugin. Step 1 : Create a new Job by clicking on item New Item. Give the some name to your job. I have given the name App1 and selected the Freestyle Project. Step 2: Give suitable description in the description box. Step 3 : Under the source code management, click on Git option and specify the repository URL of GitHub. Source Code Management None Git ? Repositories 1 Repository URL + ·hong · CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 30 N3 nirf 1750 Rank

--- Page 106 ---
Unit: III Course Code/Title:CS3V15/Devops Step 4 : Under the Build Steps section, choose the Execute Windows batch command option. And type the javac and java commands. Build Steps = Execute Windows batch command Command See the list of available environment variables javac test.java java test Advanced Add bulld stop * Step 5 : Click in Apply and Save. Step 6 : Select the Build Now option. On successful built, the test, class file gets generated in the Jenkins workspace of the App1 job. C:\ProgramData\Jenkins\.jenkins\workspace\Appl>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\Appi>java test Good Morning Pathill Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! C:\ProgramData\Jenkins\.jenkins\workspace\Appl>exit @ Finished: SUCCESS Step 7 : Now we have to copy test.class file from the workspace of Appl to another job. For that' purpose we will create another Job named App2 by clicking New Item. Select the Freestyle Project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 31 N3 nirf 1750 Rank

--- Page 5 ---
Unit: I Course Code/Title:CS3V15/Devops Example: Facebook's mobile app which is updated every two weeks effectively tells users you can have what you want and you can have it. Now ever wondered how Facebook was able to do social smoothing? It's the DevOps philosophy that helps facebook and sure that apps aren't outdated and that users get the best experience on Facebook. Facebook accomplishes this true code ownership model that makes its developers responsible that includes testing and supporting through production and delivery for each kernel of code. They write and update their true policies like this but Facebook has developed a DevOps culture and has successfully accelerated its development lifecycle. Difference between Agile and DevOps: S. No. Agile 1 It started in the year 2001. It started in the year 2007. 2 Invented by John Kern, and Martin Fowler. Invented by John Allspaw and Paul Hammond at Flickr, and the Phoenix Project by Gene Kim. 3 Agile is a method for creating software. It is not related to software development. Instead, the software that is used by DevOps is pre-built, dependable, and simple to deploy. 4 An advancement and administration approach. Typically a conclusion of administration related to designing. 5 The agile handle centers on consistent changes. DevOps centers on steady testing and conveyance. 6 A few of the finest steps embraced in Agile are recorded underneath - 1. Backlog Building 2.Sprint advancement DevOps to have a few best hones that ease the method - 1. Focus on specialized greatness. 2. Collaborate straightforwardly with clients and join their feedback 7 Agile relates generally to the way advancement is carried of, any division of the company can be spry in its hones. This may be accomplished through preparation. 8 All the group individuals working in a spry hone have a wide assortment of comparable ability sets. This is often one of the points of interest of having such a group since within the time of requirement any of the group individuals can loan help instead of holding up for the group leads or any pro impedances. 9 Spry accepts "smaller and concise". Littler the group superior it would be to convey with fewer complexities. 10 Since Agile groups are brief, a foreordained sum of time is there which are sprints. Tough, it happens that a sprint has endured longer than a month but regularly a week long. 11 A big team for your project is not required. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY DevOps DevOps centers more on program arrangement choosing the foremost dependable and most secure course. DevOps features a diverse approach and is very viable, most of the time it takes after "Divide and Conquer". Work partitioned among the improvement and operation groups. DevOps, on the other hand, accepts that "bigger is better". DevOps, on the other hand, prioritizes reliabilities. It is since of this behavior that they can center on a long-term plan that minimizes commerce's unsettling influences. It demands collaboration among different teams for the completion of work. 5 N33 nirf 1750 Rank

--- Page 6 ---
Unit: I Course Code/Title:CS3V15/Devops 12 Some of the Tools- · Bugzilla · JIRA · Kanboard and more. 13 It is suitable for managing complex projects in any department. It centers on the complete engineering process. 14 It does not focus on the automation. It focusses on automation. 15 Working system gets more significance in Agile than documentation. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Some of the Tools- · Puppet • Ansible • AWS · Chef · team City OpenStack and more. The process documentation is significant in DevOps. 6 N32 nirf 1750 Rank

--- Page 59 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Specify Archetype parameters M Group Id: com.simplileam Artifact Id: mavenproject Version: 0.0.1-SNAPSHOT Package: com,simplilearn.mavenproject . Properties available from archetype: Name Value Add ... Remove Advanced ? < Back Next > Cancel Finish The project is now created. · Open the pom.xml file You can see all the basic information that you have entered on the screen, such as the Artifact ID, Group ID, etc. You can see the junit dependencies have been added. This process takes place by default in Eclipse. There will also be some by default test cases. File Edit Source Navigate Search Project Run Window Help # Package Explorer E Jo JUnit - mavenproject/pom.xml 28 mavenproject > ( src/main/java ( src/test/java > = JRE System Library (|2SE-1:5] i Maven Dependencies target 7 <artifactIdaavenproject</artifactId> <version>0. 0. 1-SNAPSHOT</version>: <packaging>jar</packaging> <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 136 sproperties> 14 <project. build.sourceEncoding>UTF-8</project. build.sourceEncoding> </properties> 15 16 «dependenciess 18- <dependency> 19 <groupId>junit</grouptasI <artifactId=junit</artifactle> sversion>3.8.1</version> <scope>tests/scope> 21 22 223 </dependency> 24 </dependencies> 25 </project> Overview Dependencies Dependency Hierarchy |Effective POM pom.xml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 18 N32 nirf 175º Rank -

--- Page 60 ---
Course Code/Title:CS3V15/Devops Unit: II There you can find AppTest.java to be a default test case. When you click on that, you can see the test cases written in JUnit on your Eclipse screen. # Package Explorer 13 Ju JUnit mavenproject/pom.xml Z App.java AppTest.java 12 1 package com.simplilearn.mavenproject; - fl mavenproject + ( src/main/java - il com.simplileam.mavenproject U App.java = ( src/test/java il com.simplileam.mavenproject , E AppTest.java > > JRE System Library ||2SE-1.5] al Maven Dependencies - target pom.xml 3w import jumit. framework. Test:[] 6 76 / ** . Unit test for simple App. 10 public class AppTest extends TestCase 12 1 14 . Create the test case 15 16 * @param testName name of the test cole 17 18% public AppTest( String testNane ) 19 1 super( testName ); 28 21 1 22 :23% 24 * @return the suite of tests being tested 75 When it comes to adding more test cases, it will depend on the user, but these test cases and commands can easily be added in the workspace. If we try to remove certain dependencies from our file, we will receive error messages. To troubleshoot this, complete the following steps: · Go to another tab: mavenproject/pom.xml · Delete any dependencies · Save the file Immediately, there will be several error messages in the AppTest.java. R mavenproject/pom.xml App.java AppTest.java 83 1 package com.simplilearn.mavenproject; 2 3@ import junit. framework. Test;[] 6 mavenproject/src/test/java/com/simplileam/mavenproject/AppTest java 8 * Unit test for simple App. 9 18 public class AppTest extends TestCase 011 12 { 136 14 . Create the test case 15 16 * @param testName name of the test case 17 188 19 € public AppTest( String testName ) 28 super( testName ); 21 22 - 230 24 * @return the suite of tests being tested Problems E @ Javadoc ) Declaration History 7 errors, 1 warning, 0 others Description + @ Errors (7 items) a Test cannot be resolved to a type Resource Path Location AppTest.java /mavenproject/src/te line 26 Java Problem Type CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 19 N32 nirf 1750 Rank -

--- Page 73 ---
Course Code/Title:CS3V15/Devops Unit: II init - Initializes a new Gradle build. wrapper - Generates Gradle wrapper files. Help tasks Build Environment - Displays all build script dependencies declared in root project 'eclipse-workspace'. components - Displays the components produced by root project 'eclipse-workspace '. [incubating] dependencies - Displays all dependencies declared in root project 'eclipse-works pace'. Dependency Insight - Displays the insight into a specific dependency in root project 'eclipse-workspace.' Dependent Components : It displays the dependent components of components in the root project 'eclipse-workspace.' [incubating] help - Displays a help message. model - Displays the configuration model of root project 'eclipse-workspace.' [incubating] projects - Displays the sub-projects of root project 'eclipse-workspace.' properties - Displays the properties of root project 'eclipse-workspace.' tasks - Displays the tasks runnable from root project 'eclipse-workspace.' To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> To list all the tasks of the project, run the below command: 1. gradle tasks -all CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 32 N33 nirf 1750 Rank

--- Page 74 ---
Course Code/Title:CS3V15/Devops Unit: II To display more details about a task, run the below command: 1. gradle help -- task Listing Dependencies In Gradle, we can list the dependencies which are broken down by the configuration. To list the dependencies, run the below command: 1. gradle -q dependencies Difference between Gradle and Maven Basis Based on Gradle is based on developing Maven is based on developing pure Java language-based software. Configuration It uses a Groovy-based Domain-specific language (DSL) for creating project structure. Focuses on Performance It performs better than maven as it optimized for tracking only current running task. It is necessary to compile. Java Compilation Usability It is a new tool , which requires users to spend a lot This tool is a known tool for many users and is easily Gradle domain-specific language projects. It uses Extensible Markup Language (XML) for creating project structure. Developing applications by adding new features to them . It avoids compilation. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] Maven Developing applications in a given time limit. It does not create local temporary files during software creation and is hence - slower . 33 N33 nirf 1750 Rank -

--- Page 63 ---
Course Code/Title:CS3V15/Devops Unit: II 10. Introduction of Gradle: The list of features that Gradle provides, · Gradle is available with separate Domain Specific Language (DSL) based on Groovy language. · It provides the declarative language elements. Those elements also provide build-by- convention support for Java, Groovy, OSGI, Web and Scala. Language for dependency based programming The declarative language lies on a top of a general purpose task graph, which can be fully supported in the build. Structure your build Gradle allows you to apply common design principles to your build. It will give you a perfect structure for build, so that, you can design well-structured and easily maintained, comprehensible build. Deep API By using this API, you can monitor and customize its configuration and execution behavior to the core. Gradle scales Gradle can easily increase the productivity, from simple and single project builds to huge enterprise multi-project builds. Multi-project builds Gradle supports the multi-project builds and partial builds. If you build a subproject, Gradle takes care of building all the subprojects, that the subproject depends on. Different ways to manage your builds Gradle supports different strategies to manage your dependencies. Gradle is the first build integration tool Gradle is fully supported for your ANT tasks, Maven and lvy repository infrastructure for publishing and retrieving dependencies. It also provides a converter for turning a Maven pom.xml to Gradle script. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 22 N32 nirf 175º Rank -

--- Page 64 ---
Course Code/Title:CS3V15/Devops Unit: II Ease of migration Gradle can easily adapt to any structure. Therefore, you can always develop your Gradle build in the same branch, where you can build live script. Gradle Wrapper Gradle Wrapper allows you to execute the Gradle builds on machines, where Gradle is not installed. This is useful for continuous integration of servers. Free open source Gradle is an open source project, and licensed under the Apache Software License (ASL). Groovy Gradle's build script are written in Groovy programming language. The whole design of Gradle is oriented towards being used as a language and not as a rigid framework. Groovy allows you to write your own script with some abstractions. The whole Gradle API is fully designed in Groovy language. Installation of Gradle: Prerequisites to install Gradle JDK and Groovy are the prerequisites for Gradle installation. Gradle requires JDK version 6 or later to be installed in the system. It uses the JDK libraries which are installed, and sets to the JAVA_HOME environmental variable. Gradle carries its own Groovy library, therefore, we need not install Groovy explicitly. If it is installed, that is ignored by Gradle. The steps to install Gradle in your system are explained below. Step 1 - Verify JAVA Installation First of all, you need to have Java Software Development Kit (SDK) installed on your system. To verify this, execute Java -version command in any of the platform you are working on. In Windows Execute the following command to verify Java installation. I have installed JDK 1.8 in my system. C:\> java - version CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE . TECHNOLOGY 23 N32 narf 1750 Rank

--- Page 113 ---
Course Code/Title:CS3V15/Devops Unit: III 3. Configure Job Details: ○ General: I Enter a description for your job. Configure options such as discarding old builds if needed. ○ Source Code Management: I Choose Git, Subversion, or another source control system. Enter the repository URL and credentials if the repository is private. Specify the branch or tag to build from. ○ Build Triggers: Set up triggers to start the build. Common options include: Poll SCM: Jenkins will periodically check for changes in the source code repository. . Build periodically: Schedule builds at specific intervals. I GitHub hook trigger for GITScm polling: Trigger builds based on GitHub webhooks. 0 Build Environment: ■ Configure the build environment, such as setting up environment variables or cleaning up before the build starts. ○ Build Steps: Add build steps by clicking Add build step. · Common build steps include: Execute shell: Run shell commands or scripts. . Invoke Gradle script: Use Gradle to build the project. I Invoke Ant: Use Apache Ant for building. ○ Post-build Actions: · Configure actions to perform after the build completes, such as: I Archive the artifacts: Save build artifacts for later use. I Publish JUnit test result report: Display test results in Jenkins. I Send build notifications: Notify stakeholders of build status. 4. Save and Build: ○ Click Save to store the job configuration. o To start a build, click Build Now in the job dashboard. Example Build Configuration: . Job Name: MyApp-Build · Source Code Management: Git o Repository URL: https://github.com/user/myapp.git ○ Branch: main · Build Step: Execute shell ○ Command : myn clean install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 38 nirf 1750 Rank N33

--- Page 114 ---
Course Code/Title:CS3V15/Devops Unit: III · Post-build Action: Archive artifacts ○ Files to archive: target/ *. jar 2. Understanding Jenkins Workspace Overview: The Jenkins workspace is a directory where Jenkins stores files and artifacts related to a particular build. Each job has its own workspace, which is used to perform build operations. Key Aspects of Jenkins Workspace: 1. Workspace Location: o By default, the workspace is located in the Jenkins home directory, typically at /var/lib/jenkins/workspace/ on Linux systems or C:\Program Files (x86) \Jenkins\workspace\ on Windows. 2. Workspace Structure: ○ Each job gets its own subdirectory within the workspace. For example, a job named MyApp-Build will have its workspace at /var/lib/jenkins/workspace/MyApp - Build/. ○ The workspace contains: Source Code: The code pulled from the repository. . Build Artifacts: Files generated during the build process. I Logs: Logs related to the build process. 3. Workspace Usage: ○ Building: During the build process, Jenkins checks out the code into the workspace, executes build steps, and generates artifacts. ○ Archiving Artifacts: After the build, files specified in the post-build actions are archived from the workspace. ○ Cleaning Up: Jenkins may clean up workspaces based on job configurations or policies to save disk space. 4. Customizing Workspace: ○ Configure Custom Workspace Location: In the job configuration, under the Advanced Project Options, you can specify a custom workspace directory. ○ Workspace Cleanup: I Use plugins like the Workspace Cleanup Plugin to manage and clean up workspaces automatically. Example of Workspace Structure: · Workspace Directory: /var/lib/jenkins/workspace/MyApp-Build/ o Source Code: /var/lib/jenkins/workspace/MyApp-Build/src/ o Build Artifacts: /var/lib/jenkins/workspace/MyApp-Build/target/ o Logs: /var/lib/jenkins/workspace/MyApp-Build/logs/ This guide should provide a comprehensive understanding of creating a Jenkins build and managing Jenkins workspaces. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY N32 39 nirf 175º Rank

--- Page 39 ---
Course Code/Title:CS3V15/Devops Unit: I GITHUB GitHub is an increasingly popular programming resource used for code sharing. It's a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in October 2020, it is the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 56 million total developers. GitHub is a Git repository hosting service that provides a web-based graphical interface. It is the world's largest coding community. Putting a code or a project into GitHub brings it increased, widespread exposure. Programmers can find source codes in many different languages and use the command-line interface, Git, to make and keep track of any changes. GitHub helps every team member work together on a project from any location while facilitating collaboration. You can also review previous versions created at an earlier point in time. GitHub's Features? 1. Easy Project Management: GitHub is a place where project managers and developers come together to coordinate, track, and update their work so that projects are transparent and stay on schedule. 2. Increased Safety With Packages Packages can be published privately, within the team, or publicly to the open-source community. The packages can be used or reused by downloading them from GitHub. 3. Effective Team Management GitHub helps all the team members stay on the same page and organized. Moderation tools like Issue and Pull Request Locking help the team to focus on the code. 4. Improved Code Writing Pull requests help the organizations to review, develop, and propose new code. Team members can discuss any implementations and proposals through these before changing the source code. 5. Increased Code Safety GitHub uses dedicated tools to identify and analyze vulnerabilities to the code that other tools tend to miss. Development teams everywhere work together to secure the software supply chain, from start to finish. 6. Easy Code Hosting All the code and documentation are in one place. There are millions of repositories on GitHub, and each repository has its own tools to help you host and release code. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 39 N32 nirf 1750 Rank

--- Page 40 ---
Course Code/Title:CS3V15/Devops Unit: I HOSTING SERVICE FOR GIT REPOSITORY When it comes to hosting Git repositories, various platforms provide a robust infrastructure for collaborative development, version control, and project management. Choosing the right hosting service depends on factors like ease of use, collaboration features, and integration capabilities. Here are some popular Git hosting services widely utilized in the software development community: 1. GitHub: GitHub stands out as one of the most prevalent Git hosting platforms, offering a user- friendly interface, powerful collaboration features, and seamless integration with various tools. It serves as an ideal choice for open-source projects, private repositories, and team collaboration. 2. GitLab: GitLab is a comprehensive web-based Git repository manager that not only provides source code management but also includes features like continuous integration. It caters to both cloud-based and self-hosted solutions, giving users flexibility in hosting their repositories. 3. Bitbucket: Owned by Atlassian, Bitbucket is another popular Git repository hosting service. Supporting both Git and Mercurial repositories, it offers features like code collaboration, issue tracking, and continuous integration. Bitbucket is often preferred by teams using other Atlassian tools such as Jira and Confluence. 4. GitKraken Glo Boards: GitKraken Glo Boards is an integrated task and issue tracking service linked with GitKraken, a Git client. This platform allows teams to manage tasks directly associated with their Git repositories and provides a visual approach to monitoring project progress. 5. SourceForge: SourceForge, with a long history, hosts open-source software projects and offers version control, bug tracking, and project management tools. While not as prominent as some other options, it remains a viable choice for numerous projects. 6. AWS CodeCommit: As part of Amazon Web Services (AWS), AWS CodeCommit is a fully managed source control service. It seamlessly integrates with other AWS services and provides a secure and scalable environment for hosting Git repositories. Selecting the most suitable Git hosting service depends on your team's requirements, project size, and preferences for cloud-based or self-hosted solutions. Each platform has its strengths, catering to specific use cases within the software development landscape. Difference between GIT and GITHUB Git and GitHub are related concepts but serve different purposes in the context of version control and collaborative software development. Git: Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows. GitHub: GitHub is a web-based Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 40 N32 nirf 1750 Rank

--- Page 109 ---
Course Code/Title:CS3V15/Devops Unit: III Note that the git repository and the files from that repository are copied from the workspace of App1 to App2. Also note that test.class and test.java files are copied to the workspace of App2. Started by user Anuradha R Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\. jenkins\workspace\App2 Copied 16 artifacts from "Appl" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621189248227895.bat C:\ProgramData\Jenkins\.jenkins\workspace\App2>dir Volume in drive C is OS Volume Serial Number is C6C2-8A9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 .git 474 test. class 144 test.java 2 File(s) 618 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ Finished: SUCCESS 5. Extended Choice Parameter Overview: The Extended Choice Parameter Plugin provides advanced parameter types for Jenkins jobs, such as multi-select lists, checkboxes, and more complex formats. Key Features: · Parameter Types: Includes multi-select, checkboxes, and other advanced input options. . Dynamic Choices: Allows dynamic generation of choices from scripts or external sources. Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Extended Choice Parameter Plugin," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add an Extended Choice Parameter. ○ Configure the parameter type, choices, and other settings. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 110 ---
Unit: III Course Code/Title:CS3V15/Devops Usage: · Useful for scenarios where complex user input is needed or where multiple selection options are required. Configuring Jenkins to Work with Java, Git, and Maven 1. Configuring Jenkins to Work with Java Overview: Jenkins requires Java to run. Configuring Java in Jenkins involves specifying the Java Development Kit (JDK) installations Jenkins should use. Steps: 1. Install Java Development Kit (JDK): ○ Ensure that JDK is installed on your system. You can download it from the Oracle website or use OpenJDK. 2. Configure JDK in Jenkins: ○ Open Jenkins and go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to JDK section and click Add JDK. ○ Enter a name for the JDK installation (e.g., JDK 11). Check Install automatically to let Jenkins download and install the JDK, or specify the path to an existing JDK installation. ○ If specifying the path manually, provide the JAVA_HOME directory. 3. Example Configuration: ○ Name: JDK 11 ○ JAVA_HOME: /usr/lib/jvm/java-11-openjdk 4. Verify JDK Configuration: You can verify the JDK configuration by creating a simple Jenkins job and adding a build step that prints the Java version using: bash Copy code java -version 2. Configuring Jenkins to Work with Git Overview: The Git Plugin integrates Jenkins with Git repositories, enabling Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 35 1750 Rank

--- Page 7 ---
Course Code/Title:CS3V15/Devops Unit: I DevOps Tools 1. Git (GitLab, GitHub, Bitbucket) Git remains indispensable in software development and DevOps due to its pivotal role in version control, collaborative coding, and efficient project management. As technology has accelerated, the need for streamlined and organized code management has never been greater. Git empowers developers to collaborate on codebases, effortlessly creating and merging branches for new features and bug fixes. Its distributed nature ensures developers can work seamlessly offline, an increasingly valuable feature in today's remote and distributed work environments. Additionally, Git facilitates the tracking of code modifications, making it easier to identify when and why specific changes were made, a critical aspect of maintaining code quality and security. Software development is essential in driving innovation and advancing progress, and Git maintains its prominent position as the bedrock of efficient, cooperative, and secure coding methodologies. 2. Maven Due to its enduring significance in managing project dependencies, building, and project lifecycle management, Maven remains a pivotal tool in SD and DevOps. As a robust build automation and project management tool, Maven simplifies the complexities of Java-based project development by streamlining the compilation, testing, packaging, and distribution processes. It ensures consistent and reproducible builds, making it easier for development teams to collaborate efficiently and deliver high-quality software. Maven's role in managing dependencies and facilitating continuous integration and deployment remains crucial. Its ability to handle complex build scenarios and integrate seamlessly with modern DevOps practices makes it indispensable for ensuring software projects' reliability, maintainability, and scalability in 2024 and beyond. 3. Jenkins Its importance lies in its role as a powerful automation server that enables continuous integration and continuous delivery (CI/CD) pipelines. Jenkins streamlines software development by automating tasks such as building, testing, and deploying code changes, ensuring that software is delivered quickly and highly. With the growing complexity of modern applications, the need for efficient CI/CD processes has become even more paramount. Jenkins provides flexibility, extensibility, and a vast library of plugins that cater to a wide range of technologies and tools, making it adaptable to diverse development environments. As organizations prioritize speed, reliability, and collaboration in their software development practices, Jenkins stands as a cornerstone tool, enabling teams to achieve seamless automation and efficient delivery of software solutions. 4. Chef Chef, a powerful automation platform, is crucial in managing infrastructure as code. Chef empowers organizations to achieve scalability, reliability, and speed seamlessly. By allowing the automation of server provisioning, configuration, and maintenance, Chef enhances efficiency and consistency across the entire infrastructure, reducing manual errors and ensuring that infrastructure remains desired. Moreover, Chef integrates smoothly with various cloud providers, containerization technologies, and other DevOps tools, making it adaptable to the ever-evolving tech TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 7 N32 nirf 1750 Rank

--- Page 8 ---
Course Code/Title:CS3V15/Devops Unit: I landscape. As organizations prioritize agility and scalability, Chef remains a vital tool in automating complex infrastructure tasks and enabling DevOps teams to focus on innovation and delivery. 5. Puppet Puppet is essential because it simplifies the management and orchestration of complex IT infrastructures by allowing administrators to define infrastructure as code. It ensures consistency and repeatability in configuration across servers, cloud instances, and containers. Businesses increasingly rely on diverse, dynamic, and hybrid infrastructures. Puppet's importance lies in its ability to streamline provisioning, configuration, and continuous compliance, thus reducing operational complexity, minimizing errors, and accelerating software delivery. Puppet continues to empower organizations to efficiently manage and scale their infrastructure while maintaining high levels of security and compliance, making it a crucial tool for DevOps teams. 6. Ansible Ansible is a powerful and widely adopted automation and configuration management tool important in 2024 for several reasons. This tool stands out for its simplicity and versatility. It empowers organizations to automate repetitive tasks, provisioning of infrastructure, and configuration management across diverse environments, making it an invaluable asset for DevOps and IT teams. Furthermore, Ansible's agentless architecture, declarative language, and a vast library of pre- built modules make it accessible to both beginners and seasoned professionals. As organizations prioritize efficiency, scalability, and the rapid deployment of applications and services, Ansible remains an indispensable DevOps toolkit, helping teams streamline operations, enhance security, and maintain infrastructure at scale, all while reducing manual errors and increasing agility in a fast-paced technological landscape. 7. Docker Docker is crucial in modern software development and DevOps practices. It can simplify and streamline the management of applications across various environments. Docker containers encapsulate an app and its dependencies, ensuring consistent and reproducible deployments from development to production. This technology enhances portability and scalability, accelerates development cycles, and reduces the "it works on my machine" problem. In a rapidly evolving software landscape, Docker's containerization approach remains crucial for achieving efficient, isolated, and highly flexible application deployment, making it an essential component of DevOps and continuous delivery pipelines. 8. Kubernetes Kubernetes, often abbreviated as K8s, play a central role in modern software development and operations. Its importance lies in its ability to orchestrate, manage, and automate containerized applications at scale. As organizations increasingly embrace microservices architectures and containerization for their applications, Kubernetes provides the essential infrastructure for deploying, scaling, and maintaining these containers efficiently. The tool's resilience, self-healing capabilities, and support for hybrid and multi-cloud environments make it vital for achieving agility, reliability, and cost-effectiveness in application deployment. It serves as the backbone of cloud-native ecosystems, enabling organizations to accelerate software delivery, improve resource utilization, and respond effectively to the evolving demands of the digital landscape. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N32 nirf 1750 Rank

--- Page 71 ---
Unit: II Course Code/Title:CS3V15/Devops build.gradle &3 I 1 /* 2 * This file was generated by the Gradle 'init' task. 3 * 4 * This generated file contains a sample Java Library project to get you started. 5 * For more details take a look at the Java Libraries chapter in the Gradle 6 * User Manual available at https://docs.gradle.org/6.0.1/userguide/java_library_plugin.html 7 */ 8 9 plugins { 10 // Apply the java-library plugin to add support for Java Library 11 id 'java-library' 12 } 13 14 repositories { 15 // Use jcenter for resolving dependencies. 16 // You can declare any Maven/Ivy/file repository here. 17 jcenter() 18 } 19 20 dependencies { 21 // This dependency is exported to consumers, that is to say found on their compile classpath. 22 api 'org. apache. commons : commons-math3: 3.6.1' 23 24 // This dependency is used internally, and not exposed to consumers on their own compile classpath. 25 implementation 'com. google. guava: guava: 28.0-jre' 26 27 // Use JUnit test framework 30 28 testImplementation 'junit : junit: 4.12' 29 } 30 The build.gradle file contains three default sections. They are as follows: o plugins: In this section, we can apply the java-library plugin to add support for java library. o Repositories: In this section, we can declare internal and external repository for resolving dependencies. We can declare the different types of repository supported by Gradle like Maven, Ant, and Ivy. · Dependencies: In this section, we can declare dependencies that are necessary for a particular subject. Additionally, we can declare other project-related modules like a task in this file. Display the Information of the Gradle project To understand the structure, dependencies and debugging problems of a build, Gradle provides many built-in features that display information on a project. Following are some basic commands to display the information of the project: Listing projects In Gradle, all the sub-projects of a project in the workspace can be listed in a hierarchy. To do so, run the below command from the root directory of the project. 1. gradle -q projects CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 72 ---
Course Code/Title:CS3V15/Devops Unit: II To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> C:\Users \HiMaNshU\eclipse-workspace >gradle -q projects Root project Root project 'eclipse-workspace' No sub-projects To see a list of the tasks of a project, run gradle <project-path> : tasks For example, try running gradle :tasks C:\Users\HiMaNshU\eclipse-workspace> Listing Tasks Gradle allows us to list all the essential tasks of the project. To list the task, run the below command: 1. gradle -q tasks Output: C:\Users\HiMaNshUleclipse-workspace>gradle -q tasks Tasks runnable from root project Build Setup tasks CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 31 N33 nirf 1750 Rank

--- Page 55 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Environment Variables Now remove active profile from maven settings.xml and update the test profile mentioned in pom.xml. Add activation element to profile element as shown below. The test profile will trigger when the system property "env" is specified with the value "test". Create an environment variable "env" and set its value as "test". <profile> <id>test</id> <activation> <property> <name>env</name> <value>test</value> </property> </activation> </profile> Let's open command console, go to the folder containing pom.xml and execute the following mvn command. C:\MVN\project>mvn test Profile Activation via Operating System Activation element to include os detail as shown below. This test profile will trigger when the system is windows XP. <profile> <id>test</id> <activation> <os> <name>Windows XP</name> <family>Windows</family> <arch>x86</arch> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 14 N32 narf 1750 Rank -

--- Page 56 ---
Course Code/Title:CS3V15/Devops Unit: II <version>5.1.2600</version> </os> </activation> </profile> Now open command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test Profile Activation via Present/Missing File Now activation element to include OS details as shown below. The test profile will triggerwhen target/generated-sources/axistools/wsdl2java/com/companyname/group is missing. <profile> <id>test</id> <activation> <file> <missing>target/generated-sources/axistools/wsdl2java/ com/companyname/group</missing> </file> </activation> </profile> Now open the command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test 8. Maven create and build artifacts: The first step is to open Eclipse, which comes with the integrated Maven environment. The Eclipse window opens on the screen. Complete the following steps: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 15 N32 nirf 175º Rank -

--- Page 87 ---
Course Code/Title:CS3V15/Devops Unit: III 2. Go to Plugin Manager: From the dashboard, click on 'Manage Jenkins' on the left-hand side menu. On the Manage Jenkins page, click on 'Manage Plugins'. This will take you to the Plugin Manager, where you can view, install, and update plugins. 3. Exploring Available Plugins 1. Available Tab: The 'Available' tab lists all plugins that are available for installation. This includes thousands of plugins categorized by functionality (e.g., SCM, build tools, user interface enhancements). 2. Search for Plugins: Use the search box to quickly find the plugin you need. For example, if you're looking to integrate Jenkins with Git, type 'Git' in the search box. 3. Popular Plugins: Jenkins highlights popular plugins at the top of the Available tab. These are commonly used plugins that most users find essential. 4. Installing Plugins 1. Selecting Plugins to Install: Check the box next to each plugin you want to install. You can select multiple plugins at once. 2. Install Without Restart: After selecting your plugins, scroll down and click 'Install without restart'. Jenkins will install the plugins immediately, and you can continue using Jenkins during the installation. 3. Install After Restart: If you prefer, you can choose to 'Download now and install after restart'. This will install the plugins after Jenkins is restarted, ensuring that all changes are applied without interference. 4. Installation Progress: The installation process will show a progress bar for each plugin. Once a plugin is installed, it will move to the 'Installed' tab. 5. Configuring Installed Plugins 1. Plugin Configuration: After installation, some plugins may require additional configuration. Navigate to 'Manage Jenkins' > 'Configure System' or 'Configure Global Security' to adjust settings for your new plugins. 2. Global Tool Configuration: For build tools like Maven or JDKs installed via plugins, you can configure them under 'Global Tool Configuration'. Here, you define the paths to the tools or specify installation options. 6. Updating and Managing Plugins 1. Updating Plugins: Regularly update your plugins to ensure compatibility with the latest version of Jenkins and to benefit from new features or security patches. Go to the 'Updates' tab in the Plugin CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N32 nirf 12 1750 Rank

--- Page 88 ---
Unit: III Course Code/Title:CS3V15/Devops Manager. Jenkins will show you all plugins with available updates. Click 'Update' to install the latest versions. 2. Uninstalling Plugins: If a plugin is no longer needed, you can uninstall it from the 'Installed' tab. Select the plugin and click 'Uninstall'. Uninstallation typically requires a Jenkins restart to complete. 7. Commonly Used Plugins Here are a few essential plugins that you might consider adding to your Jenkins setup: 1. Git Plugin: Integrates Jenkins with Git, allowing you to pull code from GitHub, Bitbucket, or other Git repositories. 2. Pipeline Plugin: Enables the creation and management of Jenkins pipelines, a powerful way to define complex build, test, and deployment processes. 3. Maven Integration Plugin: Adds support for Maven projects, allowing you to build and manage Maven-based projects directly within Jenkins. 4. Email Extension Plugin: Provides advanced email notification options for build statuses, including customizable email templates. 5. Blue Ocean: An alternative user interface for Jenkins that simplifies pipeline creation and offers a modern, user-friendly design. 8. Troubleshooting Plugin Issues 1. Compatibility Issues: Sometimes, plugins might conflict with each other or with the Jenkins version. In such cases, refer to the plugin's documentation or Jenkins logs to diagnose the issue. 2. Rollback: If a plugin update causes problems, you can rollback to a previous version from the 'Installed' tab by selecting the desired version. 3. Restart Jenkins: Some plugin changes require a Jenkins restart. Ensure all jobs are completed before restarting to avoid any interruptions. Commonly Used Jenkins Plugins 1. Git Plugin Overview: The Git Plugin integrates Jenkins with Git repositories, allowing Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nårf 1750 Rank

--- Page 3 ---
Course Code/Title:CS3V15/Devops Unit: I Key Goals and Benefits of DevOps Goals of DevOps The fast-paced growth of the IT industry and continuous advancements in technology make it critical to set DevOps goals that are experimental and challenging for companies to compete and thrive in the market. Here are the key goals and principles that every successful DevOps program has in common. 1. Ensures effective collaboration between teams: Effective collaboration in any process relies on shared ownership. During the development process, all those involved should embrace the fact that everyone is equally responsible for the entire development process. Whether it is development, testing, or deployment, each team member should be involved. They should understand that they have an equal stake in the final outcome. In the DevOps paradigm, passing of work from one team to another is completely defined and broken down. This accelerates the entire process of development since collaboration between all the teams involved is streamlined. 2. Creates scalable infrastructure platforms: The primary focus of DevOps is to create a sustainable infrastructure for applications that make them highly scalable. According to the demands of the modern-day business world, scalable apps have become an absolute necessity. In an ideal situation, the process of scaling should be reliable and fully automated. As a result, the app will have the ability to adapt to any situation when a marketing effort goes viral. With the app being scalable, it can adjust itself to large traffic volumes and provide an immaculate user experience. 3. Builds on-demand release capabilities: Companies must focus on keeping their software in a 'releasable' state. Continuous delivery will allow the software to add new features and go live at any stage. DevOps aims to automate the process of release management because it has a plethora of advantages. Automated release management is predictable, fast, and very consistent. Moreover, through automation, companies can release new versions as per their requirements. Automated release management also has complete and thorough audit trials, as these are essential for compliance purposes. 4. Provides faster feedback: Automating monotonous tasks such as testing and reporting will accelerate the process of rapid feedback. Since the development team will know what has to change, it can roll out the updated version faster. In addition, the team can better understand the impact of the changes that it has done in the software lifecycle. A concrete understanding of changes will assist team members in working efficiently in tandem. With rapid feedback, the operations team and developers can make better decisions collectively and enhance the app's performance. Benefits of DevOps DevOps helps organizations deliver added value to their customers. Here are some compelling benefits of DevOps. 1. Smarter work and faster release: With DevOps, your development team can release the required deliverables quickly. Faster release of deliverables will keep you miles ahead of your competitors, which is very important in today's cut-throat business realm. Businesses should understand that if their review cycle is not automated, it will slow down the release process. Moreover, the inclusion of disparate tools will lead to context switching and higher costs. Thus, DevOps can help rectify this worrisome business situation. 2. Quick resolution of issues: In a business world where speed and accuracy are paramount, a fast feedback loop will help you thrive. With DevOps, the communication process becomes seamless, and, as such, it minimizes the time required to solve issues. Without open communication, key issues can slip out of mind, which will have serious repercussions in the long run. DevOps fosters open communication that helps resolve issues, thus unblocking the release pipeline faster. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 3 N32 nirf 1750 Rank

--- Page 4 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Better collaboration between teams: DevOps paves the way for more dynamic and round-the- clock communication between teams. It renders an environment for mutual collaboration and integration among teams that are distributed globally. Eliminating the traditional departmental barriers between teams forms a new sense of ownership, wherein each team member feels equally responsible for meeting delivery timelines. This collaboration contributes to happier and more engaged employees. 4. Fostering innovative mindsets: With DevOps, deployment phases of the application are more relaxed as compared to traditional methods. This is because it streamlines the entire process, ensures that there are no lapses in quality, and allows on-time and efficient release. Thus, as everything is in order, the development team is more at peace. This allows it to think out of the box and provide additional value to the user. Having a development team with an innovative mindset is a boon for any business organization. An innovative approach, in itself, has immense scope and leads to better quality and resolution of issues at hand. Thus, through DevOps, the process of expanding the horizon of an app becomes much easier. 5. Faster threat detection: Automated and continuous testing of the code will make the process of threat detection faster. As developers can locate problem areas at an early stage, they can then resolve them faster. Thus, DevOps is a vital cog in maintaining and enhancing the quality and performance of an app. As the overall build of the app is in capable hands, teams working together are empowered to share feedback as and when necessary. 6. Increased customer satisfaction: Customer satisfaction is paramount in any day and age, irrespective of the business one is involved in. DevOps is known for enhancing customer experience, which ultimately increases the level of customer satisfaction. Dissatisfied customers are never a good sign for any business. Feedback loops are an important component of DevOps. These loops empower end users to track the progress of app development at various stages. 7. In addition, they can suggest changes (if any) or give their inputs to make the app more customer- centric. Due to their dynamic nature, feedback loops help developers and customers remain on the same page. Moreover, DevOps accelerates the process of app development, which eventually lessens the delivery timer. This has a positive impact on the customer satisfaction ratio. 8. Providing the much-needed edge: Along with staying true to their development process, companies need to ensure that they sustain themselves in the cut-throat competition. Implementing DevOps can be your trump card to provide your organization with that much- needed edge. Competitive advantage is necessary, as it can become the deciding factor in the popularity of an application in many cases. Some factors set expert businesses apart from mediocre ones: · Top-quality features · Quicker and timely software releases · Maximizing return on investments · Listening to constructive feedback Difference between Agile and DevOps Agile: Agile program advancement comprises different approaches to computer program improvement beneath which prerequisites and arrangements advance through the collaborative exertion of self- organizing and cross-functional groups and their customer/end client. DevOps: DevOps could be a set of hones that combines program improvement and information- technology operations which points to abbreviating the framework's advancement life cycle and giving nonstop conveyance with tall program quality. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 4 N32 nirf 1750 Rank

--- Page 17 ---
Course Code/Title:CS3V15/Devops Unit: I that failed in a test suite gets simpler. Also, we can schedule the execution of the test cases at predefined times. After testing, the code is continuously integrated with the existing code. 4) Continuous Monitoring Monitoring is a phase that involves all the operational factors of the entire DevOps process, where important information about the use of the software is recorded and carefully processed to find out trends and identify problem areas. Usually, the monitoring is integrated within the operational capabilities of the software application. 5) Continuous Feedback The application development is consistently improved by analyzing the results from the operations of the software. This is carried out by placing the critical phase of constant feedback between the operations and the development of the next version of the current software application. The continuity is the essential factor in the DevOps as it removes the unnecessary steps which are required to take a software application from development, using it to find out its issues and then producing a better version. It kills the efficiency that may be possible with the app and reduce the number of interested customers. 6) Continuous Deployment In this phase, the code is deployed to the production servers. Also, it is essential to ensure that the code is correctly used on all the servers. The new code is deployed continuously, and configuration management tools play an essential role in executing tasks frequently and quickly. Here are some popular tools which are used in this phase, such as Chef, Puppet, Ansible, and SaltStack. Containerization tools are also playing an essential role in the deployment phase. Vagrant and Docker are popular tools that are used for this purpose. These tools help to produce consistency across development, staging, testing, and production environment. They also help in scaling up and scaling down instances softly. Containerization tools help to maintain consistency across the environments where the application is tested, developed, and deployed. There is no chance of errors or failure in the production environment as they package and replicate the same dependencies and packages used in the testing, development, and staging environment. It makes the application easy to run on different computers. 7) Continuous Operations All DevOps operations are based on the continuity with complete automation of the release process and allow the organization to accelerate the overall time to market continuingly. It is clear from the discussion that continuity is the critical factor in the DevOps in removing steps that often distract the development, take it longer to detect issues and produce a better version of the product after several months. With DevOps, we can make any software product more efficient and increase the overall count of interested customers in your product. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 17 N32 nirf 1750 Rank

--- Page 18 ---
Course Code/Title:CS3V15/Devops Unit: I AWS AWS stands for Amazon Web Services, It is an expanded cloud computing platform provided by Amazon Company. AWS provides a wide range of services with a pay-as-per-use pricing model over the Internet such as Storage, Computing power, Databases, Machine Learning services, and much more. AWS facilitates for both businesses and individual users with effectively hosting the applications, storing the data securely, and making use of a wide variety of tools and services improving management flexibility for IT resources. Advantages & Features of AWS: 1. Cost savings: One of the biggest benefits of AWS is that it can help businesses save money. As mentioned previously, businesses can avoid the high upfront costs of traditional infrastructure with AWS and pay only for the resources they use. Traditionally, businesses had to invest in hardware and software upfront, which often led to overspending. Let's look at this for example - if a business needs to run a website that gets 1000 visitors per day, they would need to purchase and maintain enough servers to support this traffic. With AWS, the business only pays for the compute resources they use when someone visits their website. This can result in significant cost savings. 2. Flexibility: Another key benefit of AWS is its flexibility. Businesses are able to customize their virtual environment - whether the operating system, database, programming language, or something else - to meet their specific needs. Especially in today's climate, the migration process to the cloud should be as frictionless as possible - and AWS makes that possible. Regardless of your use case or industry, AWS can be tailored to fit your needs, whether you're looking for a single cloud- hosted application or an entire suite of integrated solutions. 3. Reliability: AWS is known for being reliable, with an uptime of 99.9%. This makes it a great platform for mission-critical applications that need to be available 24/7. AWS also offers the ability to deploy resources across multiple availability zones for even greater reliability. The cloud platform also has a number of features that make it easier to ensure reliability, such as autoscaling and auto-healing. Autoscaling allows businesses to automatically scale their resources up or down based on demand, while auto-healing enables them to quickly identify and replace any faulty components. 4. Security: Businesses can take advantage of advanced security features, such as identity and access management, to help protect their data. Their tough infrastructure with an end-to-end approach is designed to withstand attacks and AWS provides customers with tools to help them monitor and respond to threats. When it comes to storage, Amazon S3 provides customers with a secure and reliable way to store and access data. The service is designed to be highly scalable and resilient, with built-in redundancy. Fine-grain identity and access controls can be applied to S3 buckets and objects, giving customers control over who has access to their data. Security tasks can be automated with AWS CloudFormation, making it easier for businesses to manage their security policies. And, you can rest easy knowing that AWS takes privacy seriously, with comprehensive customer data protection and compliance measures. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 18 N32 nirf 1750 Rank

--- Page 95 ---
Course Code/Title:CS3V15/Devops Unit: III C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha C: \ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>exit e Finished: SUCCESS Step 6 : Now go to GitHub repository and make some changes to your test.java file and commit these changes. BE AnurdhaP Changes in the test java Code Blame 6 lines (6 loc) · 158 Bytes 89 Code 55% faster with GitHub Copilot > public class test { 2 public static void main(String args[]) { for(int 1=1;i <= 5;1++) 3 4 System.out.println("Good Morning Path !!! "); 5 3 6 } Step 7 : Now, if you check on the Jenkins page, it automatically executes the above updated test.java file and creates a build. It is as follows .- C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test. java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Good Morning Path !!! Good Morning Pathi !! Good Morning Path !!! Good Morning Path !!! Good Morning Pathill 2. Parameter Plugin Overview: The Parameter Plugin allows Jenkins jobs to accept parameters at build time, enabling dynamic and flexible builds. Key Features: · Parameter Types: Supports various parameter types such as string, choice, boolean, and more. · Default Values: Provides default values for parameters. · Prompt for Parameters: Users are prompted to enter values for parameters when starting a build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 20

--- Page 96 ---
Course Code/Title:CS3V15/Devops Unit: III Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Parameterized Builds," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add parameters of different types (e.g., string, choice) and configure their options and default values. Usage: · Useful for creating jobs that require user input or need to be customized for different build scenarios. A Example Demo Step 1 : Click on New item and create a job. I have created a job by the name Welcome (Refer section 3.5) Step 2 : Click on the existing job and configure it with parameters. Step 3 : Check the option "This project is parameterized". Select the String Parameter. Create some string parameter. I have created a parameter named Username and default value to it. This project is parameterized ? =String Parameter ? Name ? Username Default Value ? Parth Description ? Plain text Preview Trim the string ? Step 4 : Under the Build Steps section, using echo command I tried to display the parameter value CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 21

--- Page 23 ---
Course Code/Title:CS3V15/Devops Unit: I application. Amazon EC2 enables you to increase or decrease capacity within minutes. You can use one or hundreds or even thousands of server instances simultaneously. Because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs. Amazon EC2 is integrated with most AWS services, such as Amazon Simple Storage Service (Amazon S3), Amazon Relational Database Service (Amazon RDS), and Amazon Virtual Private Cloud (Amazon VPC) to provide a complete, secure solution for computing applications. Amazon EC2 is an example of Infrastructure as a Service(IaaS). EC2 delivers secure, reliable, cost-effective compute and high-performance compute infrastructure so as to meet the needs of demanding businesses. Amazon EC2 is one of the easiest ways of providing servers on AWS Cloud and also the access to Operating system. · AWS Lambda: AWS Lambda is a serverless, event-driven compute service that allows you to run code without managing servers. You pay only for the compute time you consume and there is no charge when your code is not running. With AWS Lambda, you can run code for any type of application with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services, or you can call it directly from any web or mobile app. But triggering Lambda is possible with over 200 AWS services. You can only pay for what you have used. The compute time that you consume, you are needed to pay for it. You just only need to upload your code and everything required to run will take care of by Lambda and it automatically scales your code with high availability. · AWS Elastic Beanstalk: AWS Elastic Beanstalk is a Platform as a Service that facilitates quick deployment of your applications by providing all the application services that you need for your application. Beanstalk is a plug- and-play platform that allows working with multiple programming languages and environments. Elastic Beanstalk supports a large range of platforms like Node js, Java, PHP, Python, and Ruby. So, you can develop your application to meet your requirements and simply deploy it on Elastic Beanstalk. The main aim to use AWS Elastic Beanstalk is to allow you to focus on the deployment and management of your applications. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. 2. Networking · Amazon VPC: Amazon VPC is your network environment in the cloud. It allows you to create a private network within the AWS cloud that uses many of the same concepts and constructs as an on-premises network. Amazon VPC also gives you complete control of the network configuration. Customers can define normal networking configuration items such as IP address ranges, subnet creation, route table creation, network gateways, and security settings. Amazon VPC is an AWS foundational service and integrates with numerous AWS services. For instance, Amazon EC2 instances are deployed into your CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 24 ---
Course Code/Title:CS3V15/Devops Unit: I Amazon VPC. Similarly, Amazon Relational Database Service (Amazon RDS) database instances deploy into your Amazon VPC, where the database is protected by the structure of the network just like your on-premises network. You can easily launch AWS resources into a virtual network by Amazon Virtual Private Cloud. An isolated virtual network environment in the AWS cloud is created by Amazon VPC. . Amazon Route 53: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating human-readable names, such as www.geeksforgeeks.com, into the numeric IP addresses that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. 2. Storage · Amazon S3 (Simple Storage Service): Amazon Simple Storage Service (Amazon S3) is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the web. It is designed to provide an infinite amount of storage and it is delivered with 99.999999999% durability. You can use Amazon S3 as primary storage for cloud-native applications as a target for backup and recovery and disaster recovery. It offers industry-leading scalability, data availability, security, and performance. It's simple to move large volumes of data into or out of Amazon S3 with Amazon's cloud data migration options. Once data is stored in Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like Amazon S3 Standard - Infrequent Access and Amazon Glacier for archiving. · Amazon Glacier: Amazon Glacier is a secure, durable, and extremely low- cost storage service for data archiving and long-term backup. Data stored in Amazon Glacier takes several hours to retrieve, which is why it's ideal for archiving. The fastest access to your archive data is via Amazon Glacier. 3. Databases · Amazon RDS (Relational Database Service): Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost- efficient and resizable capacity while managing time-consuming database administration tasks, freeing you up to focus on your applications and business. You can find Amazon RDS is also available on several database instance types - optimized for memory, performance, or I/O. Amazon RDS provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. · Amazon DynamoDB (Non-Relational Database): Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. When you create a database table that can store and retrieve any amount of data you can simply use Amazon DynamoDB that will serve any level of requested traffic. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, Internet of Things (IoT), and many other applications. DynamoDB provides many features like · built-in security CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 24 N32 nirf 1750 Rank

--- Page 103 ---
Course Code/Title:CS3V15/Devops Unit: III =Publish HTML reports ? Reports HTML directory to archive ? E:\Eclipse_workplace\SeleniumDemo\test-output Index page[s] ? index.html Index page title[s] (Optional) Report title ? HTML Report Publishing options Step 11 : Click on Apply and then Save. Now build this project. Step 12 : On successful build, it creates the HTML Report as follows - x + € > C @ localhost:0000/juby/HTML/\portDemo/HTML_20Report/ Back to HTMLBepsatDemo Test results All suites Default suite 11 D myPackage.myTestPage Info . Repetir output Results . Thus using HTML Publish Report Plugin we can successfully generate HTML report of some testing framework application. · Troubleshooting : Sometimes, the raw file is displayed as HTML report. To get the HTML formatted report we must click on Manage Jenkins section, scroll down and locate Script Console section. Click on it. Tools and Actions C Broad Configuration Sium Disk Discard all the loaded alla lo memory and reloud Jerking CLI Script Console Ewecubes arbitrary suyt lor and run below given command : System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", ") . Then restart the Jenkins Page and Build the project once again. The HTML report can then be displayed as given in above step. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 28 N32 nirf 1750 Rank

--- Page 104 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Copy Artifact Overview: The Copy Artifact Plugin allows Jenkins jobs to copy build artifacts from other jobs, facilitating artifact reuse across different jobs. Key Features: · Artifact Copying: Copies files from one build to another, which can be from a specific build or the latest successful build. · Triggering Builds: Can be used in downstream jobs to fetch artifacts from upstream jobs. Configuration: 1. Install the Plugin: Go to Manage Jenkins > Manage Plugins > Available tab, search for "Copy Artifact Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Build section. ○ Add a Copy artifacts from another project build step. o Specify the project name, build number or criteria, and the target directory for copied artifacts. Usage: · Commonly used in multi-job pipelines where artifacts need to be shared between different jobs or stages. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 29 N3 nirf 1750 Rank

--- Page 35 ---
Unit: I Course Code/Title:CS3V15/Devops Push: Push is the operation that sends your committed changes to a remote repository, making them accessible to others. Git workflow v0.1 v0.2 v1.0 Master Hotfix Release Develop Feature Feature Git Flow is a structured branching model designed for projects with well- defined release cycles and a need for strict quality control. Branches: The branching model described is commonly known as the Gitflow Workflow. It's a branching strategy that defines a strict branching model designed to facilitate collaboration and streamline the release process. Let's go into detail about each branch: clone operation Working Copy edit, add, move files update operation Modify working copy status and diff operation Review changes commit and push operation push operation Commit changes push operation amend and push operation Review changes 1. Master Branch: The `master'branch represents the main codebase and contains production- ready code. This branch is typically stable and should only include thoroughly tested and approved changes. Each commit on the 'master' branch represents a new version or release of the software. 2. Develop Branch: The 'develop' branch is an integration branch where various feature branches are merged. It serves as a staging area for testing new features and ensuring they work well together before merging into the 'master' branch. This branch may have ongoing development work and is not necessarily always in a production- ready state. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 35 nirf 1750 Rank N33

--- Page 36 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Feature Branches: Feature branches are created for developing new features or implementing changes. These branches are typically based on the 'develop'branch. Once a feature is complete, the branch is merged back into the 'develop' branch. Feature branches allow developers to work on specific tasks without affecting the main codebase. 4. Release Branch: The 'release' branch is created when the 'develop' branch reaches a point where it is ready for a production release. This branch is used for final testing, bug fixes, and preparing the code for deployment. No new features should be added to the release branch. Once the release is deemed stable, it is merged into both the 'master' branch and the 'develop' branch. 5. Hotfix Branch: The 'hotfix' branch is used to quickly address critical issues or bugs in the production code. It is created directly from the 'master' branch. Hotfixes are intended to be small and focused on resolving the specific issue at hand. Once the hotfix is complete, it is merged into both the 'master'branch and the `develop'branch to ensure that the fix is applied to future releases. Here is the typical flow: · Developers work on feature branches based on the 'develop' branch. · Completed features are merged into the 'develop' branch. · When ready for a release, a 'release' branch is created from 'develop". · The release branch undergoes testing and bug fixes. · The release branch is merged into both 'master' and 'develop' once it's stable. · If a critical issue arises in production, a 'hotfix' branch is created from 'master'. · The hotfix is merged into both 'master' and 'develop to keep both branches in sync. · This Gitflow Workflow helps maintain a structured development process, ensuring that features are developed, tested, and released in a controlled manner. Example Scenario: · Imagine you're working on a large software project with a team of 10 developers. You have a major release planned for every six months. · You create a "feature/Ticket-Id" branch to develop a new login system. · Once the feature is complete, it's merged into the "develop" branch for integration and testing. · As the release date approaches, you create a "release/v1.0" branch to freeze code for the upcoming release. · Any critical issues discovered in the production environment are fixed in "hotfix" branch and merged into "master" and "develop." CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 36 N33 nirf 1750 Rank

--- Page 107 ---
Course Code/Title:CS3V15/Devops Unit: III Step 8 : Then configure the Build Steps by selecting the option Copy artifacts from another project as follows - . . First of all give the name of the project from which you want to copy the files. In my case it is App1. . Then select the option "copy from WORKSPACE of latest completed build" · Give the regular expression ** / *.*. Here's a breakdown of what ** / *.* means : o ** : The double asterisk ( ** ) is a wildcard that matches zero or more directories and subdirectories. It's often used to recursively search through all directories and subdirectories. o / : The forward slash (/) is used as a directory separator, typically on Unix-like systems. o *.* : This part of the regular expression is used to match files with an extension. In regular expressions, the * symbol represents "zero or more" of the preceding character or pattern, and the . character is used to match any character. So, *.* essentially matches any file with any extension. So, when you use the regular expression ** / *.* in a Jenkins build configuration, you are instructing Jenkins to include all files with any extension ( *.* ) located in any directory and subdirectory ( ** ) within the workspace or specified location. Build Steps E Copy artifacts from another project Project name ? App: Which build ? Copy from WORKSPACE of latest completed buld Limitation Note ? Artifacts to copy ? Artifacts not to copy ? : Toget directory CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 32

--- Page 108 ---
Course Code/Title:CS3V15/Devops Unit: III Add one more Build Step and select Execute Windows batch Command. Give the command dir so that the name of files and directories that are copied from App1 to App2 will be displayed on the Console. = Execute Windows batch command ? Command See the list of available environment variables ---- dir ----- Advanced v Add build step * Click Apply and Save. Step 8 : Now Build this project (App2). The output will be as follows - Started by user Anuradha P Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\.jenkins\workspace\App2 Copied 16 artifacts from "App1" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621109248227895.bat C:\ProgramData\Jenkins\,jenkins\workspace\App2>dir Volume in drive C Is OS Volume Serial Number is CGC2-BA9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023* 15:07 <DIR> 01-09-2023 15:07 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 474 test.class 144 test. java 2 File(s) 616 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 33 N32 narf 1750 Rank

--- Page 21 ---
Unit: I Course Code/Title:CS3V15/Devops · Gaming: AWS has been serving many gaming studios. Combining Amazon EC2 and S3 services with CloudFront enables gaming websites to deliver high-quality gaming experiences to their customers regardless of location. Use Cases of AWS · Netflix Netflix is an entertainment platform that started in the United States, but eventually, it expanded to many countries and soon became popular. However, once Netflix confronted the scalability problem because of the sudden increase in viewers. That made Netflix choose AWS services. Netflix reports that when it started using AWS services like DynamoDB and Cassandra for its distributed databases, it could handle the data easily. So, scalability is a great advantage of AWS. Netflix has adapted around 100,000 server instances from AWS for computing and storage databases, analytics, recommendation engines, and video transcoding as well. . McDonald's McDonald's is the world's largest fast-food company that serves around 64 million people per day. The growth of this company has gone to another level when it started home deliveries. By utilizing AWS services, McDonald's created a platform that integrates local restaurants with delivery partners such as Uber Eats. Scalability is also a reason for the company to choose AWS services. Moreover, with AWS Microservices Architecture, McDonald's platform can scale 20,000 orders per second and integrate with the global partners easily. · Airbnb Airbnb is an international online marketplace for rental homes. This platform connects people who are looking for rental accommodation with those who want to rent out their houses. Quite soon, Airbnb became unable to handle the constant streaming of data on the website from its customers. That is when it started using Amazon EC2 service and Elastic Load Balancing, which distributes incoming traffic to multiple Amazon EC2 instances. In this way, Airbnb could avoid traffic, and customers could use the online platform without any disruption. • Novartis Novartis is the best example for AWS use cases in healthcare. Novartis is one of the world's largest healthcare companies that provides solutions for patients' well-being. It adapted Amazon EC2 services and built a platform using other services such as Amazon Simple Storage Service, Amazon Elastic Block Store, and four availability zones. Data Analysts of Novartis are taking advantage of the AWS services and still implementing new solutions for the patients. · Expedia Expedia is a worldwide online travel agency that has always focused on the constant development and innovation of its platform to offer an extraordinary user experience for its clients. Since 2010, Expedia has been using AWS services to build a standard deployment model for better infrastructure as AWS offers the best data security through different availability zones. • Samsung If you are using Samsung mobile phones, then you may know about the Samsung app store. For setting up the apps stacked in its store, the company started using AWS services. Using AWS app development services, Samsung wanted to provide its customers with the facility to download the apps anywhere without any network traffic. • NASA NASA (National Aeronautics and Space Administration) has always wondered about creating a library to present people with all its achievements through pictures and videos of space. Later on, it created such platforms, but because it had 10 different NASA centers, it couldn't provide the best experience for viewers. So, all it wanted was to create an easy-access platform for TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 21 N32 nirf 1750 Rank

--- Page 22 ---
Unit: I Course Code/Title:CS3V15/Devops people to search for and view images and videos. Then, NASA started adopting many services from AWS to solve this problem, which included Amazon Elastic Compute Cloud, Elastic Load Balancing, Amazon Simple Storage Service, Amazon Simple Queue Service, etc. Among these, Amazon S3 helped the company store all the incoming data such as photos, videos, and audio files without any hassle. • Facebook Facebook, without a doubt, is a widespread social media platform. To build a scalable application, Facebook used services such as Amazon Elastic Compute Cloud, Amazon Simple Storage Service, Amazon Relational Database Service, Amazon SimpleDB, Amazon CloudFront, Amazon Simple Queue Service, etc. Amazon RDS helps the platform to make it easy to set up, operate, and scale the database in the cloud. Various Services offered by AWS · Amazon EC2 (Elastic Cloud computing) · Amazon RDS (Relational Database Services) · Bonus Service: Amazon Connect · Amazon S3 (Simple Storage Service) · Amazon Lambda · Amazon Cognito · Amazon Glacier · Amazon SNS (Simple Notification Service) · Bonus Service: Amazon Lex · Amazon Lightsail · Amazon VPC (Virtual Private Cloud) · Amazon Kinesis · Amazon Inspector · Amazon Auto-scaling · Amazon IAM (Identity and Access Management) · Dynamo DB · Amazon SQS (Simple Queue Service) · Amazon ElastiCache · Amazon Chime · AWS Athena · Code Catalyst · Web Application Firewall · AWS Amplify · AWS Rekognition · AWS QuickSight · AWS CloudFormation · AWS Management Console The Important Cloud Services according to various categories that are provided by AWS are given below : 1. Compute • Amazon EC2: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It allows organizations to obtain and configure virtual compute capacity in the cloud. You can select from a variety of operating systems and resource configurations like memory, CPU, and storage that are required for your TECHNOLOGY (Autonomous) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY 22 N33 narf 1750 Rank

--- Page 33 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between AWS (Amazon Web Services), Google Cloud, and Azure Technology EC2 (Elastic Compute Cloud) AWS Databases Supported Pricing Models Difficulties Many enterprises find it difficult to understand the company cost AWS fully relational and supports NoSQL databases and Big Data. Per hour - rounded up. On demand, reserved spot. structure. Google Cloud Google Engine(GCE) Compute Technologies pioneered by Google, like Big Query, Big Table, and Hadoop, are databases, and Big Data,naturally fully supported. Per minute - rounded up On demand sustained use. — Fewer services. features Azure VHD Disk) (Virtual Hard Azure supports relational and both NoSQL Windows and through AzureTable HDInsight. Per minute - rounded up. Per minute- rounded up commitments(Pre-paid or monthly) and Less "Enterprise-ready. Storage Services Simple Storage Service(S3) Elastic Block Storage. Elastic File storage. Machine Learing Sage maker. Lex. polly.And many more Blob Storage Queue Storage. File Storage Disk Storage. Data Lake Store Machine learning Azure Bot service Cognitive service GIT Cloud storage. Persistent Disk Transfer appliance. Cloud speech AI Cloud Video Intelligence. Cloud Machine learning engine Git is a distributed version control system (DVCS) that helps manage and track changes in source code during software development. It was created by Linus Torvalds in 2005 and has become one of the most widely used version control systems in the software development industry. Some key concepts and features of Git: · Version Control: Git allows developers to keep track of changes made to their code over time. This includes modifications, additions, and deletions of files. · Distributed System: Git is a distributed version control system, meaning that each developer has a complete copy of the entire repository, including its full history. This allows developers to work independently and merge their changes when necessary. · Branching: Git enables developers to create branches, which are essentially separate lines of development. This allows for the parallel development of features or bug fixes without affecting the main codebase. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) LEN 33 nirf 1750 Rank

--- Page 34 ---
Course Code/Title:CS3V15/Devops Unit: I · Merging: Git provides tools for merging changes from one branch into another. This is essential when multiple developers are working on different branches and need to bring their changes together. · History Tracking: Git maintains a complete history of changes made to the codebase. Developers can view, revert, or analyze changes made over time. · Remote Repositories: Git supports remote repositories, allowing developers to collaborate with others by pushing and pulling changes to and from a shared repository. Platforms like GitHub, GitLab, and Bitbucket provide hosting services for Git repositories. · Staging Area: Git uses a staging area (also known as the index) to prepare and review changes before committing them to the repository. This allows developers to selectively include or exclude specific changes. · Open Source: Git is an open-source tool, and its source code is freely available for modification and distribution. Various GIT Components: Git is composed of several key components that work together to enable version control and collaborative development. Here are the main components of Git: · Repository (Repo): A repository is a directory or storage space where your project and its version history are stored. It contains all the files and directories associated with your project, along with the metadata and configuration information. · Working Directory: The working directory is the directory on your local machine where you manipulate files and make changes to your project. It is essentially your local copy of the repository. · Index (Staging Area): The index, also known as the staging area, is a middle ground where changes are prepared before being committed to the repository. It allows you to selectively stage changes, which means you can choose which modifications to include in the next commit. · Commit: A commit is a snapshot of the changes made to the files in the repository. It represents a specific point in the project's history and is accompanied by a commit message that describes the changes. · Branch: A branch is a parallel line of development within a repository. It allows developers to work on different features or bug fixes simultaneously without affecting the main codebase. Branches can be merged to incorporate changes into other branches. . Head: HEAD is a reference to the latest commit in the currently checked-out branch. It essentially points to the tip of the branch you are currently on. · Remote: A remote is a version of the repository stored on a different server. Git supports collaboration by allowing developers to push and pull changes between their local repository and remote repositories. Platforms like GitHub, GitLab, and Bitbucket are examples of remote repositories. · Clone: Cloning is the process of creating a copy of a remote repository on your local machine. This allows you to start working on your own copy of a project. · Fetch: The fetch operation retrieves changes from a remote repository but does not automatically merge them into your working directory. It is useful for reviewing changes before deciding to merge. · Pull: Pull is a combination of fetch and merge. It retrieves changes from a remote repository and automatically merges them into your working directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 43 ---
Course Code/Title:CS3V15/Devops Unit: II o Generates source code (if auto-generated code is used) o Generates documentation from source code o Compiles source code o Packages compiled code into JAR of ZIP file o Installs the packaged code in local repository, server repository, or central repository 2. Installation of Maven: To install maven on windows, you need to perform following steps: 1. Download maven and extract it 2. Add JAVA_HOME and MAVEN_HOME in environment variable 3. Add maven path in environment variable 4. Verify Maven 1) Download Maven To install maven on windows, you need to download apache maven first. Download Maven latest Maven software from Download latest version of Maven For example: apache-maven-3.1.1-bin.zip 2) Add MAVEN_HOME in environment variable Right click on MyComputer -> properties -> Advanced > Environment variables -> click new button System Settings - Now add MAVEN_HOME in variable name and path of maven in variable value. It must be the home directory of maven i.e. outer directory of bin. For example: E:\apache-maven-3.1.1 3) Add Maven Path in environment variable Click on new tab if path is not set, then set the path of maven. If it is set, edit the path and append the path of maven.Here, we have installed JDK and its path is set by default, so we are going to append the path of maven. The path of maven should be %maven home%/bin. For example, E:\apache-maven- 3.1.1\bin . 4) Verify maven To verify whether maven is installed or not, open the command prompt and write: mvn -version Now it will display the version of maven and jdk including the maven home and java home. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 2 N32 nirf 175º Rank

--- Page 44 ---
Unit: II Course Code/Title:CS3V15/Devops 3. POM FILES: POM is an acronym for Project Object Model. The pom.xml file contains information of project and configuration information for the maven to build the project such as dependencies, build directory, source directory, test source directory, plugin, goals etc. Maven reads the pom.xml file, then executes the goal. Before maven 2, it was named as project.xml file. But, since maven 2 (also in maven 3), it is renamed as pom.xml. Elements of maven pom.xml file For creating the simple pom.xml file, you need to have following elements: Element Description project It is the root element of pom.xml file. modelVersion It is the sub element of project. It specifies the modelVersion. It should be set to 4.0.0. groupId It is the sub element of project. It specifies the id for the project group. artifactId It is the sub element of project. It specifies the id for the artifact (project). An artifact is something that is either produced or used by a project. Examples of artifacts produced by Maven for a project include: JARs, source and binary distributions, and WARs. version It is the sub element of project. It specifies the version of the artifact under given group. Maven pom.xml file with additional elements Here, we are going to add other elements in pom.xml file such as: Element Description packaging defines packaging type such as jar, war etc. name defines name of the maven project. url defines url of the project. dependencies defines dependencies for this project. dependency defines a dependency. It is used inside dependencies. scope defines scope for this maven project. It can be compile, provided, runtime, test and system. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 3 N32 narf 1750 Rank -

--- Page 9 ---
Course Code/Title:CS3V15/Devops Unit: I 9. Slack Slack is a crucial tool for businesses and organizations worldwide. Its significance lies in facilitating seamless communication and collaboration among teams, whether working in the same office or remotely. Slack's real-time messaging, file sharing, and integration capabilities streamline workflow, enhance productivity and keep teams connected across different time zones and locations. As the work landscape evolves, with more companies embracing hybrid and remote work models, Slack is a vital hub for quick decision-making, project coordination, and knowledge sharing. With an ever- expanding ecosystem of integrations and features, Slack remains at the forefront of modern workplace communication, making it essential for businesses to stay agile, efficient, and competitive. 10. AWS Cloud Computing and Storage in DevOps AWS (Amazon Web Services) Cloud Computing and Storage are crucial in DevOps because they provide scalable, flexible, and cost-effective infrastructure for DevOps practices. AWS offers many services, including compute resources, databases, container orchestration, and serverless computing, which align perfectly with modern software development and deployment demands. Organizations adopt DevOps to accelerate software delivery. AWS provides the foundation for rapidly deploying and scaling applications, supporting continuous integration and continuous delivery (CI/CD) pipelines, and automating infrastructure provisioning through tools like AWS CloudFormation. Furthermore, AWS's storage solutions enable efficient data management, backup, and recovery, ensuring the resilience and reliability required for DevOps operations. As cloud technology evolves, AWS remains at the forefront, enabling DevOps teams to focus on innovation and efficiency. 11. Azure Cloud Computing and Storage in DevOps Azure Cloud Computing and Storage will be pivotal in DevOps practices in 2024 and beyond. Azure offers a comprehensive cloud ecosystem that enables organizations to scale their infrastructure, deploy applications, and store data efficiently. Azure provides essential services for continuous integration and continuous deployment (CI/CD), automation, monitoring, and security. Its cloud computing capabilities facilitate the provisioning of resources on demand, ensuring that development and testing environments are readily available. Azure's storage solutions, including Azure Blob Storage, Azure Files, and Azure SQL Database, enable secure data storage and retrieval, supporting the data-driven aspects of DevOps. Besides, Azure's integration with DevOps tools like Azure DevOps Services streamlines the software development lifecycle, enhancing collaboration and automation. 12. GCP Cloud Computing and Storage in DevOps Google Cloud Platform (GCP) offers robust cloud computing and storage solutions. GCP provides a scalable, reliable, and highly available infrastructure essential for modern DevOps practices. With its comprehensive set of services, including Google Compute Engine, Google Kubernetes Engine, Cloud Storage, and BigQuery, GCP empowers DevOps teams to build, deploy, and manage applications easily. Its emphasis on automation, infrastructure as code, and container orchestration aligns seamlessly with DevOps principles. Moreover, GCP's cutting-edge technologies, such as AI and machine learning capabilities, provide DevOps practitioners with advanced tools for monitoring, analytics, and automation, making it a powerful choice for organizations seeking to optimize their software development and delivery processes. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 10 ---
Course Code/Title:CS3V15/Devops Unit: I 13. Monitoring, Alerting, and Incident Response Tools: SignalFx Monitoring, alerting, and incident response tools like SignalFx are pivotal in DevOps and software development. As software systems become complex and distributed, the need for real-time visibility into performance and the ability to respond swiftly to incidents is significant. SignalFx excels in this regard by providing advanced monitoring and observability solutions that enable organizations to detect anomalies, trace issues across microservices proactively, and set up intelligent alerts. As applications scale, cloud-native architectures become the norm, and user expectations for reliability grow, SignalFx's capabilities are crucial. It empowers DevOps teams to ensure high availability, optimize resource utilization, and maintain a seamless user experience by identifying and addressing performance issues before they impact end-users. It is one of the most essential tools for modern software operations. 14. Appdynamics AppDynamics, a leading application performance management and monitoring platform, remains critically important as it ensures the optimal performance of modern digital businesses. As organizations rely on complex and distributed software systems, proactively monitoring, troubleshooting, and optimizing these applications becomes essential. AppDynamics provides real-time visibility into application performance, allowing businesses to swiftly identify bottlenecks, latency issues, and errors. With the ever-growing complexity of applications, the importance of AppDynamics lies in its ability to empower organizations to deliver exceptional user experiences, maintain application reliability, and swiftly respond to performance issues, thereby ensuring the continued success and competitiveness of digital businesses. 15. Raygun It is a crucial tool in software development and DevOps because it ensures application reliability and performance. Raygun is an application monitoring and error-tracking platform that empowers development teams to identify, diagnose, and resolve real-time issues. With software systems growing in complexity and the increased demand for seamless user experiences, Raygun's importance lies in providing actionable insights into application errors and performance bottlenecks. It enables organizations to proactively address issues, reduce downtime, and enhance user satisfaction, leading to higher software quality and improved customer experiences. Software is central to businesses across industries. Raygun's role in maintaining application health and facilitating rapid issue resolution makes it a fundamental tool for DevOps professionals and software developers. 16. Splunk Cloud Splunk Cloud helps organizations gain critical insights from the ever-expanding volume of data generated in today's digital landscape. As businesses increasingly rely on data- driven decision-making, Splunk Cloud stands out as a robust and scalable platform for monitoring, searching, analyzing, and visualizing machine-generated data. Its importance lies in providing real-time visibility into the health and performance of complex systems, applications, and infrastructures, enabling rapid incident detection and response. As cybersecurity threats evolve, Splunk Cloud's advanced security analytics and threat detection capabilities remain indispensable for safeguarding against cyberattacks and ensuring data integrity. In a world where data is a strategic asset, Splunk Cloud's role in harnessing the power of data for operational excellence and security cannot be overstated. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 10 N32 nirf 1750 Rank

--- Page 79 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Plugins: ○ Role: Extend Jenkins functionality without modifying the core software. ○ Types: I Source Control Plugins: Git, SVN, Mercurial. Build Tools Plugins: Maven, Gradle, Ant. Notification Plugins: Email, Slack, HipChat. · Reporting Plugins: JUnit, Cobertura, Checkstyle. ○ Management: Plugins can be installed, updated, and configured via the Jenkins web interface. 5. Master-Slave Architecture: ○ Concept: The Jenkins master distributes build tasks to multiple agents, enabling distributed builds. ○ Benefits: Scalability: Handle more builds simultaneously. I Resource Management: Assign builds to agents with the necessary resources. Isolation: Run builds in isolated environments to avoid conflicts. Workflow and Data Flow 1. Job Trigger: ○ Jobs can be triggered by various events: Manual Trigger: Initiated by a user through the Jenkins interface. Scheduled Trigger: Using CRON-like syntax to schedule jobs. · SCM Trigger: Automatically triggered by changes in the source code repository. Upstream/Downstream Trigger: Triggered by the completion of other jobs. 2. Build Execution: ○ Job Assignment: The master assigns the job to an available agent based on labels, availability, and resource requirements. Build Environment Setup: The agent sets up the environment, including checking out the code, installing dependencies, and configuring the workspace. ○ Execution: The agent runs the build steps as defined in the job configuration. 3. Build Results: ○ Logs: Captures console output and logs from the build process. ○ Artifacts: Stores build artifacts like binaries, packages, and reports. ○ Test Results: Collects and displays test results and code coverage reports. ○ Build History: Maintains a history of all builds, including status, duration, and changes. 4. Notification and Reporting: ○ Notifications: Jenkins can send notifications through various channels (email, chat, etc.) upon build completion or failure. ○ Dashboards: Provides dashboards for visualizing build status, trends, and metrics. ○ Reports: Generates and displays reports on test results, code quality, and other metrics. INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 4 1750 Rank

--- Page 80 ---
Unit: III Course Code/Title:CS3V15/Devops Summary Jenkins' architecture is designed to be flexible and scalable. The master-agent model allows for efficient distribution of build tasks, while plugins provide extensive customization options. By leveraging Jenkins' comprehensive job configuration and robust notification and reporting capabilities, teams can streamline their CI/CD pipelines and improve software quality and delivery speed. Creating a Jenkins Job Creating a Jenkins job involves setting up a new project within Jenkins to automate various tasks such as building code, running tests, and deploying applications. Here is a step-by-step guide to creating a Jenkins job. Prerequisites · Jenkins installed and running. · Basic understanding of the Jenkins interface. . Access to the Jenkins dashboard. Step-by-Step Guide 1. Access Jenkins Dashboard: . Open your web browser and navigate to your Jenkins instance URL (e.g., http://localhost:8080). 2. Create a New Job: ○ On the Jenkins dashboard, click on the "New Item" link on the left-hand side menu. 3. Enter Job Name: ○ Enter a name for your new job in the "Enter an item name" field. o Choose the type of job you want to create. For most uses, "Freestyle project" is a good starting point. ○ Click "OK". 4. Configure the Job: ○ After clicking "OK", you will be directed to the job configuration page. 5. General Configuration: ○ Description: Provide a brief description of the job. ○ Discard Old Builds: Optionally set this to limit the number of old builds Jenkins keeps. ○ Select Version Control System: Choose the version control system (e.g., Git, Subversion). 6. Source Code Management: ○ Repository URL: Enter the repository URL. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 5 nirf 175 Rank N33

--- Page 13 ---
Course Code/Title:CS3V15/Devops Unit: I 2024. Terraform provides the agility and scalability required to keep up with the dynamic demands of modern applications. Terraform's importance lies in its ability to bring consistency, version control, and automation to infrastructure operations, thereby reducing manual errors, streamlining DevOps workflows, and facilitating applications' rapid and reliable deployment in an increasingly complex and cloud-centric environment. As organizations adopt cloud-native technologies, Terraform remains essential to ensure efficient and consistent infrastructure management. 25. Phantom Phantom enhances security automation and incident response capabilities. In today's rapidly evolving threat landscape, organizations face a constant barrage of cybersecurity incidents, and the ability to respond swiftly and effectively is necessary. It provides a platform for automating security workflows, from detecting and investigating potential threats to orchestrating responses and mitigating risks. Phantom's importance lies in its capacity to reduce response times, increase consistency in incident handling, and free up manual resources from repetitive tasks. With the growing complexity of cyber threats, Phantom empowers security teams to defend against attacks and safeguard critical assets proactively. 26. Nagios Nagios, an open-source monitoring and alerting system, remains vital due to its enduring significance in maintaining the reliability and performance of IT infrastructure and applications. Organizations increasingly rely on complex systems and services. Nagios plays a crucial role by providing real-time monitoring and alerting capabilities, allowing IT teams to detect and address issues before they impact users or cause system outages. Its versatility, extensibility, and support for both on-premises and cloud environments make Nagios a valuable tool for ensuring critical systems' availability, stability, and security, aligning perfectly with the demands of modern IT operations and DevOps practices. 27. Vagrant Vagrant continues to play a crucial role in software development and DevOps. It is a tool that simplifies creating and managing reproducible development environments. Its importance lies in its ability to provide developers and DevOps teams with a consistent and isolated environment for software development, testing, and deployment. With the ever-evolving complexity of software stacks, dependencies, and infrastructure configurations, Vagrant remains essential in ensuring these environments are easily shareable, scalable, and maintainable. It allows developers to work seamlessly across various operating systems and provides a standardized setup that minimizes compatibility issues. 28. Sentry Sentry plays a critical role in modern software development and DevOps practices. With software applications' increasing complexity and scale, identifying and addressing errors and issues has become crucial. Sentry is vital because it provides real-time error tracking and monitoring, allowing development teams to proactively detect and diagnose issues, whether they occur in production or during development. Its importance is minimizing downtime, improving user experience, and maintaining software systems' overall health and reliability. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nirf 1750 Rank

--- Page 14 ---
Course Code/Title:CS3V15/Devops Unit: I 29. Gradle Gradle continues to be a vital tool in software development and DevOps. Gradle is an advanced build automation system that plays a crucial role in managing dependencies, building projects, and orchestrating complex workflows efficiently. Its importance lies in its versatility and scalability, as it caters to various project sizes and types. Gradle's ability to easily handle multi-language, multi-project builds and its support for plugin-based customization make it indispensable in modern software development. As organizations increasingly adopt microservices architectures and cloud-native technologies, Gradle's capabilities are instrumental in managing the complexity of building, testing, and deploying applications across diverse environments. DevOps Architecture DevOps Architecture Plan Release Code Dep Deploy Development and operations both play essential roles in order to deliver applications. The deployment comprises analyzing the requirements, designing, developing, and testing of the software components or frameworks. The operation consists of the administrative processes, services, and support for the software. When both the development and operations cture is the solution to fix the gap between Test Monitor Build Operate ined with collaborating, then the DevOps al www deployment and operation terms; therefore, delivery can be faster. DevOps architecture is used for the applications hosted on the cloud platform and large distributed applications. Agile Development is used in the DevOps architecture so that integration and delivery can be contiguous. When the development and operations team works separately from each other, then it is time- consuming to design, test, and deploy. And if the terms are not in sync with each other, then it may cause a delay in the delivery. So DevOps enables the teams to change their shortcomings and increases productivity. Below are the various components that are used in the DevOps architecture: 1) Build: Without DevOps, the cost of the consumption of the resources was evaluated based on the pre-defined individual usage with fixed hardware allocation. And with DevOps, the usage of cloud, sharing of resources comes into the picture, and the build is dependent upon the user's need, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Build Code Test Plan DevOps Components Moniter Dev Ops Deploy Operate Release 14 N33 nirf 1750 Rank

--- Page 65 ---
Course Code/Title:CS3V15/Devops Unit: II Output The output is as follows - java version "1.8.0 66" Java(TM) SE Runtime Environment (build 1.8.0_66-b18) Java HotSpot(TM) 64-Bit Server VM (build 25.66-b18, mixed mode) Step 2 - Download Gradle Build File Download the latest https://gradle.org/install/. version of Gradle from the link available at Step 3 - Set Up Environment for Gradle Setting up environment means, we have to extract the distribution file and copy the library files into proper location. Set up GRADLE_HOME and PATH environmental variables. This step is platform dependent. In Windows Extract the downloaded zip file named gradle-2.11-all.zip and copy the distribution files from Downloads\gradle-2.11\ to C:\gradlel location. After that, add the C: \gradle and C: \gradlelbin directories to the GRADLE_HOME and PATH system variables. Follow the given instructions - Right Click On My Computers -> Click On Properties -> Advanced System Settings -> Click On Environmental Variables. There you will find a dialog box for creating and editing system variables. Click on new button for creating GRADLE_HOME variable (follow the left side screenshot). Click on Edit for editing the existing Path system variable (follow the right side screenshot). Follow the below given screenshots. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 24 N33 nirf 1750 Rank

--- Page 66 ---
Course Code/Title:CS3V15/Devops System Properties X Computer Name Hardware Advanced System Protection Remote Environment Variables x Unit: II System Properties × Computer Name |Hardware Advanced System Protection Remote Environment Variables × New System Variable × Variable name: GRADLE_HOME| Variable value: C:\grade Cancel System variables Variable Value ComSpec C:\Windows\system32\and.exe FP_NO_HOST_C ... NO GRADLE_HOME E:\work\22- Orientdb \gradle-2.11 Variable name: Path Variable value: #C:\gradle bin - System variables Variable Value Edit System Variable × OK Cancel < ORIENTDB_HOME E:\work\22- Orientdblorientdb-communi ... " JAVA_HOME C:\Program Files\Java\jdk1.7.0_60 New ... Edt ... Delete OK Cancel Step 4 - Verify the Gradle installation In windows os Windows_NT Path C:\oraclexe \app \prade\product\11.2.0\, .. PATHEXT .COM ;. EXE ;. BAT ;. CMD ;. VBS ;. VBE ;. JS ;.... v New ... Edit ... Delete OK Cancel You can execute the following command in command prompt. C:\> gradle -v Output Here you will find the Gradle version. Gradle 2.11 Build time: 2016-02-08 07:59:16 UTC Build number: none Revision: 584db1c7c90bdd1de1d1c4c51271c665bfcba978 Groovy: 2.4.4 CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 25 N33 nirf 1750 Rank

--- Page 19 ---
Course Code/Title:CS3V15/Devops Unit: I 5. Compliance: By compliance, we mean that certain businesses are required to follow specific regulations. Financial services companies in the United States, for example, must comply with the Sarbanes-Oxley Act, while healthcare, education, and energy companies must comply with HIPAA and other regulations. AWS provides a number of compliance-related features and services, such as data encryption and identity and access management, to help businesses meet these requirements. 6. High-Performance: Interested in delivering your applications quickly and efficiently? Taking advantage of AWS features such as auto-scaling and load balancing will help ensure your applications are always available and running optimally. AWS can help businesses improve their performance by offering a variety of cloud-based services, including Amazon Elastic Compute Cloud (EC2), which provides high-performance computing resources, and Amazon CloudFront, which delivers content quickly and securely to users around the world. Others include machine learning (ML) and analytics services, such as Amazon SageMaker and Amazon Athena. These services provide the tools businesses need to quickly and easily analyze their data for insights. Fast networking in the cloud is also possible with AWS, thanks to its Elastic Load Balancing (ELB) and Amazon Virtual Private Cloud (VPC). With ELB, businesses can balance their workloads across multiple instances for increased performance, while VPC allows businesses to create isolated private networks in the cloud. 7. Developer Tools: Developer tools are designed to make it easier for developers to create, deploy, and manage applications - and AWS provides developers with what they need to build applications quickly and easily. By leveraging developer tools, developers can save time and money by automating tedious tasks. They also benefit from access to AWS's extensive library of pre-built applications that can help them get their projects off the ground quickly. Services such as Amazon Elastic Beanstalk and Amazon CloudFormation can help them automate the process of creating and deploying applications. Other ways developers can improve productivity with AWS include using AWS Code Commit to store and manage source code. 8. Integration: Thanks to its many integrations with other Amazon services, as well as third-party services, AWS makes it easy for businesses to get started with cloud computing. AWS provides a wide range of services that can be easily integrated into existing business infrastructure. This allows businesses to add new features and capabilities without having to make major changes or invest in new hardware or software. For instance, if a business wants to add mobile capabilities to its website, it can take advantage of Amazon's Mobile SDK and Web Services. These tools allow businesses to quickly develop and deploy mobile apps that connect directly with their existing infrastructure. 9. Management Console: The AWS management console is a web-based interface that provides users with a simple way to interact with and manage their AWS resources - essentially a place where you can access and manage everything on the cloud. It provides a graphical view of all the resources associated with an account, as well as tools for creating and configuring new resources. Compared to traditional command- line interfaces, the AWS management console saves time and makes it easier for users to get the most out of their AWS services. Not only that, but your business gets access to 350+ free digital training courses through the AWS Academy, covering topics such as cloud fundamentals, DevOps, security, and big data. This means you can train your employees on how to use AWS, and in turn, help them become more efficient at their jobs. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 19 N32 nirf 1750 Rank

--- Page 20 ---
Course Code/Title:CS3V15/Devops Unit: I 10. Scalability: With an on-demand service, businesses can quickly spin up new servers as needed with just a few clicks. This makes it much easier to scale resources up or down as demand changes, allowing businesses to save costs and maintain performance even during peak periods. For example, if a business is expecting a sudden surge in traffic due to an advertising campaign or seasonal event, they can easily add more capacity to their server infrastructure to handle the increased load. Bru Textiles, a specialty textile company in Belgium, was able to quickly scale its infrastructure by leveraging AWS. Bru Textiles went digital to grow and offer new services. Embracing technology, they brought in digital twin technology to give their customers an idea of the texture and essence of their physical fabrics. AWS Applications · Storage and Backup: Storage and backup are important for any Cloud Computing service. AWS provides you with reliable storage services like Amazon Simple Storage Service to store large- scale data and backup services like AWS Backup to take backups of this data, which is stored in other AWS services. AWS stores the data in three different availability zones so that if one fails, you can still access your data. This makes AWS storage reliable and easily accessible. Therefore, companies with huge application data to store and backup securely can use AWS. · Big Data: One of the biggest challenges faced by companies these days is Big Data. The companies are struggling to store their large amounts of data using traditional methods. With AWS Big Data storage services, they can manage to store their data even if the data limit increases unexpectedly as AWS provides virtually unlimited data storage with scale-in and scale- out options. AWS offers easy access and faster data retrieval as well. For data processing, it offers services like EMR, with which the companies can easily set up, operate, and scale their big data. Therefore, efficiently storing and managing Big Data is among the top AWS applications. · Enterprise IT: AWS is a one-stop solution for any IT business. Many features of it such as secure storage, scalability, flexibility, and elasticity support companies to innovate faster than ever before. Using AWS for IT enterprises makes them profitable in terms of both money and time. As AWS maintains its cloud architecture, it need not waste time and money on professionals to do the same. · Social Networking: Social networking is essential for businesses in the present-day scenario where Digital Marketing is key, and it is easier with AWS. Companies can connect with customers and stakeholders and communicate through social networking sites and develop their business. Services like AWS social networking engine, which is powered by Turn Key GNU/Linux (HVM) AMI stack, are used for performance and scalability to help companies build a suitable social networking site and gain profits. · Mobile Apps: Mobile applications are embedded with day-to-day life. With AWS, you have the facility to create an app in your desired programming language. You can also keep up the applications that are consistently accessible and solid with high compute, storage, database, and application services. You can take advantage of AWS auto-scaling and managed relational database service for the better performance of your apps. · Websites: AWS offers a wide range of website hosting options to create the best website for customers. Its services like Amazon Light sail have everything, such as a virtual machine, SSD- based storage, data transfer, DNS management, and a static IP, to launch a website in such a way that the user can manage the website easily. Amazon EC2, AWS Lambda, Elastic Load Balancing, AWS Amplify, Amazon S3, etc. also help users build reliable and scalable websites. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 139 ---
Course Code/Title:CS3V15/Devops Unit: IV Create the file, if it does NOT exist (or) absent $ ansible testservers -a "touch /tmp/testfile creates=/tmp/testfile" -i ansible_hosts As Playbook - name: "Validate if a file is present or not present using Ansible Command module" hosts: testservers tasks: - name: "Create a file if it does not exist" command: "touch /tmp/latestfile" args: creates: "/tmp/latestfile" register: createif - name: "Display the file to make sure its created" command: "ls -lrt /tmp/latestfile" register: displayif when: createif is changed - debug: var=displayif.stdout - name: "Remove the file if it exist" command: "rm -rf /tmp/latestfile" args: removes: "/tmp/latestfile" register: removeif In the playbook we execute three tasks, • First one is a create a file if it does not exist • Second to display if the creation is successful • Third one is to delete the file if it exists Example 5: Execute or Run the Script when a file exists or not exists Now for this example let us take something relatable to real world scenario. Like Start the Server instance (or) program if the PID file or LOCK file does not exist Here is the sample playbook for you to get started. I am giving a general playbook here as I do not want to pick some server/technology of my own which you cannot relate to. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 140 ---
Course Code/Title:CS3V15/Devops Unit:IV you can feel free to modify the script to your needs as this is just a template. - name: Start of Stop Server instance based on PID/LOCK file availability hosts: appservers tasks: - name: Start the instance when the PID file is not present become: yes become_user: appuser command: "startserver.sh" args: creates: "/path/to/pid/instance.pid" register: startinst - name: Stop the instance when the lock file is present become: yes become_user: appuser command: "stopserver.sh" args: removes: "/path/to/lockfile/instance.lck" register: stopinst CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 26 N33 nirf 1750 Rank

--- Page 151 ---
Azure DevOps Taking you to your Azure DevOps organization ... Congratulations, you're an organization owner! Sign in to your organization at any time, https://dev.azure.com/{yourorganization}. With your organization, the following aspects are included in the free tier: First five users free (Basic license): Azure Pipelines: o One Microsoft-hosted CI/CD (one concurrent job, upto 30 hours per month) o One self-hosted CI/CD concurrent job Azure Boards: Work item tracking and Kanban boards Azure Repos: Unlimited private Git repos Azure Artifacts: Two GB free per organization Build applications with Azure: Azure DevOps enables you to build, test, and deploy anyapplication to any cloud or on premises To configure build pipelines that continuously build, test, and verify your applications. 11

--- Page 152 ---
Part 1: Get started with Azure DevOps Part 2: Build applications with Azure DevOps Part 3: Deploy applications with Azure DevOps Create a build pipeline with Azure Pipelines: Prerequisites: Familiarity with forking and cloning a GitHub repo Account requirements: An Azure DevOps organization o To use Microsoft-hosted agents, your Azure DevOps organization must have access to Microsoft- hosted parallel jobs. Check your parallel jobs and request a free grant. O You can use GitHub Code spaces to complete the module, even if your Azure DevOps organization doesn't have any parallel jobs. A GitHub account Software requirements: If using GitHub Code spaces to complete the module, there are no software requirements as all software is included in the Codespace If using a local development environment withMicrosoft-hosted agents, you must have the following software installed: o Visual Studio Code o .NET 6.0 SDK o Git 12

--- Page 31 ---
Course Code/Title:CS3V15/Devops Unit: I our work simpler. The pricing of Azure is also simpler and cost-effective. Popularly termed as "Pay As You Go", which means how much you use, pay only for that. Microsoft Azure Used for · Deployment Of applications: You can develop and deploy the application in the azure cloud by using the service called Azure App Service and Azure Functions after deploying the applications end users can access it. · Identity and Access Managment: The application and data which is deployed and stored in the Microsoft Azure can be secured with the help of Identity and Access Managment. It's commonly used for single sign-on, multi-factor authentication, and identity governance. · Data Storage and Databases: You can store the data in Microsoft azure in service like blob storage for unstructured data, table storage for NoSQL data, file storage, and Azure SQL Database for relational databases. The service can be scaled depending on the amount of data we are getting. · DevOps and Continuous Integration/Continuous Deployment (CI/CD): Azure DevOps will provide some tools like ncluding version control, build automation, release management, and application monitoring Following are some of the services Microsoft Azure offers: 1. Compute: Includes Virtual Machines, Virtual Machine Scale Sets, Functions for serverless computing, Batch for containerized batch workloads, Service Fabric for microservices and container orchestration, and Cloud Services for building cloud-based apps and APIs. 2. Networking: With Azure, you can use a variety of networking tools, like the Virtual Network, which can connect to on-premise data centers; Load Balancer; Application Gateway; VPN Gateway; Azure DNS for domain hosting, Content Delivery Network, Traffic Manager, ExpressRoute dedicated private network fiber connections; and Network Watcher monitoring and diagnostics 3. Storage: Includes Blob, Queue, File, and Disk Storage, as well as a Data Lake Store, Backup, and Site Recovery, among others. 4. Web + Mobile: Creating Web + Mobile applications is very easy as it includes several services for building and deploying applications. 5. Containers: Azure has a property that includes Container Service, which supports Kubernetes, DC/OS or Docker Swarm, and Container Registry, as well as tools for microservices. 6. Databases: Azure also included several SQL-based databases and related tools. 7. Data + Analytics: Azure has some big data tools like HDInsight for Hadoop Spark, R Server, HBase, and Storm clusters 8. AI + Cognitive Services: With Azure developing applications with artificial intelligence capabilities, like the Computer Vision API, Face API, Bing Web Search, Video Indexer, and Language Understanding Intelligent. 9. Internet of Things: Includes IoT Hub and IoT Edge services that can be combined with a variety of machine learning, analytics, and communications services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 31 N32 nirf 1750 Rank

--- Page 32 ---
Unit: I Course Code/Title:CS3V15/Devops 10. Security + Identity: Includes Security Center, Azure Active Directory, Key Vault, and Multi-Factor Authentication Services. 11. Developer Tools: Includes cloud development services like Visual Studio Team Services, Azure DevTest Labs, HockeyApp mobile app deployment and monitoring, Xamarin cross- platform mobile development, and more. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 32 N32 nirf 1750 Rank

--- Page 145 ---
Course Code/Title: CS3V15/Devops Unit: V Step 7:Note the types of plans offered by GitHub. There are a few different plans to choose from, varying in the amount of features provided. Learn to ship software like a pro. GitHub gives students free access to the best developer tools so they can learn by doing. Free Get additional student benefits GitHub Pro Unlimited public/private repositories 2,000 CI/CD minutes/month Free for public repositories 500MB of Packages storage Free for public repositories Protect your branches Ensure that collaborators on your repository cannot make irrevocable chang branches. O Draft pull requests O 120 core-hours of Codespaces compute O Pages and Wikis 15GB of Codespaces storage O Community support ® 3,000 CI/CD minutes/month Free for public repositories 2GB of Packages storage Free for public repositories O 180 core-hours of Codespaces compute 20GB of Codespaces storage Web-based support wikiHow Step 8: Select the free plan. On the plan selection page, scrolldown to click the button for choosing a free plan. This will immediately take you to your GitHub dashboard. Continue for free wiki How UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 5 N33 In 175º Rank ---

--- Page 146 ---
Course Code/Title: CS3V15/Devops Unit: V · If you choose a paid plan, you'll have to enter yourpayment information as requested before you can continue. . If you want to upgrade your Github account in the future, click the menu at the top- right corner, select Settings, andchoose Billing and plans to view your options. GitHub essentials are: · Repositories · Branches · Commits · Pull Requests · Git (the version control software GitHub is built on) Repository: A GitHub repository can be used to store a developmentproject. It can contain folders and any type of files (HTML, CSS,Javascript, Documents, Data, Images). A GitHub repository should also include a licence file and a README file about the project. A GitHub repository can also be used to store ideas, or any resources that you want to share. Github Repository Creation Step 1: Click on the new repository option Marketplace Explore New repository and GitHub without any code! Catch Universe Join a community Universe in San Fr ticket pricing-he Import repository × New gist bird New organization New project llo World guide, you'll create a repository, start a , write comments, and open a pull request. GitHub Sponsors Matching Fund × Ready to support open source? GitHub will match your contribution to developers during their first year in GitHub Sponsors. d the guide Start a project × Welcome to the new dashboard. Get closer to the stuff you care about most. Explore repositories atlas-engineer/next Next browser - Be productive. UnENVIVAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 175° Rank 6

--- Page 137 ---
Course Code/Title:CS3V15/Devops Unit:IV I Example 2: Get the Hostname and Version of remote servers with UNAME we have used the command module and executing an uname -a command as AD-HOC command $ ansible testservers -m command -a "uname -a" -i ansible hosts As Playbook - name: Check the remote host Hostname, Version, Distribution with UNAME hosts: testservers tasks: - name: Execute the UNAME command register: unameout command: "uname -a" - debug: var: unameout.stdout lines I Example 3: Check the Disk Usage of Remote server To get the disk usage, we are using df -h , here -h is human readable As Ad Hoc command $ ansible testservers -m command -a "df -h" -i ansible hosts As Playbook - name: Check the disk usage of all the file system in the remote servers hosts: testservers tasks: - name: Execute the df command register: dfout command: "df -h" - debug: var: dfout.stdout lines CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 138 ---
Course Code/Title:CS3V15/Devops Unit:IV Example 4: Restart Apache Server using Ansible Command Module So far, In all the sample we have seen we are using one application server and one web server. Totally two servers. But in this example, we need to limit our execution only to the web server as we are going to check the restart the apache web server. this is done using -- limit parameter As Ad hoc command $ ansible testservers -m command -a "httpd -k restart" -i ansible_hosts -b - limit As Playbook - name: restart apache web server hosts: testservers tasks: - name: restartapache register: httpdresout become: yes command: "httpd -k restart" when: ansible_hostname == "mwiweb02" - debug: var: httpdresout.stdout_lines I Example 5: Execute a command when a file exists or not exists There are two most useful parameters in ansible command module such as removes and creates removes - used to tell ansible to Execute the command only if the file exist creates - used to tell ansible to Execute the specified command only if the file does not exist Here we are going to do a very simple file creation and removal based on the file availability or existence. In General using the ansible command module, This method of creating and removing the file is not recommended as Ansible has a dedicated module named file to do the same effortlessly. As Ad hoc command Remove the file, if it does exist (or) present $ ansible testservers -a "rm -rf /tmp/testfile removes=/tmp/testfile" -i ansible_hosts CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 24 N33 nirf 1750 Rank

--- Page 149 ---
Course Code/Title: CS3V15/Devops Unit: V Step 6: Now you will see that all of our files uploaded in our github. Namanbhatia7 Add initial resume website files Latest commit f47be5b 1 minute ago Display_Picture.png Add initial resume website files 1 minute ago My_Resume.html Add initial resume website files 1 minute ago README.md Initial commit 15 minutes ago Styles.css Add initial resume website files 1 minute ago Branch: · A GitHub branch is used to work with different versions of a repository at the same time. · By default a repository has a master branch (aproduction branch). · Any other branch is a copy of the master branch (as itwas at a point in time). · New Branches are for bug fixes and feature work separate from the master branch. When changes are ready, they can be merged into the master branch. If youmake changes to the master branch while working on a new branch, these updates can be pulled in. Commits: At GitHub, changes are called commits. Each commit (change) has a description explaining why achange was made. Pull Requests : . Pull Requests are the heart of GitHub collaboration. · With a pull request you are proposing that your changesshould be merged (pulled in) with the master. · Pull requests show content differences, changes, additions, and subtractions in colors (green and red). · As soon as you have a commit, you can open a pull request and start a discussion, even before the code isfinished. Git: · Git was created by Linus Torvalds in 2005 to develop Linux Kernel · Git is an open-source distributed version control system. It is designed to handle minor to major projectswith high speed and efficiency. . It is developed to co-ordinate the work among the developers. The version control allows us to track andwork together with our team members at the same workspace. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 9 175" Rank

--- Page 150 ---
CREATE AZURE ORGANIZATION AZURE: Microsoft Azure, often referred to as Azure cloud computing platform run by Microsoft. It offers access, management, and the development of applications and services through global data centers It also provides a range of capabilities, including software as a service (SaaS), platform as a service, and infrastructure as a service (IaaS). It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014 Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Prerequisites: To plan your organizational structure. Microsoft accounts or authenticate users with Microsoft Entra ID. For more information, see Choosing your organization administrator account type. Create an organization: 1. Sign in to Azure DevOps. 2. Select New organization 3. Confirm information, and then select Continue 10

--- Page 123 ---
Course Code/Title:CS3V15/Devops Unit: IV Ad-Hoc Limitations: While ad-hoc commands are ideal for single, immediate tasks, they are not designed for complex, multi- step automation. Playbooks come to the rescue when tasks have interdependencies and need to be executed in a coordinated manner. Readability and Reusability: Playbooks are authored in YAML, a human-readable and straightforward format. This not only makes them easy to write but also facilitates sharing and collaboration. You can reuse playbooks across various scenarios, saving time and effort. Conditional and Looping Logic: Playbooks offer advanced features, including conditional statements and looping, that allow you to adapt automation to different situations. This flexibility makes playbooks versatile and capable of handling a wide range of automation needs. In essence, playbooks are your tool of choice when automation tasks become multi-faceted and require a structured and logical approach. They provide the power to streamline and automate complex workflows with precision and efficiency. Example: Ping All Inventory Devices To illustrate the use of a playbook, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: playbook.yaml 1 --- 2 -name : My First Play #Name of play 3 hosts: virtualmachines #defining host can be all 4 tasks: 5 -name:Ping My Hosts #Name of task 6 ansible.builtin.ping: # can also write ping To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 124 ---
Course Code/Title:CS3V15/Devops Unit: IV Output and Result: Activities Terminal - Nov 2 12:29 . O auriga@auriga-Latitude-E7470: - /Desktop/ansible X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook.eg1.yaml PLAY [My First Play] TASK [Gathering Facts] ok: [ vri01] TASK [Ping Devices] ok: [vm01] PLAY RECAP vn01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$[] rescued=0 ignored=0 X As you can see there is gathering facts task which we didn't create is running it is the default task which ping the connection. Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: playbook.yaml 1 --- 2- name: Install Nginx 3 hosts: all 4 become: yes 5 tasks: 6 - name: Install Nginx using apt 7 apt: 8 name: nginx 9 state: present To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml -- ask-become-pass CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 10 N33 nirf 1750 Rank

--- Page 141 ---
Course Code/Title: CS3V15/Devops Unit: V UNIT V - BUILDING DEVOPS PIPELINES USING AZURE Create Github Account, Create Repository, Create Azure Organization, Create a new pipeline, Build a sample code, Modify azure-pipelines.yaml file Github Account Creation What is Github? GitHub is a code hosting platform for collaboration and version control. GitHub lets you and others work together on projects fromanywhere. Github is owned by Microsoft, provides access to public(free) and private(paid) repositories. Stepsto create Github Account: Join GitHub · GitHub × + < C https://github.com/join f Facebook - Log In o ... Twitter G Google f Facebook YouTub Product v Solu wiki How Step 1:Go to https://github.com/join in a web browser. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 1 175" Rank

--- Page 142 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: Enter your personal details. In addition to creating a username and entering an email address, you'll also have to create a password. Your password must be at least 15 characters in length or at least 8 characters with at least one number and lowercase letter. Join GitHub First, let's create your user account Username * wikihowneveconcepts Email address * Password * .......... Make sure it's at least 15 characters OR at least 8 characters including a number and a lowercase letter. Learn more. Email preferences Send me occasional product updates, announcements, and offers. Verify your account wiki How Step 3: Click Verify to start the verification puzzle. The instructions vary by puzzle, so just follow the on-screen instructions to confirm that you are a human. A green checkmark will appear after completing the puzzle. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 2

--- Page 153 ---
Create a pipeline in azure What is pipeline Azure Pipelines supports continuous integration (CI)and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline. The latest way to build pipelines is with the YAML pipeline editor. You can also use Classic pipelines with the Classic editor. code release build CI plan continuous testing CD deploy monitor operate Create your first pipeline This is a step-by-step guide to using Azure Pipelines to build a sample application from a Git repository. This guide uses YAML pipelines configured with the YAML pipeline editor. If you'd like to use Classic pipelines instead, see Define your Classic pipeline. For guidance on using TFVC, see Build TFVC repositories. Prerequisites - Azure DevOps A GitHub account where you can create are pository. An Azure DevOps organization. Create one for free. If your team already has one, then make sure you're an administrator of the Azure Devops. 12

--- Page 154 ---
Get the Java sample code To get started, fork the following repository into your GitHub account. https://github.com/MicrosoftDocs/pipelines-java Create your first Java pipeline 1. Sign-in to your Azure DevOps organization and go to your project. 2. Go to Pipelines, and then select New pipeline. 3. Do the steps of the wizard by first selecting GitHub as the location of your source code. 4. You might be redirected to GitHub to sign in. If so, enter your GitHub credentials. 5. When you see the list of repositories, select your repository. 6. You might be redirected to GitHub to install the Azure Pipelines app. If so,select Approve & install. 7. Azure Pipelines will analyze your repository and recommend the Maven pipeline template. 8. When your new pipeline appears, take a look at the YAML to see what it does. When you're ready, select Save and run. 9. You're prompted to commit a new azure pipelines.yml file to your repository. After you're happy with the message, select Save and run again. If you want to watch your pipeline in action, select the build job. You just created and ran a pipeline that we automatically created for you, because your code appeared to be a good match for the Maven template. You now have a working YAML pipeline (azure-pipelines.yml) in your repository that's ready for you to customize! 14

--- Page 127 ---
Course Code/Title:CS3V15/Devops Unit:IV Abbreviation You can also use abbreviation to represent lists. Example Countries: ['America', 'China', 'Canada', 'Iceland'] List inside Dictionaries We can use list inside dictionaries, i.e., value of key is list. Example james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english … List of Dictionaries We can also make list of dictionaries. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english - robert: name: robert richardson rollNo: 53 div: B sex: male CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 13 N32 nirf 1750 Rank

--- Page 128 ---
Course Code/Title:CS3V15/Devops Unit:IV likes: - biology - chemistry … YAML uses "|" to include newlines while showing multiple lines and ">" to suppress newlines while showing multiple lines. Due to this we can read and edit large lines. In both the cases intendentation will be ignored. We can also represent Boolean (True/false) values in YAML. where boolean values can be case insensitive. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english result: maths: 87 chemistry: 45 biology: 56 physics: 70 english: 80 passed: TRUE messageIncludeNewLines: | Congratulation !! You passed with 79% messageExcludeNewLines: > Congratulation !! You passed with 79% CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 14 N33 nirf 1750 Rank

--- Page 89 ---
Course Code/Title:CS3V15/Devops Unit: III Key Features: · Source Code Management: Configures Jenkins to use Git repositories as the source code for jobs. Example Demo Step 1 : Create a simple Java program. I have created a folder named MyJavaPrograms and inside it created a simple Java program as follows - e.g., SSH test.java public class test { public static void main(String args[I) { for(int i=1;i <= 5;1++) System.out.printin("Welcome Anuradha"); } Plugin," Step 2 : Open the command prompt, switch to that folder and execute the above Java program. It is illustrated by following screenshot. 1 C:\Windows\System32\cmd.e X + Microsoft Windows [Version 10.0.22621.2134] (c) Microsoft Corporation. All rights reserved. 0 X before E:\MyJavaPrograms>javac test. java E: \MyJavaPrograms>java test Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha E: \MyJavaPrograms> Step 3 : Now we will create a Git repository and push this repository on GitHub. First of all we will initialise the Git repository my using git init command. CHENNAI INSTITUTE TECHNOLOGY 14 - N32 nirf 1750 Rank CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt)

--- Page 90 ---
Unit: III Course Code/Title:CS3V15/Devops C:\Windows\System32\cmd.e x X E:\MyJavaPrograms>git init Initialized empty Git repository in E:/MyJavaPrograms/.git/ E:\MyJavaPrograms>git status On branch master No commits yet Untracked files: (use "git add <file> ... " to include in what will be committed) test.class test. java nothing added to commit but untracked files present (use "git add" to track) E:\MyJavaPrograms> then add the java and class files to git repository - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git add . E:\MyJavaPrograms>git status On branch master No commits yet Changes to be committed: (use "git rm -- cached <file> ... " to unstage) new file: test.class new file: test. java E: \MyJavaPrograms> Now we will commit the changes - C:\Windows\System32\cmd.e X E:\MyJavaPrograms>git commit -m "First Commit for Java program" [master (root-commit) 4cb04db] First Commit for Java program 2 files changed, 6 insertions(+) create mode 100644 test.class create mode 100644 test. java E:\MyJavaPrograms> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) - INSTITUTE . TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 129 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible ad-hoc Commands The Ansible ad-hoc command uses the /usr/bin/ansible command-line tool to automate a single task on one or more managed nodes. The Ad-hoc commands are quick and easy, but they are not re-usable. The Ad-hoc commands demonstrate the simplicity and power of Ansible. Syntax ansible <hosts> [-m <module_name>] -a <"arguments"> -u <username> [ -- become] Explanation Hosts: It can be an entry in the inventory file. For specifying all hosts in the inventory, use all or "*" . module_name: It is an optional parameter. There are hundreds of modules available in the Ansible, such as shell, yum, apt, file, and copy. By default, it is the command. Arguments: We should pass values that are required by the module. It can change according to the module used. Username: It specifies the user account in which Ansible can execute commands. Become: It's an optional parameter specified when we want to run operations that need sudo privilege. By default, it becomes false. 1. Parallelism and shell commands You can reboot your company server in 12 parallel forks at the same time. For this, you need to set up the SSHagent for connection. $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa To run reboot for all your company servers in the group, 'abc', in 12 parallel forks: $ ansible abc -a "/sbin/reboot" -f 12 By default, Ansible will run the above ad-hoc commands from the current user account. If you want to change then pass the username in ad-hoc command as follows: $ ansible abc -a "/sbin/reboot" -f 12 -u username 2. File Transfer You can use ad-hoc commands for doing SCP (secure copy protocol) which means lots of files in parallel on multiple machines or servers. CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 15 N32 nirf 175º Rank

--- Page 130 ---
Course Code/Title:CS3V15/Devops Unit:IV Transferring file on many machines or servers $ ansible abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf" Creating new directory $ ansible abc -m file - a "dest = /path/user1/new mode = 888 owner = user group = user1 state = directory" Deleting all directory and files $ ansible abc -m file -a "dest = /path/user1/new state = absent" 3. Managing Packages Ad-hoc commands are available for apt and yum module. Here are the following ad-hoc commands using yum. Below command checks, if the yum package is installed or not, but not update it. $ ansible abc -m yum -a "name = demo-tomcat-1 state = present" Below command checks the package is not installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = absent" And below command checks the latest version of package is installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = latest" 4. Managing Users and Groups You can manage, create, and remove a user account on your managed nodes with ad-hoc commands. $ ansible all -m user -a "name=foo password =< crypted password here>" $ ansible all -m user -a "name=foo state=absent" 5. Managing Services Ensure a service is started on all the webservers. $ ansible webservers -m service -a "name=httpd state=started" Alternatively, restart a service on all webservers: $ ansible webservers -m service -a "name=httpd state=restarted" CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 nirf 1750 Rank

--- Page 133 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible Roles Roles provide a framework for fully independent or interdependent collections of files, tasks, templates, variables, and modules. The role is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks and makes them easier to reuse. The breaking of the playbook allows you to break the playbook into reusable components. Each role is limited to a particular functionality or desired output, with all the necessary steps to provide that result either within the same role itself or in other roles listed as dependencies. Roles are not playbooks. Roles are small functionality that can be used within the playbooks independently. Roles have no specific setting for which hosts the role will apply. Top-level playbooks are the bridge holding the hosts from your inventory file to roles that should be applied to those hosts. Creating a Role The directory structure for roles is essential to creating a new role, such as: Role Structure The roles have a structured layout on the file system. You can change the default structured of the roles as well. For example, let us stick to the default structure of the roles. Each role is a directory tree in itself. So the role name is the directory name within the /roles directory. $ ansible-galaxy -h Usage ansible-galaxy[delete|import|info|init|install|list|login|remove|search|setup][ -- help] [options] ... Options o -h: (help) it shows this help message and exit. o -v: (verbose) Verbose mode (-vvv for more, -vvvv to enable connection debugging). o -- version: it shows program version number and exit. Roles are stored in separate directories and have a particular directory structure CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 19 N33 nirf 1750 Rank

--- Page 134 ---
Course Code/Title:CS3V15/Devops Unit: IV Typical structure of ansible role myweb defaults L main.yml files · handlers L main.yml meta. L main.yml README.md tasks L_ main.yml templates tests inventory test.yml vars L main.yml · defaults : It stores the default variable for the role. For example the default port number is http is 8080, then it can be stored in defaults. · files : This folder contains the files required to transfer or deploy to the target machines. · handlers : It contains handlers, which may be used by some role. · meta : It defines some data or information about the role. · tasks : It contains the main list of tasks to be executed by the role. These tasks might be defined in separate files as per the functionalities. · templates : It contains the templates which can be deployed via a role. Ansible Module Ansible Command module is used to execute commands on a remote node. The Command module, is used mostly to run simple Linux commands on a remote node/server which is part of a host group or Stand alone server mentioned in the host group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 125 ---
Course Code/Title:CS3V15/Devops Unit: IV This command executes the specified playbook while prompting for the necessary privilege escalation password. Output and Result: Activities Terminal - Nov 2 13:00 . auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook-eg2.yaml PLAY [Install Nginx] TASK [Gathering Facts] Total: [vnet]: FAILEDT .. ['msg": "Missing sudo password') PLAY RECAP : ok=0 changed=0 unreachable=0 Fatled =! skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory. yaml playbook-eg2.yaml - - ask-becone-pass BECOME password: PLAY [Install Nginx] TASK [Gathering Facts] ok: [vri01] TASK [Install Nginx using apt] ok: [vri01] PLAY RECAP * VMD1 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ [] As shown in the example image above, the first command encountered an error due to the absence of the -ask-become-pass flag. This flag is essential when your task requires elevated privileges, as it prompts Ansible to request the sudo password for authentication. The corrected command includes the -ask-become-pass flag, ensuring that the necessary privileges are obtained before executing the task. This is particularly important when working with tasks that require administrative access, such as package installations or system configurations. Note: In this blog, we primarily focus on the core and fundamental components of Ansible, providing an introductory overview and understanding of its key concepts. While Ansible offers advanced features such as "Collections," "Ansible Galaxy," "Roles," "Dynamic Inventory," and "Custom Modules and Plugins," we recognize that these topics are extensive and may require separate dedicated discussions. For the purpose of this blog, we aim to establish a strong foundation of Ansible's core components. If you're interested in diving deeper into these advanced topics, we recommend exploring Ansible's official documentation and additional resources dedicated to each subject. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 11 N32 nirf 1750 Rank

--- Page 126 ---
Course Code/Title:CS3V15/Devops Unit:IV Understanding YAML Ansible uses YAML syntax for expressing Ansible playbooks. This chapter provides an overview of YAML. Ansible uses YAML because it is very easy for humans to understand, read and write when compared to other data formats like XML and JSON. Every YAML file optionally starts with " --- " and ends with " ... ". In this section, we will learn the different ways in which the YAML data is represented. key-value pair YAML uses simple key-value pair to represent the data. The dictionary is represented in key: value pair. Note - There should be space between : and value. Example: A student record --- #Optional YAML start syntax james: name: james john rollNo: 34 div: B sex: male ... #Optional YAML end syntax Abbreviation You can also use abbreviation to represent dictionaries. Example James: {name: james john, rollNo: 34, div: B, sex: male} Representing List We can also represent List in YAML. Every element(member) of list should be written in a new line with same indentation starting with "- " (- and space). Example countries: - America - China - Canada - Iceland CHENNAI INSTITUTE OF TECHNOLOGY CHENNAI INSTITUTE . TECHNOLOGY (Autonomous) 12 N33 nirf 1750 Rank

--- Page 1 ---
Course Code/Title:CS3V15/Devops CHENNAI INSTITUTE OF TECHNOLOGY Transforming Lives CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) NAME OF THE PROGRAMME B. E (CSE) YEAR III SEMESTER V REGULATIONS 2022R COURSE CODE CS3V15 COURSE NAME DEVOPS FACULTY NAME (Prepared by) NAME OF SUBJECT EXPERT (Verified by) M.SUNDHARI CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Unit: I N33 INATIONAL BOARD . ACCREDITATION 100% Accreditation All eligible UG & PG Programs NIRF 151 - 200 Band Engineering 2023 A+ DE NAAC Contact 8681927167 Contact 1 N32 nirf 1750 Rank

--- Page 2 ---
Course Code/Title:CS3V15/Devops Unit: I UNIT - I Introduction to Devops Devops Essentials - Introduction To AWS, GCP, Azure - Version control systems: Git and Github. The word "DevOps" was coined in 2009 by Patrick Debois, who became one of its gurus. The term was formed by combining "development" and "operations," which provides a starting point for understanding exactly what people typically mean when they say "DevOps." Notably, DevOps isn't a process or a technology or a standard. Many devotees refer to DevOps as a "culture"-a viewpoint that New Relic favors. We also use the term "DevOps movement" when talking about topics such as adoption rates and trends for the future, and "DevOps environment" to refer to an IT organization that has adopted a DevOps culture. "DevOps represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile, lean practices in the context of a system-oriented approach. DevOps emphasizes people (and culture), and seeks to improve collaboration between operations and development teams. DevOps implementations utilize technology- especially automation tools that can leverage an increasingly programmable and dynamic infrastructure from a life cycle perspective." DevOps is defined as a combination of processes and tools created to facilitate organizations in delivering services and applications much faster than they can through conventional software development processes. It helps increase customers' confidence in the applications that an organization offers, thereby allowing the company to flourish and achieve its business goals faster. Development 3 3 DevOps 13 51 Operations Quality Assurance (QA) How DevOps Works? A DevOps process can be summarized as an infinite loop that comprises the following stages - build, test, and release through the delivery pipeline and plan and monitor through feedback, which resets the loop again. With such an amazing combination, teams use tech stack and tooling that assists them in reliably developing apps. Moreover, going away from the Under the DevOps model, development and operations teams work in constant cohesion throughout the entire project lifecycle, starting right from development to deployment. When security is the main focus, the quality assurance team is tightly knitted with the DevOps team throughout the app lifecycle. In this situation, some DevOps teams are also referred to as DevSecOps. Close coordination with the QA team ensures that no loopholes are left unchecked in the provided service/app. COMPANY HOW DEVOPS WORKS DELIVERY PIPELINE BUILD TEST RELEASE PLAN MONITOR FEEDBACK LOOP TOOLBOX" CUSTOMER norm, teams use automated processes here. DevOps tools also allow engineers to complete different tasks independently. Be it provisioning infrastructure or deploying code, they can accomplish these tasks without being dependent on one another. As such, the DevOps model accelerates the overall application development process. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 135 ---
Course Code/Title:CS3V15/Devops Unit: IV Syntax for writing the module on command line While writing the module we use the options such as -m and -a. The -m is for module and -a is for argument, after the -a option we specify the executable command in double quotes. here 'all' stands for all the remote hosts, otherwise we can mention the group name of the hosts 1 $ ansible all -m command -a "echo 'Hello hosts'" The -m option followed by module name 'command' Command to be executed on remote hosts, given in double quotes Commonly used modules 1. Command : Following is a command module that executes a command on remote hosts $ ansible all -m command -a "echo 'Hello hosts'" 2. User : The user module is used to create, modify or delete users. For example - To create a user named 'new_user' on all the hosts in the 'dev' group using ansible, you can use the user module. Here's the ansible command to do that : $ ansible dev -m user -a "name=new_user state=present" Sometimes the above command fails as there is no privilege to create user on remote host. Then we can issue the command as follows $ ansible all -m user -a "name=new_user state=present" -- become -- ask-become-pass The above command asks for the password for the remote host to login. On supplying the password the user will be created. We can verify. if new user is getting created or not on remote host by issuing following command on each remote host $ less /etc/passwd If we want to create new user without asking for password on all the remote hosts then we can issue the command as - $ ansible all -m user -a "name=new_user1 state=present" -- become -b The -b option is used to become superuser. By this the sudo or su privileges can be granted. .3. Ping : The ping module is used to connectivity to remote hosts $ ansible all -m ping 4. Copy : The copy module is used to copy files from control machine to remote hosts $ ansible dev -m copy -a "src=/home/ansible_user/myfile.txt dest=/home/ansible_user/myfile.txt" The above command will copy the myfile.txt file from control node to all the managed nodes of dev group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 21 N32 nirf 1750 Rank

--- Page 136 ---
Course Code/Title:CS3V15/Devops Unit: IV 5. Package : The package module is used to install, remove or upgrade the packages, For example, the following command installs the nginx web server package : $ ansible package -a "name=nginx state=present" Here name indicates the name of the package to be installed, the state=present means install the package. Normally "present" is for installation, "remove" is for removing the package and "latest" is used to upgrade the package. 6. Shell : The shell module is used to run shell commands with more flexibility on remote hosts. For example - Following shell command executes on remote hosts. $ ansible dev -m shell -a "cmd=is" 7. Service : The service module is used to start, stop or restart a specific service on remote hosts. For example - Following service will start the apache web service on all the remote hosts. $ ansible dev -m service -a "name=apache2 state=started" Note that the state can started for starting the service, stopped for stopping the service. o started/stopped are idempotent actions that will not run commands unless necessary. o restarted will always bounce the service. o reloaded will always reload. I Example 1: Get the Uptime of remote servers We have used command module to run the uptime command and we have given both the ad hoc and the playbook form of execution. as AD-HOC Command $ ansible testservers -m command -a uptime -i ansible_hosts as Playbook - name: Check the remote host uptime hosts: testservers tasks: - name: Execute the Uptime command over Command module register: uptimeoutput command: "uptime" - debug: var: uptimeoutput.stdout_lines CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 22 N32 nirf 1750 Rank

--- Page 147 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: After clicking new repository option, we will have to initialize some things like, naming our project, choosing the visibility etc. After performing these steps click Create Repository button. Create a new repository A repository contains all project files, including the revision history. Already have a project repository elsewhere? Import a repository. Owner Repository name * / Namanbhatia7 - Resume This is going to be name of our project Great repository names are short and memorable. Need inspiration? How about animated-memory? Description (optional) Public Anyone Keep this as public selected ository. You choose who can commit. see this rer Private You choose who can see and commit to this repository. We can add a project description if we want. Skip this step if you're importing an existing repository. Initialize this repository with a README This will let you immediately clone the repository to your comp Tick the README option Add .gitignore: None - Add a license: None ~ Create repository After performing above steps, Click this button Step 3: After clicking the button, we will be directed to below page. Right now the only file we have is a readme file. Namanbhatia7 / Resume o Unwatch - 1 * Star 0 Y Fork 0 <> Code Issues 0 (*) Pull requests 0 ||| Projects 0 BB Wiki Security the Insights # Settings No description, website, or topics provided. Edit Manage topics T 1 commit įº 1 branch > 0 releases 2 1 contributor Branch: master New pull request Create new file Upload files Find File Clone or download TT Namanbhatia7 Initial commit Latest commit 676ac98 now README.md Initial commit now DO README.md Resume UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 7 175" Rank

--- Page 148 ---
Course Code/Title: CS3V15/Devops Unit: V Step 4: Now click on the "Upload files" button. 4 > C https://github.com/avicnotes/html-css-projects ... Search or jump to ... Click to End Session Total: $80.83 Pull requests Issues Marketplace Explore Google" Custom Search @ avicnotes / html-css-projects Unwatch - 1 Star 0 Fork 0 <> Code Issues 1'] Pull requests Actions Projects Wiki O Security ~ Insights ... go main - ¿º 1 branch 0 tags Go to file Add file - { Code About avicnotes screenshots Create new file Upload files 2 24 commits No description, website, or topics provided. README.md Update README.md 2 days ago Readme Screenshot (10).png screenshots 2 days ago Screenshot (11).png screenshots 2 days ago Releases Screenshot (12).png screenshots 2 days ago No releases published Create a new release 19 Screenshot (6).png screenshots 2 days ago accordion.JPG Add files via upload 4 days ago Packages background-dots.png Add files via upload 2 days ago clone-project-1.html Updated navbar items to my original ones 2 days ago index.html Create index.html 's ago No packages published Publish your first package Activate Windows Go to Settings to activate Windows, Environments 1 O Type here to search O W. 11:32 AM 2/1/2021 Step 5:Follow the steps mentioned and click "commitchanges". Drag additional files here to add them to your repository Or choose your files Display_Picture.png Simply drag or add files to be uploaded × My_Resume.html X Styles.css × Commit changes Add initial resume website files Add an optional ended des Give this particular version a name -o- Commit directly to the master branch. Ensure this option is checked !] Create a new branch for this commit and start a pull request. Learn more about pull requests. Commit changes Cancel UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 8

--- Page 157 ---
Select a pipeline run to view information about that run. You can choose to Retain or Delete a run from the context menu. For more information on run retention, see Build and release retention policies. Stages La 51m ago > ... 6 42s Retain Delete Ld 2h ago 0 1m 13s - View pipeline details The details page for a pipeline allows you to view and manage that pipeline. Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. Azure DevOps · Search F 4 FabrikamFiber Edit Run pipeline : + Runs Branches Analytics Y Description Stages Ac #20191209.3 Set up CI with Azure Pipelines Manually triggered { main 2b4b23c #20191209.2 Set up CI with Azure Pipelines & Manually triggered & main 2b4b23c Là 54m ago Č 1m 13s Là 55m ago Č 1m 55s #20191209.1 Set up CI with Azure Pipelines Individual CI 1º main 2b4b23c Lô 56m ago 1m 6s 17

--- Page 158 ---
Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. View pipeline run details From the pipeline run summary you can view the status of your run, both while itis running and when it is complete. #20191210.2 Update azure-pipelines.yml for Azure Pipe ... on FabrikamFiber Summary Environments Triggered by Steve Danielson FabrikamFiber & main b2f795e Today at 12:56 PM Duration: 1m 9s Tests: Get started Changes: ¢ 2 commits Work items: ₾ 1 linked Artifacts: 1 published Stages Jobs Build 1 job completed 415 Run new : Deploy 1 job completed 13s 1 artifact From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, test results, and work Items From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, testresults, and work items Jobs and stages The jobs pane displays an overview of the status of your stages and jobs. This pane may have multiple tabs depending on whether your pipeline has stages and jobs, or just jobs. In this example, the pipeline has two stages named Build and Deploy. You can drill down into the pipeline steps by choosing the job from either the Stages or Jobs pane. 10

--- Page 155 ---
10. When you're ready to make changes to your pipeline, select it in the Pipelines page, and then Edit the azure-pipelines.yml file. View and manage your pipelines You can view and manage your pipelines by choosing Pipelines from the left-hand menu to go to the pipelines landing page. Azure DevOps : FabrikamFiber 1 Pipelines 0 Search F FabrikamFiber + Pipelines New pipeline Overview Recent All Runs Y Filter pipelines Boards Repos Pipelines Pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... La 32m ago & Manually triggered 8º main & 42s Environments Releases FabrikamFiber #20191209.3 · Set up CI with Az ... La 1h ago & Manually triggered & main 1m 13s Library Task groups *** Deployment groups From the pipelines landing page you can view pipelines and pipeline runs, create and import pipelines, manage security, and drill down into pipeline and run details. Choose Recent to view recently run pipelines (the default view), or choose All to view all pipelines. Pipelines New pipeline : Recent All Runs Filter pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... & Manually triggered & main Là 36m ago ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Az ... Là 1h ago 1m 13s & Manually triggered & main 15

--- Page 156 ---
Select a pipeline to manage that pipeline and view the runs. Select the build number for the last run to view the results of that build, select the branch name toview the branch for that run, or select the context menu to run the pipeline and perform other management actions. Recently run pipelines Pipeline Last run #20191209.2 · Set up CI with Azure Pip ... > pipelines-dotnet-core 8 Manually triggered 12º main Là 1h ago * ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Azure Pip ... 試 2h 8 Manually triggered 8 main Edit in Run pipeline Manage security Rename/move Delete Select Runs to view all pipeline runs. You canoptionally filter the displayed runs. Pipelines Recent All Runs Filter by keywords State V Repository V New pipeline Requested forV Tags X All pipeline runs Description Stages Set up CI with Azure Pipelines & #20191209.2 on pipelines-dotnet-core & main d4964 .. Set up CI with Azure Pipelines #20191209.1 on pipelines-dotnet-core åº main d4964 ... Set up CI with Azure Pipelines & #20191209.3 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines & #20191209.2 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines #20191209.1 on FabrikamFiber & main 2b4b23c La 44m ago 42s La 53m ago ₾ 46s Là 1h ago 0 1m 13s Ld 1h ago 1m 555 Lo 1h ago 1m 6s 16

--- Page 165 ---
Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. Customize CI triggers Pipeline triggers cause a pipeline to run. You can use trigger: to cause a pipeline to run whenever you push an update to a branch. YAML pipelines are configured by default with a CI trigger on your default branch (which is usually main). You can set up triggers for specific branches or for pull request validation. For a pull request validation trigger, just replace the trigger: step with pr: as shown in the two examples below. By default, the pipeline runs for each pull request change. . If you'd like to set up triggers, add either of the following snippets at the beginning of your azure-pipelines.yml file. YAML I Copy trigger: - main - releases/* YAML Copy pr: - main - releases/* You can specify the full name of the branch (for example, main) or a prefix-matching · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. Build using multiple versions Add steps You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can use tasks for building, testing, publishing, or deploying your app. For Java, the Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. YAML Copy - task: PublishCodeCoverageResults@1 inputs: codeCoverageTool: "JaCoco" summaryFileLocation: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco/ reportDirectory: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco" failIfCoverageEmpty: true · Select Save and then confirm the changes. . You can view your test and code coverage results by selecting your build and going to the Test and Coverage tabs.

--- Page 166 ---
Build using multiple versions Rename/move pipeline Name ScheduledTriggerTest 1234 Select folder 1 .. . Cancel Save Pipeline settings X Processing of new run requests Enabled Paused Disabled YAML file path azure-pipelines.yml Automatically link work items included in this run Cancel Save YAML trigger: main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdkVersion: "1.10" jdk11_windows: imageName: "windows-latest" jdkversion: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdkVersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: **** /TEST -*. xml" goals: "package"

--- Page 163 ---
‹ FabrikamFiber Runs Branches Analytics Description Edit Run pipeline : Stages #20210609.3 Update azure-pipelines.yml for Azure Pipelines Manually triggered for main 9 61bba8c x2 La Wednesday Ø 215 #20210609.2 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for 1º main 9 bc6c105 x La Wednesday ₾ 305 #20210609.1 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for main 9 53eb92b x • #20210520.1 Updated README.md Individual CI for & new-branch 0 1ada387 ‹ azure-cli-example go main V azure-cli-example / azure-pipelines.yml 30 pool: default 31 32 steps: 33 # Specify python version and install if needed Settings 34 - task: UsePythonVersion@0 35 condition: false 36 inputs: 37 -versionSpec: '3.x' 38 architecture: 'x64' 39 40 # Update pip to latest 41 - bash: python -m pip install -- upgrade pip 42 condition: false 43 displayName: 'Upgrade pip' 44 45 container 46 &continueOnError 47 48 88988 name & parameters & resources & schedules 49 50 51 services 52 strategy 53 & variables 54 & workspace La Wednesday .455 Là May 20 ₾ 135 Variables Run : Tasks 7 Search tasks dotnet .NET Core Build, test, package, or publish a dotnet applicatio. Android signing Sign and align Android APK files Ant Build with Apache Ant 80 App Center distribute Distribute app builds to testers and users via Visu .. App Center test Test app packages with Visual Studio App Center Archive files Compress files into .7z, tar.gz on zip ARM template deployment Deploy an Azure Resource Manager (ARM) templ .. Azure App Service deploy Deploy to Azure App Service a web, mobile, or AP. · The YAML pipeline editor provides several keyboard shortcuts, which we show in the following examples. . Choose Ctrl+Space for Intellisense support while you're editing the YAML pipeline.

--- Page 164 ---
· The task assistant provides a method for adding tasksto your YAML pipeline. · To display the task assistant, edit your YAML pipeline and choose Show assistant. Understand the azure-pipelines.yml file · A pipeline is defined using a YAML file in your repo. Usually, this file is named azure-pipelines.yml and is located at the root of your repo. · Navigate to the Pipelines page in Azure Pipelines, select the pipeline you created, and choose Edit in the context menu of the pipeline to open the YAMLeditor for the pipeline. · This pipeline runs whenever your team pushes a change to the main branch of your repo or creates a pull request. It runs on a Microsoft-hosted Linux machine. · The pipeline process has a single step, which is to run the Maven task. YAML:Code trigger: - main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdk Version: "1.10" jdk11_windows: imageName: "windows-latest" jdk Version: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdk VersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: " ** /TEST -*. xml"goals: "package" Change the platform to build · Navigate to the editor for your pipeline by selectingEdit pipeline action on the build, or by selecting Edit from the pipeline's main page. · To choose a different platform like Windows or Mac, change thevmImage value: · pool: · vmImage: "windows-latest" Add steps . You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can usetasks for building, testing, publishing, or deploying your app. For Java, the

--- Page 159 ---
Choose a job to see the steps for that job From the < Jobs in run #20191 ... FabrikamFiber Build O Build Build 40s Initialize job 1s 2 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 Duration: 40s 6 Pool: Azure Pipelines Image: Ubuntu-16.04 7 > Job preparation parameters Checkout 3s CmdLine 25 Component Detect 32s Post-job: Checkout <1s Finalize Job <1s Deploy DeployWeb 10s Finalize build Report build status <1s steps view, you can review the status and details of eachstep. From the Moreactions you can toggle timestamps or view a raw log of all steps in the pipeline. Build 9 Pool: Azure Pipelines 2 Image: Ubuntu-22.04 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 View job raw log Toggle timestamps 6 Duration: 40s Job preparation parameters 10

--- Page 160 ---
Cancel and re-run a pipeline If the pipeline is running, you can cancel it by choosing Cancel. If the run has completed, you can re-run the pipeline by choosing Run new. Pipeline run more actions menu: #20191210.3 Update azure-pipelines.yml for Azure Pip ... on FabrikamFiber Cancel Stages Jobs € Build C Deploy 0/1 compléted 48s Not started Build 48s Cancel From the More actions menu you can download logs, add tags, edit the pipeline, delete the run, and configure retention for the run. cure Pipe ... Run new Download logs Add tags Edit pipeline View retention leases Retain Delete 20

--- Page 161 ---
Add a status badge to your repository Many developers like to show that they're keeping their code quality high by displaying a status badge in their repo. Azure Pipelines succeeded To copy the status badge to your clipboard: 1. In Azure Pipelines, go to the Pipelines page to view the list of pipelines.Select the pipeline you created in the previous section. 2. Select , and then select Status badge. 3. Select Status badge. 4. Copy the sample Markdown from the Sample markdown section. Now with the badge Markdown in your clipboard, take the following steps in GitHub: 1. Go to the list of files and select Readme.md. Select the pencil icon to edit. 2. Paste the status badge Markdown at the beginning of the file. 3. Commit the change to the main branch. 4. Notice that the status badge appears in the description of your repository. To configure anonymous access to badges for private projects: 1. Navigate to Project Settings in the bottom left corner of the page 2. Open the Settings tab under Pipelines 3. Toggle the Disable anonymous access to badges slider under General 21

--- Page 162 ---
Azure-Pipeline • Azure Pipelineș provides a YAML pipeline editor that you can use to author and edit your pipelines. • The YAML editor is based on the Monaco Editor. • The editor provides tools like Intellisense support anda task assistant to provide guidance while you edit a pipeline. • You can also edit pipelines by modifying the azure pipelines.yml file directly in your pipeline's repository using a text editor of your choice Edit a YAML pipeline To access the YAML pipeline editor, do the following steps. • Sign in to your organization • (https://dev.azure.com/{yourorganization}). • Select your project, choose Pipelines, and then select the pipeline you want to edit. You can browse pipelines by Recent, All, and Runs. • Choose Edit. • Make edits to your pipeline using Intellisense and the task assistant forguidance. Azure DevOps fabrikam-tailspin FabrikamFiber F FabrikamFiber + Pipelines Overview Recent All Runs Boards Pipelines Recently run pipelines Repos 1 Pipeline Pipelines Pipelines FabrikamFiber 2 3 17

--- Page 131 ---
Course Code/Title:CS3V15/Devops Unit: IV Ensure a service is stopped: $ ansible webservers -m service -a "name=httpd state=stopped" 6. Gathering Facts Fact represents the discovered variables about a system. You can use the facts to implement conditional execution of tasks, and also used to get ad-hoc information about your systems. To see all the facts: $ ansible all -m setup Ansible Playbooks Playbooks are the files where the Ansible code is written. Playbooks are written in YAML format. YAML means "Yet Another Markup Language," so there is not much syntax needed. Playbooks are one of the core features of Ansible and tell Ansible what to execute, and it is used in complex scenarios. They offer increased flexibility. Playbooks contain the steps which the user wants to execute on a particular machine. And playbooks are run sequentially. Playbooks are the building blocks for all the use cases of Ansible. Ansible playbooks tend to be more configuration language than a programming language. Through a playbook, you can designate specific roles to some of the hosts and other roles to other hosts. By doing this, you can orchestrate multiple servers in very different scenarios, all in one playbook. Playbook Structure Each playbook is a collection of one or more plays. Playbooks are structured by using Plays. There can be more than one play inside a playbook. Playbook Play Task Module Target Environment O Task Module Play Task Target Environment Play Module O Task CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY 17 N33 nirf 1750 Rank

--- Page 132 ---
Course Code/Title:CS3V15/Devops Unit:IV name: install and configure DB hosts: testServer become: yes vars: oracle_db_port_value : 1521 tasks: -name: Install the Oracle DB yum: < code to install the DB> -name: Ensure the installed service is enabled and running service: name: < your service name> The Different YAML Tags name This tag specifies the name of the Ansible playbook. As in what this playbook will be doing. Any logical name can be given to the playbook. hosts This tag specifies the lists of hosts or host group against which we want to run the task. The hosts field/tag is mandatory. It tells Ansible on which hosts to run the listed tasks. The tasks can be run on the same machine or on a remote machine. One can run the tasks on multiple machines and hence hosts tag can have a group of hosts' entry as well. vars Vars tag lets you define the variables which you can use in your playbook. Usage is similar to variables in any programming language. tasks All playbooks should contain tasks or a list of tasks to be executed. Tasks are a list of actions one needs to perform. A tasks field contains the name of the task. This works as the help text for the user. It is not mandatory but proves useful in debugging the playbook. Each task internally links to a piece of code called a module. A module that should be executed, and arguments that are required for the module you want to execute. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 18 N33 nirf 1750 Rank

--- Page 143 ---
Unit: V Course Code/Title: CS3V15/Devops Email preferences Send me occasional product updates, announcements, and offers. Verify your account Pick one square that shows two identical objects. wiki How Step 4: Click the green Create account button. It's belowthe form, at the bottom of the page. This will take you to an email verification page. Verify your account Create account By creating an account, you agree to the Terms of Service. For more information about GitHub's privacy practices, see the GitHub Privacy Statement. We'll occasionally send you account-related wikiHow UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 3

--- Page 144 ---
Course Code/Title: CS3V15/Devops Unit: V Carefully review the Terms of Service at https://help.github.com/en/articles/github- terms-of-service and the Privacy Statement at https://help.github.com/en/articles/github- privacy-statement before you continue. You're almost done! We sent a launch code to wikihowneveconcepts@gmail.com + Enter code Didn't get your email? Resend the code or update your email address. wiki How Step 5: Verify your email by entering the code. After clicking Create account, you'll receive an email with a code.Enter this code on the verification page. Entering the code will automatically take you to the welcome page. Step 6: Select your preferences and click Continue. GitHub displays a quick survey that can help you tailor your experience to match what you're looking for. You'll be sent tothe plan selection page after completing the survey. How many team members will be working with you? This will help us guide you to the tools that are best suited for your projects. Just me 2 - 5 5 - 10 10 - 20 20 - 50 50+ Are you a student or teacher? Student Teacher Continue wikiHow CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 4 175° Rank ---

--- Page 121 ---
Course Code/Title:CS3V15/Devops Unit: IV · - m ping specifies the "ping" module, a basic module that checks the reachability of the managed nodes. . - i inventory.yaml points to the inventory file containing the list of devices. · The "ping" module sends a test command to the target devices and reports their status, confirming whether they are responsive. Output and Result: auriga@auriga-Latitude-E7470 :- /Desktop/ansibleŞ ansible all -[ inventory.yaml -m ping VOOS | UNREACHABLET => ( "changed's false. "mg's "Failed to connect to the host via s "unreachable": true auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ton denied (publickey_ password) In the above example the ssh host is not reachable. Activities Terminal Nov 2 12:13 . auriga@auriga-Latitude-E7470: - /Desktop/ansible C auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -i inventory.yaml -m ping vn81 | SUCCESS => 'ansible_facts": { "discovered_interpreter_python": "/usr/bin/python3" "changed": false, "ping": "pong" auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ In the above output image it is working fine Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: 1ansible all -i inventory.yaml -m apt -a "name=nginx state=present" -- become -- ask-become-pass · - m apt specifies the "apt" module, which is responsible for package management. · - a "name=nginx state=present" includes variables for the module to act upon, such as specifying that Nginx should be installed (state=present). · - become signifies that the command should run with elevated privileges (sudo). · - ask-become-pass prompts for the sudo password of the managed node to ensure the installation proceeds smoothly. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 7 N32 nirf 1750 Rank

--- Page 122 ---
Course Code/Title:CS3V15/Devops Unit:IV Output and Result: Activities Terminal * Nov 2 12:24 · auriga@auriga-Latitude-E7470: - /Desktop/ansible F X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -[ inventory.yaml -m apt -a "name=ansible state=present" -- become - - ask-become-pass BECOME password: vm01 | CHANGED => { "ansible_facts": { "discovered_interpreter_python": * /usr/bin/python3" "cache_update_time": 1698985056, "cache_updated": false, "changed": true, "stderr": "" 'stderr_Lines": []. stdout": "Reading package lists ... \nBuilding dependency tree ... \nReading state information ... \nThe following packages were automatically installed and are no longer required:\n gtr1.2-goa-1.0 hplip-data libcgt-fast-perl libcgt-pm-perl\n libdouble-conversion3 Libevent-core-2.1-7 libevent-pthreads-2.1-7\n \ibfcgt-perl libfprint-2-todi libfwupdplugin1 libht ml-template-perl\n Libpcre2-16-0 libqtScoresa libqtsdbuss libqtsguis libqt5network5\n libqtSpositionings libqt5printsupports libqt5qm15 Libqtsquick5\n libqtSsensors5 libqt5svg5 Li bqt5webchannel5 libqt5webkit5 libqt5widgets5\n Libxcb-xinerama@ Libxcb-xinpute Libxmlbi printer-driver-postscript-hp\n python3-renderpm python3-reportlab python3-reportlab-accel\n qt5-gtk-platformthene qttranslations5-lien shim\nUse 'sudo apt autoremove' to remove them. InThe following additional packages will be installed:\n teee-data python3-argcomplete pyt hon3-crypto python3-dnspython\n python3-jinja2 python3 . jmespath python3-kerberos python3-libcloud\n python3-netaddr python3-ntin-auth python3-requests-kerberos\n python3-requests- ntlm python3-selinux python3-winrm python3-xmltodict\nSuggested packages: \n cowsay sshpass python- jinja2-doc ipython3 python-netaddr-docs\nThe following NEW packages will be install ed:\n ansible leee-data python3-argcomplete python3-crypto python3-dnspython\n python3- jinja2 python3-jmespath python3-kerberos python3 . Libcloud\n python3-netaddr python3-ntlm-aut h python3-requests-kerberos\n python3-requests-ntIm python3-selinux python3-winrm python3-xmltodict\n0 upgraded, 16 newly installed, 0 to remove and 50 not upgraded. InNeed to get 97 25 kb of archives. \nAfter this operation, 98.6 MB of additional disk space will be used. \nGet:1 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-jinja2 all 2.10.1-2 [95.5 KB]\nGet:2 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-crypto amd64 2.6.1-13ubuntu2 [237 KB]\nGet:3 http://in.archive.ubuntu.com/ubuntu focal-updates/main amd64 pyt hon3-dnspython all 1.16.0-1ubuntu1 [89.2 KB]\nGet:4 http://in.archive.ubuntu.com/ubuntu focal/main amd64 ieee-data all 20180805.1 [1589 KB]\nGet: 5 http://in.archive. ubuntu.com/ubuntu focal-updates/main amd64 python3-netaddr all 0.7.19-3ubuntu1 [236 KB]\nGet:6 http://in.archive.ubuntu.com/ubuntu focal/universe and64 ansible all 2.9.6+dfsg-1 [5794 KB]\nGet:7 http: //\n.archive.ubuntu.com/ubuntu focal/universe amd64 python3-argcomplete all 1.8.1-1.3ubuntu1 [27.2 kB]\nGet:8 http://in.archive.ubuntu.com/ubuntu focal-updates/main and64 python3- jne spath all 0.9.4-2ubuntu1 [21.5 kB]\nGet:9 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-kerberos amd64 1.1.14-3.1build1 [22.6 k8]\nGet: 10 http://in.archive.ubuntu. con/ubuntu focal/universe amd64 python3-libcloud all 2.8.0-1 [1483 KB]\nGet:11 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-ntlm-auth all 1.1.0-1 [19.6 KB]\nGet: 1 2 http://in.archive.ubuntu.com/ubuntu focal/universe and64 python3-requests-kerberos all 0.12.0-2 [11.9 KB]\nGet: 13 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-r equests-ntlm all 1.1.0-1 [6004 B]\nGet: 14 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-selinux amd64 3.0-1build2 [139 KB]\nGet : 15 http://in.archive.ubuntu.com/ubu ntu focal/universe and64 python3-xmltodict all 0.12.0-1 [12.6 KB]\nGet:16 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-winrm all 0.3.0-2 [21.7 KB]\nFetched 9725 k B in 7s (1454 KB/s)\nSelecting previously unselected package python3-jinja2.\r\n(Reading database ... \r(Reading database ... 5*\r (Reading database .. . 10%\r(Reading database ... 15% \r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database . .. 45%\r (Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 68%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading da tabase ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 212542 files and directories currently installed.)\r\nPreparin g to unpack ... /00-python3-jinja2_2.10.1-2_all.deb ... \r\nUnpacking python3-jinja2 (2.10.1-2) ... \r\nSelecting previously unselected package python3-crypto. \r\nPreparing to unpack .. ./01-python3-crypto_2.6.1-13ubuntu2_amd64.deb ... \r\nUnpacking python3-crypto (2.6. 1-13ubuntu2) ... \r\nSelecting previously unselected package python3-dnspython. \r\nPreparing to unpa ck ... /02-python3-dnspython_1.16.0-lubuntu1_all.deb ... \r\nUnpacking python3-dnspython (1. 16.0-1ubuntu1) ... \r\nSelecting previously unselected package leee-data. \r\nPreparing to unp ack ... /03-ieee-data_20180805.1_all.deb ... \r\nUnpacking teee-data (20188805.1) ... \r\nSelecting previously unselected package python3-netaddr. \r\nPreparing to unpack ... /04-python3. netaddr_0.7.19-3ubuntu1_all.deb ... \r\nUnpacking python3-netaddr (0.7.19-3ubuntu1) ... \r\nSelecting previously unselected package ansible. \r\nPreparing to unpack .. . /05-ansible_2.9.6 +dfsg-1_all.deb ... \r\nUnpacking ansible (2.9.6+dfsg-1) ... \r\nSelecting previously unselected package python3-argcomplete. \r\nPreparing to unpack .. . /06-python3-argcomplete_1.8.1-1. 3ubuntu1_all.deb ... \r\nUnpacking python3-argcomplete (1.8.1-1.3ubuntu1) ... \r\nSelecting previously unselected package python3- jnespath. \r\nPreparing to unpack ... /07-python3- jnespa th_0.9.4-2ubuntul_all.deb ... \r\nUnpacking python3-jmespath (0.9.4-2ubuntu1) ... \r\nSelecting previously unselected package python3-kerberos. \r\nPreparing to unpack ... /08-python3-ke rberos_1.1.14-3.1build1_and64.deb ... \r\nUnpacking python3-kerberos (1.1.14-3.1build1) ... \r\nSelecting previously unselected package python3-libcloud. \r\nPreparing to unpack .. . /09- python3-libcloud_2.8.0-1_all.deb ... \r\nUnpacking python3-libcloud (2.8.0-1) ... \r\nSelecting previously unselected package python3-ntlm-auth. \r\nPreparing to unpack ... /10-python3-n tim-auth_1.1.0-1_all.deb .. . \r\nUnpacking python3-ntlm-auth (1.1.0-1) ... \r\nSelecting previously unselected package python3-requests-kerberos. \r\nPreparing to unpack ... /11-python3. requests-kerberos 0.12.8-2 all. deb .... \r\nUnpacking python3-requests-kerberos (0.12.0-2) ... \r\nSelecting previously unselected package python3-requests-ntln. \r\nPreparing to unpack By using these ad-hoc commands, you can perform quick, task-specific operations on your managed nodes, saving time and effort in your automation tasks. CommonModules: Ansible offers an extensive library of modules for various tasks, from system administration to application deployment. You can explore the full list of Ansible modules in the official documentation to find the most suitable module for your specific requirements. BestPractices: When working with ad-hoc commands, consider using SSH key-based authentication for secure, passwordless access. It streamlines the authentication process and enhances the security of your automation tasks. Using Ansible Playbooks Introduction: In the world of Ansible, playbooks are the cornerstone of structured and complex automation. While ad- hoc commands are perfect for quick and isolated tasks, playbooks step in when you need a well- orchestrated sequence of operations. They allow you to tackle multi-step automation scenarios where tasks depend on each other, and conditional actions are required. Let's delve deeper into the importance of playbooks: The Role of Playbooks: Playbooks serve as the framework for orchestrating automation tasks that involve a sequence of operations. Whether it's configuring servers, deploying applications, or managing infrastructure, playbooks provide a structured way to define how these tasks are executed. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N33 nirf 1750 Rank

--- Page 53 ---
Course Code/Title:CS3V15/Devops Unit: II <execution> <phase>test</phase> <goals> <goal>run</goal> </goals> <configuration> <tasks> <echo>Using env.test.properties</echo> <copy file="src/main/resources/env.test.properties" tofile="${project.build.outputDirectory}/env.properties"/> </tasks> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> Now open the command console, go to the folder containing pom.xml and execute the following mvn command. Pass the profile name as argument using -P option. C:\MVN\project>mvn test -Ptest Maven will start processing and displaying the result of test build profile. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 12 N32 nirf 1750 Rank

--- Page 54 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Maven Settings Open Maven settings.xml file available in %USER_HOME%/.m2 directory where %USER_HOME% represents the user home directory. If settings.xml file is not there, then create a new one. Add test profile as an active profile using active Profiles node as shown below in example. <settings xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"> <mirrors> <mirror> <id>maven.dev.snaponglobal.com</id> <name>Internal Artifactory Maven repository</name> <url>http://repo1.maven.org/maven2/</url> <mirrorOf> *< /mirrorOf> /mirror </mirrors> <activeProfiles> <activeProfile>test</activeProfile> </activeProfiles> </settings> Now open command console, go to the folder containing pom.xml and execute the following mvn command. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 13 N33 nirf 1750 Rank

--- Page 51 ---
Course Code/Title:CS3V15/Devops Unit: II 7. Maven Profiles: A Build profile is a set of configuration values, which can be used to set or override default values of Maven build. Using a build profile, you can customize build for different environments such as Production v/s Development environments. Types of Build Profile Build profiles are majorly of three types. Type Where it is defined Per Project Defined in the project POM file, pom.xml Per User Defined in Maven settings xml file (%USER_HOME%/.m2/settings.xml) Global Defined in Maven global settings xml file (%M2_HOME%/conf/settings.xml) Profile Activation A Maven Build Profile can be activated in various ways. · Explicitly using command console input. · Through maven settings. · Based on environment variables (User/System variables). · OS Settings (for example, Windows family). · Present/missing files. Now, under src/main/resources, there are three environment specific files - Sl.No. File Name & Description env.properties 1 default configuration used if no profile is mentioned. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 10 N33 nirf 1750 Rank

--- Page 52 ---
Course Code/Title:CS3V15/Devops Unit: II env.test.properties 2 test configuration when test profile is used. 3 env.prod.properties production configuration when prod profile is used. Explicit Profile Activation In the following example, we will attach maven-antrun-plugin:run goal to test the phase. This will allow us to echo text messages for different profiles. We will be using pom.xml to define different profiles and will activate profile at command console using maven command. Assume, we've created the following pom.xml in C:\MVN\project folder. <project xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation = "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.companyname.projectgroup</groupId> <artifactId>project</artifactId> <version>1.0</version> <profiles> <profile> <id>test</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <version>1.1</version> <executions> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 11 N33 nirf 1750 Rank

--- Page 45 ---
Course Code/Title:CS3V15/Devops Unit: II Example for pom.xml file <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> 4. Maven Repository A maven repository is a directory of packaged JAR file with pom.xml file. Maven searches for dependencies in the repositories. There are 3 types of maven repository: 1. Local Repository 2. Central Repository 3. Remote Repository Maven searches for the dependencies in the following order: Local repository then Central repository then Remote repository. Local Central Remote Repository Repository Repository CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 4 N32 nirf 1750 Rank -

--- Page 46 ---
Course Code/Title:CS3V15/Devops Unit: II If dependency is not found in these repositories, maven stops processing and throws an error. 1) Maven Local Repository Maven local repository is located in your local system. It is created by the maven when you run any maven command. By default, maven local repository is %USER_HOME%/.m2 directory. For example: C:\Users\SSS IT\.m2. Update location of Local Repository We can change the location of maven local repository by changing the settings.xml file. It is located in MAVEN_HOME/conf/settings.xml, for example: E:\apache-maven- 3.1.1\conf\settings.xml. 2) Maven Central Repository Maven central repository is located on the web. It has been created by the apache maven community itself. The path of central repository is: http://repo1.maven.org/maven2/. The central repository contains a lot of common libraries that can be viewed by this url http://search.maven.org/#browse. 3) Maven Remote Repository Maven remote repository is located on the web. Most of libraries can be missing from the central repository such as JBoss library etc, so we need to define remote repository in pom.xml file. Let's see the code to add the jUnit library in pom.xml file. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 5 N32 narf 1750 Rank -

--- Page 57 ---
Course Code/Title:CS3V15/Devops · Go to the File option · In the drop-down menu, select New · Select the Project option Unit: II If you want to create a Java project, you can select the "Java Project" option. Since we are not creating a Java project specifically, we have chosen the "Project" option. eclipse-workspace - Eclipse File Edit Source Refactor Navigate Search Project Run Window Help New Open File ... G) Open Projects from File System ... Shift+Alt+N Close Close All Shift + Cb1+W Save Save As. Save All Shift+Ctrl+S Revert Move .:. Rename 2 Refresh ES Convert Line Delimiters To Print Import ... Export ... Ctrl+P de java Project ui Project ... El Package G Class Interface G Enum @ Annotation 69 Source Folder 18 Java Working Set Folder File Untitled Text File Task JUnit Test Case Example ... Other ... Ctrl+N The dialog box that appears on the screen will display different types of projects. · Select the Maven Project option . Click on Next New Project + Select a wizard Create a Maven Project Wizards: type filter text irroject Gradle Java 2º Java Project #Java Project from Existing Ant Buildfile - @Maven Lu Check out Maven Projects from SCM M& Maven Module Maven Project Examples ? < Back Next > X Cancel Finish A dialog box will appear. Select the default workspace. . Click on "Next" Several Group IDs, Artifact IDs, and Versions will then appear. · Select a plugin there and click on "Next" CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 16 N33 narf 1750 Rank

--- Page 58 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Select an Archetype M Catalog: All Catalogs Configure .. Biter: × Group Id Artifact id Version org.apache.maven.archetypes maven-archetype-archetype 1.0 org.apache.maven.archetypes maven-archetype-|2ee-simple 1.0 org.apache.maven archetypes maven-archetype-plugin 1.2 org.apache.maven.archetypes maven-archetype-plugin-site 1.1 org.apache.maven.archetypes maven-archetype-portlet 1.0.1 org.apache.maven.archetypes maven-archetype-profiles 1.0-alpha-4 org.apache.maven.archetypes maven-archetype-quickstart 1.1 org.apache.maven.archetypes maven-archetype-site 11 org anacha mauan archabenes masan.archchina.cito.cimnia 11 An archetype which contains a sample Maven project. Show the last version of Archetype only Include snapshot archetypes Add Archetype ... > Advanced ? < Back Next > Cancel Finish In the next dialog box that appears, you'll complete the following steps: · Enter the Group ID “com.xyz" · Enter the Artifact ID "mavenproject" · The version will appear on the screen These items can all be modified at a later time if needed. · Click on "Finish" CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 17 N33 nirf 1750 Rank

--- Page 21 ---
Unit: I Course Code/Title:CS3V15/Devops · Gaming: AWS has been serving many gaming studios. Combining Amazon EC2 and S3 services with CloudFront enables gaming websites to deliver high-quality gaming experiences to their customers regardless of location. Use Cases of AWS · Netflix Netflix is an entertainment platform that started in the United States, but eventually, it expanded to many countries and soon became popular. However, once Netflix confronted the scalability problem because of the sudden increase in viewers. That made Netflix choose AWS services. Netflix reports that when it started using AWS services like DynamoDB and Cassandra for its distributed databases, it could handle the data easily. So, scalability is a great advantage of AWS. Netflix has adapted around 100,000 server instances from AWS for computing and storage databases, analytics, recommendation engines, and video transcoding as well. . McDonald's McDonald's is the world's largest fast-food company that serves around 64 million people per day. The growth of this company has gone to another level when it started home deliveries. By utilizing AWS services, McDonald's created a platform that integrates local restaurants with delivery partners such as Uber Eats. Scalability is also a reason for the company to choose AWS services. Moreover, with AWS Microservices Architecture, McDonald's platform can scale 20,000 orders per second and integrate with the global partners easily. · Airbnb Airbnb is an international online marketplace for rental homes. This platform connects people who are looking for rental accommodation with those who want to rent out their houses. Quite soon, Airbnb became unable to handle the constant streaming of data on the website from its customers. That is when it started using Amazon EC2 service and Elastic Load Balancing, which distributes incoming traffic to multiple Amazon EC2 instances. In this way, Airbnb could avoid traffic, and customers could use the online platform without any disruption. • Novartis Novartis is the best example for AWS use cases in healthcare. Novartis is one of the world's largest healthcare companies that provides solutions for patients' well-being. It adapted Amazon EC2 services and built a platform using other services such as Amazon Simple Storage Service, Amazon Elastic Block Store, and four availability zones. Data Analysts of Novartis are taking advantage of the AWS services and still implementing new solutions for the patients. · Expedia Expedia is a worldwide online travel agency that has always focused on the constant development and innovation of its platform to offer an extraordinary user experience for its clients. Since 2010, Expedia has been using AWS services to build a standard deployment model for better infrastructure as AWS offers the best data security through different availability zones. • Samsung If you are using Samsung mobile phones, then you may know about the Samsung app store. For setting up the apps stacked in its store, the company started using AWS services. Using AWS app development services, Samsung wanted to provide its customers with the facility to download the apps anywhere without any network traffic. • NASA NASA (National Aeronautics and Space Administration) has always wondered about creating a library to present people with all its achievements through pictures and videos of space. Later on, it created such platforms, but because it had 10 different NASA centers, it couldn't provide the best experience for viewers. So, all it wanted was to create an easy-access platform for TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 21 N32 nirf 1750 Rank

--- Page 22 ---
Unit: I Course Code/Title:CS3V15/Devops people to search for and view images and videos. Then, NASA started adopting many services from AWS to solve this problem, which included Amazon Elastic Compute Cloud, Elastic Load Balancing, Amazon Simple Storage Service, Amazon Simple Queue Service, etc. Among these, Amazon S3 helped the company store all the incoming data such as photos, videos, and audio files without any hassle. • Facebook Facebook, without a doubt, is a widespread social media platform. To build a scalable application, Facebook used services such as Amazon Elastic Compute Cloud, Amazon Simple Storage Service, Amazon Relational Database Service, Amazon SimpleDB, Amazon CloudFront, Amazon Simple Queue Service, etc. Amazon RDS helps the platform to make it easy to set up, operate, and scale the database in the cloud. Various Services offered by AWS · Amazon EC2 (Elastic Cloud computing) · Amazon RDS (Relational Database Services) · Bonus Service: Amazon Connect · Amazon S3 (Simple Storage Service) · Amazon Lambda · Amazon Cognito · Amazon Glacier · Amazon SNS (Simple Notification Service) · Bonus Service: Amazon Lex · Amazon Lightsail · Amazon VPC (Virtual Private Cloud) · Amazon Kinesis · Amazon Inspector · Amazon Auto-scaling · Amazon IAM (Identity and Access Management) · Dynamo DB · Amazon SQS (Simple Queue Service) · Amazon ElastiCache · Amazon Chime · AWS Athena · Code Catalyst · Web Application Firewall · AWS Amplify · AWS Rekognition · AWS QuickSight · AWS CloudFormation · AWS Management Console The Important Cloud Services according to various categories that are provided by AWS are given below : 1. Compute • Amazon EC2: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It allows organizations to obtain and configure virtual compute capacity in the cloud. You can select from a variety of operating systems and resource configurations like memory, CPU, and storage that are required for your TECHNOLOGY (Autonomous) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY 22 N33 narf 1750 Rank

--- Page 7 ---
Course Code/Title:CS3V15/Devops Unit: I DevOps Tools 1. Git (GitLab, GitHub, Bitbucket) Git remains indispensable in software development and DevOps due to its pivotal role in version control, collaborative coding, and efficient project management. As technology has accelerated, the need for streamlined and organized code management has never been greater. Git empowers developers to collaborate on codebases, effortlessly creating and merging branches for new features and bug fixes. Its distributed nature ensures developers can work seamlessly offline, an increasingly valuable feature in today's remote and distributed work environments. Additionally, Git facilitates the tracking of code modifications, making it easier to identify when and why specific changes were made, a critical aspect of maintaining code quality and security. Software development is essential in driving innovation and advancing progress, and Git maintains its prominent position as the bedrock of efficient, cooperative, and secure coding methodologies. 2. Maven Due to its enduring significance in managing project dependencies, building, and project lifecycle management, Maven remains a pivotal tool in SD and DevOps. As a robust build automation and project management tool, Maven simplifies the complexities of Java-based project development by streamlining the compilation, testing, packaging, and distribution processes. It ensures consistent and reproducible builds, making it easier for development teams to collaborate efficiently and deliver high-quality software. Maven's role in managing dependencies and facilitating continuous integration and deployment remains crucial. Its ability to handle complex build scenarios and integrate seamlessly with modern DevOps practices makes it indispensable for ensuring software projects' reliability, maintainability, and scalability in 2024 and beyond. 3. Jenkins Its importance lies in its role as a powerful automation server that enables continuous integration and continuous delivery (CI/CD) pipelines. Jenkins streamlines software development by automating tasks such as building, testing, and deploying code changes, ensuring that software is delivered quickly and highly. With the growing complexity of modern applications, the need for efficient CI/CD processes has become even more paramount. Jenkins provides flexibility, extensibility, and a vast library of plugins that cater to a wide range of technologies and tools, making it adaptable to diverse development environments. As organizations prioritize speed, reliability, and collaboration in their software development practices, Jenkins stands as a cornerstone tool, enabling teams to achieve seamless automation and efficient delivery of software solutions. 4. Chef Chef, a powerful automation platform, is crucial in managing infrastructure as code. Chef empowers organizations to achieve scalability, reliability, and speed seamlessly. By allowing the automation of server provisioning, configuration, and maintenance, Chef enhances efficiency and consistency across the entire infrastructure, reducing manual errors and ensuring that infrastructure remains desired. Moreover, Chef integrates smoothly with various cloud providers, containerization technologies, and other DevOps tools, making it adaptable to the ever-evolving tech TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 7 N32 nirf 1750 Rank

--- Page 8 ---
Course Code/Title:CS3V15/Devops Unit: I landscape. As organizations prioritize agility and scalability, Chef remains a vital tool in automating complex infrastructure tasks and enabling DevOps teams to focus on innovation and delivery. 5. Puppet Puppet is essential because it simplifies the management and orchestration of complex IT infrastructures by allowing administrators to define infrastructure as code. It ensures consistency and repeatability in configuration across servers, cloud instances, and containers. Businesses increasingly rely on diverse, dynamic, and hybrid infrastructures. Puppet's importance lies in its ability to streamline provisioning, configuration, and continuous compliance, thus reducing operational complexity, minimizing errors, and accelerating software delivery. Puppet continues to empower organizations to efficiently manage and scale their infrastructure while maintaining high levels of security and compliance, making it a crucial tool for DevOps teams. 6. Ansible Ansible is a powerful and widely adopted automation and configuration management tool important in 2024 for several reasons. This tool stands out for its simplicity and versatility. It empowers organizations to automate repetitive tasks, provisioning of infrastructure, and configuration management across diverse environments, making it an invaluable asset for DevOps and IT teams. Furthermore, Ansible's agentless architecture, declarative language, and a vast library of pre- built modules make it accessible to both beginners and seasoned professionals. As organizations prioritize efficiency, scalability, and the rapid deployment of applications and services, Ansible remains an indispensable DevOps toolkit, helping teams streamline operations, enhance security, and maintain infrastructure at scale, all while reducing manual errors and increasing agility in a fast-paced technological landscape. 7. Docker Docker is crucial in modern software development and DevOps practices. It can simplify and streamline the management of applications across various environments. Docker containers encapsulate an app and its dependencies, ensuring consistent and reproducible deployments from development to production. This technology enhances portability and scalability, accelerates development cycles, and reduces the "it works on my machine" problem. In a rapidly evolving software landscape, Docker's containerization approach remains crucial for achieving efficient, isolated, and highly flexible application deployment, making it an essential component of DevOps and continuous delivery pipelines. 8. Kubernetes Kubernetes, often abbreviated as K8s, play a central role in modern software development and operations. Its importance lies in its ability to orchestrate, manage, and automate containerized applications at scale. As organizations increasingly embrace microservices architectures and containerization for their applications, Kubernetes provides the essential infrastructure for deploying, scaling, and maintaining these containers efficiently. The tool's resilience, self-healing capabilities, and support for hybrid and multi-cloud environments make it vital for achieving agility, reliability, and cost-effectiveness in application deployment. It serves as the backbone of cloud-native ecosystems, enabling organizations to accelerate software delivery, improve resource utilization, and respond effectively to the evolving demands of the digital landscape. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N32 nirf 1750 Rank

--- Page 17 ---
Course Code/Title:CS3V15/Devops Unit: I that failed in a test suite gets simpler. Also, we can schedule the execution of the test cases at predefined times. After testing, the code is continuously integrated with the existing code. 4) Continuous Monitoring Monitoring is a phase that involves all the operational factors of the entire DevOps process, where important information about the use of the software is recorded and carefully processed to find out trends and identify problem areas. Usually, the monitoring is integrated within the operational capabilities of the software application. 5) Continuous Feedback The application development is consistently improved by analyzing the results from the operations of the software. This is carried out by placing the critical phase of constant feedback between the operations and the development of the next version of the current software application. The continuity is the essential factor in the DevOps as it removes the unnecessary steps which are required to take a software application from development, using it to find out its issues and then producing a better version. It kills the efficiency that may be possible with the app and reduce the number of interested customers. 6) Continuous Deployment In this phase, the code is deployed to the production servers. Also, it is essential to ensure that the code is correctly used on all the servers. The new code is deployed continuously, and configuration management tools play an essential role in executing tasks frequently and quickly. Here are some popular tools which are used in this phase, such as Chef, Puppet, Ansible, and SaltStack. Containerization tools are also playing an essential role in the deployment phase. Vagrant and Docker are popular tools that are used for this purpose. These tools help to produce consistency across development, staging, testing, and production environment. They also help in scaling up and scaling down instances softly. Containerization tools help to maintain consistency across the environments where the application is tested, developed, and deployed. There is no chance of errors or failure in the production environment as they package and replicate the same dependencies and packages used in the testing, development, and staging environment. It makes the application easy to run on different computers. 7) Continuous Operations All DevOps operations are based on the continuity with complete automation of the release process and allow the organization to accelerate the overall time to market continuingly. It is clear from the discussion that continuity is the critical factor in the DevOps in removing steps that often distract the development, take it longer to detect issues and produce a better version of the product after several months. With DevOps, we can make any software product more efficient and increase the overall count of interested customers in your product. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 17 N32 nirf 1750 Rank

--- Page 18 ---
Course Code/Title:CS3V15/Devops Unit: I AWS AWS stands for Amazon Web Services, It is an expanded cloud computing platform provided by Amazon Company. AWS provides a wide range of services with a pay-as-per-use pricing model over the Internet such as Storage, Computing power, Databases, Machine Learning services, and much more. AWS facilitates for both businesses and individual users with effectively hosting the applications, storing the data securely, and making use of a wide variety of tools and services improving management flexibility for IT resources. Advantages & Features of AWS: 1. Cost savings: One of the biggest benefits of AWS is that it can help businesses save money. As mentioned previously, businesses can avoid the high upfront costs of traditional infrastructure with AWS and pay only for the resources they use. Traditionally, businesses had to invest in hardware and software upfront, which often led to overspending. Let's look at this for example - if a business needs to run a website that gets 1000 visitors per day, they would need to purchase and maintain enough servers to support this traffic. With AWS, the business only pays for the compute resources they use when someone visits their website. This can result in significant cost savings. 2. Flexibility: Another key benefit of AWS is its flexibility. Businesses are able to customize their virtual environment - whether the operating system, database, programming language, or something else - to meet their specific needs. Especially in today's climate, the migration process to the cloud should be as frictionless as possible - and AWS makes that possible. Regardless of your use case or industry, AWS can be tailored to fit your needs, whether you're looking for a single cloud- hosted application or an entire suite of integrated solutions. 3. Reliability: AWS is known for being reliable, with an uptime of 99.9%. This makes it a great platform for mission-critical applications that need to be available 24/7. AWS also offers the ability to deploy resources across multiple availability zones for even greater reliability. The cloud platform also has a number of features that make it easier to ensure reliability, such as autoscaling and auto-healing. Autoscaling allows businesses to automatically scale their resources up or down based on demand, while auto-healing enables them to quickly identify and replace any faulty components. 4. Security: Businesses can take advantage of advanced security features, such as identity and access management, to help protect their data. Their tough infrastructure with an end-to-end approach is designed to withstand attacks and AWS provides customers with tools to help them monitor and respond to threats. When it comes to storage, Amazon S3 provides customers with a secure and reliable way to store and access data. The service is designed to be highly scalable and resilient, with built-in redundancy. Fine-grain identity and access controls can be applied to S3 buckets and objects, giving customers control over who has access to their data. Security tasks can be automated with AWS CloudFormation, making it easier for businesses to manage their security policies. And, you can rest easy knowing that AWS takes privacy seriously, with comprehensive customer data protection and compliance measures. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 18 N32 nirf 1750 Rank

--- Page 9 ---
Course Code/Title:CS3V15/Devops Unit: I 9. Slack Slack is a crucial tool for businesses and organizations worldwide. Its significance lies in facilitating seamless communication and collaboration among teams, whether working in the same office or remotely. Slack's real-time messaging, file sharing, and integration capabilities streamline workflow, enhance productivity and keep teams connected across different time zones and locations. As the work landscape evolves, with more companies embracing hybrid and remote work models, Slack is a vital hub for quick decision-making, project coordination, and knowledge sharing. With an ever- expanding ecosystem of integrations and features, Slack remains at the forefront of modern workplace communication, making it essential for businesses to stay agile, efficient, and competitive. 10. AWS Cloud Computing and Storage in DevOps AWS (Amazon Web Services) Cloud Computing and Storage are crucial in DevOps because they provide scalable, flexible, and cost-effective infrastructure for DevOps practices. AWS offers many services, including compute resources, databases, container orchestration, and serverless computing, which align perfectly with modern software development and deployment demands. Organizations adopt DevOps to accelerate software delivery. AWS provides the foundation for rapidly deploying and scaling applications, supporting continuous integration and continuous delivery (CI/CD) pipelines, and automating infrastructure provisioning through tools like AWS CloudFormation. Furthermore, AWS's storage solutions enable efficient data management, backup, and recovery, ensuring the resilience and reliability required for DevOps operations. As cloud technology evolves, AWS remains at the forefront, enabling DevOps teams to focus on innovation and efficiency. 11. Azure Cloud Computing and Storage in DevOps Azure Cloud Computing and Storage will be pivotal in DevOps practices in 2024 and beyond. Azure offers a comprehensive cloud ecosystem that enables organizations to scale their infrastructure, deploy applications, and store data efficiently. Azure provides essential services for continuous integration and continuous deployment (CI/CD), automation, monitoring, and security. Its cloud computing capabilities facilitate the provisioning of resources on demand, ensuring that development and testing environments are readily available. Azure's storage solutions, including Azure Blob Storage, Azure Files, and Azure SQL Database, enable secure data storage and retrieval, supporting the data-driven aspects of DevOps. Besides, Azure's integration with DevOps tools like Azure DevOps Services streamlines the software development lifecycle, enhancing collaboration and automation. 12. GCP Cloud Computing and Storage in DevOps Google Cloud Platform (GCP) offers robust cloud computing and storage solutions. GCP provides a scalable, reliable, and highly available infrastructure essential for modern DevOps practices. With its comprehensive set of services, including Google Compute Engine, Google Kubernetes Engine, Cloud Storage, and BigQuery, GCP empowers DevOps teams to build, deploy, and manage applications easily. Its emphasis on automation, infrastructure as code, and container orchestration aligns seamlessly with DevOps principles. Moreover, GCP's cutting-edge technologies, such as AI and machine learning capabilities, provide DevOps practitioners with advanced tools for monitoring, analytics, and automation, making it a powerful choice for organizations seeking to optimize their software development and delivery processes. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 10 ---
Course Code/Title:CS3V15/Devops Unit: I 13. Monitoring, Alerting, and Incident Response Tools: SignalFx Monitoring, alerting, and incident response tools like SignalFx are pivotal in DevOps and software development. As software systems become complex and distributed, the need for real-time visibility into performance and the ability to respond swiftly to incidents is significant. SignalFx excels in this regard by providing advanced monitoring and observability solutions that enable organizations to detect anomalies, trace issues across microservices proactively, and set up intelligent alerts. As applications scale, cloud-native architectures become the norm, and user expectations for reliability grow, SignalFx's capabilities are crucial. It empowers DevOps teams to ensure high availability, optimize resource utilization, and maintain a seamless user experience by identifying and addressing performance issues before they impact end-users. It is one of the most essential tools for modern software operations. 14. Appdynamics AppDynamics, a leading application performance management and monitoring platform, remains critically important as it ensures the optimal performance of modern digital businesses. As organizations rely on complex and distributed software systems, proactively monitoring, troubleshooting, and optimizing these applications becomes essential. AppDynamics provides real-time visibility into application performance, allowing businesses to swiftly identify bottlenecks, latency issues, and errors. With the ever-growing complexity of applications, the importance of AppDynamics lies in its ability to empower organizations to deliver exceptional user experiences, maintain application reliability, and swiftly respond to performance issues, thereby ensuring the continued success and competitiveness of digital businesses. 15. Raygun It is a crucial tool in software development and DevOps because it ensures application reliability and performance. Raygun is an application monitoring and error-tracking platform that empowers development teams to identify, diagnose, and resolve real-time issues. With software systems growing in complexity and the increased demand for seamless user experiences, Raygun's importance lies in providing actionable insights into application errors and performance bottlenecks. It enables organizations to proactively address issues, reduce downtime, and enhance user satisfaction, leading to higher software quality and improved customer experiences. Software is central to businesses across industries. Raygun's role in maintaining application health and facilitating rapid issue resolution makes it a fundamental tool for DevOps professionals and software developers. 16. Splunk Cloud Splunk Cloud helps organizations gain critical insights from the ever-expanding volume of data generated in today's digital landscape. As businesses increasingly rely on data- driven decision-making, Splunk Cloud stands out as a robust and scalable platform for monitoring, searching, analyzing, and visualizing machine-generated data. Its importance lies in providing real-time visibility into the health and performance of complex systems, applications, and infrastructures, enabling rapid incident detection and response. As cybersecurity threats evolve, Splunk Cloud's advanced security analytics and threat detection capabilities remain indispensable for safeguarding against cyberattacks and ensuring data integrity. In a world where data is a strategic asset, Splunk Cloud's role in harnessing the power of data for operational excellence and security cannot be overstated. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 10 N32 nirf 1750 Rank

--- Page 63 ---
Course Code/Title:CS3V15/Devops Unit: II 10. Introduction of Gradle: The list of features that Gradle provides, · Gradle is available with separate Domain Specific Language (DSL) based on Groovy language. · It provides the declarative language elements. Those elements also provide build-by- convention support for Java, Groovy, OSGI, Web and Scala. Language for dependency based programming The declarative language lies on a top of a general purpose task graph, which can be fully supported in the build. Structure your build Gradle allows you to apply common design principles to your build. It will give you a perfect structure for build, so that, you can design well-structured and easily maintained, comprehensible build. Deep API By using this API, you can monitor and customize its configuration and execution behavior to the core. Gradle scales Gradle can easily increase the productivity, from simple and single project builds to huge enterprise multi-project builds. Multi-project builds Gradle supports the multi-project builds and partial builds. If you build a subproject, Gradle takes care of building all the subprojects, that the subproject depends on. Different ways to manage your builds Gradle supports different strategies to manage your dependencies. Gradle is the first build integration tool Gradle is fully supported for your ANT tasks, Maven and lvy repository infrastructure for publishing and retrieving dependencies. It also provides a converter for turning a Maven pom.xml to Gradle script. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 22 N32 nirf 175º Rank -

--- Page 64 ---
Course Code/Title:CS3V15/Devops Unit: II Ease of migration Gradle can easily adapt to any structure. Therefore, you can always develop your Gradle build in the same branch, where you can build live script. Gradle Wrapper Gradle Wrapper allows you to execute the Gradle builds on machines, where Gradle is not installed. This is useful for continuous integration of servers. Free open source Gradle is an open source project, and licensed under the Apache Software License (ASL). Groovy Gradle's build script are written in Groovy programming language. The whole design of Gradle is oriented towards being used as a language and not as a rigid framework. Groovy allows you to write your own script with some abstractions. The whole Gradle API is fully designed in Groovy language. Installation of Gradle: Prerequisites to install Gradle JDK and Groovy are the prerequisites for Gradle installation. Gradle requires JDK version 6 or later to be installed in the system. It uses the JDK libraries which are installed, and sets to the JAVA_HOME environmental variable. Gradle carries its own Groovy library, therefore, we need not install Groovy explicitly. If it is installed, that is ignored by Gradle. The steps to install Gradle in your system are explained below. Step 1 - Verify JAVA Installation First of all, you need to have Java Software Development Kit (SDK) installed on your system. To verify this, execute Java -version command in any of the platform you are working on. In Windows Execute the following command to verify Java installation. I have installed JDK 1.8 in my system. C:\> java - version CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE . TECHNOLOGY 23 N32 narf 1750 Rank

--- Page 71 ---
Unit: II Course Code/Title:CS3V15/Devops build.gradle &3 I 1 /* 2 * This file was generated by the Gradle 'init' task. 3 * 4 * This generated file contains a sample Java Library project to get you started. 5 * For more details take a look at the Java Libraries chapter in the Gradle 6 * User Manual available at https://docs.gradle.org/6.0.1/userguide/java_library_plugin.html 7 */ 8 9 plugins { 10 // Apply the java-library plugin to add support for Java Library 11 id 'java-library' 12 } 13 14 repositories { 15 // Use jcenter for resolving dependencies. 16 // You can declare any Maven/Ivy/file repository here. 17 jcenter() 18 } 19 20 dependencies { 21 // This dependency is exported to consumers, that is to say found on their compile classpath. 22 api 'org. apache. commons : commons-math3: 3.6.1' 23 24 // This dependency is used internally, and not exposed to consumers on their own compile classpath. 25 implementation 'com. google. guava: guava: 28.0-jre' 26 27 // Use JUnit test framework 30 28 testImplementation 'junit : junit: 4.12' 29 } 30 The build.gradle file contains three default sections. They are as follows: o plugins: In this section, we can apply the java-library plugin to add support for java library. o Repositories: In this section, we can declare internal and external repository for resolving dependencies. We can declare the different types of repository supported by Gradle like Maven, Ant, and Ivy. · Dependencies: In this section, we can declare dependencies that are necessary for a particular subject. Additionally, we can declare other project-related modules like a task in this file. Display the Information of the Gradle project To understand the structure, dependencies and debugging problems of a build, Gradle provides many built-in features that display information on a project. Following are some basic commands to display the information of the project: Listing projects In Gradle, all the sub-projects of a project in the workspace can be listed in a hierarchy. To do so, run the below command from the root directory of the project. 1. gradle -q projects CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 72 ---
Course Code/Title:CS3V15/Devops Unit: II To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> C:\Users \HiMaNshU\eclipse-workspace >gradle -q projects Root project Root project 'eclipse-workspace' No sub-projects To see a list of the tasks of a project, run gradle <project-path> : tasks For example, try running gradle :tasks C:\Users\HiMaNshU\eclipse-workspace> Listing Tasks Gradle allows us to list all the essential tasks of the project. To list the task, run the below command: 1. gradle -q tasks Output: C:\Users\HiMaNshUleclipse-workspace>gradle -q tasks Tasks runnable from root project Build Setup tasks CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 31 N33 nirf 1750 Rank

--- Page 69 ---
Unit: II Course Code/Title:CS3V15/Devops 1 build.gradle gradle wrapper 2 gradle-wrapper.jar - gradle-wrapper.properties 3 gradlew 4 gradlew.bat 5 settings.gradle 6 It is the default structure of a Gradle project. Gradle will generate the following things for us: 1. The gradle file is build script for configuring the current project. 2. An executable JAR file is used as a Gradle wrapper. 3. Configuration properties for Gradle Wrapper. 4. The gradlew is a Gradle wrapper script for UNIX based OS. 5. The bat is the Gradle Wrapper script for Windows. 6. The settings script for configuring the Gradle build. Step3: Create a task Gradle supports APIs for creating and managing tasks through a Groovy-based DSL or Kotlin-based DSL. Every project contains a collection of tasks for some basic operation. Gradle supports a library of tasks that configure the project. For example, there is a Copy task, which copies files from one location to another. The Copy task is one of the most used tasks In Gradle. To use the Copy task in build script, follow the below process. Step1: Create a directory called src C: \Users \HiMaNshU\demo>mkdir src C: \Users \HiMaNshU\demo>cd src Step2: Add a file called myfile.txt in the src directory. Add the single line "Hello, World!" to it, also, we can leave it empty. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 28 N32 nirf 1750 Rank

--- Page 70 ---
Unit: II Course Code/Title:CS3V15/Devops C:\Users \HiMaNshU\demo\src>echo myfile.txt myfile.txt Define a task called Copy in build.gradle file. It will copy the src directory to a new directory called dest. We don't have to create the dest directory; the Copy task will do it for us. 1. task copy(type: Copy, group: "Custom", description: "The sources are copied to dest directory") { 2. from "src" 3. into "dest" 4. } We can provide anything in a group and description. Also, we can omit them, but doing so will also be omitted from the report of the task used later. Now execute our new copy task: C: \Users \HiMaNshU\demo>gradle copy BUILD SUCCESSFUL in 2s 1 actionable task: 1 executed C: \Users \HiMaNshU\demo>_ After the successful execution of the task, we will get BUILD SUCCESSFUL message. The build.gradle file The build.gradle file is build script of a Gradle project. All the tasks and plugins are defined in this file. When we run a gradle command, it looks for a file called build.gradle in the current directory. Although we have called it a build script, strictly, it is a build configuration script. The build script defines a project and its tasks. The default build.gradle file looks like as follows: CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 29 N33 nirf 1750 Rank

--- Page 41 ---
Unit: I Course Code/Title:CS3V15/Devops S.No. Git 1 Git is a software. 2 Git is a command-line tool GitHub is a graphical user interface 3 Git is installed locally on the system GitHub is hosted on the web 4 Git is maintained by linux. GitHub is maintained by Microsoft. 5 Git is focused on version control and code sharing. GitHub is a hosting service for Git repositories. 6 Git is a version control system to manage source code history. 7 Git was first released in 2005. GHub was launched in 2008. 8 Git has no user management feature. 9 Git is open-source licensed. GitHub includes a free-tier and pay- for- use tier. 10 Git has minimal external tool configuration. 11 Git provides a Desktop interface named Git Gui. GitHub provides a Desktop interface named GitHub Desktop. 12 Git competes with CVS, Azure DevOps Server, Subversion, Mercurial, etc. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY GitHub GitHub is a service. GitHub is focused on centralized source code hosting. GitHub has a built-in user management feature. GitHub has an active marketplace for tool integration. GitHub competes with GitLab, Bit Bucket, AWS Code Commit, etc. 41 N32 nirf 1750 Rank

--- Page 42 ---
Course Code/Title:CS2V15/Devops Unit: II UNIT - II COMPILE AND BUILD USING MAVEN AND GRADLE Introduction, Installation of Maven, POM files, Maven Build lifecycle, Build phases(compile build, test, package) Maven Profiles, Maven repositories(local, central, global),Maven plugins, Maven create and build Artifacts, Dependency management, Installation of Gradle, Understand build using Gradle. 1. Introduction of Maven: Apache Maven is an automation tool. The tool is written in Java. It was initially released on 13 July 2004. It is developed by the Apache software foundation. It is part of the Jakarta Project. It is working on two aspects: how software is built, and its dependencies. It was created by Jason van Zyl. It is built by using a plugin-based architecture that allows it to make the use of any application controllable by standard input. It dynamically downloads Java libraries. Understanding the problem without Maven There are many problems that we face during the project development. They are discussed below: 1) Adding set of Jars in each project: In case of struts, spring, hibernate frameworks, we need to add set of jar files in each project. It must include all the dependencies of jars also. 2) Creating the right project structure: We must create the right project structure in servlet, struts etc, otherwise it will not be executed. 3) Building and Deploying the project: We must have to build and deploy the project so that it may work. Maven simplifies the above mentioned problems. It does mainly following tasks. 1. It makes a project easy to build 2. It provides uniform build process (maven project can be shared by all the maven projects) 3. It provides project information (log document, cross referenced sources, mailing list, dependency list, unit test reports etc.) 4. It is easy to migrate for new features of Maven Apache Maven helps to manage o Builds Documentation o Reporing o SCMs o Releases o Distribution What is Build Tool? A build tool is used for building a process. It does following: CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE TECHNOLOGY N32 nirf 1 1750 Rank

--- Page 29 ---
Unit: I Course Code/Title:CS3V15/Devops Enterprise services. AWS virtual private cloud. Outsourced cloud services. Salesforce User Controls Operating System, Runtime, Middleware, and Application data Others It is highly scalable and flexible. Microsoft Azure. Force.com, Gigaspaces. Data of the application It is highly scalable to suit the different businesses according to resources. Google cloud services Google offers a seven wide range of Services: · Compute · Networking · Storage and Databases · Big Data · Machine Learning · Identity & Security · Management and Developer Tools IBM cloud analysis. AWS, Terremark Nothing It is highly scalable to suit the small, mid and enterprise level business 1. Compute: GCP provides a scalable range of computing options you can tailor to match your needs. It provides highly customizable virtual machines. and the option to deploy your code directly or via containers. · Google Compute Engine · Google App Engine · Google Kubernetes Engine · Google Cloud Container Registry · Cloud Functions 2. Networking: The Storage domain includes services related to networking, it includes the following services • Google Virtual Private Cloud (VPC) . Google Cloud Load Balancing · Content Delivery Network · What is Google Cloud Connect · Google Cloud DNS · What is Google Cloud Web Hosting 3. Storage and Databases: The Storage domain includes services related to data storage, it includes the following services · Google Cloud Storage . Cloud SQL · Cloud Bigtable · Google Cloud Datastore • Persistent Disk TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 29 N32 nirf 1750 Rank

--- Page 30 ---
Unit: I Course Code/Title:CS3V15/Devops 4. Big Data: The Storage domain includes services related to big data, it includes the following services · Google BigQuery · Google Cloud Dataproc · Google Cloud Datalab · Google Cloud Pub/Sub 5. Cloud AI: The Storage domain includes services related to machine learning, it includes the following services · Cloud Machine Learning · Vision API · Speech API · Natural Language API · Translation API • Jobs API 6. Identity & Security: The Storage domain includes services related to security, it includes the following services · Cloud Resource Manager · Cloud IAM · Cloud Security Scanner · Cloud Platform Security 7. Management Tools: The Storage domain includes services related to monitoring and management, it includes the following services • Stackdriver · Monitoring · Logging · Error Reporting · Trace · Cloud Console 8. Developer Tools: The Storage domain includes services related to development, it includes the following services . Cloud SDK · Deployment Manager · Cloud Source Repositories · Cloud Test Lab AZURE Azure is Microsoft's cloud platform, just like Google has its Google Cloud and Amazon has its Amazon Web Service or AWS.000. Generally, it is a platform through which we can use Microsoft's resources. For example, to set up a huge server, we will require huge investment, effort, physical space, and so on. In such situations, Microsoft Azure comes to our rescue. It will provide us with virtual machines, fast processing of data, analytical and monitoring tools, and so on to make TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 30 N33 narf 1750 Rank

--- Page 73 ---
Course Code/Title:CS3V15/Devops Unit: II init - Initializes a new Gradle build. wrapper - Generates Gradle wrapper files. Help tasks Build Environment - Displays all build script dependencies declared in root project 'eclipse-workspace'. components - Displays the components produced by root project 'eclipse-workspace '. [incubating] dependencies - Displays all dependencies declared in root project 'eclipse-works pace'. Dependency Insight - Displays the insight into a specific dependency in root project 'eclipse-workspace.' Dependent Components : It displays the dependent components of components in the root project 'eclipse-workspace.' [incubating] help - Displays a help message. model - Displays the configuration model of root project 'eclipse-workspace.' [incubating] projects - Displays the sub-projects of root project 'eclipse-workspace.' properties - Displays the properties of root project 'eclipse-workspace.' tasks - Displays the tasks runnable from root project 'eclipse-workspace.' To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> To list all the tasks of the project, run the below command: 1. gradle tasks -all CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 32 N33 nirf 1750 Rank

--- Page 74 ---
Course Code/Title:CS3V15/Devops Unit: II To display more details about a task, run the below command: 1. gradle help -- task Listing Dependencies In Gradle, we can list the dependencies which are broken down by the configuration. To list the dependencies, run the below command: 1. gradle -q dependencies Difference between Gradle and Maven Basis Based on Gradle is based on developing Maven is based on developing pure Java language-based software. Configuration It uses a Groovy-based Domain-specific language (DSL) for creating project structure. Focuses on Performance It performs better than maven as it optimized for tracking only current running task. It is necessary to compile. Java Compilation Usability It is a new tool , which requires users to spend a lot This tool is a known tool for many users and is easily Gradle domain-specific language projects. It uses Extensible Markup Language (XML) for creating project structure. Developing applications by adding new features to them . It avoids compilation. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] Maven Developing applications in a given time limit. It does not create local temporary files during software creation and is hence - slower . 33 N33 nirf 1750 Rank -

--- Page 55 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Environment Variables Now remove active profile from maven settings.xml and update the test profile mentioned in pom.xml. Add activation element to profile element as shown below. The test profile will trigger when the system property "env" is specified with the value "test". Create an environment variable "env" and set its value as "test". <profile> <id>test</id> <activation> <property> <name>env</name> <value>test</value> </property> </activation> </profile> Let's open command console, go to the folder containing pom.xml and execute the following mvn command. C:\MVN\project>mvn test Profile Activation via Operating System Activation element to include os detail as shown below. This test profile will trigger when the system is windows XP. <profile> <id>test</id> <activation> <os> <name>Windows XP</name> <family>Windows</family> <arch>x86</arch> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 14 N32 narf 1750 Rank -

--- Page 56 ---
Course Code/Title:CS3V15/Devops Unit: II <version>5.1.2600</version> </os> </activation> </profile> Now open command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test Profile Activation via Present/Missing File Now activation element to include OS details as shown below. The test profile will triggerwhen target/generated-sources/axistools/wsdl2java/com/companyname/group is missing. <profile> <id>test</id> <activation> <file> <missing>target/generated-sources/axistools/wsdl2java/ com/companyname/group</missing> </file> </activation> </profile> Now open the command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test 8. Maven create and build artifacts: The first step is to open Eclipse, which comes with the integrated Maven environment. The Eclipse window opens on the screen. Complete the following steps: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 15 N32 nirf 175º Rank -

--- Page 47 ---
Course Code/Title:CS3V15/Devops Unit: II <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> Any repository of maven is available in mvnrepository.com 5. Maven Plugins: The maven plugins are central part of maven framework, it is used to perform specific goal. According to Apache Maven, there are 2 types of maven plugins. 1. Build Plugins 2. Reporting Plugins Build Plugins These plugins are executed at the time of build. These plugins should be declared inside the <build> element. Reporting Plugins These plugins are executed at the time of site generation. These plugins should be declared inside the <reporting> element. Maven Core Plugins A list of maven core plugins are given below: Plugin Description clean clean up after build. compiler compiles java source code. deploy deploys the artifact to the remote repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY N32 nirf 6 175º Rank -

--- Page 48 ---
Course Code/Title:CS3V15/Devops Unit: II failsafe runs the JUnit integration tests in an isolated classloader. install installs the built artifact into the local repository. resources copies the resources to the output directory for including in the JAR. site generates a site for the current project. surefire runs the JUnit unit tests in an isolated classloader. verifier verifies the existence of certain conditions. It is useful for integration tests. Example for maven plugin: The compiler plugin is used to compile the source code of a Maven project. This plugin has two goals, which are already bound to specific phases of the default lifecycle: · compile - compile main source files · testCompile - compile test source files Here's the compiler plugin in the POM: <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.12.1</version> <configuration> <source>1.8</source> <target>1.8</target> </configuration> </plugin> 6. Maven Build Lifecycle: A Build Lifecycle is a well-defined sequence of phases, which define the order in which the goals are to be executed. Here phase represents a stage in life cycle. As an example, a typical Maven Build Lifecycle consists of the following sequence of phases. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 7 N33 nirf 175º Rank -

--- Page 3 ---
Course Code/Title:CS3V15/Devops Unit: I Key Goals and Benefits of DevOps Goals of DevOps The fast-paced growth of the IT industry and continuous advancements in technology make it critical to set DevOps goals that are experimental and challenging for companies to compete and thrive in the market. Here are the key goals and principles that every successful DevOps program has in common. 1. Ensures effective collaboration between teams: Effective collaboration in any process relies on shared ownership. During the development process, all those involved should embrace the fact that everyone is equally responsible for the entire development process. Whether it is development, testing, or deployment, each team member should be involved. They should understand that they have an equal stake in the final outcome. In the DevOps paradigm, passing of work from one team to another is completely defined and broken down. This accelerates the entire process of development since collaboration between all the teams involved is streamlined. 2. Creates scalable infrastructure platforms: The primary focus of DevOps is to create a sustainable infrastructure for applications that make them highly scalable. According to the demands of the modern-day business world, scalable apps have become an absolute necessity. In an ideal situation, the process of scaling should be reliable and fully automated. As a result, the app will have the ability to adapt to any situation when a marketing effort goes viral. With the app being scalable, it can adjust itself to large traffic volumes and provide an immaculate user experience. 3. Builds on-demand release capabilities: Companies must focus on keeping their software in a 'releasable' state. Continuous delivery will allow the software to add new features and go live at any stage. DevOps aims to automate the process of release management because it has a plethora of advantages. Automated release management is predictable, fast, and very consistent. Moreover, through automation, companies can release new versions as per their requirements. Automated release management also has complete and thorough audit trials, as these are essential for compliance purposes. 4. Provides faster feedback: Automating monotonous tasks such as testing and reporting will accelerate the process of rapid feedback. Since the development team will know what has to change, it can roll out the updated version faster. In addition, the team can better understand the impact of the changes that it has done in the software lifecycle. A concrete understanding of changes will assist team members in working efficiently in tandem. With rapid feedback, the operations team and developers can make better decisions collectively and enhance the app's performance. Benefits of DevOps DevOps helps organizations deliver added value to their customers. Here are some compelling benefits of DevOps. 1. Smarter work and faster release: With DevOps, your development team can release the required deliverables quickly. Faster release of deliverables will keep you miles ahead of your competitors, which is very important in today's cut-throat business realm. Businesses should understand that if their review cycle is not automated, it will slow down the release process. Moreover, the inclusion of disparate tools will lead to context switching and higher costs. Thus, DevOps can help rectify this worrisome business situation. 2. Quick resolution of issues: In a business world where speed and accuracy are paramount, a fast feedback loop will help you thrive. With DevOps, the communication process becomes seamless, and, as such, it minimizes the time required to solve issues. Without open communication, key issues can slip out of mind, which will have serious repercussions in the long run. DevOps fosters open communication that helps resolve issues, thus unblocking the release pipeline faster. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 3 N32 nirf 1750 Rank

--- Page 4 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Better collaboration between teams: DevOps paves the way for more dynamic and round-the- clock communication between teams. It renders an environment for mutual collaboration and integration among teams that are distributed globally. Eliminating the traditional departmental barriers between teams forms a new sense of ownership, wherein each team member feels equally responsible for meeting delivery timelines. This collaboration contributes to happier and more engaged employees. 4. Fostering innovative mindsets: With DevOps, deployment phases of the application are more relaxed as compared to traditional methods. This is because it streamlines the entire process, ensures that there are no lapses in quality, and allows on-time and efficient release. Thus, as everything is in order, the development team is more at peace. This allows it to think out of the box and provide additional value to the user. Having a development team with an innovative mindset is a boon for any business organization. An innovative approach, in itself, has immense scope and leads to better quality and resolution of issues at hand. Thus, through DevOps, the process of expanding the horizon of an app becomes much easier. 5. Faster threat detection: Automated and continuous testing of the code will make the process of threat detection faster. As developers can locate problem areas at an early stage, they can then resolve them faster. Thus, DevOps is a vital cog in maintaining and enhancing the quality and performance of an app. As the overall build of the app is in capable hands, teams working together are empowered to share feedback as and when necessary. 6. Increased customer satisfaction: Customer satisfaction is paramount in any day and age, irrespective of the business one is involved in. DevOps is known for enhancing customer experience, which ultimately increases the level of customer satisfaction. Dissatisfied customers are never a good sign for any business. Feedback loops are an important component of DevOps. These loops empower end users to track the progress of app development at various stages. 7. In addition, they can suggest changes (if any) or give their inputs to make the app more customer- centric. Due to their dynamic nature, feedback loops help developers and customers remain on the same page. Moreover, DevOps accelerates the process of app development, which eventually lessens the delivery timer. This has a positive impact on the customer satisfaction ratio. 8. Providing the much-needed edge: Along with staying true to their development process, companies need to ensure that they sustain themselves in the cut-throat competition. Implementing DevOps can be your trump card to provide your organization with that much- needed edge. Competitive advantage is necessary, as it can become the deciding factor in the popularity of an application in many cases. Some factors set expert businesses apart from mediocre ones: · Top-quality features · Quicker and timely software releases · Maximizing return on investments · Listening to constructive feedback Difference between Agile and DevOps Agile: Agile program advancement comprises different approaches to computer program improvement beneath which prerequisites and arrangements advance through the collaborative exertion of self- organizing and cross-functional groups and their customer/end client. DevOps: DevOps could be a set of hones that combines program improvement and information- technology operations which points to abbreviating the framework's advancement life cycle and giving nonstop conveyance with tall program quality. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 4 N32 nirf 1750 Rank

--- Page 23 ---
Course Code/Title:CS3V15/Devops Unit: I application. Amazon EC2 enables you to increase or decrease capacity within minutes. You can use one or hundreds or even thousands of server instances simultaneously. Because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs. Amazon EC2 is integrated with most AWS services, such as Amazon Simple Storage Service (Amazon S3), Amazon Relational Database Service (Amazon RDS), and Amazon Virtual Private Cloud (Amazon VPC) to provide a complete, secure solution for computing applications. Amazon EC2 is an example of Infrastructure as a Service(IaaS). EC2 delivers secure, reliable, cost-effective compute and high-performance compute infrastructure so as to meet the needs of demanding businesses. Amazon EC2 is one of the easiest ways of providing servers on AWS Cloud and also the access to Operating system. · AWS Lambda: AWS Lambda is a serverless, event-driven compute service that allows you to run code without managing servers. You pay only for the compute time you consume and there is no charge when your code is not running. With AWS Lambda, you can run code for any type of application with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services, or you can call it directly from any web or mobile app. But triggering Lambda is possible with over 200 AWS services. You can only pay for what you have used. The compute time that you consume, you are needed to pay for it. You just only need to upload your code and everything required to run will take care of by Lambda and it automatically scales your code with high availability. · AWS Elastic Beanstalk: AWS Elastic Beanstalk is a Platform as a Service that facilitates quick deployment of your applications by providing all the application services that you need for your application. Beanstalk is a plug- and-play platform that allows working with multiple programming languages and environments. Elastic Beanstalk supports a large range of platforms like Node js, Java, PHP, Python, and Ruby. So, you can develop your application to meet your requirements and simply deploy it on Elastic Beanstalk. The main aim to use AWS Elastic Beanstalk is to allow you to focus on the deployment and management of your applications. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. 2. Networking · Amazon VPC: Amazon VPC is your network environment in the cloud. It allows you to create a private network within the AWS cloud that uses many of the same concepts and constructs as an on-premises network. Amazon VPC also gives you complete control of the network configuration. Customers can define normal networking configuration items such as IP address ranges, subnet creation, route table creation, network gateways, and security settings. Amazon VPC is an AWS foundational service and integrates with numerous AWS services. For instance, Amazon EC2 instances are deployed into your CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 24 ---
Course Code/Title:CS3V15/Devops Unit: I Amazon VPC. Similarly, Amazon Relational Database Service (Amazon RDS) database instances deploy into your Amazon VPC, where the database is protected by the structure of the network just like your on-premises network. You can easily launch AWS resources into a virtual network by Amazon Virtual Private Cloud. An isolated virtual network environment in the AWS cloud is created by Amazon VPC. . Amazon Route 53: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating human-readable names, such as www.geeksforgeeks.com, into the numeric IP addresses that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. 2. Storage · Amazon S3 (Simple Storage Service): Amazon Simple Storage Service (Amazon S3) is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the web. It is designed to provide an infinite amount of storage and it is delivered with 99.999999999% durability. You can use Amazon S3 as primary storage for cloud-native applications as a target for backup and recovery and disaster recovery. It offers industry-leading scalability, data availability, security, and performance. It's simple to move large volumes of data into or out of Amazon S3 with Amazon's cloud data migration options. Once data is stored in Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like Amazon S3 Standard - Infrequent Access and Amazon Glacier for archiving. · Amazon Glacier: Amazon Glacier is a secure, durable, and extremely low- cost storage service for data archiving and long-term backup. Data stored in Amazon Glacier takes several hours to retrieve, which is why it's ideal for archiving. The fastest access to your archive data is via Amazon Glacier. 3. Databases · Amazon RDS (Relational Database Service): Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost- efficient and resizable capacity while managing time-consuming database administration tasks, freeing you up to focus on your applications and business. You can find Amazon RDS is also available on several database instance types - optimized for memory, performance, or I/O. Amazon RDS provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. · Amazon DynamoDB (Non-Relational Database): Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. When you create a database table that can store and retrieve any amount of data you can simply use Amazon DynamoDB that will serve any level of requested traffic. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, Internet of Things (IoT), and many other applications. DynamoDB provides many features like · built-in security CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 24 N32 nirf 1750 Rank

--- Page 15 ---
Course Code/Title:CS3V15/Devops Unit: I which is a mechanism to control the usage of resources or capacity. 2) Code: Many good practices such as Git enables the code to be used, which ensures writing the code for business, helps to track changes, getting notified about the reason behind the difference in the actual and the expected output, and if necessary reverting to the original code developed. The code can be appropriately arranged in files, folders, etc. And they can be reused. 3) Test: The application will be ready for production after testing. In the case of manual testing, it consumes more time in testing and moving the code to the output. The testing can be automated, which decreases the time for testing so that the time to deploy the code to production can be reduced as automating the running of the scripts will remove many manual steps. 4) Plan: DevOps use Agile methodology to plan the development. With the operations and development team in sync, it helps in organizing the work to plan accordingly to increase productivity. 5) Monitor: Continuous monitoring is used to identify any risk of failure. Also, it helps in tracking the system accurately so that the health of the application can be checked. The monitoring becomes more comfortable with services where the log data may get monitored through many third-party tools such as Splunk. 6) Deploy: Many systems can support the scheduler for automated deployment. The cloud management platform enables users to capture accurate insights and view the optimization scenario, analytics on trends by the deployment of dashboards. 7) Operate: DevOps changes the way traditional approach of developing and testing separately. The teams operate in a collaborative way where both the teams actively participate throughout the service lifecycle. The operation team interacts with developers, and they come up with a monitoring plan which serves the IT and business requirements. 8) Release: Deployment to an environment can be done by automation. But when the deployment is made to the production environment, it is done by manual triggering. Many processes involved in release management commonly used to do the deployment in the production environment manually to lessen the impact on the customers. DevOps Lifecycle DevOps defines an agile relationship between operations and Development. It is a process that is practiced by the development team and operational engineers Learning DevOps is not complete without understanding the DevOps lifecycle phases. The DevOps lifecycle includes seven phases as given below: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 16 ---
Unit: I Course Code/Title:CS3V15/Devops together from beginning to the final stage of the product. Development Operations Integration Deployment DevOps Lidecycle Feedback 2) Continuous Integration Testing Monitering 1) Continuous Development This phase involves the planning and coding of the software. The vision of the project is decided during the planning phase. And the developers begin developing the code for the application. There are no DevOps tools that are required for planning, but there are several tools for maintaining the code. This stage is the heart of the entire DevOps lifecycle. It is a software development practice in which the developers require to commit changes to the source code more frequently. This may be on a daily or weekly basis. Then every commit is built, and this allows early detection of problems if they are present. Building code is not only involved compilation, but it also includes unit testing, integration testing, code review, and packaging. The code supporting new functionality is continuously integrated with the existing code. Therefore, there is continuous development of software. The updated code needs to be integrated continuously and smoothly with the systems to reflect changes to the end-users. Commit Build O O Test O Stage Deploy Dev/QA O O > Continuous Integration/Delivery Development </> git Code Commit = production server. 3) Continuous Testing Production Jenkins is a popular tool used in this phase. Whenever there is a change in the Git repository, then Jenkins fetches the updated code and prepares a build of that code, which is an executable file in the form of war or jar. Then this build is forwarded to the test server or the This phase, where the developed software is continuously testing for bugs. For constant testing, automation testing tools such as TestNG, JUnit, Selenium, etc are used. These tools allow QAs to test multiple code- bases thoroughly in parallel to ensure that there is no flaw in the functionality. In this phase, Docker Containers can be used for simulating the test environment. It may occur in the form of documentation files or maybe produce large-scale data about the application parameters when it is in a continuous use position. The system errors such as server not reachable, low memory, etc are resolved in this phase. It maintains the security and availability of the service. Automation testing saves a lot of time and effort for executing the tests instead of doing this manually. Apart from that, report generation is a big plus. The task of evaluating the test cases INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 narf 1750 Rank

--- Page 43 ---
Course Code/Title:CS3V15/Devops Unit: II o Generates source code (if auto-generated code is used) o Generates documentation from source code o Compiles source code o Packages compiled code into JAR of ZIP file o Installs the packaged code in local repository, server repository, or central repository 2. Installation of Maven: To install maven on windows, you need to perform following steps: 1. Download maven and extract it 2. Add JAVA_HOME and MAVEN_HOME in environment variable 3. Add maven path in environment variable 4. Verify Maven 1) Download Maven To install maven on windows, you need to download apache maven first. Download Maven latest Maven software from Download latest version of Maven For example: apache-maven-3.1.1-bin.zip 2) Add MAVEN_HOME in environment variable Right click on MyComputer -> properties -> Advanced > Environment variables -> click new button System Settings - Now add MAVEN_HOME in variable name and path of maven in variable value. It must be the home directory of maven i.e. outer directory of bin. For example: E:\apache-maven-3.1.1 3) Add Maven Path in environment variable Click on new tab if path is not set, then set the path of maven. If it is set, edit the path and append the path of maven.Here, we have installed JDK and its path is set by default, so we are going to append the path of maven. The path of maven should be %maven home%/bin. For example, E:\apache-maven- 3.1.1\bin . 4) Verify maven To verify whether maven is installed or not, open the command prompt and write: mvn -version Now it will display the version of maven and jdk including the maven home and java home. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 2 N32 nirf 175º Rank

--- Page 44 ---
Unit: II Course Code/Title:CS3V15/Devops 3. POM FILES: POM is an acronym for Project Object Model. The pom.xml file contains information of project and configuration information for the maven to build the project such as dependencies, build directory, source directory, test source directory, plugin, goals etc. Maven reads the pom.xml file, then executes the goal. Before maven 2, it was named as project.xml file. But, since maven 2 (also in maven 3), it is renamed as pom.xml. Elements of maven pom.xml file For creating the simple pom.xml file, you need to have following elements: Element Description project It is the root element of pom.xml file. modelVersion It is the sub element of project. It specifies the modelVersion. It should be set to 4.0.0. groupId It is the sub element of project. It specifies the id for the project group. artifactId It is the sub element of project. It specifies the id for the artifact (project). An artifact is something that is either produced or used by a project. Examples of artifacts produced by Maven for a project include: JARs, source and binary distributions, and WARs. version It is the sub element of project. It specifies the version of the artifact under given group. Maven pom.xml file with additional elements Here, we are going to add other elements in pom.xml file such as: Element Description packaging defines packaging type such as jar, war etc. name defines name of the maven project. url defines url of the project. dependencies defines dependencies for this project. dependency defines a dependency. It is used inside dependencies. scope defines scope for this maven project. It can be compile, provided, runtime, test and system. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 3 N32 narf 1750 Rank -

--- Page 59 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Specify Archetype parameters M Group Id: com.simplileam Artifact Id: mavenproject Version: 0.0.1-SNAPSHOT Package: com,simplilearn.mavenproject . Properties available from archetype: Name Value Add ... Remove Advanced ? < Back Next > Cancel Finish The project is now created. · Open the pom.xml file You can see all the basic information that you have entered on the screen, such as the Artifact ID, Group ID, etc. You can see the junit dependencies have been added. This process takes place by default in Eclipse. There will also be some by default test cases. File Edit Source Navigate Search Project Run Window Help # Package Explorer E Jo JUnit - mavenproject/pom.xml 28 mavenproject > ( src/main/java ( src/test/java > = JRE System Library (|2SE-1:5] i Maven Dependencies target 7 <artifactIdaavenproject</artifactId> <version>0. 0. 1-SNAPSHOT</version>: <packaging>jar</packaging> <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 136 sproperties> 14 <project. build.sourceEncoding>UTF-8</project. build.sourceEncoding> </properties> 15 16 «dependenciess 18- <dependency> 19 <groupId>junit</grouptasI <artifactId=junit</artifactle> sversion>3.8.1</version> <scope>tests/scope> 21 22 223 </dependency> 24 </dependencies> 25 </project> Overview Dependencies Dependency Hierarchy |Effective POM pom.xml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 18 N32 nirf 175º Rank -

--- Page 60 ---
Course Code/Title:CS3V15/Devops Unit: II There you can find AppTest.java to be a default test case. When you click on that, you can see the test cases written in JUnit on your Eclipse screen. # Package Explorer 13 Ju JUnit mavenproject/pom.xml Z App.java AppTest.java 12 1 package com.simplilearn.mavenproject; - fl mavenproject + ( src/main/java - il com.simplileam.mavenproject U App.java = ( src/test/java il com.simplileam.mavenproject , E AppTest.java > > JRE System Library ||2SE-1.5] al Maven Dependencies - target pom.xml 3w import jumit. framework. Test:[] 6 76 / ** . Unit test for simple App. 10 public class AppTest extends TestCase 12 1 14 . Create the test case 15 16 * @param testName name of the test cole 17 18% public AppTest( String testNane ) 19 1 super( testName ); 28 21 1 22 :23% 24 * @return the suite of tests being tested 75 When it comes to adding more test cases, it will depend on the user, but these test cases and commands can easily be added in the workspace. If we try to remove certain dependencies from our file, we will receive error messages. To troubleshoot this, complete the following steps: · Go to another tab: mavenproject/pom.xml · Delete any dependencies · Save the file Immediately, there will be several error messages in the AppTest.java. R mavenproject/pom.xml App.java AppTest.java 83 1 package com.simplilearn.mavenproject; 2 3@ import junit. framework. Test;[] 6 mavenproject/src/test/java/com/simplileam/mavenproject/AppTest java 8 * Unit test for simple App. 9 18 public class AppTest extends TestCase 011 12 { 136 14 . Create the test case 15 16 * @param testName name of the test case 17 188 19 € public AppTest( String testName ) 28 super( testName ); 21 22 - 230 24 * @return the suite of tests being tested Problems E @ Javadoc ) Declaration History 7 errors, 1 warning, 0 others Description + @ Errors (7 items) a Test cannot be resolved to a type Resource Path Location AppTest.java /mavenproject/src/te line 26 Java Problem Type CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 19 N32 nirf 1750 Rank -

--- Page 49 ---
Course Code/Title:CS3V15/Devops Unit: II Validate Deploy Compile Install Test Verify Package Integration Test · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. There are always pre and post phases to register goals, which must run prior to, or after a particular phase. When Maven starts building a project, it steps through a defined sequence of phases and executes goals, which are registered with each phase. Maven has the following three standard lifecycles - • default: This is the main lifecycle, as it's responsible for project deployment. CHENNAI CHENNAI INSTITUTE . TECHNOLOGY 8 clean: Handles project cleaning, ensuring that all artifacts generated by previous INSTITUTE OF TECHNOLOGY 175º Rank (Autonomous]

--- Page 50 ---
Course Code/Title:CS3V15/Devops Unit: II builds are removed. · site: Manages the creation of the project's site documentation. Default Lifecycle (default) Most Maven users will be familiar with the default lifecycle. It is a general model of a build process for a software application. The first phase is validate and the last phase is deploy. · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. Clean Lifecycle (clean) The first lifecycle in Maven. Running mvn clean invokes the clean lifecycle which consists of three lifecycle phases: · pre-clean : execute processes needed prior to the actual project cleaning · clean : remove all files generated by the previous build · post-clean : execute processes needed to finalize the project cleaning Site Lifecycle (site) Maven does more than build software artifacts from project, it can also generate project documentation and reports about the project, or a collection of projects. Project documentation and site generation have a dedicated lifecycle which contains four phases: 1. pre-site : execute processes needed prior to the actual project site generation 2. site : generate the project's site documentation 3. post-site : execute processes needed to finalize the site generation, and to prepare for site deployment 4. site-deploy: deploy the generated site documentation to the specified web server CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 89 ---
Course Code/Title:CS3V15/Devops Unit: III Key Features: · Source Code Management: Configures Jenkins to use Git repositories as the source code for jobs. Example Demo Step 1 : Create a simple Java program. I have created a folder named MyJavaPrograms and inside it created a simple Java program as follows - e.g., SSH test.java public class test { public static void main(String args[I) { for(int i=1;i <= 5;1++) System.out.printin("Welcome Anuradha"); } Plugin," Step 2 : Open the command prompt, switch to that folder and execute the above Java program. It is illustrated by following screenshot. 1 C:\Windows\System32\cmd.e X + Microsoft Windows [Version 10.0.22621.2134] (c) Microsoft Corporation. All rights reserved. 0 X before E:\MyJavaPrograms>javac test. java E: \MyJavaPrograms>java test Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha E: \MyJavaPrograms> Step 3 : Now we will create a Git repository and push this repository on GitHub. First of all we will initialise the Git repository my using git init command. CHENNAI INSTITUTE TECHNOLOGY 14 - N32 nirf 1750 Rank CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt)

--- Page 90 ---
Unit: III Course Code/Title:CS3V15/Devops C:\Windows\System32\cmd.e x X E:\MyJavaPrograms>git init Initialized empty Git repository in E:/MyJavaPrograms/.git/ E:\MyJavaPrograms>git status On branch master No commits yet Untracked files: (use "git add <file> ... " to include in what will be committed) test.class test. java nothing added to commit but untracked files present (use "git add" to track) E:\MyJavaPrograms> then add the java and class files to git repository - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git add . E:\MyJavaPrograms>git status On branch master No commits yet Changes to be committed: (use "git rm -- cached <file> ... " to unstage) new file: test.class new file: test. java E: \MyJavaPrograms> Now we will commit the changes - C:\Windows\System32\cmd.e X E:\MyJavaPrograms>git commit -m "First Commit for Java program" [master (root-commit) 4cb04db] First Commit for Java program 2 files changed, 6 insertions(+) create mode 100644 test.class create mode 100644 test. java E:\MyJavaPrograms> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) - INSTITUTE . TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 31 ---
Course Code/Title:CS3V15/Devops Unit: I our work simpler. The pricing of Azure is also simpler and cost-effective. Popularly termed as "Pay As You Go", which means how much you use, pay only for that. Microsoft Azure Used for · Deployment Of applications: You can develop and deploy the application in the azure cloud by using the service called Azure App Service and Azure Functions after deploying the applications end users can access it. · Identity and Access Managment: The application and data which is deployed and stored in the Microsoft Azure can be secured with the help of Identity and Access Managment. It's commonly used for single sign-on, multi-factor authentication, and identity governance. · Data Storage and Databases: You can store the data in Microsoft azure in service like blob storage for unstructured data, table storage for NoSQL data, file storage, and Azure SQL Database for relational databases. The service can be scaled depending on the amount of data we are getting. · DevOps and Continuous Integration/Continuous Deployment (CI/CD): Azure DevOps will provide some tools like ncluding version control, build automation, release management, and application monitoring Following are some of the services Microsoft Azure offers: 1. Compute: Includes Virtual Machines, Virtual Machine Scale Sets, Functions for serverless computing, Batch for containerized batch workloads, Service Fabric for microservices and container orchestration, and Cloud Services for building cloud-based apps and APIs. 2. Networking: With Azure, you can use a variety of networking tools, like the Virtual Network, which can connect to on-premise data centers; Load Balancer; Application Gateway; VPN Gateway; Azure DNS for domain hosting, Content Delivery Network, Traffic Manager, ExpressRoute dedicated private network fiber connections; and Network Watcher monitoring and diagnostics 3. Storage: Includes Blob, Queue, File, and Disk Storage, as well as a Data Lake Store, Backup, and Site Recovery, among others. 4. Web + Mobile: Creating Web + Mobile applications is very easy as it includes several services for building and deploying applications. 5. Containers: Azure has a property that includes Container Service, which supports Kubernetes, DC/OS or Docker Swarm, and Container Registry, as well as tools for microservices. 6. Databases: Azure also included several SQL-based databases and related tools. 7. Data + Analytics: Azure has some big data tools like HDInsight for Hadoop Spark, R Server, HBase, and Storm clusters 8. AI + Cognitive Services: With Azure developing applications with artificial intelligence capabilities, like the Computer Vision API, Face API, Bing Web Search, Video Indexer, and Language Understanding Intelligent. 9. Internet of Things: Includes IoT Hub and IoT Edge services that can be combined with a variety of machine learning, analytics, and communications services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 31 N32 nirf 1750 Rank

--- Page 32 ---
Unit: I Course Code/Title:CS3V15/Devops 10. Security + Identity: Includes Security Center, Azure Active Directory, Key Vault, and Multi-Factor Authentication Services. 11. Developer Tools: Includes cloud development services like Visual Studio Team Services, Azure DevTest Labs, HockeyApp mobile app deployment and monitoring, Xamarin cross- platform mobile development, and more. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 32 N32 nirf 1750 Rank

--- Page 65 ---
Course Code/Title:CS3V15/Devops Unit: II Output The output is as follows - java version "1.8.0 66" Java(TM) SE Runtime Environment (build 1.8.0_66-b18) Java HotSpot(TM) 64-Bit Server VM (build 25.66-b18, mixed mode) Step 2 - Download Gradle Build File Download the latest https://gradle.org/install/. version of Gradle from the link available at Step 3 - Set Up Environment for Gradle Setting up environment means, we have to extract the distribution file and copy the library files into proper location. Set up GRADLE_HOME and PATH environmental variables. This step is platform dependent. In Windows Extract the downloaded zip file named gradle-2.11-all.zip and copy the distribution files from Downloads\gradle-2.11\ to C:\gradlel location. After that, add the C: \gradle and C: \gradlelbin directories to the GRADLE_HOME and PATH system variables. Follow the given instructions - Right Click On My Computers -> Click On Properties -> Advanced System Settings -> Click On Environmental Variables. There you will find a dialog box for creating and editing system variables. Click on new button for creating GRADLE_HOME variable (follow the left side screenshot). Click on Edit for editing the existing Path system variable (follow the right side screenshot). Follow the below given screenshots. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 24 N33 nirf 1750 Rank

--- Page 66 ---
Course Code/Title:CS3V15/Devops System Properties X Computer Name Hardware Advanced System Protection Remote Environment Variables x Unit: II System Properties × Computer Name |Hardware Advanced System Protection Remote Environment Variables × New System Variable × Variable name: GRADLE_HOME| Variable value: C:\grade Cancel System variables Variable Value ComSpec C:\Windows\system32\and.exe FP_NO_HOST_C ... NO GRADLE_HOME E:\work\22- Orientdb \gradle-2.11 Variable name: Path Variable value: #C:\gradle bin - System variables Variable Value Edit System Variable × OK Cancel < ORIENTDB_HOME E:\work\22- Orientdblorientdb-communi ... " JAVA_HOME C:\Program Files\Java\jdk1.7.0_60 New ... Edt ... Delete OK Cancel Step 4 - Verify the Gradle installation In windows os Windows_NT Path C:\oraclexe \app \prade\product\11.2.0\, .. PATHEXT .COM ;. EXE ;. BAT ;. CMD ;. VBS ;. VBE ;. JS ;.... v New ... Edit ... Delete OK Cancel You can execute the following command in command prompt. C:\> gradle -v Output Here you will find the Gradle version. Gradle 2.11 Build time: 2016-02-08 07:59:16 UTC Build number: none Revision: 584db1c7c90bdd1de1d1c4c51271c665bfcba978 Groovy: 2.4.4 CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 25 N33 nirf 1750 Rank

--- Page 25 ---
Course Code/Title:CS3V15/Devops Unit: I · backups · automated multi-region replication in-memory caching · data export tools. Global Infrastructure of AWS · AWS is a cloud computing platform which is globally available. . Global infrastructure is a region around the world in which AWS is based. Global infrastructure is a bunch of high-level IT services which is shown below: AWS is available in 19 regions, and 57 availability zones in December 2018 and 5 more regions 15 more availability zones for 2019. The following are the components that make up the AWS infrastructure: o Availability Zones o Region o Edge locations o Regional Edge Caches Compnents of Global Infrastructure Availability zones Region Edge Locations Regional Edge Caches Availability zone as a Data Center An availability zone is a facility that can be somewhere in a country or in a city. Inside this facility, i.e., Data Centre, we can have multiple servers, switches, load balancing, firewalls. The things which interact with the cloud sits inside the data centers. o An availability zone can be a several data centers, but if they are close together, they are counted as 1 availability zone. Region o A region is a geographical area. Each region consists of 2 more availability zones. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 26 ---
Course Code/Title:CS3V15/Devops Unit: I o A region is a collection of data centers which are completely isolated from other regions. · A region consists of more than two availability zones connected to each other through links. AZ-a AZ-b AZ-c Availability zones are connected through redundant and isolated metro fibers. Edge Locations o Edge locations are the endpoints for AWS used for caching content. o Edge locations consist of CloudFront, Amazon's Content Delivery Network (CDN). o Edge locations are more than regions. Currently, there are over 150 edge locations. Edge location is not a region but a small location that AWS have. It is used for caching the content. o Edge locations are mainly located in most of the major cities to distribute the content to end users with reduced latency. o For example, some user accesses your website from Singapore; then this request would be redirected to the edge location closest to Singapore where cached data can be read. Regional Edge Cache o AWS announced a new type of edge location in November 2016, known as a Regional Edge Cache. o Regional Edge cache lies between CloudFront Origin servers and the edge locations. o A regional edge cache has a large cache than an individual edge location. o Data is removed from the cache at the edge location while the data is retained at the Regional Edge Caches. o When the user requests the data, then data is no longer available at the edge location. Therefore, the edge location retrieves the cached data from the Regional edge cache instead of the Origin servers that have high latency. What Is Cloud Computing? Cloud computing is the use of hardware and software components in an off-premises location to deliver a service to a network. Users can access files and applications from any device that can access the internet. Some features and capabilities include: · Cloud providers can pull the computing resources to provide services to multiple customers with the help of a multi-tenant model · Cloud computing proves an on-demand self-service, which helps administrators monitor performance CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 26 N32 nirf 1750 Rank -

--- Page 83 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Click "OK" to proceed. 4. Configure the Job General Settings 1. Project Description: ○ Enter a description for your job. 2. Discard Old Builds: ○ Set up a policy to manage the number of builds Jenkins keeps. Source Code Management (SCM) 1. Select SCM: ○ Choose your source code management system (e.g., Git). 2. Configure Repository: ○ Enter the repository URL and provide credentials if needed. 3. Branch Specification: ○ Specify the branch to build (e.g., main, develop). Build Triggers 1. Choose Build Triggers: ○ Poll SCM: Check for changes at intervals. ○ Build Periodically: Schedule builds. ○ GitHub hook trigger: Trigger builds on GitHub push. ○ Build after other projects: Trigger builds based on other jobs. Build Environment 1. Configure Build Environment: ○ Clean workspace: Delete workspace before build starts. ○ Manage secrets: Use secret texts or files. ○ Node selection: Specify where to run the job. Build Steps 1. Add Build Steps: ○ Click "Add build step". ○ Choose the build step type (e.g., Execute shell, Invoke Gradle script). ○ Enter the commands or scripts needed for the build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 nirf 8 1750 Rank

--- Page 84 ---
Course Code/Title:CS3V15/Devops Unit: III Post-build Actions 1. Add Post-build Actions: Click "Add post-build action". ○ Common actions include: Archive artifacts: Store build artifacts. Email notification: Send build status emails. I Publish test results: Aggregate test results. Trigger other jobs: Trigger other jobs based on results. 5. Save and Build 1. Save the Configuration: ○ Click "Save". 2. Build the Job: ○ Click "Build Now" to manually trigger a build. ○ View progress and logs by clicking on the build number. 6. Monitor and Manage Builds 1. Check Build History: ○ View past builds and their statuses. 2. Console Output: ○ Click on a build number to see detailed logs. 3. Build Artifacts: ○ Access any artifacts produced by the build. 7. Advanced Configuration (Optional) 1. Pipeline as Code: 0 For complex workflows, use Jenkins Pipelines. Create a Jenkinsfile in your repository. 2. Plugins: ○ Extend Jenkins functionality with plugins from the Jenkins Plugin Manager. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) 9 N32 nirf 1750 Rank

--- Page 75 ---
Course Code/Title:CS3V15/Devops Customization This tool is highly customizable as it supports a variety of IDE's . Languages supported Project Configuration Based on Goal of time to get used to it. available. It supports software development in Java, C, C++, and Groovy. For declaring the project configuration, it does not use the XML files. On the phases of the fixed and linear model. Graph of task dependencies that do the work. To add functionality in the project is the main goal of the Gradle. To finish the project in the given timeline is the main goal of the Maven. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY Unit: II This tool serves a limited number of developers and is not that customizable. It supports software development in Java, Scala, C# , and Ruby and it does not natively support C and C+ but can support through plugins like "maven-native- plugin" or we can integrate other build systems like CMake or Makefile. For declaring the project configuration, it uses the XML files. 34 N33 nirf 1750 Rank

--- Page 76 ---
Course Code/Title:CS3V15/Devops Unit: III Unit - III CONTINUOUS INTEGRATION USING JENKINS Install & Configure Jenkins, Jenkins Architecture Overview, creating a Jenkins Job, configuring a Jenkins job, Introduction to Plugins, Adding Plugins to Jenkins, commonly used plugins (Git Plugin, Parameter Plugin, HTML Publisher, Copy Artifact and Extended choice parameters). Configuring Jenkins to work with java, Git and Maven, creating a Jenkins Build and Jenkins workspace. Installing and Configuring Jenkins Overview Jenkins is a widely-used open-source automation server that helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration and continuous delivery (CI/CD). Below are detailed steps and considerations for installing and configuring Jenkins. Installation 1. System Requirements: ○ Operating System: Jenkins can run on any major operating system. ○ Java: Jenkins requires Java Runtime Environment (JRE) 8 or later. 2. Download Jenkins: ○ Download the latest version of Jenkins from the official Jenkins website. 3. Installation Steps: ○ Windows: I Run the .msi installer and follow the installation wizard. ■ After installation, Jenkins will start automatically as a Windows service. ○ Linux: Add Jenkins repository and import the GPG key. I Use package manager to install Jenkins (e.g., apt-get install jenkins). I Start Jenkins using systemctl (systemctl start jenkins). 4. Initial Setup: ○ After installation, Jenkins can be accessed through a web browser at http : //< your _server _ip_or_domain>: 8080. o Unlock Jenkins by entering the initial admin password, which can be found in the jenkins_home directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 1 N32 nirf 1750 Rank

--- Page 77 ---
Unit: III Course Code/Title:CS3V15/Devops 5. Customize Jenkins: ○ Install suggested plugins or select the required plugins manually. ○ Create the first admin user. Configuration 1. Global Configuration: ○ Navigate to Manage Jenkins > Configure System. ○ Set up global environment variables, email notifications, and other system-wide settings. 2. Security Configuration: ○ Configure security settings under Manage Jenkins > Configure Global Security. o Use the built-in user database or integrate with external authentication mechanisms like LDAP. 3. Node Configuration: ○ Jenkins can distribute build loads to multiple nodes. ○ Configure nodes by navigating to Manage Jenkins > Manage Nodes and Clouds. 4. Job Configuration: ○ Create new jobs by clicking New Item on the Jenkins dashboard. o Configure job-specific settings such as source code management, build triggers, and post-build actions. 5. Pipeline Configuration: ○ Jenkins supports pipeline as code using Jenkinsfile. ○ Define your pipeline stages in a Jenkinsfile placed in the root of your project repository. 6. Plugins: ○ Enhance Jenkins functionality by installing plugins from the Manage Jenkins > Manage Plugins section. 7. Backup and Restore: ○ Regularly back up the jenkins_home directory, which contains Jenkins configurations, job configurations, and build history. ○ Use plugins like ThinBackup for automated backups. Ref: https://youtube.com/playlist?list=PL6flErFppaj35spJjPy41-IruDjw2kRV-&si=Vzd9AAMv53xwWB S Ref: https://bit.ly/youtube_jenkins (click here) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 78 ---
Course Code/Title:CS3V15/Devops Unit: III Jenkins Architecture Overview Jenkins is a robust automation server widely used for continuous integration and continuous delivery (CI/CD) processes. Here's a detailed breakdown of the Jenkins architecture and its core components. Core Components of Jenkins Architecture 1. Jenkins Master: ○ Role: The Jenkins master is the central control unit. 0 Functions: Scheduling Jobs: Assigns build jobs to the appropriate agents. Monitoring Agents: Keeps track of the state and health of agents. I Job Execution: Executes build jobs directly if no agents are available. I User Interface: Provides a web-based interface for users to configure jobs, view build results, and manage the Jenkins environment. 2. Build Agents (Nodes): ○ Role: Agents are responsible for executing the build jobs assigned by the master. ○ Types: Agents can run on different operating systems and hardware configurations. ○ Configuration: Static Agents: Permanently assigned to the Jenkins master. . Dynamic Agents: Created and destroyed as needed, often used in cloud environments. ○ Communication: Agents communicate with the master using the Jenkins Remoting protocol. 3. Job Configuration: ○ Types of Jobs: I Freestyle Projects: Basic job type with simple configurations. Pipeline Jobs: Scripted or declarative pipelines that define the entire build process as code. I Multi-Configuration (Matrix) Projects: Allows testing across different environments and configurations. ○ Components: I Source Code Management: Integration with version control systems like Git, SVN, etc. . Build Triggers: Conditions that start the job, such as code commits, scheduled times, or manual triggers. Build Steps: Actions performed during the build, such as compiling code, running tests, and packaging artifacts. . Post-Build Actions: Steps executed after the build, like deploying artifacts, sending notifications, or archiving results. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 99 ---
Unit: III Course Code/Title:CS3V15/Devops 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "HTML Publisher Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Post-build Actions section. ○ Select Publish HTML reports. ○ Specify the directory containing HTML reports and the index page. Usage: . Ideal for displaying build reports such as test results or code coverage metrics in a user-friendly HTML format. DEMO: Step 1 : We will create a Maven project using Selenium and TestNG framework. Open Eclipse : IDE and Create a Maven Project. Select the Create Simple Project (Skip archetype selection) and enter the Group Id and Artifact Id as per your choice. My project name is SeleniumDemo. Step 2 : Add the Selenium dependency in your pom.xml file as follows - <dependency> <groupId>org.seleniumhq.selenium</groupId> <artifactId> selenium-java</artifactId> <version>3.141.59</version> </dependency > Save the pom.xml file. Step 3 : Now install testNG plugin. For that, from Menu select Help-> Eclipse Marketplace Help Welcome 7 Help Contents Search Show Context Help Show Active Keybindings ... Ctrl+Shift+L Tip of the Day Tips and Tricks ... Cheat Sheets ... @ Eclipse User Storage Open the Eclipse Marketpla ** Perform Setup Tasks ... % Check for Updates Install New Software ... Eclipse Marketplace ... = About Eclipse IDE · Donate Contribute Type TestNG in the search window. The testNG installation option will be available. click Install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 24 N32 nirf 1750 Rank ---

--- Page 100 ---
Course Code/Title:CS3V15/Devops Unit: III C Eclipse Meketplace × Eclipse Marketplace Select solutions to initiit, Prese leuralt Now to proceed with lutallation. Press the "more info" link to learn more about a solution Search Recent Popular Favorites itwled - Research at the Eclipse × All Categories Go TestNG for Eclipse . This plug-in late you run your TextNG wits from Erfiore. You can run sulter, groupe er individuel methods. Errors are reported in a pepwane tab that lets you. more info by Chat Bent Apache 20 tantog zielt testingunit integration functional selection * 737 ** * stal: 1,83M (25,021 last month) Install MoreUnit 3.3.0 Morelinit is an Solipoe plugin that should mult you in wilting mory unk with. It supports ali programming languages (witching between teds and clanset under. mone.info Install Infinitest 5.4.0.3 Infinitest is a continuont tout runner for tova,'and is valuable to developers valing a Marketplaces (7) Cancel Then confirm the selected features by clicking on Confirm button. x Confirá Selected Features These Confirm to gorthus with the induction. Cs go back te chocie bis sules it CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 25 N3 nirf 1750 Rank

--- Page 27 ---
Course Code/Title:CS3V15/Devops Unit: I · Servers are maintained easily and there is nearly zero downtime · Users can access cloud data and upload it on the cloud from any device with a solid internet connection · Cloud environments can be modified according to the user's requirements and is easily accessible · Clouds are highly secure, making data breaches more unlikely · Migrating to the cloud eliminates the need to buy on-premises infrastructure . It offers pay-as-you-go pricing, meaning you only pay for the resources you use 1. Infrastructure as a Service: IaaS delivers virtualized computing resources over the Internet. Users can rent virtual machines, storage, and networking infrastructure, allowing for easy scalability without investing in physical hardware. Examples include AWS EC2 and Azure Virtual Machines. 2. Platform as a Service: PaaS offers a robust platform for developers to build, deploy, and manage apps without worrying about the underlying infrastructure. It simplifies application development and deployment, with services like Google App Engine and Heroku leading the way. 3. Software as a Service: SaaS offers software applications on a subscription basis, accessible via a web browser. Users don't need to install or maintain software locally, making it ideal for collaboration tools (e.g., Microsoft 365, Google Workspace) and CRM systems (e.g., Salesforce). 4. Function as a Service: FaaS allows developers to execute code responding to events without managing servers. It's highly scalable and cost-efficient, exemplified by AWS Lambda and Azure Functions. FaaS is also known as serverless computing. 5. Container as a Service: CaaS enables the deployment and management of containerized applications using orchestration tools like Kubernetes. It provides portability and scalability for applications across different cloud environments. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 27 N33 nif 1750 Rank

--- Page 28 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between main cloud computing services Terms Stands for Uses Access Model Technical understanding. Popularity Percentage rise Usage Cloud services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY IAAS Infrastructure as a service. IAAS is used by network architects. IAAS gives access to the resources like virtual machines and virtual storage. It is a service model that provides virtualized computing resources over the internet. It requires technical knowledge. It is popular among developers and researchers. It has around a 12% increment. Used by the skilled developer to develop unique applications. Amazon Web Services, sun, vCloud Express. PAAS Platform as a service. PAAS is used by developers. PAAS gives access to run time environment to deployment and development tools for application. It is a cloud computing model that delivers tools that are used for the development of applications. Some knowledge is required for the basic setup. It is popular among developers who focus on the development of apps and scripts. It has around 32% increment. Used by mid-level developers to build applications. Facebook, and Google search engine. SAAS Software as a service. SAAS is used by the end user. SAAS gives access to the end user. It is a service model in cloud computing that hosts software to make it available to clients. There is no requirement about technicalities company handles everything. It is popular among consumers and companies, such as file sharing, email, and networking. It has about a 27 % rise in the cloud computing model. Used among the users of entertainment. MS Office web, Facebook and Google Apps. 28 N32 nirf 1750 Rank

--- Page 61 ---
Course Code/Title:CS3V15/Devops Unit: II Return to the previous screen and undo the deletion. The errors that occurred will disappear. *mavenproject/pom.xml & IApp.java AppTest.java 6 <artifactId>mavenproject</artifactId> 7 <version>0.0.1-SNAPSHOT</version> <packaging>jar</packaging> 8 9 16 <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 138 <properties> 14 15 16 17% <dependencies> 18 19 <groupId>junit</groupId> I 28 21 22 23 24 <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> cdependency> <artifactId>junit</artifactid> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> 25 </project> 26 9. Dependency Management: The dependencyManagement and dependencies are especially useful for multi-module projects. Dependency Management : This tag consists of a dependencies tag which itself might contain multiple dependency tags. Each dependency is supposed to have at least three maintags: groupId, artifactId, and version. For example: <dependencyManagement> <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 20 N33 nirf 1750 Rank -

--- Page 62 ---
Course Code/Title:CS3V15/Devops Unit: II The above code just declares the new artifact commons-lang3, but it doesn't really add it to the project dependency resource list. Dependencies: This tag contains a list of dependency tags. Each dependency is supposed to have at least two main tags, which are groupId and artifactId. For example: <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> The version and scope tags can be inherited implicitly if we have used the dependencyManagement tag before in the POM file. DependencyManagement is just a declaration, and it does not really add a dependency. Dependencies tag adds the actual dependency to the project. An example for adding the JUnit library dependency: <dependencyManagement> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.2</version> <scope>test</scope> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 21 N33 nirf 175º Rank

--- Page 19 ---
Course Code/Title:CS3V15/Devops Unit: I 5. Compliance: By compliance, we mean that certain businesses are required to follow specific regulations. Financial services companies in the United States, for example, must comply with the Sarbanes-Oxley Act, while healthcare, education, and energy companies must comply with HIPAA and other regulations. AWS provides a number of compliance-related features and services, such as data encryption and identity and access management, to help businesses meet these requirements. 6. High-Performance: Interested in delivering your applications quickly and efficiently? Taking advantage of AWS features such as auto-scaling and load balancing will help ensure your applications are always available and running optimally. AWS can help businesses improve their performance by offering a variety of cloud-based services, including Amazon Elastic Compute Cloud (EC2), which provides high-performance computing resources, and Amazon CloudFront, which delivers content quickly and securely to users around the world. Others include machine learning (ML) and analytics services, such as Amazon SageMaker and Amazon Athena. These services provide the tools businesses need to quickly and easily analyze their data for insights. Fast networking in the cloud is also possible with AWS, thanks to its Elastic Load Balancing (ELB) and Amazon Virtual Private Cloud (VPC). With ELB, businesses can balance their workloads across multiple instances for increased performance, while VPC allows businesses to create isolated private networks in the cloud. 7. Developer Tools: Developer tools are designed to make it easier for developers to create, deploy, and manage applications - and AWS provides developers with what they need to build applications quickly and easily. By leveraging developer tools, developers can save time and money by automating tedious tasks. They also benefit from access to AWS's extensive library of pre-built applications that can help them get their projects off the ground quickly. Services such as Amazon Elastic Beanstalk and Amazon CloudFormation can help them automate the process of creating and deploying applications. Other ways developers can improve productivity with AWS include using AWS Code Commit to store and manage source code. 8. Integration: Thanks to its many integrations with other Amazon services, as well as third-party services, AWS makes it easy for businesses to get started with cloud computing. AWS provides a wide range of services that can be easily integrated into existing business infrastructure. This allows businesses to add new features and capabilities without having to make major changes or invest in new hardware or software. For instance, if a business wants to add mobile capabilities to its website, it can take advantage of Amazon's Mobile SDK and Web Services. These tools allow businesses to quickly develop and deploy mobile apps that connect directly with their existing infrastructure. 9. Management Console: The AWS management console is a web-based interface that provides users with a simple way to interact with and manage their AWS resources - essentially a place where you can access and manage everything on the cloud. It provides a graphical view of all the resources associated with an account, as well as tools for creating and configuring new resources. Compared to traditional command- line interfaces, the AWS management console saves time and makes it easier for users to get the most out of their AWS services. Not only that, but your business gets access to 350+ free digital training courses through the AWS Academy, covering topics such as cloud fundamentals, DevOps, security, and big data. This means you can train your employees on how to use AWS, and in turn, help them become more efficient at their jobs. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 19 N32 nirf 1750 Rank

--- Page 20 ---
Course Code/Title:CS3V15/Devops Unit: I 10. Scalability: With an on-demand service, businesses can quickly spin up new servers as needed with just a few clicks. This makes it much easier to scale resources up or down as demand changes, allowing businesses to save costs and maintain performance even during peak periods. For example, if a business is expecting a sudden surge in traffic due to an advertising campaign or seasonal event, they can easily add more capacity to their server infrastructure to handle the increased load. Bru Textiles, a specialty textile company in Belgium, was able to quickly scale its infrastructure by leveraging AWS. Bru Textiles went digital to grow and offer new services. Embracing technology, they brought in digital twin technology to give their customers an idea of the texture and essence of their physical fabrics. AWS Applications · Storage and Backup: Storage and backup are important for any Cloud Computing service. AWS provides you with reliable storage services like Amazon Simple Storage Service to store large- scale data and backup services like AWS Backup to take backups of this data, which is stored in other AWS services. AWS stores the data in three different availability zones so that if one fails, you can still access your data. This makes AWS storage reliable and easily accessible. Therefore, companies with huge application data to store and backup securely can use AWS. · Big Data: One of the biggest challenges faced by companies these days is Big Data. The companies are struggling to store their large amounts of data using traditional methods. With AWS Big Data storage services, they can manage to store their data even if the data limit increases unexpectedly as AWS provides virtually unlimited data storage with scale-in and scale- out options. AWS offers easy access and faster data retrieval as well. For data processing, it offers services like EMR, with which the companies can easily set up, operate, and scale their big data. Therefore, efficiently storing and managing Big Data is among the top AWS applications. · Enterprise IT: AWS is a one-stop solution for any IT business. Many features of it such as secure storage, scalability, flexibility, and elasticity support companies to innovate faster than ever before. Using AWS for IT enterprises makes them profitable in terms of both money and time. As AWS maintains its cloud architecture, it need not waste time and money on professionals to do the same. · Social Networking: Social networking is essential for businesses in the present-day scenario where Digital Marketing is key, and it is easier with AWS. Companies can connect with customers and stakeholders and communicate through social networking sites and develop their business. Services like AWS social networking engine, which is powered by Turn Key GNU/Linux (HVM) AMI stack, are used for performance and scalability to help companies build a suitable social networking site and gain profits. · Mobile Apps: Mobile applications are embedded with day-to-day life. With AWS, you have the facility to create an app in your desired programming language. You can also keep up the applications that are consistently accessible and solid with high compute, storage, database, and application services. You can take advantage of AWS auto-scaling and managed relational database service for the better performance of your apps. · Websites: AWS offers a wide range of website hosting options to create the best website for customers. Its services like Amazon Light sail have everything, such as a virtual machine, SSD- based storage, data transfer, DNS management, and a static IP, to launch a website in such a way that the user can manage the website easily. Amazon EC2, AWS Lambda, Elastic Load Balancing, AWS Amplify, Amazon S3, etc. also help users build reliable and scalable websites. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 67 ---
Course Code/Title:CS3V15/Devops Unit: II Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013 JVM: 1.7.0_60 (Oracle Corporation 24.60-b09) OS: Windows 8.1 6.3 amd64 Understanding build using Gradle: The Gradle build is a process of creating a Gradle project. When we run a gradle command, it will look for a file called build.gradle in the current directory. This file is also called the Gradle build script. The build configuration, tasks, and plugins are described in this file. The build script describes a project and its tasks. Let's create a small Gradle project, run some of the basic Gradle commands, and understand how Gradle manages the project. Follow the steps below to create and test a Gradle project. Step1: Open the command line and create a directory First, Open the command line and create a directory for the project and change directory to it. Let's create a demo directory. C: \Users \HiMaNshU>mkdir demo C: \Users \HiMaNshU>cd demo Step2: Initialize a Gradle project To generate a Gradle project, run the gradle init command. It will generate a simple project. With this project, we will explore and understand everything that is generated. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 26 N33 nirf 1750 Rank

--- Page 68 ---
Course Code/Title:CS3V15/Devops Unit: II When we run the gradle init command, it will ask for some basic requirements. First, it will ask the type of project that we want to create. It will give four options: 1. 1: basic 2. 2: application 3. 3: library 4. 4: Gradle plugin Select our requirements. Hence, we are just making a demo project so that we will select the basic option. To select basic option, press 1 and Enter key. Consider the below output: C: \Users \HiMaNshU\demo>gradle init Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could ed, use -- status for details Select type of project to generate: 1: basic 2: application 3: library 4: Gradle plugin Enter selection (default: basic) [1 .. 4] 1 Next, it will ask for DSL. There are two options that are available for DSL: 1. 1: Groovy 2. 2: Kotlin Groovy is the default DSL provided by Gradle. Select build script DSL. Select build script DSL: 1: Groovy 2: Kotlin Enter selection (default: Groovy) [1 .. 2] 1 Next, it will ask for the project name. Type the project name and press Enter key. It will take a while to build a project. After the successful execution of the project, we will get a message BUILD SUCCESSFUL. Project name (default: demo): First_Gradle > Task : init Get more help with your project: https: //guides. gradle.org/creating- uilds BUILD SUCCESSFUL in 2m 56s 2 actionable tasks: 2 executed C: \Users\HiMaNshU\demo>mkdir src Now we have successfully created a Gradle project. Now, what will happen to our specified directory? Consider the below structure of the Gradle project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 27 N33 narf 1750 Rank

--- Page 109 ---
Course Code/Title:CS3V15/Devops Unit: III Note that the git repository and the files from that repository are copied from the workspace of App1 to App2. Also note that test.class and test.java files are copied to the workspace of App2. Started by user Anuradha R Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\. jenkins\workspace\App2 Copied 16 artifacts from "Appl" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621189248227895.bat C:\ProgramData\Jenkins\.jenkins\workspace\App2>dir Volume in drive C is OS Volume Serial Number is C6C2-8A9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 .git 474 test. class 144 test.java 2 File(s) 618 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ Finished: SUCCESS 5. Extended Choice Parameter Overview: The Extended Choice Parameter Plugin provides advanced parameter types for Jenkins jobs, such as multi-select lists, checkboxes, and more complex formats. Key Features: · Parameter Types: Includes multi-select, checkboxes, and other advanced input options. . Dynamic Choices: Allows dynamic generation of choices from scripts or external sources. Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Extended Choice Parameter Plugin," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add an Extended Choice Parameter. ○ Configure the parameter type, choices, and other settings. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 110 ---
Unit: III Course Code/Title:CS3V15/Devops Usage: · Useful for scenarios where complex user input is needed or where multiple selection options are required. Configuring Jenkins to Work with Java, Git, and Maven 1. Configuring Jenkins to Work with Java Overview: Jenkins requires Java to run. Configuring Java in Jenkins involves specifying the Java Development Kit (JDK) installations Jenkins should use. Steps: 1. Install Java Development Kit (JDK): ○ Ensure that JDK is installed on your system. You can download it from the Oracle website or use OpenJDK. 2. Configure JDK in Jenkins: ○ Open Jenkins and go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to JDK section and click Add JDK. ○ Enter a name for the JDK installation (e.g., JDK 11). Check Install automatically to let Jenkins download and install the JDK, or specify the path to an existing JDK installation. ○ If specifying the path manually, provide the JAVA_HOME directory. 3. Example Configuration: ○ Name: JDK 11 ○ JAVA_HOME: /usr/lib/jvm/java-11-openjdk 4. Verify JDK Configuration: You can verify the JDK configuration by creating a simple Jenkins job and adding a build step that prints the Java version using: bash Copy code java -version 2. Configuring Jenkins to Work with Git Overview: The Git Plugin integrates Jenkins with Git repositories, enabling Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 35 1750 Rank

--- Page 35 ---
Unit: I Course Code/Title:CS3V15/Devops Push: Push is the operation that sends your committed changes to a remote repository, making them accessible to others. Git workflow v0.1 v0.2 v1.0 Master Hotfix Release Develop Feature Feature Git Flow is a structured branching model designed for projects with well- defined release cycles and a need for strict quality control. Branches: The branching model described is commonly known as the Gitflow Workflow. It's a branching strategy that defines a strict branching model designed to facilitate collaboration and streamline the release process. Let's go into detail about each branch: clone operation Working Copy edit, add, move files update operation Modify working copy status and diff operation Review changes commit and push operation push operation Commit changes push operation amend and push operation Review changes 1. Master Branch: The `master'branch represents the main codebase and contains production- ready code. This branch is typically stable and should only include thoroughly tested and approved changes. Each commit on the 'master' branch represents a new version or release of the software. 2. Develop Branch: The 'develop' branch is an integration branch where various feature branches are merged. It serves as a staging area for testing new features and ensuring they work well together before merging into the 'master' branch. This branch may have ongoing development work and is not necessarily always in a production- ready state. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 35 nirf 1750 Rank N33

--- Page 36 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Feature Branches: Feature branches are created for developing new features or implementing changes. These branches are typically based on the 'develop'branch. Once a feature is complete, the branch is merged back into the 'develop' branch. Feature branches allow developers to work on specific tasks without affecting the main codebase. 4. Release Branch: The 'release' branch is created when the 'develop' branch reaches a point where it is ready for a production release. This branch is used for final testing, bug fixes, and preparing the code for deployment. No new features should be added to the release branch. Once the release is deemed stable, it is merged into both the 'master' branch and the 'develop' branch. 5. Hotfix Branch: The 'hotfix' branch is used to quickly address critical issues or bugs in the production code. It is created directly from the 'master' branch. Hotfixes are intended to be small and focused on resolving the specific issue at hand. Once the hotfix is complete, it is merged into both the 'master'branch and the `develop'branch to ensure that the fix is applied to future releases. Here is the typical flow: · Developers work on feature branches based on the 'develop' branch. · Completed features are merged into the 'develop' branch. · When ready for a release, a 'release' branch is created from 'develop". · The release branch undergoes testing and bug fixes. · The release branch is merged into both 'master' and 'develop' once it's stable. · If a critical issue arises in production, a 'hotfix' branch is created from 'master'. · The hotfix is merged into both 'master' and 'develop to keep both branches in sync. · This Gitflow Workflow helps maintain a structured development process, ensuring that features are developed, tested, and released in a controlled manner. Example Scenario: · Imagine you're working on a large software project with a team of 10 developers. You have a major release planned for every six months. · You create a "feature/Ticket-Id" branch to develop a new login system. · Once the feature is complete, it's merged into the "develop" branch for integration and testing. · As the release date approaches, you create a "release/v1.0" branch to freeze code for the upcoming release. · Any critical issues discovered in the production environment are fixed in "hotfix" branch and merged into "master" and "develop." CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 36 N33 nirf 1750 Rank

--- Page 103 ---
Course Code/Title:CS3V15/Devops Unit: III =Publish HTML reports ? Reports HTML directory to archive ? E:\Eclipse_workplace\SeleniumDemo\test-output Index page[s] ? index.html Index page title[s] (Optional) Report title ? HTML Report Publishing options Step 11 : Click on Apply and then Save. Now build this project. Step 12 : On successful build, it creates the HTML Report as follows - x + € > C @ localhost:0000/juby/HTML/\portDemo/HTML_20Report/ Back to HTMLBepsatDemo Test results All suites Default suite 11 D myPackage.myTestPage Info . Repetir output Results . Thus using HTML Publish Report Plugin we can successfully generate HTML report of some testing framework application. · Troubleshooting : Sometimes, the raw file is displayed as HTML report. To get the HTML formatted report we must click on Manage Jenkins section, scroll down and locate Script Console section. Click on it. Tools and Actions C Broad Configuration Sium Disk Discard all the loaded alla lo memory and reloud Jerking CLI Script Console Ewecubes arbitrary suyt lor and run below given command : System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", ") . Then restart the Jenkins Page and Build the project once again. The HTML report can then be displayed as given in above step. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 28 N32 nirf 1750 Rank

--- Page 104 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Copy Artifact Overview: The Copy Artifact Plugin allows Jenkins jobs to copy build artifacts from other jobs, facilitating artifact reuse across different jobs. Key Features: · Artifact Copying: Copies files from one build to another, which can be from a specific build or the latest successful build. · Triggering Builds: Can be used in downstream jobs to fetch artifacts from upstream jobs. Configuration: 1. Install the Plugin: Go to Manage Jenkins > Manage Plugins > Available tab, search for "Copy Artifact Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Build section. ○ Add a Copy artifacts from another project build step. o Specify the project name, build number or criteria, and the target directory for copied artifacts. Usage: · Commonly used in multi-job pipelines where artifacts need to be shared between different jobs or stages. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 29 N3 nirf 1750 Rank

--- Page 39 ---
Course Code/Title:CS3V15/Devops Unit: I GITHUB GitHub is an increasingly popular programming resource used for code sharing. It's a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in October 2020, it is the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 56 million total developers. GitHub is a Git repository hosting service that provides a web-based graphical interface. It is the world's largest coding community. Putting a code or a project into GitHub brings it increased, widespread exposure. Programmers can find source codes in many different languages and use the command-line interface, Git, to make and keep track of any changes. GitHub helps every team member work together on a project from any location while facilitating collaboration. You can also review previous versions created at an earlier point in time. GitHub's Features? 1. Easy Project Management: GitHub is a place where project managers and developers come together to coordinate, track, and update their work so that projects are transparent and stay on schedule. 2. Increased Safety With Packages Packages can be published privately, within the team, or publicly to the open-source community. The packages can be used or reused by downloading them from GitHub. 3. Effective Team Management GitHub helps all the team members stay on the same page and organized. Moderation tools like Issue and Pull Request Locking help the team to focus on the code. 4. Improved Code Writing Pull requests help the organizations to review, develop, and propose new code. Team members can discuss any implementations and proposals through these before changing the source code. 5. Increased Code Safety GitHub uses dedicated tools to identify and analyze vulnerabilities to the code that other tools tend to miss. Development teams everywhere work together to secure the software supply chain, from start to finish. 6. Easy Code Hosting All the code and documentation are in one place. There are millions of repositories on GitHub, and each repository has its own tools to help you host and release code. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 39 N32 nirf 1750 Rank

--- Page 40 ---
Course Code/Title:CS3V15/Devops Unit: I HOSTING SERVICE FOR GIT REPOSITORY When it comes to hosting Git repositories, various platforms provide a robust infrastructure for collaborative development, version control, and project management. Choosing the right hosting service depends on factors like ease of use, collaboration features, and integration capabilities. Here are some popular Git hosting services widely utilized in the software development community: 1. GitHub: GitHub stands out as one of the most prevalent Git hosting platforms, offering a user- friendly interface, powerful collaboration features, and seamless integration with various tools. It serves as an ideal choice for open-source projects, private repositories, and team collaboration. 2. GitLab: GitLab is a comprehensive web-based Git repository manager that not only provides source code management but also includes features like continuous integration. It caters to both cloud-based and self-hosted solutions, giving users flexibility in hosting their repositories. 3. Bitbucket: Owned by Atlassian, Bitbucket is another popular Git repository hosting service. Supporting both Git and Mercurial repositories, it offers features like code collaboration, issue tracking, and continuous integration. Bitbucket is often preferred by teams using other Atlassian tools such as Jira and Confluence. 4. GitKraken Glo Boards: GitKraken Glo Boards is an integrated task and issue tracking service linked with GitKraken, a Git client. This platform allows teams to manage tasks directly associated with their Git repositories and provides a visual approach to monitoring project progress. 5. SourceForge: SourceForge, with a long history, hosts open-source software projects and offers version control, bug tracking, and project management tools. While not as prominent as some other options, it remains a viable choice for numerous projects. 6. AWS CodeCommit: As part of Amazon Web Services (AWS), AWS CodeCommit is a fully managed source control service. It seamlessly integrates with other AWS services and provides a secure and scalable environment for hosting Git repositories. Selecting the most suitable Git hosting service depends on your team's requirements, project size, and preferences for cloud-based or self-hosted solutions. Each platform has its strengths, catering to specific use cases within the software development landscape. Difference between GIT and GITHUB Git and GitHub are related concepts but serve different purposes in the context of version control and collaborative software development. Git: Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows. GitHub: GitHub is a web-based Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 40 N32 nirf 1750 Rank

--- Page 105 ---
Course Code/Title:CS3V15/Devops Unit: III How to install copy artifact plugin ? Step 1 : Start the Jenkins tool by entering the login and password. Step 2 : Click on Manage Jenkins. Click on Plugins. Step 3 : In the search window, type the name copy Artifact. Step 4 : The plugin name will be displayed if it is not already installed. Select it. Name Adde a build atop to copy artifacts from another project. Thủa plugin le up for adoption! We are looking for naw maintainent. Velt our Adopt a Plugin intuitive for mort information. then click on Install. The plugin gets installed. Example Demo · Let us discuss how to use Copy Artifact plugin. Step 1 : Create a new Job by clicking on item New Item. Give the some name to your job. I have given the name App1 and selected the Freestyle Project. Step 2: Give suitable description in the description box. Step 3 : Under the source code management, click on Git option and specify the repository URL of GitHub. Source Code Management None Git ? Repositories 1 Repository URL + ·hong · CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 30 N3 nirf 1750 Rank

--- Page 106 ---
Unit: III Course Code/Title:CS3V15/Devops Step 4 : Under the Build Steps section, choose the Execute Windows batch command option. And type the javac and java commands. Build Steps = Execute Windows batch command Command See the list of available environment variables javac test.java java test Advanced Add bulld stop * Step 5 : Click in Apply and Save. Step 6 : Select the Build Now option. On successful built, the test, class file gets generated in the Jenkins workspace of the App1 job. C:\ProgramData\Jenkins\.jenkins\workspace\Appl>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\Appi>java test Good Morning Pathill Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! C:\ProgramData\Jenkins\.jenkins\workspace\Appl>exit @ Finished: SUCCESS Step 7 : Now we have to copy test.class file from the workspace of Appl to another job. For that' purpose we will create another Job named App2 by clicking New Item. Select the Freestyle Project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 31 N3 nirf 1750 Rank

--- Page 107 ---
Course Code/Title:CS3V15/Devops Unit: III Step 8 : Then configure the Build Steps by selecting the option Copy artifacts from another project as follows - . . First of all give the name of the project from which you want to copy the files. In my case it is App1. . Then select the option "copy from WORKSPACE of latest completed build" · Give the regular expression ** / *.*. Here's a breakdown of what ** / *.* means : o ** : The double asterisk ( ** ) is a wildcard that matches zero or more directories and subdirectories. It's often used to recursively search through all directories and subdirectories. o / : The forward slash (/) is used as a directory separator, typically on Unix-like systems. o *.* : This part of the regular expression is used to match files with an extension. In regular expressions, the * symbol represents "zero or more" of the preceding character or pattern, and the . character is used to match any character. So, *.* essentially matches any file with any extension. So, when you use the regular expression ** / *.* in a Jenkins build configuration, you are instructing Jenkins to include all files with any extension ( *.* ) located in any directory and subdirectory ( ** ) within the workspace or specified location. Build Steps E Copy artifacts from another project Project name ? App: Which build ? Copy from WORKSPACE of latest completed buld Limitation Note ? Artifacts to copy ? Artifacts not to copy ? : Toget directory CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 32

--- Page 108 ---
Course Code/Title:CS3V15/Devops Unit: III Add one more Build Step and select Execute Windows batch Command. Give the command dir so that the name of files and directories that are copied from App1 to App2 will be displayed on the Console. = Execute Windows batch command ? Command See the list of available environment variables ---- dir ----- Advanced v Add build step * Click Apply and Save. Step 8 : Now Build this project (App2). The output will be as follows - Started by user Anuradha P Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\.jenkins\workspace\App2 Copied 16 artifacts from "App1" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621109248227895.bat C:\ProgramData\Jenkins\,jenkins\workspace\App2>dir Volume in drive C Is OS Volume Serial Number is CGC2-BA9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023* 15:07 <DIR> 01-09-2023 15:07 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 474 test.class 144 test. java 2 File(s) 616 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 33 N32 narf 1750 Rank

--- Page 33 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between AWS (Amazon Web Services), Google Cloud, and Azure Technology EC2 (Elastic Compute Cloud) AWS Databases Supported Pricing Models Difficulties Many enterprises find it difficult to understand the company cost AWS fully relational and supports NoSQL databases and Big Data. Per hour - rounded up. On demand, reserved spot. structure. Google Cloud Google Engine(GCE) Compute Technologies pioneered by Google, like Big Query, Big Table, and Hadoop, are databases, and Big Data,naturally fully supported. Per minute - rounded up On demand sustained use. — Fewer services. features Azure VHD Disk) (Virtual Hard Azure supports relational and both NoSQL Windows and through AzureTable HDInsight. Per minute - rounded up. Per minute- rounded up commitments(Pre-paid or monthly) and Less "Enterprise-ready. Storage Services Simple Storage Service(S3) Elastic Block Storage. Elastic File storage. Machine Learing Sage maker. Lex. polly.And many more Blob Storage Queue Storage. File Storage Disk Storage. Data Lake Store Machine learning Azure Bot service Cognitive service GIT Cloud storage. Persistent Disk Transfer appliance. Cloud speech AI Cloud Video Intelligence. Cloud Machine learning engine Git is a distributed version control system (DVCS) that helps manage and track changes in source code during software development. It was created by Linus Torvalds in 2005 and has become one of the most widely used version control systems in the software development industry. Some key concepts and features of Git: · Version Control: Git allows developers to keep track of changes made to their code over time. This includes modifications, additions, and deletions of files. · Distributed System: Git is a distributed version control system, meaning that each developer has a complete copy of the entire repository, including its full history. This allows developers to work independently and merge their changes when necessary. · Branching: Git enables developers to create branches, which are essentially separate lines of development. This allows for the parallel development of features or bug fixes without affecting the main codebase. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) LEN 33 nirf 1750 Rank

--- Page 34 ---
Course Code/Title:CS3V15/Devops Unit: I · Merging: Git provides tools for merging changes from one branch into another. This is essential when multiple developers are working on different branches and need to bring their changes together. · History Tracking: Git maintains a complete history of changes made to the codebase. Developers can view, revert, or analyze changes made over time. · Remote Repositories: Git supports remote repositories, allowing developers to collaborate with others by pushing and pulling changes to and from a shared repository. Platforms like GitHub, GitLab, and Bitbucket provide hosting services for Git repositories. · Staging Area: Git uses a staging area (also known as the index) to prepare and review changes before committing them to the repository. This allows developers to selectively include or exclude specific changes. · Open Source: Git is an open-source tool, and its source code is freely available for modification and distribution. Various GIT Components: Git is composed of several key components that work together to enable version control and collaborative development. Here are the main components of Git: · Repository (Repo): A repository is a directory or storage space where your project and its version history are stored. It contains all the files and directories associated with your project, along with the metadata and configuration information. · Working Directory: The working directory is the directory on your local machine where you manipulate files and make changes to your project. It is essentially your local copy of the repository. · Index (Staging Area): The index, also known as the staging area, is a middle ground where changes are prepared before being committed to the repository. It allows you to selectively stage changes, which means you can choose which modifications to include in the next commit. · Commit: A commit is a snapshot of the changes made to the files in the repository. It represents a specific point in the project's history and is accompanied by a commit message that describes the changes. · Branch: A branch is a parallel line of development within a repository. It allows developers to work on different features or bug fixes simultaneously without affecting the main codebase. Branches can be merged to incorporate changes into other branches. . Head: HEAD is a reference to the latest commit in the currently checked-out branch. It essentially points to the tip of the branch you are currently on. · Remote: A remote is a version of the repository stored on a different server. Git supports collaboration by allowing developers to push and pull changes between their local repository and remote repositories. Platforms like GitHub, GitLab, and Bitbucket are examples of remote repositories. · Clone: Cloning is the process of creating a copy of a remote repository on your local machine. This allows you to start working on your own copy of a project. · Fetch: The fetch operation retrieves changes from a remote repository but does not automatically merge them into your working directory. It is useful for reviewing changes before deciding to merge. · Pull: Pull is a combination of fetch and merge. It retrieves changes from a remote repository and automatically merges them into your working directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 79 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Plugins: ○ Role: Extend Jenkins functionality without modifying the core software. ○ Types: I Source Control Plugins: Git, SVN, Mercurial. Build Tools Plugins: Maven, Gradle, Ant. Notification Plugins: Email, Slack, HipChat. · Reporting Plugins: JUnit, Cobertura, Checkstyle. ○ Management: Plugins can be installed, updated, and configured via the Jenkins web interface. 5. Master-Slave Architecture: ○ Concept: The Jenkins master distributes build tasks to multiple agents, enabling distributed builds. ○ Benefits: Scalability: Handle more builds simultaneously. I Resource Management: Assign builds to agents with the necessary resources. Isolation: Run builds in isolated environments to avoid conflicts. Workflow and Data Flow 1. Job Trigger: ○ Jobs can be triggered by various events: Manual Trigger: Initiated by a user through the Jenkins interface. Scheduled Trigger: Using CRON-like syntax to schedule jobs. · SCM Trigger: Automatically triggered by changes in the source code repository. Upstream/Downstream Trigger: Triggered by the completion of other jobs. 2. Build Execution: ○ Job Assignment: The master assigns the job to an available agent based on labels, availability, and resource requirements. Build Environment Setup: The agent sets up the environment, including checking out the code, installing dependencies, and configuring the workspace. ○ Execution: The agent runs the build steps as defined in the job configuration. 3. Build Results: ○ Logs: Captures console output and logs from the build process. ○ Artifacts: Stores build artifacts like binaries, packages, and reports. ○ Test Results: Collects and displays test results and code coverage reports. ○ Build History: Maintains a history of all builds, including status, duration, and changes. 4. Notification and Reporting: ○ Notifications: Jenkins can send notifications through various channels (email, chat, etc.) upon build completion or failure. ○ Dashboards: Provides dashboards for visualizing build status, trends, and metrics. ○ Reports: Generates and displays reports on test results, code quality, and other metrics. INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 4 1750 Rank

--- Page 80 ---
Unit: III Course Code/Title:CS3V15/Devops Summary Jenkins' architecture is designed to be flexible and scalable. The master-agent model allows for efficient distribution of build tasks, while plugins provide extensive customization options. By leveraging Jenkins' comprehensive job configuration and robust notification and reporting capabilities, teams can streamline their CI/CD pipelines and improve software quality and delivery speed. Creating a Jenkins Job Creating a Jenkins job involves setting up a new project within Jenkins to automate various tasks such as building code, running tests, and deploying applications. Here is a step-by-step guide to creating a Jenkins job. Prerequisites · Jenkins installed and running. · Basic understanding of the Jenkins interface. . Access to the Jenkins dashboard. Step-by-Step Guide 1. Access Jenkins Dashboard: . Open your web browser and navigate to your Jenkins instance URL (e.g., http://localhost:8080). 2. Create a New Job: ○ On the Jenkins dashboard, click on the "New Item" link on the left-hand side menu. 3. Enter Job Name: ○ Enter a name for your new job in the "Enter an item name" field. o Choose the type of job you want to create. For most uses, "Freestyle project" is a good starting point. ○ Click "OK". 4. Configure the Job: ○ After clicking "OK", you will be directed to the job configuration page. 5. General Configuration: ○ Description: Provide a brief description of the job. ○ Discard Old Builds: Optionally set this to limit the number of old builds Jenkins keeps. ○ Select Version Control System: Choose the version control system (e.g., Git, Subversion). 6. Source Code Management: ○ Repository URL: Enter the repository URL. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 5 nirf 175 Rank N33

--- Page 5 ---
Unit: I Course Code/Title:CS3V15/Devops Example: Facebook's mobile app which is updated every two weeks effectively tells users you can have what you want and you can have it. Now ever wondered how Facebook was able to do social smoothing? It's the DevOps philosophy that helps facebook and sure that apps aren't outdated and that users get the best experience on Facebook. Facebook accomplishes this true code ownership model that makes its developers responsible that includes testing and supporting through production and delivery for each kernel of code. They write and update their true policies like this but Facebook has developed a DevOps culture and has successfully accelerated its development lifecycle. Difference between Agile and DevOps: S. No. Agile 1 It started in the year 2001. It started in the year 2007. 2 Invented by John Kern, and Martin Fowler. Invented by John Allspaw and Paul Hammond at Flickr, and the Phoenix Project by Gene Kim. 3 Agile is a method for creating software. It is not related to software development. Instead, the software that is used by DevOps is pre-built, dependable, and simple to deploy. 4 An advancement and administration approach. Typically a conclusion of administration related to designing. 5 The agile handle centers on consistent changes. DevOps centers on steady testing and conveyance. 6 A few of the finest steps embraced in Agile are recorded underneath - 1. Backlog Building 2.Sprint advancement DevOps to have a few best hones that ease the method - 1. Focus on specialized greatness. 2. Collaborate straightforwardly with clients and join their feedback 7 Agile relates generally to the way advancement is carried of, any division of the company can be spry in its hones. This may be accomplished through preparation. 8 All the group individuals working in a spry hone have a wide assortment of comparable ability sets. This is often one of the points of interest of having such a group since within the time of requirement any of the group individuals can loan help instead of holding up for the group leads or any pro impedances. 9 Spry accepts "smaller and concise". Littler the group superior it would be to convey with fewer complexities. 10 Since Agile groups are brief, a foreordained sum of time is there which are sprints. Tough, it happens that a sprint has endured longer than a month but regularly a week long. 11 A big team for your project is not required. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY DevOps DevOps centers more on program arrangement choosing the foremost dependable and most secure course. DevOps features a diverse approach and is very viable, most of the time it takes after "Divide and Conquer". Work partitioned among the improvement and operation groups. DevOps, on the other hand, accepts that "bigger is better". DevOps, on the other hand, prioritizes reliabilities. It is since of this behavior that they can center on a long-term plan that minimizes commerce's unsettling influences. It demands collaboration among different teams for the completion of work. 5 N33 nirf 1750 Rank

--- Page 6 ---
Unit: I Course Code/Title:CS3V15/Devops 12 Some of the Tools- · Bugzilla · JIRA · Kanboard and more. 13 It is suitable for managing complex projects in any department. It centers on the complete engineering process. 14 It does not focus on the automation. It focusses on automation. 15 Working system gets more significance in Agile than documentation. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Some of the Tools- · Puppet • Ansible • AWS · Chef · team City OpenStack and more. The process documentation is significant in DevOps. 6 N32 nirf 1750 Rank

--- Page 91 ---
Course Code/Title:CS3V15/Devops Unit: III Now we will create a repository on GitHub and then push the above committed Git repository on the GitHub. Open a web browser, and login to www.GitHub.com. Create a repository by the name MyJavaPrograms. Start a new repository A repository contains all of your project's files, revision history, and collaborator discussion. AnurdhaP / MyJavaPrograms Public Anyone on the internet can see this repository Private You choose who can see and commit to this repository Create a new repository 1 then copy the URL of this repository. HTTPS SSH https://github.com/AnurdhaP/MyJavaPrograms.git Now open the command prompt window and go to the MyJavaPrograms directory and issue the following command. git remote add origin https://github.com/AnurdhaP/MyJavaPrograms.git then push the Git to GitHub repository using the command push -u origin master CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 16

--- Page 92 ---
Course Code/Title:CS3V15/Devops Unit: III It is illustrated as follows - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git push -u origin master Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 719 bytes | 719.00 KiB/s, done. Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 To https://github.com/AnurdhaP/MyJavaPrograms.git * [new branch] master -> master branch 'master' set up to track 'origin/master'. E: \MyJavaPrograms> · Now it we refresh the repository on GitHub then we can see our test.java and test.class programs are present in that repository. MyJavaPrograms Public !" master - 1º 1 branch 0 tags Go to f AnurdhaP First Commit for Java program .4cb test.class First Commit for Java program test.java First Commit for Java program Help people interested in this repository understand your project by adding a README. Step 4 : Now create a job in Jenkins that makes use of GitHub plugin in order to access the GitHub repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 17 N3 nirf 1750 Rank

--- Page 95 ---
Course Code/Title:CS3V15/Devops Unit: III C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha C: \ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>exit e Finished: SUCCESS Step 6 : Now go to GitHub repository and make some changes to your test.java file and commit these changes. BE AnurdhaP Changes in the test java Code Blame 6 lines (6 loc) · 158 Bytes 89 Code 55% faster with GitHub Copilot > public class test { 2 public static void main(String args[]) { for(int 1=1;i <= 5;1++) 3 4 System.out.println("Good Morning Path !!! "); 5 3 6 } Step 7 : Now, if you check on the Jenkins page, it automatically executes the above updated test.java file and creates a build. It is as follows .- C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test. java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Good Morning Path !!! Good Morning Pathi !! Good Morning Path !!! Good Morning Path !!! Good Morning Pathill 2. Parameter Plugin Overview: The Parameter Plugin allows Jenkins jobs to accept parameters at build time, enabling dynamic and flexible builds. Key Features: · Parameter Types: Supports various parameter types such as string, choice, boolean, and more. · Default Values: Provides default values for parameters. · Prompt for Parameters: Users are prompted to enter values for parameters when starting a build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 20

--- Page 96 ---
Course Code/Title:CS3V15/Devops Unit: III Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Parameterized Builds," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add parameters of different types (e.g., string, choice) and configure their options and default values. Usage: · Useful for creating jobs that require user input or need to be customized for different build scenarios. A Example Demo Step 1 : Click on New item and create a job. I have created a job by the name Welcome (Refer section 3.5) Step 2 : Click on the existing job and configure it with parameters. Step 3 : Check the option "This project is parameterized". Select the String Parameter. Create some string parameter. I have created a parameter named Username and default value to it. This project is parameterized ? =String Parameter ? Name ? Username Default Value ? Parth Description ? Plain text Preview Trim the string ? Step 4 : Under the Build Steps section, using echo command I tried to display the parameter value CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 21

--- Page 11 ---
Course Code/Title:CS3V15/Devops Unit: I 17. Selenium It remains a vital tool in software testing and automation due to its enduring relevance in ensuring the quality of web applications. As technology evolves, web applications become increasingly complex, requiring thorough testing across various browsers and platforms. With its robust automation capabilities and extensive browser compatibility, Selenium allows developers and QA teams to automate repetitive testing tasks efficiently, conduct cross- browser testing, and ensure that web applications function flawlessly across diverse environments. Its open-source nature, active community support, and integration with other DevOps tools make Selenium a go-to choice for organizations striving for continuous delivery and the rapid deployment of high-quality software, a cornerstone of modern software development practices. 18. Gremlin Gremlin is an essential tool in chaos engineering, which has become increasingly critical for ensuring the resilience and reliability of modern software systems. As technology advances and complex distributed systems become the norm, the potential for unexpected failures and outages also rises. Gremlin allows organizations to proactively identify weaknesses and vulnerabilities in their infrastructure and applications by simulating controlled failures, such as network disruptions, service outages, and resource constraints. By intentionally inducing chaos and monitoring the system's response, teams can uncover weaknesses before they lead to costly downtime or security breaches. Gremlin facilitates organizations to build more robust, fault-tolerant systems that can withstand real-world challenges and deliver uninterrupted services to users. 19. ServiceNow ServiceNow is a vital platform for organizations seeking to streamline their IT service management and beyond. Its significance lies in its ability to provide a unified, cloud-based solution for automating and optimizing various business processes, including ITSM, ITOM, HR, customer service, and more. Due to the rapid digitization of services, remote work, and the growing complexity of technology infrastructures, ServiceNow offers a comprehensive approach to managing workflows, resolving issues, and delivering services efficiently. Its intelligent automation capabilities, analytics, and AI-driven insights empower organizations to enhance productivity, agility, and customer satisfaction while reducing operational costs. ServiceNow's role in orchestrating and integrating diverse systems and processes makes it an indispensable tool for driving digital transformation and ensuring smooth operations in the ever-evolving business landscape of 2024. 20. Status Service Updates: The Status Page "Status Service Updates: The Status Page" is a critical tool for organizations and businesses of all sizes. In today's world, where online services and applications are integral to operations, ensuring their availability and reliability is essential. It provides real-time information to users and stakeholders about the operational status of services, applications, and infrastructure. The Status Page plays a crucial role in transparency, trust-building, and customer satisfaction by promptly communicating service disruptions, planned maintenance, and incident resolutions. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 1750 Rank

--- Page 12 ---
Course Code/Title:CS3V15/Devops Unit: I Downtime can often lead to significant financial losses and damage to a company's reputation, so having a practical Status Page becomes not just a convenience but a necessity. It allows organizations to showcase their commitment to transparency and responsiveness in addressing service-related issues, ultimately fostering stronger customer relationships and trust. 21. ELK (Elasticsearch, Logstash and Kibana) ELK, which stands for Elasticsearch, Logstash, and Kibana, continues to shine in DevOps and IT operations. This powerful trio of tools remains essential for organizations seeking effective log management, monitoring, and data visualization. Elasticsearch is a highly scalable and fast search engine that enables real-time data indexing and search. Logstash facilitates the collection, processing, and transformation of log data from various sources, making it compatible with Elasticsearch. Kibana, on the other hand, provides a user- friendly interface for visualizing and analyzing data, offering customizable dashboards and powerful data exploration capabilities. ELK's significance in 2024 lies in its ability to empower organizations with comprehensive insights into their systems, applications, and infrastructure. It ultimately facilitates quick problem resolution, proactive monitoring, and data-driven decision-making in an increasingly complex and fast-paced technological landscape. 22. GitLab CI/CD GitLab CI/CD's significance lies in its ability to automate the complete software delivery pipeline, from code changes to deployment, in a single integrated environment. GitLab CI/CD ensures rapid and reliable delivery of software updates. It enables continuous integration (CI) by automatically building and testing code changes, allowing teams to catch issues early in the development cycle. Furthermore, the continuous deployment (CD) aspect automates the release and deployment process, reducing the risk of human errors and enabling organizations to deliver features and updates to users swiftly and confidently. GitLab CI/CD's importance is further accentuated as businesses seek to accelerate digital transformation efforts, respond rapidly to changing market demands, and maintain a competitive edge through efficient and automated software delivery practices. 23. Scripting Scripting remains vital due to its pivotal role in automating and streamlining various aspects of software development, system administration, and DevOps practices. Scripting languages like Python, Bash, and PowerShell empower tech professionals to write code that can execute repetitive tasks, manipulate data, and orchestrate complex processes efficiently. Scripting facilitates rapid prototyping, configuration management, and the creation of automated deployment pipelines. It enhances productivity, ensures consistency and reduces human error in tasks ranging from software testing and deployment to infrastructure provisioning and monitoring. As organizations increasingly embrace DevOps and cloud- native technologies, scripting stays competitive and adaptive in the tech landscape. 24. Terraform Terraform plays a crucial role in modern infrastructure provisioning and management. It allows organizations to define and deploy infrastructure as code, enabling the automated creation and configuration of cloud resources, containers, and other infrastructure components. Cloud computing, microservices, and containerization have become the norm in CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 12 N32 nirf 1750 Rank

--- Page 1 ---
Course Code/Title:CS3V15/Devops CHENNAI INSTITUTE OF TECHNOLOGY Transforming Lives CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) NAME OF THE PROGRAMME B. E (CSE) YEAR III SEMESTER V REGULATIONS 2022R COURSE CODE CS3V15 COURSE NAME DEVOPS FACULTY NAME (Prepared by) NAME OF SUBJECT EXPERT (Verified by) M.SUNDHARI CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Unit: I N33 INATIONAL BOARD . ACCREDITATION 100% Accreditation All eligible UG & PG Programs NIRF 151 - 200 Band Engineering 2023 A+ DE NAAC Contact 8681927167 Contact 1 N32 nirf 1750 Rank

--- Page 2 ---
Course Code/Title:CS3V15/Devops Unit: I UNIT - I Introduction to Devops Devops Essentials - Introduction To AWS, GCP, Azure - Version control systems: Git and Github. The word "DevOps" was coined in 2009 by Patrick Debois, who became one of its gurus. The term was formed by combining "development" and "operations," which provides a starting point for understanding exactly what people typically mean when they say "DevOps." Notably, DevOps isn't a process or a technology or a standard. Many devotees refer to DevOps as a "culture"-a viewpoint that New Relic favors. We also use the term "DevOps movement" when talking about topics such as adoption rates and trends for the future, and "DevOps environment" to refer to an IT organization that has adopted a DevOps culture. "DevOps represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile, lean practices in the context of a system-oriented approach. DevOps emphasizes people (and culture), and seeks to improve collaboration between operations and development teams. DevOps implementations utilize technology- especially automation tools that can leverage an increasingly programmable and dynamic infrastructure from a life cycle perspective." DevOps is defined as a combination of processes and tools created to facilitate organizations in delivering services and applications much faster than they can through conventional software development processes. It helps increase customers' confidence in the applications that an organization offers, thereby allowing the company to flourish and achieve its business goals faster. Development 3 3 DevOps 13 51 Operations Quality Assurance (QA) How DevOps Works? A DevOps process can be summarized as an infinite loop that comprises the following stages - build, test, and release through the delivery pipeline and plan and monitor through feedback, which resets the loop again. With such an amazing combination, teams use tech stack and tooling that assists them in reliably developing apps. Moreover, going away from the Under the DevOps model, development and operations teams work in constant cohesion throughout the entire project lifecycle, starting right from development to deployment. When security is the main focus, the quality assurance team is tightly knitted with the DevOps team throughout the app lifecycle. In this situation, some DevOps teams are also referred to as DevSecOps. Close coordination with the QA team ensures that no loopholes are left unchecked in the provided service/app. COMPANY HOW DEVOPS WORKS DELIVERY PIPELINE BUILD TEST RELEASE PLAN MONITOR FEEDBACK LOOP TOOLBOX" CUSTOMER norm, teams use automated processes here. DevOps tools also allow engineers to complete different tasks independently. Be it provisioning infrastructure or deploying code, they can accomplish these tasks without being dependent on one another. As such, the DevOps model accelerates the overall application development process. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 93 ---
Course Code/Title:CS3V15/Devops Unit: III And configure the Jenkins job as follows - General Description In this job we have used GitHub plugin Plain text Preview Discard old builds ? GitHub project This project is parameterized ? Throttle builds Execute concurrent builds if necessary Advanced V Source Code Management None Git ? Repositories ? Repository URL https://github.com/AnurdhaP/MyJavaPrograms.gil Please enter Git repository. Credentials ? - none - Add Advanced CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 18 N3 nirf 1750 Rank

--- Page 94 ---
Course Code/Title:CS3V15/Devops Unit: III Build Triggers Trigger builds remotely (e.g. from scripts) ? Build after other projects are built ? Build periodically ? GitHub hook trigger for GITSem polling ? Poll SCM ? Schedule ? A Do you really mean "every minute" when you say "" "" " ""? Perhaps you meant "H" "" "" to poll once per hour Would last have run at Wednesday, 30 August, 2023 at 4:11:02 pm india Standard Time; would'nest run at Wednesday, 30.A Ignore post-commit hooks ? Build Steps Execute Windows batch command ? Command See the list of available environment variables javas test.java java test. java ----- Advanced V Add build sten Save this configuration. Step 5 : Now if we build this job the output that we see is as follows on Console Output. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 19 N3 nirf 1750 Rank

--- Page 119 ---
Course Code/Title:CS3V15/Devops Unit: IV · Groupings in inventories enable you to organize your infrastructure, allowing you to target specific subsets of nodes based on your needs. For instance, when you need to perform tasks in a particular location, groupings become essential. Creating Inventories There are two primary methods to create an inventory on the control node: using a YAML file or an INI file. 1. INI Inventory Example (inventory.ini): 1[virtualmachines] 2host1 ansible_host:192.168.0.1 3192.168.0.2 2. In this INI file, it captures the IP addresses of managed nodes. The default username used for SSH connections is typically the working user of your control node. 3. ansible_host: This parameter is used to specify the IP address, although you can write the IP directly. 4. Inventory.yaml 1 --- 2virtualmachines: # Define a class name to represent a particular group of devices. 1 hosts: # Define the hosts. 2 vm01: # Define a parent name to indicate a class of parent type. 3 ansible_host: 192.168.0.1 # Specify the IP address. 4 http_port: 80 # (optional) Define the HTTP port. 5 ansible_user: auriga # (optional) Provide the username if it's different from the control node's user. o ansible_user: Use this to specify a different user if the managed node's user is not the same as the control node's user. o http_port: Specify a port if necessary. In real-world scenarios, inventory files can be more complex, reflecting the diverse and extensive infrastructure of organizations. You can explore more configurations here. Verifying Your Inventory After creating your inventory, it's a good practice to verify its correctness. This step ensures that the inventory is correctly structured and accessible to Ansible. To do this, use the following command: 1ansible-inventory -i inventory.yaml -list · - i: Indicates the inventory file you want to use. . - list: Requests the listing of the inventory content. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 5 N33 nirf 1750 Rank

--- Page 120 ---
Course Code/Title:CS3V15/Devops Unit:IV output: auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-inventory ·i inventory.yaml -- list "_meta": { "hostvars": { "vm01": "ansible_host": "192.168.0.208", "ansible_user"; "au" "children": [ "ungrouped" "ungrouped": { "hosts": "vn01 1 By following these steps, you have effectively set up and verified your inventory, ensuring a solid foundation for your Ansible automation. Using Ansible Ad-Hoc Commands Introduction: In Ansible, ad-hoc commands are your go-to solution for executing quick, one-off tasks on remote nodes. They provide a straightforward and efficient way to interact with managed nodes without the need for creating full-fledged playbooks. Ad-hoc commands are particularly useful when you need immediate results without the overhead of playbook development. When to Use Ad-Hoc Commands: Ad-hoc commands are best suited for scenarios where the task at hand is simple and doesn't require the complexity of a playbook. They are perfect for tasks like system health checks, package installation, service management, or any other single-operation job. Ad-Hoc Command Syntax: Ad-hoc commands follow a specific syntax that comprises various components, each serving a unique role: . Target Group: This is the group of hosts you intend to target with the ad-hoc command. · Module (-m): Specifies the module to execute. Modules are Ansible's building blocks for performing tasks, and they can range from basic operations like "ping" to more advanced tasks such as package management. · Inventory File (-i): Indicates the location of your inventory file, which defines the list of target devices. Example: Ping All Inventory Devices To illustrate the use of ad-hoc commands, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: 1ansible virtualmachines -m ping -i inventory.yaml virtualmachines is the target group. In this case, it could be any group, or you can use all to target all devices in your inventory. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 141 ---
Course Code/Title: CS3V15/Devops Unit: V UNIT V - BUILDING DEVOPS PIPELINES USING AZURE Create Github Account, Create Repository, Create Azure Organization, Create a new pipeline, Build a sample code, Modify azure-pipelines.yaml file Github Account Creation What is Github? GitHub is a code hosting platform for collaboration and version control. GitHub lets you and others work together on projects fromanywhere. Github is owned by Microsoft, provides access to public(free) and private(paid) repositories. Stepsto create Github Account: Join GitHub · GitHub × + < C https://github.com/join f Facebook - Log In o ... Twitter G Google f Facebook YouTub Product v Solu wiki How Step 1:Go to https://github.com/join in a web browser. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 1 175" Rank

--- Page 142 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: Enter your personal details. In addition to creating a username and entering an email address, you'll also have to create a password. Your password must be at least 15 characters in length or at least 8 characters with at least one number and lowercase letter. Join GitHub First, let's create your user account Username * wikihowneveconcepts Email address * Password * .......... Make sure it's at least 15 characters OR at least 8 characters including a number and a lowercase letter. Learn more. Email preferences Send me occasional product updates, announcements, and offers. Verify your account wiki How Step 3: Click Verify to start the verification puzzle. The instructions vary by puzzle, so just follow the on-screen instructions to confirm that you are a human. A green checkmark will appear after completing the puzzle. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 2

--- Page 153 ---
Create a pipeline in azure What is pipeline Azure Pipelines supports continuous integration (CI)and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline. The latest way to build pipelines is with the YAML pipeline editor. You can also use Classic pipelines with the Classic editor. code release build CI plan continuous testing CD deploy monitor operate Create your first pipeline This is a step-by-step guide to using Azure Pipelines to build a sample application from a Git repository. This guide uses YAML pipelines configured with the YAML pipeline editor. If you'd like to use Classic pipelines instead, see Define your Classic pipeline. For guidance on using TFVC, see Build TFVC repositories. Prerequisites - Azure DevOps A GitHub account where you can create are pository. An Azure DevOps organization. Create one for free. If your team already has one, then make sure you're an administrator of the Azure Devops. 12

--- Page 154 ---
Get the Java sample code To get started, fork the following repository into your GitHub account. https://github.com/MicrosoftDocs/pipelines-java Create your first Java pipeline 1. Sign-in to your Azure DevOps organization and go to your project. 2. Go to Pipelines, and then select New pipeline. 3. Do the steps of the wizard by first selecting GitHub as the location of your source code. 4. You might be redirected to GitHub to sign in. If so, enter your GitHub credentials. 5. When you see the list of repositories, select your repository. 6. You might be redirected to GitHub to install the Azure Pipelines app. If so,select Approve & install. 7. Azure Pipelines will analyze your repository and recommend the Maven pipeline template. 8. When your new pipeline appears, take a look at the YAML to see what it does. When you're ready, select Save and run. 9. You're prompted to commit a new azure pipelines.yml file to your repository. After you're happy with the message, select Save and run again. If you want to watch your pipeline in action, select the build job. You just created and ran a pipeline that we automatically created for you, because your code appeared to be a good match for the Maven template. You now have a working YAML pipeline (azure-pipelines.yml) in your repository that's ready for you to customize! 14

--- Page 131 ---
Course Code/Title:CS3V15/Devops Unit: IV Ensure a service is stopped: $ ansible webservers -m service -a "name=httpd state=stopped" 6. Gathering Facts Fact represents the discovered variables about a system. You can use the facts to implement conditional execution of tasks, and also used to get ad-hoc information about your systems. To see all the facts: $ ansible all -m setup Ansible Playbooks Playbooks are the files where the Ansible code is written. Playbooks are written in YAML format. YAML means "Yet Another Markup Language," so there is not much syntax needed. Playbooks are one of the core features of Ansible and tell Ansible what to execute, and it is used in complex scenarios. They offer increased flexibility. Playbooks contain the steps which the user wants to execute on a particular machine. And playbooks are run sequentially. Playbooks are the building blocks for all the use cases of Ansible. Ansible playbooks tend to be more configuration language than a programming language. Through a playbook, you can designate specific roles to some of the hosts and other roles to other hosts. By doing this, you can orchestrate multiple servers in very different scenarios, all in one playbook. Playbook Structure Each playbook is a collection of one or more plays. Playbooks are structured by using Plays. There can be more than one play inside a playbook. Playbook Play Task Module Target Environment O Task Module Play Task Target Environment Play Module O Task CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY 17 N33 nirf 1750 Rank

--- Page 132 ---
Course Code/Title:CS3V15/Devops Unit:IV name: install and configure DB hosts: testServer become: yes vars: oracle_db_port_value : 1521 tasks: -name: Install the Oracle DB yum: < code to install the DB> -name: Ensure the installed service is enabled and running service: name: < your service name> The Different YAML Tags name This tag specifies the name of the Ansible playbook. As in what this playbook will be doing. Any logical name can be given to the playbook. hosts This tag specifies the lists of hosts or host group against which we want to run the task. The hosts field/tag is mandatory. It tells Ansible on which hosts to run the listed tasks. The tasks can be run on the same machine or on a remote machine. One can run the tasks on multiple machines and hence hosts tag can have a group of hosts' entry as well. vars Vars tag lets you define the variables which you can use in your playbook. Usage is similar to variables in any programming language. tasks All playbooks should contain tasks or a list of tasks to be executed. Tasks are a list of actions one needs to perform. A tasks field contains the name of the task. This works as the help text for the user. It is not mandatory but proves useful in debugging the playbook. Each task internally links to a piece of code called a module. A module that should be executed, and arguments that are required for the module you want to execute. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 18 N33 nirf 1750 Rank

--- Page 127 ---
Course Code/Title:CS3V15/Devops Unit:IV Abbreviation You can also use abbreviation to represent lists. Example Countries: ['America', 'China', 'Canada', 'Iceland'] List inside Dictionaries We can use list inside dictionaries, i.e., value of key is list. Example james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english … List of Dictionaries We can also make list of dictionaries. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english - robert: name: robert richardson rollNo: 53 div: B sex: male CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 13 N32 nirf 1750 Rank

--- Page 128 ---
Course Code/Title:CS3V15/Devops Unit:IV likes: - biology - chemistry … YAML uses "|" to include newlines while showing multiple lines and ">" to suppress newlines while showing multiple lines. Due to this we can read and edit large lines. In both the cases intendentation will be ignored. We can also represent Boolean (True/false) values in YAML. where boolean values can be case insensitive. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english result: maths: 87 chemistry: 45 biology: 56 physics: 70 english: 80 passed: TRUE messageIncludeNewLines: | Congratulation !! You passed with 79% messageExcludeNewLines: > Congratulation !! You passed with 79% CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 14 N33 nirf 1750 Rank

--- Page 113 ---
Course Code/Title:CS3V15/Devops Unit: III 3. Configure Job Details: ○ General: I Enter a description for your job. Configure options such as discarding old builds if needed. ○ Source Code Management: I Choose Git, Subversion, or another source control system. Enter the repository URL and credentials if the repository is private. Specify the branch or tag to build from. ○ Build Triggers: Set up triggers to start the build. Common options include: Poll SCM: Jenkins will periodically check for changes in the source code repository. . Build periodically: Schedule builds at specific intervals. I GitHub hook trigger for GITScm polling: Trigger builds based on GitHub webhooks. 0 Build Environment: ■ Configure the build environment, such as setting up environment variables or cleaning up before the build starts. ○ Build Steps: Add build steps by clicking Add build step. · Common build steps include: Execute shell: Run shell commands or scripts. . Invoke Gradle script: Use Gradle to build the project. I Invoke Ant: Use Apache Ant for building. ○ Post-build Actions: · Configure actions to perform after the build completes, such as: I Archive the artifacts: Save build artifacts for later use. I Publish JUnit test result report: Display test results in Jenkins. I Send build notifications: Notify stakeholders of build status. 4. Save and Build: ○ Click Save to store the job configuration. o To start a build, click Build Now in the job dashboard. Example Build Configuration: . Job Name: MyApp-Build · Source Code Management: Git o Repository URL: https://github.com/user/myapp.git ○ Branch: main · Build Step: Execute shell ○ Command : myn clean install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 38 nirf 1750 Rank N33

--- Page 114 ---
Course Code/Title:CS3V15/Devops Unit: III · Post-build Action: Archive artifacts ○ Files to archive: target/ *. jar 2. Understanding Jenkins Workspace Overview: The Jenkins workspace is a directory where Jenkins stores files and artifacts related to a particular build. Each job has its own workspace, which is used to perform build operations. Key Aspects of Jenkins Workspace: 1. Workspace Location: o By default, the workspace is located in the Jenkins home directory, typically at /var/lib/jenkins/workspace/ on Linux systems or C:\Program Files (x86) \Jenkins\workspace\ on Windows. 2. Workspace Structure: ○ Each job gets its own subdirectory within the workspace. For example, a job named MyApp-Build will have its workspace at /var/lib/jenkins/workspace/MyApp - Build/. ○ The workspace contains: Source Code: The code pulled from the repository. . Build Artifacts: Files generated during the build process. I Logs: Logs related to the build process. 3. Workspace Usage: ○ Building: During the build process, Jenkins checks out the code into the workspace, executes build steps, and generates artifacts. ○ Archiving Artifacts: After the build, files specified in the post-build actions are archived from the workspace. ○ Cleaning Up: Jenkins may clean up workspaces based on job configurations or policies to save disk space. 4. Customizing Workspace: ○ Configure Custom Workspace Location: In the job configuration, under the Advanced Project Options, you can specify a custom workspace directory. ○ Workspace Cleanup: I Use plugins like the Workspace Cleanup Plugin to manage and clean up workspaces automatically. Example of Workspace Structure: · Workspace Directory: /var/lib/jenkins/workspace/MyApp-Build/ o Source Code: /var/lib/jenkins/workspace/MyApp-Build/src/ o Build Artifacts: /var/lib/jenkins/workspace/MyApp-Build/target/ o Logs: /var/lib/jenkins/workspace/MyApp-Build/logs/ This guide should provide a comprehensive understanding of creating a Jenkins build and managing Jenkins workspaces. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY N32 39 nirf 175º Rank

--- Page 125 ---
Course Code/Title:CS3V15/Devops Unit: IV This command executes the specified playbook while prompting for the necessary privilege escalation password. Output and Result: Activities Terminal - Nov 2 13:00 . auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook-eg2.yaml PLAY [Install Nginx] TASK [Gathering Facts] Total: [vnet]: FAILEDT .. ['msg": "Missing sudo password') PLAY RECAP : ok=0 changed=0 unreachable=0 Fatled =! skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory. yaml playbook-eg2.yaml - - ask-becone-pass BECOME password: PLAY [Install Nginx] TASK [Gathering Facts] ok: [vri01] TASK [Install Nginx using apt] ok: [vri01] PLAY RECAP * VMD1 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ [] As shown in the example image above, the first command encountered an error due to the absence of the -ask-become-pass flag. This flag is essential when your task requires elevated privileges, as it prompts Ansible to request the sudo password for authentication. The corrected command includes the -ask-become-pass flag, ensuring that the necessary privileges are obtained before executing the task. This is particularly important when working with tasks that require administrative access, such as package installations or system configurations. Note: In this blog, we primarily focus on the core and fundamental components of Ansible, providing an introductory overview and understanding of its key concepts. While Ansible offers advanced features such as "Collections," "Ansible Galaxy," "Roles," "Dynamic Inventory," and "Custom Modules and Plugins," we recognize that these topics are extensive and may require separate dedicated discussions. For the purpose of this blog, we aim to establish a strong foundation of Ansible's core components. If you're interested in diving deeper into these advanced topics, we recommend exploring Ansible's official documentation and additional resources dedicated to each subject. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 11 N32 nirf 1750 Rank

--- Page 126 ---
Course Code/Title:CS3V15/Devops Unit:IV Understanding YAML Ansible uses YAML syntax for expressing Ansible playbooks. This chapter provides an overview of YAML. Ansible uses YAML because it is very easy for humans to understand, read and write when compared to other data formats like XML and JSON. Every YAML file optionally starts with " --- " and ends with " ... ". In this section, we will learn the different ways in which the YAML data is represented. key-value pair YAML uses simple key-value pair to represent the data. The dictionary is represented in key: value pair. Note - There should be space between : and value. Example: A student record --- #Optional YAML start syntax james: name: james john rollNo: 34 div: B sex: male ... #Optional YAML end syntax Abbreviation You can also use abbreviation to represent dictionaries. Example James: {name: james john, rollNo: 34, div: B, sex: male} Representing List We can also represent List in YAML. Every element(member) of list should be written in a new line with same indentation starting with "- " (- and space). Example countries: - America - China - Canada - Iceland CHENNAI INSTITUTE OF TECHNOLOGY CHENNAI INSTITUTE . TECHNOLOGY (Autonomous) 12 N33 nirf 1750 Rank

--- Page 111 ---
Course Code/Title:CS3V15/Devops Unit: III Steps: 1. Install the Git Plugin: 0 Go to Manage Jenkins > Manage Plugins. ○ Under the Available tab, search for Git Plugin and install it. 2. Configure Git in Jenkins: ○ Navigate to Manage Jenkins> Global Tool Configuration. ○ Scroll down to the Git section and click Add Git. ○ Specify the path to the Git executable or use the default path. ○ Optionally, configure additional settings such as Git installation locations. 3. Example Configuration: ○ Name: Git ○ Path to Git executable: /usr/bin/ git (or use the default if Git is installed in a standard location). 4. Configure Job to Use Git: ○ Create or edit a Jenkins job and go to Source Code Management. ○ Select Git. ○ Enter the repository URL (e.g., https://github.com/user/repository.git). ○ Configure credentials if the repository is private. ○ Specify the branch to build (e.g., main). 5. Example Repository URL: ○ Repository URL: https://github.com/example/repo.git o Branch Specifier: main 3. Configuring Jenkins to Work with Maven Overview: The Maven Plugin integrates Jenkins with Apache Maven, allowing Jenkins to use Maven to build projects. Steps: 1. Install Maven: ○ Ensure that Apache Maven is installed on your system. You can download it from the Apache Maven website. 2. Configure Maven in Jenkins: ○ Go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to the Maven section and click Add Maven. ○ Enter a name for the Maven installation (e.g., Maven 3.8.6). ○ Check Install automatically to let Jenkins download Maven, or specify the path to an existing Maven installation. ○ CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 36 1750 Rank

--- Page 112 ---
Unit: III Course Code/Title:CS3V15/Devops 3. Example Configuration: ○ Name: Maven 3.8.6 ○ MAVEN_HOME: / usr / share / maven 4. Configure Maven in a Jenkins Job: ○ Create or edit a Jenkins job and go to Build section. 0 Add a build step and select Invoke top-level Maven targets. o Choose the Maven version configured earlier. ○ Specify the goals to run (e.g., clean install). 5. Example Maven Goals: ○ Goals: clean install 6. Configure Build Environment: ○ Ensure that the job's build environment is set up to use Maven. This might involve setting environment variables or configuring build scripts. 7. Example Environment Variables: ○ MAVEN_OPTS: - Xms512m -Xmx2048m (if you need to set specific JVM options for Maven). By following these steps, Jenkins will be properly configured to work with Java, Git, and Maven, allowing you to set up and manage continuous integration and delivery pipelines effectively. If you need further customization or run into issues, consulting the documentation for each tool or plugin can provide additional guidance. Creating a Jenkins Build and Understanding Jenkins Workspace 1. Creating a Jenkins Build Overview: A Jenkins build is a process where Jenkins executes a series of steps defined in a job configuration. This typically involves compiling code, running tests, and generating artifacts. Steps to Create a Jenkins Build: 1. Access Jenkins Dashboard: ○ Open your Jenkins dashboard, usually found at http://localhost: 8080. 2. Create a New Job: ○ Click on New Item on the left sidebar. Enter a name for your job. ○ Choose a job type. Common types include: I Freestyle project: For simple build jobs with basic configuration. · Pipeline: For more complex build processes using a Jenkinsfile. ○ Click OK to proceed. ○ TECHNOLOGY N33 37 nirf 1750 Rank CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous)

--- Page 163 ---
‹ FabrikamFiber Runs Branches Analytics Description Edit Run pipeline : Stages #20210609.3 Update azure-pipelines.yml for Azure Pipelines Manually triggered for main 9 61bba8c x2 La Wednesday Ø 215 #20210609.2 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for 1º main 9 bc6c105 x La Wednesday ₾ 305 #20210609.1 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for main 9 53eb92b x • #20210520.1 Updated README.md Individual CI for & new-branch 0 1ada387 ‹ azure-cli-example go main V azure-cli-example / azure-pipelines.yml 30 pool: default 31 32 steps: 33 # Specify python version and install if needed Settings 34 - task: UsePythonVersion@0 35 condition: false 36 inputs: 37 -versionSpec: '3.x' 38 architecture: 'x64' 39 40 # Update pip to latest 41 - bash: python -m pip install -- upgrade pip 42 condition: false 43 displayName: 'Upgrade pip' 44 45 container 46 &continueOnError 47 48 88988 name & parameters & resources & schedules 49 50 51 services 52 strategy 53 & variables 54 & workspace La Wednesday .455 Là May 20 ₾ 135 Variables Run : Tasks 7 Search tasks dotnet .NET Core Build, test, package, or publish a dotnet applicatio. Android signing Sign and align Android APK files Ant Build with Apache Ant 80 App Center distribute Distribute app builds to testers and users via Visu .. App Center test Test app packages with Visual Studio App Center Archive files Compress files into .7z, tar.gz on zip ARM template deployment Deploy an Azure Resource Manager (ARM) templ .. Azure App Service deploy Deploy to Azure App Service a web, mobile, or AP. · The YAML pipeline editor provides several keyboard shortcuts, which we show in the following examples. . Choose Ctrl+Space for Intellisense support while you're editing the YAML pipeline.

--- Page 164 ---
· The task assistant provides a method for adding tasksto your YAML pipeline. · To display the task assistant, edit your YAML pipeline and choose Show assistant. Understand the azure-pipelines.yml file · A pipeline is defined using a YAML file in your repo. Usually, this file is named azure-pipelines.yml and is located at the root of your repo. · Navigate to the Pipelines page in Azure Pipelines, select the pipeline you created, and choose Edit in the context menu of the pipeline to open the YAMLeditor for the pipeline. · This pipeline runs whenever your team pushes a change to the main branch of your repo or creates a pull request. It runs on a Microsoft-hosted Linux machine. · The pipeline process has a single step, which is to run the Maven task. YAML:Code trigger: - main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdk Version: "1.10" jdk11_windows: imageName: "windows-latest" jdk Version: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdk VersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: " ** /TEST -*. xml"goals: "package" Change the platform to build · Navigate to the editor for your pipeline by selectingEdit pipeline action on the build, or by selecting Edit from the pipeline's main page. · To choose a different platform like Windows or Mac, change thevmImage value: · pool: · vmImage: "windows-latest" Add steps . You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can usetasks for building, testing, publishing, or deploying your app. For Java, the

--- Page 137 ---
Course Code/Title:CS3V15/Devops Unit:IV I Example 2: Get the Hostname and Version of remote servers with UNAME we have used the command module and executing an uname -a command as AD-HOC command $ ansible testservers -m command -a "uname -a" -i ansible hosts As Playbook - name: Check the remote host Hostname, Version, Distribution with UNAME hosts: testservers tasks: - name: Execute the UNAME command register: unameout command: "uname -a" - debug: var: unameout.stdout lines I Example 3: Check the Disk Usage of Remote server To get the disk usage, we are using df -h , here -h is human readable As Ad Hoc command $ ansible testservers -m command -a "df -h" -i ansible hosts As Playbook - name: Check the disk usage of all the file system in the remote servers hosts: testservers tasks: - name: Execute the df command register: dfout command: "df -h" - debug: var: dfout.stdout lines CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 138 ---
Course Code/Title:CS3V15/Devops Unit:IV Example 4: Restart Apache Server using Ansible Command Module So far, In all the sample we have seen we are using one application server and one web server. Totally two servers. But in this example, we need to limit our execution only to the web server as we are going to check the restart the apache web server. this is done using -- limit parameter As Ad hoc command $ ansible testservers -m command -a "httpd -k restart" -i ansible_hosts -b - limit As Playbook - name: restart apache web server hosts: testservers tasks: - name: restartapache register: httpdresout become: yes command: "httpd -k restart" when: ansible_hostname == "mwiweb02" - debug: var: httpdresout.stdout_lines I Example 5: Execute a command when a file exists or not exists There are two most useful parameters in ansible command module such as removes and creates removes - used to tell ansible to Execute the command only if the file exist creates - used to tell ansible to Execute the specified command only if the file does not exist Here we are going to do a very simple file creation and removal based on the file availability or existence. In General using the ansible command module, This method of creating and removing the file is not recommended as Ansible has a dedicated module named file to do the same effortlessly. As Ad hoc command Remove the file, if it does exist (or) present $ ansible testservers -a "rm -rf /tmp/testfile removes=/tmp/testfile" -i ansible_hosts CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 24 N33 nirf 1750 Rank

--- Page 149 ---
Course Code/Title: CS3V15/Devops Unit: V Step 6: Now you will see that all of our files uploaded in our github. Namanbhatia7 Add initial resume website files Latest commit f47be5b 1 minute ago Display_Picture.png Add initial resume website files 1 minute ago My_Resume.html Add initial resume website files 1 minute ago README.md Initial commit 15 minutes ago Styles.css Add initial resume website files 1 minute ago Branch: · A GitHub branch is used to work with different versions of a repository at the same time. · By default a repository has a master branch (aproduction branch). · Any other branch is a copy of the master branch (as itwas at a point in time). · New Branches are for bug fixes and feature work separate from the master branch. When changes are ready, they can be merged into the master branch. If youmake changes to the master branch while working on a new branch, these updates can be pulled in. Commits: At GitHub, changes are called commits. Each commit (change) has a description explaining why achange was made. Pull Requests : . Pull Requests are the heart of GitHub collaboration. · With a pull request you are proposing that your changesshould be merged (pulled in) with the master. · Pull requests show content differences, changes, additions, and subtractions in colors (green and red). · As soon as you have a commit, you can open a pull request and start a discussion, even before the code isfinished. Git: · Git was created by Linus Torvalds in 2005 to develop Linux Kernel · Git is an open-source distributed version control system. It is designed to handle minor to major projectswith high speed and efficiency. . It is developed to co-ordinate the work among the developers. The version control allows us to track andwork together with our team members at the same workspace. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 9 175" Rank

--- Page 150 ---
CREATE AZURE ORGANIZATION AZURE: Microsoft Azure, often referred to as Azure cloud computing platform run by Microsoft. It offers access, management, and the development of applications and services through global data centers It also provides a range of capabilities, including software as a service (SaaS), platform as a service, and infrastructure as a service (IaaS). It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014 Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Prerequisites: To plan your organizational structure. Microsoft accounts or authenticate users with Microsoft Entra ID. For more information, see Choosing your organization administrator account type. Create an organization: 1. Sign in to Azure DevOps. 2. Select New organization 3. Confirm information, and then select Continue 10

--- Page 161 ---
Add a status badge to your repository Many developers like to show that they're keeping their code quality high by displaying a status badge in their repo. Azure Pipelines succeeded To copy the status badge to your clipboard: 1. In Azure Pipelines, go to the Pipelines page to view the list of pipelines.Select the pipeline you created in the previous section. 2. Select , and then select Status badge. 3. Select Status badge. 4. Copy the sample Markdown from the Sample markdown section. Now with the badge Markdown in your clipboard, take the following steps in GitHub: 1. Go to the list of files and select Readme.md. Select the pencil icon to edit. 2. Paste the status badge Markdown at the beginning of the file. 3. Commit the change to the main branch. 4. Notice that the status badge appears in the description of your repository. To configure anonymous access to badges for private projects: 1. Navigate to Project Settings in the bottom left corner of the page 2. Open the Settings tab under Pipelines 3. Toggle the Disable anonymous access to badges slider under General 21

--- Page 162 ---
Azure-Pipeline • Azure Pipelineș provides a YAML pipeline editor that you can use to author and edit your pipelines. • The YAML editor is based on the Monaco Editor. • The editor provides tools like Intellisense support anda task assistant to provide guidance while you edit a pipeline. • You can also edit pipelines by modifying the azure pipelines.yml file directly in your pipeline's repository using a text editor of your choice Edit a YAML pipeline To access the YAML pipeline editor, do the following steps. • Sign in to your organization • (https://dev.azure.com/{yourorganization}). • Select your project, choose Pipelines, and then select the pipeline you want to edit. You can browse pipelines by Recent, All, and Runs. • Choose Edit. • Make edits to your pipeline using Intellisense and the task assistant forguidance. Azure DevOps fabrikam-tailspin FabrikamFiber F FabrikamFiber + Pipelines Overview Recent All Runs Boards Pipelines Recently run pipelines Repos 1 Pipeline Pipelines Pipelines FabrikamFiber 2 3 17

--- Page 117 ---
Course Code/Title:CS3V15/Devops Unit: IV • Explore Ansible modules. 4. Playbook - · Playbooks are a sequence of plays that define the order of tasks executed by Ansible. . A play consists of a list of tasks that target managed nodes in an inventory. · Tasks, in turn, are composed of one or more modules that specify operations. · Playbooks provide a structured way to define and automate complex workflows. 5. Roles - · Roles provide an organized environment for managing complex tasks. · They include templates, playbooks, inventories, error handlers, vars, and meta information. · Ideal for handling larger, multi-step automation processes. 6. Collections - · Collections are distribution formats for Ansible content. · They encompass playbooks, roles, modules, and plugins. · Find Ansible collections. 7. Galaxy - · Ansible Galaxy is a platform for sharing and downloading collections. . It allows the Ansible community to collaborate and exchange automation content. Setting Up the Ansible Control Node Prerequisite: Before you begin setting up Ansible, it's essential to ensure you meet the following prerequisites: · SSH: Make sure SSH is installed on your Linux system. SSH is crucial for secure communication between the control node and managed nodes. Installation Steps: 1. Open your terminal. 2. Run the following command to install Ansible: 1python3 -m pip install -- user ansible 3. SSH Key: o For secure and direct connections to managed nodes, you'll need to provide a public SSH key. You can generate one using the ssh-keygen command. o Follow the on-screen instructions to create your SSH key pair. This key pair is a crucial element of Ansible's secure communications. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 118 ---
Course Code/Title:CS3V15/Devops Unit: IV 4. For specific installation instructions tailored to your Linux distribution, click here. 5. Try to connect ssh using once your mange node is set up. 1ssh <user>@<ip> Note: If you're using a different operating system, find the appropriate installation instructions here. Setting Up the Ansible Mange Node Steps: 1. Install OpenSSH-Server: o To enable remote management of the managed node, you'll need to install the OpenSSH server. Use the following command (assuming you're using a Debian-based Linux distribution like Ubuntu): 1sudo apt install openssh-server o This command installs the OpenSSH server, allowing secure remote access. 2. Create a User: o For convenience and consistency, it's helpful to create a user on the managed node with the same name as the user on the control node. This makes it easier to manage SSH keys and ensures a smoother experience. 3. Configure SSH Key: o To establish secure and direct connections to the managed node, you need to paste the public SSH key from the control node into the authorized_keys file located in the .ssh directory of the user's home folder on the managed node. o The path is typically /home/<user>/.ssh/authorized_keys, where <user> is the username you created or are using on the managed node. You can use the ssh-copy-id command to automate the process of copying your public key to the managed node. For example: 1ssh-copy-id <user>@<managed_node_ip> o This command securely copies your public key to the authorized_keys file, allowing passwordless SSH authentication. Managing Ansible Inventories In the world of Ansible, inventories are like the backbone of your automation infrastructure. They serve as a vital component that lists and organizes the managed nodes, making automation tasks seamless and organized. Inventories use a parent-child concept that allows you to create groupings, which come in handy during specific tasks or scenarios. Why Are Inventories Important? · Inventories provide a comprehensive list of managed nodes, making it easy to interact with and manage them. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 4 N33 nirf 1750 Rank

--- Page 97 ---
Course Code/Title:CS3V15/Devops Unit: III Build Steps Execute Windows batch command ? Command See the list of available environment variables javac welcome. java java welcome Advanced = Execute Windows batch command 7 Command See the list of avalable chuironment variables acho User name is Nuernane% Advanced Step 5 : Click on Apply and then Save. Step 6 : Select Build with parameters option. Dashboard > welcome > Status Changes Workspace Build with Parameters Configure Delete Project 0 Rename CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 22 N3 nirf 1750 Rank

--- Page 98 ---
Course Code/Title:CS3V15/Devops Unit: III Project welcome This build requires parameters: Username Parth D Build Cancel We can change the username as well. Suppose I changed the Username as Anand then click the Build, we get the console output as follows - C: \ProgramData\Jenkins\.jenkins\workspace\welcome>javac welcome.java C:\ProgramData\Jenkins\.jenkins\workspace\welcomeyjava welcome Welcome to first Job of Jenkins [welcome] $ cmd /c call C:\WINDOWS\TEMP\jenkins16582464065687933173.bat C:\ProgramData\Jenkins\.jenkins\workspace\welcome>echo User name is Anand User name is Anand C:\ProgramData\Jenkins\.jenkins\workspace\welcome>exit 0 Finished: SUCCESS Thus we set variety of parameters to the Jenkins job. We can set the Boolean parameter, choice parameter, credentials, file parameter, password parameter and so on. 3. HTML Publisher Filter Plain Boolean Parameter Choice Parameter Credentials Parameter File Parameter Multi-line String Parameter Password Parameter Run Parameter String Parameter Add Parameter Overview: The HTML Publisher Plugin allows Jenkins to publish HTML reports and artifacts generated during the build process. Key Features: · Report Publishing: Publishes HTML reports, dashboards, or other HTML artifacts. · Report Directory: Allows specifying the directory containing HTML reports. · Index Page: Sets a default HTML file to be displayed when accessing the report. Configuration: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 23 nirf 1750 Rank N33

--- Page 101 ---
Course Code/Title:CS3V15/Devops Unit: III v => SeleniumDemo src/main/java > src/main/resources > src/test/java > 15 src/test/resources > al JRE System Library [J2SE-1.5] › al Maven Dependencies » drivers v src > @ main v fs test v java v 2> myPackage 10 myTestPage.java 23 resources › › target > > test-output 2 pom.xml > Servers myTestPage.java package myPackage; import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver; import org.testng.annotations.Test; public class myTestPage { @Test public void myFun() { System.setProperty("webdriver.chrome.driver","./drivers/chromedriver.exe"); WebDriver driver = new ChromeDriver(); driver.get("https://google.com"); String title = driver.getTitle(); System.out.println("Hey Anuradha, the tile of this page is: "+title); driver.quit(); } Program explanation : In above program, we have written one test case by the function myFun. In this function, using the Chromedriver which we have installed, we open the web page google.com Then we try to get the tile of this page and display it on the console using System.out statement. Finally we quit the driver. Step 7 : Right click on the Project name in Project Explorer window, Run As-> Maven Test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 26 N3 nirf 1750 Rank

--- Page 102 ---
Unit: III Course Code/Title:CS3V15/Devops Step 8 : Now, locate the Java file myTestPage.java and right click Run As->TestNG Test. It is as follows - 0 Run As # Debug As Profile As Web Services > > 1 TestNG Test > Run Configurations ... > Alt+Shift+X, N Step 9 : We will get the output as follows - Hey Anuradha, the tile of this page is: Google PASSED: myFun Default test Tests run: 1, Failures: 0, Skips: 0 Default suite Total tests run: 1, Failures: 0, Skips: 0 Step 10 : Now, we will open the Jenkin's page and create the HTML Publisher report for the above SeleniumDemo project. The prerequisite to this activity is that the HTML Publisher report plugin must be installed in Jenkins. · Click on New Item, Enter the Item Name and select as Freestyle project, I have given the name HTMLReportDemo. · Then click on configure. Write the suitable description in General section. · The in Build Steps and Add Build steps as Invoke top-level Maven targets provide the Maven version, and under goals write clean test. . Under Post Build Actions, click on Add Post Build Action and select Publish HTML Report. Now give the path of index.html file which is created by Selenium testNG framework in Maven. It is the path of test-output folder. Also give some suitable HTML report title. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 27 N3 nirf 1750 Rank

--- Page 151 ---
Azure DevOps Taking you to your Azure DevOps organization ... Congratulations, you're an organization owner! Sign in to your organization at any time, https://dev.azure.com/{yourorganization}. With your organization, the following aspects are included in the free tier: First five users free (Basic license): Azure Pipelines: o One Microsoft-hosted CI/CD (one concurrent job, upto 30 hours per month) o One self-hosted CI/CD concurrent job Azure Boards: Work item tracking and Kanban boards Azure Repos: Unlimited private Git repos Azure Artifacts: Two GB free per organization Build applications with Azure: Azure DevOps enables you to build, test, and deploy anyapplication to any cloud or on premises To configure build pipelines that continuously build, test, and verify your applications. 11

--- Page 152 ---
Part 1: Get started with Azure DevOps Part 2: Build applications with Azure DevOps Part 3: Deploy applications with Azure DevOps Create a build pipeline with Azure Pipelines: Prerequisites: Familiarity with forking and cloning a GitHub repo Account requirements: An Azure DevOps organization o To use Microsoft-hosted agents, your Azure DevOps organization must have access to Microsoft- hosted parallel jobs. Check your parallel jobs and request a free grant. O You can use GitHub Code spaces to complete the module, even if your Azure DevOps organization doesn't have any parallel jobs. A GitHub account Software requirements: If using GitHub Code spaces to complete the module, there are no software requirements as all software is included in the Codespace If using a local development environment withMicrosoft-hosted agents, you must have the following software installed: o Visual Studio Code o .NET 6.0 SDK o Git 12

--- Page 13 ---
Course Code/Title:CS3V15/Devops Unit: I 2024. Terraform provides the agility and scalability required to keep up with the dynamic demands of modern applications. Terraform's importance lies in its ability to bring consistency, version control, and automation to infrastructure operations, thereby reducing manual errors, streamlining DevOps workflows, and facilitating applications' rapid and reliable deployment in an increasingly complex and cloud-centric environment. As organizations adopt cloud-native technologies, Terraform remains essential to ensure efficient and consistent infrastructure management. 25. Phantom Phantom enhances security automation and incident response capabilities. In today's rapidly evolving threat landscape, organizations face a constant barrage of cybersecurity incidents, and the ability to respond swiftly and effectively is necessary. It provides a platform for automating security workflows, from detecting and investigating potential threats to orchestrating responses and mitigating risks. Phantom's importance lies in its capacity to reduce response times, increase consistency in incident handling, and free up manual resources from repetitive tasks. With the growing complexity of cyber threats, Phantom empowers security teams to defend against attacks and safeguard critical assets proactively. 26. Nagios Nagios, an open-source monitoring and alerting system, remains vital due to its enduring significance in maintaining the reliability and performance of IT infrastructure and applications. Organizations increasingly rely on complex systems and services. Nagios plays a crucial role by providing real-time monitoring and alerting capabilities, allowing IT teams to detect and address issues before they impact users or cause system outages. Its versatility, extensibility, and support for both on-premises and cloud environments make Nagios a valuable tool for ensuring critical systems' availability, stability, and security, aligning perfectly with the demands of modern IT operations and DevOps practices. 27. Vagrant Vagrant continues to play a crucial role in software development and DevOps. It is a tool that simplifies creating and managing reproducible development environments. Its importance lies in its ability to provide developers and DevOps teams with a consistent and isolated environment for software development, testing, and deployment. With the ever-evolving complexity of software stacks, dependencies, and infrastructure configurations, Vagrant remains essential in ensuring these environments are easily shareable, scalable, and maintainable. It allows developers to work seamlessly across various operating systems and provides a standardized setup that minimizes compatibility issues. 28. Sentry Sentry plays a critical role in modern software development and DevOps practices. With software applications' increasing complexity and scale, identifying and addressing errors and issues has become crucial. Sentry is vital because it provides real-time error tracking and monitoring, allowing development teams to proactively detect and diagnose issues, whether they occur in production or during development. Its importance is minimizing downtime, improving user experience, and maintaining software systems' overall health and reliability. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nirf 1750 Rank

--- Page 14 ---
Course Code/Title:CS3V15/Devops Unit: I 29. Gradle Gradle continues to be a vital tool in software development and DevOps. Gradle is an advanced build automation system that plays a crucial role in managing dependencies, building projects, and orchestrating complex workflows efficiently. Its importance lies in its versatility and scalability, as it caters to various project sizes and types. Gradle's ability to easily handle multi-language, multi-project builds and its support for plugin-based customization make it indispensable in modern software development. As organizations increasingly adopt microservices architectures and cloud-native technologies, Gradle's capabilities are instrumental in managing the complexity of building, testing, and deploying applications across diverse environments. DevOps Architecture DevOps Architecture Plan Release Code Dep Deploy Development and operations both play essential roles in order to deliver applications. The deployment comprises analyzing the requirements, designing, developing, and testing of the software components or frameworks. The operation consists of the administrative processes, services, and support for the software. When both the development and operations cture is the solution to fix the gap between Test Monitor Build Operate ined with collaborating, then the DevOps al www deployment and operation terms; therefore, delivery can be faster. DevOps architecture is used for the applications hosted on the cloud platform and large distributed applications. Agile Development is used in the DevOps architecture so that integration and delivery can be contiguous. When the development and operations team works separately from each other, then it is time- consuming to design, test, and deploy. And if the terms are not in sync with each other, then it may cause a delay in the delivery. So DevOps enables the teams to change their shortcomings and increases productivity. Below are the various components that are used in the DevOps architecture: 1) Build: Without DevOps, the cost of the consumption of the resources was evaluated based on the pre-defined individual usage with fixed hardware allocation. And with DevOps, the usage of cloud, sharing of resources comes into the picture, and the build is dependent upon the user's need, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Build Code Test Plan DevOps Components Moniter Dev Ops Deploy Operate Release 14 N33 nirf 1750 Rank

--- Page 133 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible Roles Roles provide a framework for fully independent or interdependent collections of files, tasks, templates, variables, and modules. The role is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks and makes them easier to reuse. The breaking of the playbook allows you to break the playbook into reusable components. Each role is limited to a particular functionality or desired output, with all the necessary steps to provide that result either within the same role itself or in other roles listed as dependencies. Roles are not playbooks. Roles are small functionality that can be used within the playbooks independently. Roles have no specific setting for which hosts the role will apply. Top-level playbooks are the bridge holding the hosts from your inventory file to roles that should be applied to those hosts. Creating a Role The directory structure for roles is essential to creating a new role, such as: Role Structure The roles have a structured layout on the file system. You can change the default structured of the roles as well. For example, let us stick to the default structure of the roles. Each role is a directory tree in itself. So the role name is the directory name within the /roles directory. $ ansible-galaxy -h Usage ansible-galaxy[delete|import|info|init|install|list|login|remove|search|setup][ -- help] [options] ... Options o -h: (help) it shows this help message and exit. o -v: (verbose) Verbose mode (-vvv for more, -vvvv to enable connection debugging). o -- version: it shows program version number and exit. Roles are stored in separate directories and have a particular directory structure CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 19 N33 nirf 1750 Rank

--- Page 134 ---
Course Code/Title:CS3V15/Devops Unit: IV Typical structure of ansible role myweb defaults L main.yml files · handlers L main.yml meta. L main.yml README.md tasks L_ main.yml templates tests inventory test.yml vars L main.yml · defaults : It stores the default variable for the role. For example the default port number is http is 8080, then it can be stored in defaults. · files : This folder contains the files required to transfer or deploy to the target machines. · handlers : It contains handlers, which may be used by some role. · meta : It defines some data or information about the role. · tasks : It contains the main list of tasks to be executed by the role. These tasks might be defined in separate files as per the functionalities. · templates : It contains the templates which can be deployed via a role. Ansible Module Ansible Command module is used to execute commands on a remote node. The Command module, is used mostly to run simple Linux commands on a remote node/server which is part of a host group or Stand alone server mentioned in the host group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 85 ---
Course Code/Title:CS3V15/Devops Unit: III By following these steps, you can set up a Jenkins job to automate builds, integrate with your source code management system, and perform various actions based on build results. Introduction to Jenkins Plugins What Are Jenkins Plugins? Jenkins plugins are extensions that add extra functionality to Jenkins. They allow you to customize and extend Jenkins to better fit your specific needs. Plugins can provide integrations with other tools, add new features, and improve existing functionalities. Why Use Plugins? . Extend Functionality: Add features not available in the core Jenkins. . Integrate Tools: Seamlessly integrate with other tools and platforms (e.g., GitHub, Docker). . Improve Productivity: Automate more tasks and streamline your workflow. · Customize UI: Tailor the Jenkins interface to better suit your preferences and needs. How to Manage Plugins 1. Accessing the Plugin Manager 1. Go to Jenkins Dashboard: ○ Open your Jenkins dashboard. 2. Navigate to Plugin Manager: ○ Click on "Manage Jenkins". ○ Select "Manage Plugins". 2. Installing Plugins 1. Available Plugins: ○ Go to the "Available" tab to see a list of plugins that can be installed. 2. Search for Plugins: ○ Use the search bar to find specific plugins. 3. Select Plugins: ○ Check the box next to the plugins you want to install. 4. Install Plugins: ○ Click "Install without restart" or "Download now and install after restart". 3. Updating Plugins CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 narf 10 1750 Rank

--- Page 86 ---
Course Code/Title:CS3V15/Devops Unit: III 1. Go to Updates: ○ Navigate to the "Updates" tab to see plugins with available updates. 2. Select Plugins: ○ Check the box next to the plugins you want to update. 3. Update Plugins: ○ Click "Download now and install after restart". 4. Managing Installed Plugins 1. Installed Plugins: ○ Go to the "Installed" tab to see all installed plugins. 2. Uninstall Plugins: ○ Click the "Uninstall" button next to the plugin you want to remove. 3. Check Plugin Versions: ○ View the current version of each installed plugin. Adding Plugins to Jenkins Plugins are a crucial part of Jenkins, allowing you to extend its functionality to suit your project's specific needs. Whether you're integrating Jenkins with other tools, customizing your job configuration, or adding new build steps, plugins provide the flexibility you need. Here's a detailed guide on how to add plugins to Jenkins. 1. Why Plugins Are Important - Extending Jenkins Functionality: Plugins allow Jenkins to integrate with various tools and technologies, such as Git, Maven, Docker, and more. - Customizing Jobs: With plugins, you can add specific steps, triggers, and post-build actions to your jobs, making Jenkins adaptable to any workflow. - Automation: Plugins help automate processes like code quality checks, notifications, and deployments. 2. Accessing the Plugin Manager 1. Navigate to Jenkins Dashboard: Open your Jenkins dashboard in a web browser (e.g., http://localhost:8080). CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 175º Rank

--- Page 145 ---
Course Code/Title: CS3V15/Devops Unit: V Step 7:Note the types of plans offered by GitHub. There are a few different plans to choose from, varying in the amount of features provided. Learn to ship software like a pro. GitHub gives students free access to the best developer tools so they can learn by doing. Free Get additional student benefits GitHub Pro Unlimited public/private repositories 2,000 CI/CD minutes/month Free for public repositories 500MB of Packages storage Free for public repositories Protect your branches Ensure that collaborators on your repository cannot make irrevocable chang branches. O Draft pull requests O 120 core-hours of Codespaces compute O Pages and Wikis 15GB of Codespaces storage O Community support ® 3,000 CI/CD minutes/month Free for public repositories 2GB of Packages storage Free for public repositories O 180 core-hours of Codespaces compute 20GB of Codespaces storage Web-based support wikiHow Step 8: Select the free plan. On the plan selection page, scrolldown to click the button for choosing a free plan. This will immediately take you to your GitHub dashboard. Continue for free wiki How UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 5 N33 In 175º Rank ---

--- Page 146 ---
Course Code/Title: CS3V15/Devops Unit: V · If you choose a paid plan, you'll have to enter yourpayment information as requested before you can continue. . If you want to upgrade your Github account in the future, click the menu at the top- right corner, select Settings, andchoose Billing and plans to view your options. GitHub essentials are: · Repositories · Branches · Commits · Pull Requests · Git (the version control software GitHub is built on) Repository: A GitHub repository can be used to store a developmentproject. It can contain folders and any type of files (HTML, CSS,Javascript, Documents, Data, Images). A GitHub repository should also include a licence file and a README file about the project. A GitHub repository can also be used to store ideas, or any resources that you want to share. Github Repository Creation Step 1: Click on the new repository option Marketplace Explore New repository and GitHub without any code! Catch Universe Join a community Universe in San Fr ticket pricing-he Import repository × New gist bird New organization New project llo World guide, you'll create a repository, start a , write comments, and open a pull request. GitHub Sponsors Matching Fund × Ready to support open source? GitHub will match your contribution to developers during their first year in GitHub Sponsors. d the guide Start a project × Welcome to the new dashboard. Get closer to the stuff you care about most. Explore repositories atlas-engineer/next Next browser - Be productive. UnENVIVAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 175° Rank 6

--- Page 159 ---
Choose a job to see the steps for that job From the < Jobs in run #20191 ... FabrikamFiber Build O Build Build 40s Initialize job 1s 2 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 Duration: 40s 6 Pool: Azure Pipelines Image: Ubuntu-16.04 7 > Job preparation parameters Checkout 3s CmdLine 25 Component Detect 32s Post-job: Checkout <1s Finalize Job <1s Deploy DeployWeb 10s Finalize build Report build status <1s steps view, you can review the status and details of eachstep. From the Moreactions you can toggle timestamps or view a raw log of all steps in the pipeline. Build 9 Pool: Azure Pipelines 2 Image: Ubuntu-22.04 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 View job raw log Toggle timestamps 6 Duration: 40s Job preparation parameters 10

--- Page 160 ---
Cancel and re-run a pipeline If the pipeline is running, you can cancel it by choosing Cancel. If the run has completed, you can re-run the pipeline by choosing Run new. Pipeline run more actions menu: #20191210.3 Update azure-pipelines.yml for Azure Pip ... on FabrikamFiber Cancel Stages Jobs € Build C Deploy 0/1 compléted 48s Not started Build 48s Cancel From the More actions menu you can download logs, add tags, edit the pipeline, delete the run, and configure retention for the run. cure Pipe ... Run new Download logs Add tags Edit pipeline View retention leases Retain Delete 20

--- Page 139 ---
Course Code/Title:CS3V15/Devops Unit: IV Create the file, if it does NOT exist (or) absent $ ansible testservers -a "touch /tmp/testfile creates=/tmp/testfile" -i ansible_hosts As Playbook - name: "Validate if a file is present or not present using Ansible Command module" hosts: testservers tasks: - name: "Create a file if it does not exist" command: "touch /tmp/latestfile" args: creates: "/tmp/latestfile" register: createif - name: "Display the file to make sure its created" command: "ls -lrt /tmp/latestfile" register: displayif when: createif is changed - debug: var=displayif.stdout - name: "Remove the file if it exist" command: "rm -rf /tmp/latestfile" args: removes: "/tmp/latestfile" register: removeif In the playbook we execute three tasks, • First one is a create a file if it does not exist • Second to display if the creation is successful • Third one is to delete the file if it exists Example 5: Execute or Run the Script when a file exists or not exists Now for this example let us take something relatable to real world scenario. Like Start the Server instance (or) program if the PID file or LOCK file does not exist Here is the sample playbook for you to get started. I am giving a general playbook here as I do not want to pick some server/technology of my own which you cannot relate to. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 140 ---
Course Code/Title:CS3V15/Devops Unit:IV you can feel free to modify the script to your needs as this is just a template. - name: Start of Stop Server instance based on PID/LOCK file availability hosts: appservers tasks: - name: Start the instance when the PID file is not present become: yes become_user: appuser command: "startserver.sh" args: creates: "/path/to/pid/instance.pid" register: startinst - name: Stop the instance when the lock file is present become: yes become_user: appuser command: "stopserver.sh" args: removes: "/path/to/lockfile/instance.lck" register: stopinst CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 26 N33 nirf 1750 Rank

--- Page 115 ---
Unit: IV Course Code/Title:CS3V15/Devops UNIT IV CONFIGURATION MANAGEMENT USING ANSIBLE Ansible Introduction, Installation, Ansible master/slave configuration, YAML basics, Ansible modules, Ansible Inventory files, Ansible playbooks, Ansible Roles, adhoc commands in ansible. Ansible A SSH ANSIBLE TARGET NODE HOSTS PLAYBOOK Ansible is a powerful DevOps tool designed for automating tasks on remote servers or nodes. In simple terms, it allows you to automate commands and functions on multiple remote machines from a central 'master' node. To illustrate its usefulness, consider a scenario where you need to reboot dozens or even hundreds of remote hosts. You could manually SSH into each one and initiate the reboot, or you can use Ansible to streamline the process, making it efficient and offering a wide range of additional functionalities. In essence, Ansible operates much like the second method mentioned, using SSH (Secure Shell), a secure communication protocol, to control remote nodes in a secure and optimized manner. Ansible Components Ansible Core Components Control node Ansible Inventory Managed node 1 Managed node 2 Managed node 3 1. Control Node - · The central or main node where Ansible is installed. · Used to trigger commands like ansible and ansible-inventory on other nodes. · Acts as the orchestrator for Ansible operations. • TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 1 N33 nirf 1750 Rank

--- Page 116 ---
Course Code/Title:CS3V15/Devops Unit: IV 2. Manage Node- · A remote or slave node where tasks are executed or controlled by Ansible. . These are the servers or devices you want to manage or automate. 3. Inventory - · A list of managed node IPs and configurations. · Logically organized, typically using file formats like YAML or INI. · Created on the control node to describe the deployment of hosts to Ansible. Ansible Additional Components Ansible Architecture CMDB Users Public / Private Cloud Hosts ... ... Inventory .. Modules 000 Ansible Playbook API Plugins Networking www.educba.com 1. Ad-Hoc Commands - . These are one-off commands that you can execute using the ansible command. · Useful for quick tasks or tests on remote nodes. 2. Plugins - · Plugins are pieces of code that extend Ansible's core functionality. · Ansible uses a plugin architecture for flexibility and expandability. · Examples include connectivity plugins for establishing connections and cache plugins. · Learn more about Ansible plugins. 3. Module- · Modules are built-in functions that can be used to perform various tasks. . They eliminate the need to write custom code for common operations. · Examples include modules for package management (apt, yum), service management, and more. CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) CHENNAI INSTITUTE . TECHNOLOGY 2 N33 nirf 1750 Rank

--- Page 129 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible ad-hoc Commands The Ansible ad-hoc command uses the /usr/bin/ansible command-line tool to automate a single task on one or more managed nodes. The Ad-hoc commands are quick and easy, but they are not re-usable. The Ad-hoc commands demonstrate the simplicity and power of Ansible. Syntax ansible <hosts> [-m <module_name>] -a <"arguments"> -u <username> [ -- become] Explanation Hosts: It can be an entry in the inventory file. For specifying all hosts in the inventory, use all or "*" . module_name: It is an optional parameter. There are hundreds of modules available in the Ansible, such as shell, yum, apt, file, and copy. By default, it is the command. Arguments: We should pass values that are required by the module. It can change according to the module used. Username: It specifies the user account in which Ansible can execute commands. Become: It's an optional parameter specified when we want to run operations that need sudo privilege. By default, it becomes false. 1. Parallelism and shell commands You can reboot your company server in 12 parallel forks at the same time. For this, you need to set up the SSHagent for connection. $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa To run reboot for all your company servers in the group, 'abc', in 12 parallel forks: $ ansible abc -a "/sbin/reboot" -f 12 By default, Ansible will run the above ad-hoc commands from the current user account. If you want to change then pass the username in ad-hoc command as follows: $ ansible abc -a "/sbin/reboot" -f 12 -u username 2. File Transfer You can use ad-hoc commands for doing SCP (secure copy protocol) which means lots of files in parallel on multiple machines or servers. CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 15 N32 nirf 175º Rank

--- Page 130 ---
Course Code/Title:CS3V15/Devops Unit:IV Transferring file on many machines or servers $ ansible abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf" Creating new directory $ ansible abc -m file - a "dest = /path/user1/new mode = 888 owner = user group = user1 state = directory" Deleting all directory and files $ ansible abc -m file -a "dest = /path/user1/new state = absent" 3. Managing Packages Ad-hoc commands are available for apt and yum module. Here are the following ad-hoc commands using yum. Below command checks, if the yum package is installed or not, but not update it. $ ansible abc -m yum -a "name = demo-tomcat-1 state = present" Below command checks the package is not installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = absent" And below command checks the latest version of package is installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = latest" 4. Managing Users and Groups You can manage, create, and remove a user account on your managed nodes with ad-hoc commands. $ ansible all -m user -a "name=foo password =< crypted password here>" $ ansible all -m user -a "name=foo state=absent" 5. Managing Services Ensure a service is started on all the webservers. $ ansible webservers -m service -a "name=httpd state=started" Alternatively, restart a service on all webservers: $ ansible webservers -m service -a "name=httpd state=restarted" CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 nirf 1750 Rank

--- Page 81 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Credentials: Add credentials if required. ○ Branch Specifier: Specify the branch (e.g., */main). 7. Build Triggers: ○ Build Periodically: Use CRON syntax to schedule builds. ○ Poll SCM: Jenkins will check the repository for changes at specified intervals. ○ Other Triggers: Configure other triggers such as GitHub hooks, upstream projects, etc. 8. Build Environment: o Configure the build environment settings like setting environment variables, running scripts before the build, etc. 9. Build Steps: ○ Click on "Add Build Step" and choose the appropriate build step (e.g., "Execute Shell", "Invoke Ant", "Invoke Gradle script"). ○ Example: For executing a shell script, add the necessary shell commands. 10. Post-build Actions: ○ Click on "Add post-build action" and choose the appropriate action (e.g., "Archive the artifacts", "Publish JUnit test result report"). ○ Configure the settings for each post-build action. 11. Save the Configuration: ○ Once you have configured all the necessary settings, click "Save" at the bottom of the page. 12. Run the Job: ○ On the job's main page, click "Build Now" to run the job immediately. 13. Monitor the Job: ○ Click on the build number in the "Build History" to view the detailed output and logs of the build process. ○ Check the console output for any errors or warnings. Practical Example Setting up a Freestyle Project for a Maven Build: 1. Create New Item: ○ Name: MyMavenProject o Type: Freestyle project 2. Source Code Management: ○ Git: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 82 ---
Unit: III Course Code/Title:CS3V15/Devops I Repository URL: https://github.com/example/my-maven-project.git · Branch Specifier: * /main 3. Build Triggers: ○ Poll SCM: H/15 * (poll every 15 minutes) 4. Build Steps: ○ Invoke Top-Level Maven Targets: Goals: clean install 5. Post-build Actions: ○ Publish JUnit test result report: I Test report XMLs: * /target/surefire-reports/ *. xml 6. Save and Build: ○ Click "Save". ○ Click "Build Now" on the job's main page. By following these steps, you can set up a Jenkins job to automate your build process for various types of projects. Jenkins provides extensive customization options, allowing you to tailor the job to meet your specific requirements. Configuring a Jenkins Job 1. Install Jenkins First, make sure Jenkins is installed. You can download it from the official Jenkins website. 2. Access the Jenkins Dashboard Open your web browser and go to http://your-server-ip: 8080 to access the Jenkins dashboard. 3. Create a New Job 1. Start a New Job: ○ Click on "New Item" on the left-hand menu. 2. Name the Job: ○ Enter a name for your job in the "Enter an item name" field. 3. Select Job Type: ○ Choose the type of job (e.g., Freestyle project, Pipeline). CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) - TECHNOLOGY N32 nirf 7 1750 Rank

--- Page 165 ---
Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. Customize CI triggers Pipeline triggers cause a pipeline to run. You can use trigger: to cause a pipeline to run whenever you push an update to a branch. YAML pipelines are configured by default with a CI trigger on your default branch (which is usually main). You can set up triggers for specific branches or for pull request validation. For a pull request validation trigger, just replace the trigger: step with pr: as shown in the two examples below. By default, the pipeline runs for each pull request change. . If you'd like to set up triggers, add either of the following snippets at the beginning of your azure-pipelines.yml file. YAML I Copy trigger: - main - releases/* YAML Copy pr: - main - releases/* You can specify the full name of the branch (for example, main) or a prefix-matching · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. Build using multiple versions Add steps You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can use tasks for building, testing, publishing, or deploying your app. For Java, the Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. YAML Copy - task: PublishCodeCoverageResults@1 inputs: codeCoverageTool: "JaCoco" summaryFileLocation: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco/ reportDirectory: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco" failIfCoverageEmpty: true · Select Save and then confirm the changes. . You can view your test and code coverage results by selecting your build and going to the Test and Coverage tabs.

--- Page 166 ---
Build using multiple versions Rename/move pipeline Name ScheduledTriggerTest 1234 Select folder 1 .. . Cancel Save Pipeline settings X Processing of new run requests Enabled Paused Disabled YAML file path azure-pipelines.yml Automatically link work items included in this run Cancel Save YAML trigger: main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdkVersion: "1.10" jdk11_windows: imageName: "windows-latest" jdkversion: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdkVersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: **** /TEST -*. xml" goals: "package"

--- Page 155 ---
10. When you're ready to make changes to your pipeline, select it in the Pipelines page, and then Edit the azure-pipelines.yml file. View and manage your pipelines You can view and manage your pipelines by choosing Pipelines from the left-hand menu to go to the pipelines landing page. Azure DevOps : FabrikamFiber 1 Pipelines 0 Search F FabrikamFiber + Pipelines New pipeline Overview Recent All Runs Y Filter pipelines Boards Repos Pipelines Pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... La 32m ago & Manually triggered 8º main & 42s Environments Releases FabrikamFiber #20191209.3 · Set up CI with Az ... La 1h ago & Manually triggered & main 1m 13s Library Task groups *** Deployment groups From the pipelines landing page you can view pipelines and pipeline runs, create and import pipelines, manage security, and drill down into pipeline and run details. Choose Recent to view recently run pipelines (the default view), or choose All to view all pipelines. Pipelines New pipeline : Recent All Runs Filter pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... & Manually triggered & main Là 36m ago ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Az ... Là 1h ago 1m 13s & Manually triggered & main 15

--- Page 156 ---
Select a pipeline to manage that pipeline and view the runs. Select the build number for the last run to view the results of that build, select the branch name toview the branch for that run, or select the context menu to run the pipeline and perform other management actions. Recently run pipelines Pipeline Last run #20191209.2 · Set up CI with Azure Pip ... > pipelines-dotnet-core 8 Manually triggered 12º main Là 1h ago * ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Azure Pip ... 試 2h 8 Manually triggered 8 main Edit in Run pipeline Manage security Rename/move Delete Select Runs to view all pipeline runs. You canoptionally filter the displayed runs. Pipelines Recent All Runs Filter by keywords State V Repository V New pipeline Requested forV Tags X All pipeline runs Description Stages Set up CI with Azure Pipelines & #20191209.2 on pipelines-dotnet-core & main d4964 .. Set up CI with Azure Pipelines #20191209.1 on pipelines-dotnet-core åº main d4964 ... Set up CI with Azure Pipelines & #20191209.3 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines & #20191209.2 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines #20191209.1 on FabrikamFiber & main 2b4b23c La 44m ago 42s La 53m ago ₾ 46s Là 1h ago 0 1m 13s Ld 1h ago 1m 555 Lo 1h ago 1m 6s 16

--- Page 147 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: After clicking new repository option, we will have to initialize some things like, naming our project, choosing the visibility etc. After performing these steps click Create Repository button. Create a new repository A repository contains all project files, including the revision history. Already have a project repository elsewhere? Import a repository. Owner Repository name * / Namanbhatia7 - Resume This is going to be name of our project Great repository names are short and memorable. Need inspiration? How about animated-memory? Description (optional) Public Anyone Keep this as public selected ository. You choose who can commit. see this rer Private You choose who can see and commit to this repository. We can add a project description if we want. Skip this step if you're importing an existing repository. Initialize this repository with a README This will let you immediately clone the repository to your comp Tick the README option Add .gitignore: None - Add a license: None ~ Create repository After performing above steps, Click this button Step 3: After clicking the button, we will be directed to below page. Right now the only file we have is a readme file. Namanbhatia7 / Resume o Unwatch - 1 * Star 0 Y Fork 0 <> Code Issues 0 (*) Pull requests 0 ||| Projects 0 BB Wiki Security the Insights # Settings No description, website, or topics provided. Edit Manage topics T 1 commit įº 1 branch > 0 releases 2 1 contributor Branch: master New pull request Create new file Upload files Find File Clone or download TT Namanbhatia7 Initial commit Latest commit 676ac98 now README.md Initial commit now DO README.md Resume UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 7 175" Rank

--- Page 148 ---
Course Code/Title: CS3V15/Devops Unit: V Step 4: Now click on the "Upload files" button. 4 > C https://github.com/avicnotes/html-css-projects ... Search or jump to ... Click to End Session Total: $80.83 Pull requests Issues Marketplace Explore Google" Custom Search @ avicnotes / html-css-projects Unwatch - 1 Star 0 Fork 0 <> Code Issues 1'] Pull requests Actions Projects Wiki O Security ~ Insights ... go main - ¿º 1 branch 0 tags Go to file Add file - { Code About avicnotes screenshots Create new file Upload files 2 24 commits No description, website, or topics provided. README.md Update README.md 2 days ago Readme Screenshot (10).png screenshots 2 days ago Screenshot (11).png screenshots 2 days ago Releases Screenshot (12).png screenshots 2 days ago No releases published Create a new release 19 Screenshot (6).png screenshots 2 days ago accordion.JPG Add files via upload 4 days ago Packages background-dots.png Add files via upload 2 days ago clone-project-1.html Updated navbar items to my original ones 2 days ago index.html Create index.html 's ago No packages published Publish your first package Activate Windows Go to Settings to activate Windows, Environments 1 O Type here to search O W. 11:32 AM 2/1/2021 Step 5:Follow the steps mentioned and click "commitchanges". Drag additional files here to add them to your repository Or choose your files Display_Picture.png Simply drag or add files to be uploaded × My_Resume.html X Styles.css × Commit changes Add initial resume website files Add an optional ended des Give this particular version a name -o- Commit directly to the master branch. Ensure this option is checked !] Create a new branch for this commit and start a pull request. Learn more about pull requests. Commit changes Cancel UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 8

--- Page 123 ---
Course Code/Title:CS3V15/Devops Unit: IV Ad-Hoc Limitations: While ad-hoc commands are ideal for single, immediate tasks, they are not designed for complex, multi- step automation. Playbooks come to the rescue when tasks have interdependencies and need to be executed in a coordinated manner. Readability and Reusability: Playbooks are authored in YAML, a human-readable and straightforward format. This not only makes them easy to write but also facilitates sharing and collaboration. You can reuse playbooks across various scenarios, saving time and effort. Conditional and Looping Logic: Playbooks offer advanced features, including conditional statements and looping, that allow you to adapt automation to different situations. This flexibility makes playbooks versatile and capable of handling a wide range of automation needs. In essence, playbooks are your tool of choice when automation tasks become multi-faceted and require a structured and logical approach. They provide the power to streamline and automate complex workflows with precision and efficiency. Example: Ping All Inventory Devices To illustrate the use of a playbook, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: playbook.yaml 1 --- 2 -name : My First Play #Name of play 3 hosts: virtualmachines #defining host can be all 4 tasks: 5 -name:Ping My Hosts #Name of task 6 ansible.builtin.ping: # can also write ping To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 124 ---
Course Code/Title:CS3V15/Devops Unit: IV Output and Result: Activities Terminal - Nov 2 12:29 . O auriga@auriga-Latitude-E7470: - /Desktop/ansible X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook.eg1.yaml PLAY [My First Play] TASK [Gathering Facts] ok: [ vri01] TASK [Ping Devices] ok: [vm01] PLAY RECAP vn01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$[] rescued=0 ignored=0 X As you can see there is gathering facts task which we didn't create is running it is the default task which ping the connection. Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: playbook.yaml 1 --- 2- name: Install Nginx 3 hosts: all 4 become: yes 5 tasks: 6 - name: Install Nginx using apt 7 apt: 8 name: nginx 9 state: present To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml -- ask-become-pass CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 10 N33 nirf 1750 Rank

--- Page 157 ---
Select a pipeline run to view information about that run. You can choose to Retain or Delete a run from the context menu. For more information on run retention, see Build and release retention policies. Stages La 51m ago > ... 6 42s Retain Delete Ld 2h ago 0 1m 13s - View pipeline details The details page for a pipeline allows you to view and manage that pipeline. Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. Azure DevOps · Search F 4 FabrikamFiber Edit Run pipeline : + Runs Branches Analytics Y Description Stages Ac #20191209.3 Set up CI with Azure Pipelines Manually triggered { main 2b4b23c #20191209.2 Set up CI with Azure Pipelines & Manually triggered & main 2b4b23c Là 54m ago Č 1m 13s Là 55m ago Č 1m 55s #20191209.1 Set up CI with Azure Pipelines Individual CI 1º main 2b4b23c Lô 56m ago 1m 6s 17

--- Page 158 ---
Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. View pipeline run details From the pipeline run summary you can view the status of your run, both while itis running and when it is complete. #20191210.2 Update azure-pipelines.yml for Azure Pipe ... on FabrikamFiber Summary Environments Triggered by Steve Danielson FabrikamFiber & main b2f795e Today at 12:56 PM Duration: 1m 9s Tests: Get started Changes: ¢ 2 commits Work items: ₾ 1 linked Artifacts: 1 published Stages Jobs Build 1 job completed 415 Run new : Deploy 1 job completed 13s 1 artifact From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, test results, and work Items From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, testresults, and work items Jobs and stages The jobs pane displays an overview of the status of your stages and jobs. This pane may have multiple tabs depending on whether your pipeline has stages and jobs, or just jobs. In this example, the pipeline has two stages named Build and Deploy. You can drill down into the pipeline steps by choosing the job from either the Stages or Jobs pane. 10

--- Page 121 ---
Course Code/Title:CS3V15/Devops Unit: IV · - m ping specifies the "ping" module, a basic module that checks the reachability of the managed nodes. . - i inventory.yaml points to the inventory file containing the list of devices. · The "ping" module sends a test command to the target devices and reports their status, confirming whether they are responsive. Output and Result: auriga@auriga-Latitude-E7470 :- /Desktop/ansibleŞ ansible all -[ inventory.yaml -m ping VOOS | UNREACHABLET => ( "changed's false. "mg's "Failed to connect to the host via s "unreachable": true auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ton denied (publickey_ password) In the above example the ssh host is not reachable. Activities Terminal Nov 2 12:13 . auriga@auriga-Latitude-E7470: - /Desktop/ansible C auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -i inventory.yaml -m ping vn81 | SUCCESS => 'ansible_facts": { "discovered_interpreter_python": "/usr/bin/python3" "changed": false, "ping": "pong" auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ In the above output image it is working fine Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: 1ansible all -i inventory.yaml -m apt -a "name=nginx state=present" -- become -- ask-become-pass · - m apt specifies the "apt" module, which is responsible for package management. · - a "name=nginx state=present" includes variables for the module to act upon, such as specifying that Nginx should be installed (state=present). · - become signifies that the command should run with elevated privileges (sudo). · - ask-become-pass prompts for the sudo password of the managed node to ensure the installation proceeds smoothly. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 7 N32 nirf 1750 Rank

--- Page 122 ---
Course Code/Title:CS3V15/Devops Unit:IV Output and Result: Activities Terminal * Nov 2 12:24 · auriga@auriga-Latitude-E7470: - /Desktop/ansible F X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -[ inventory.yaml -m apt -a "name=ansible state=present" -- become - - ask-become-pass BECOME password: vm01 | CHANGED => { "ansible_facts": { "discovered_interpreter_python": * /usr/bin/python3" "cache_update_time": 1698985056, "cache_updated": false, "changed": true, "stderr": "" 'stderr_Lines": []. stdout": "Reading package lists ... \nBuilding dependency tree ... \nReading state information ... \nThe following packages were automatically installed and are no longer required:\n gtr1.2-goa-1.0 hplip-data libcgt-fast-perl libcgt-pm-perl\n libdouble-conversion3 Libevent-core-2.1-7 libevent-pthreads-2.1-7\n \ibfcgt-perl libfprint-2-todi libfwupdplugin1 libht ml-template-perl\n Libpcre2-16-0 libqtScoresa libqtsdbuss libqtsguis libqt5network5\n libqtSpositionings libqt5printsupports libqt5qm15 Libqtsquick5\n libqtSsensors5 libqt5svg5 Li bqt5webchannel5 libqt5webkit5 libqt5widgets5\n Libxcb-xinerama@ Libxcb-xinpute Libxmlbi printer-driver-postscript-hp\n python3-renderpm python3-reportlab python3-reportlab-accel\n qt5-gtk-platformthene qttranslations5-lien shim\nUse 'sudo apt autoremove' to remove them. InThe following additional packages will be installed:\n teee-data python3-argcomplete pyt hon3-crypto python3-dnspython\n python3-jinja2 python3 . jmespath python3-kerberos python3-libcloud\n python3-netaddr python3-ntin-auth python3-requests-kerberos\n python3-requests- ntlm python3-selinux python3-winrm python3-xmltodict\nSuggested packages: \n cowsay sshpass python- jinja2-doc ipython3 python-netaddr-docs\nThe following NEW packages will be install ed:\n ansible leee-data python3-argcomplete python3-crypto python3-dnspython\n python3- jinja2 python3-jmespath python3-kerberos python3 . Libcloud\n python3-netaddr python3-ntlm-aut h python3-requests-kerberos\n python3-requests-ntIm python3-selinux python3-winrm python3-xmltodict\n0 upgraded, 16 newly installed, 0 to remove and 50 not upgraded. InNeed to get 97 25 kb of archives. \nAfter this operation, 98.6 MB of additional disk space will be used. \nGet:1 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-jinja2 all 2.10.1-2 [95.5 KB]\nGet:2 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-crypto amd64 2.6.1-13ubuntu2 [237 KB]\nGet:3 http://in.archive.ubuntu.com/ubuntu focal-updates/main amd64 pyt hon3-dnspython all 1.16.0-1ubuntu1 [89.2 KB]\nGet:4 http://in.archive.ubuntu.com/ubuntu focal/main amd64 ieee-data all 20180805.1 [1589 KB]\nGet: 5 http://in.archive. ubuntu.com/ubuntu focal-updates/main amd64 python3-netaddr all 0.7.19-3ubuntu1 [236 KB]\nGet:6 http://in.archive.ubuntu.com/ubuntu focal/universe and64 ansible all 2.9.6+dfsg-1 [5794 KB]\nGet:7 http: //\n.archive.ubuntu.com/ubuntu focal/universe amd64 python3-argcomplete all 1.8.1-1.3ubuntu1 [27.2 kB]\nGet:8 http://in.archive.ubuntu.com/ubuntu focal-updates/main and64 python3- jne spath all 0.9.4-2ubuntu1 [21.5 kB]\nGet:9 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-kerberos amd64 1.1.14-3.1build1 [22.6 k8]\nGet: 10 http://in.archive.ubuntu. con/ubuntu focal/universe amd64 python3-libcloud all 2.8.0-1 [1483 KB]\nGet:11 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-ntlm-auth all 1.1.0-1 [19.6 KB]\nGet: 1 2 http://in.archive.ubuntu.com/ubuntu focal/universe and64 python3-requests-kerberos all 0.12.0-2 [11.9 KB]\nGet: 13 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-r equests-ntlm all 1.1.0-1 [6004 B]\nGet: 14 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-selinux amd64 3.0-1build2 [139 KB]\nGet : 15 http://in.archive.ubuntu.com/ubu ntu focal/universe and64 python3-xmltodict all 0.12.0-1 [12.6 KB]\nGet:16 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-winrm all 0.3.0-2 [21.7 KB]\nFetched 9725 k B in 7s (1454 KB/s)\nSelecting previously unselected package python3-jinja2.\r\n(Reading database ... \r(Reading database ... 5*\r (Reading database .. . 10%\r(Reading database ... 15% \r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database . .. 45%\r (Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 68%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading da tabase ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 212542 files and directories currently installed.)\r\nPreparin g to unpack ... /00-python3-jinja2_2.10.1-2_all.deb ... \r\nUnpacking python3-jinja2 (2.10.1-2) ... \r\nSelecting previously unselected package python3-crypto. \r\nPreparing to unpack .. ./01-python3-crypto_2.6.1-13ubuntu2_amd64.deb ... \r\nUnpacking python3-crypto (2.6. 1-13ubuntu2) ... \r\nSelecting previously unselected package python3-dnspython. \r\nPreparing to unpa ck ... /02-python3-dnspython_1.16.0-lubuntu1_all.deb ... \r\nUnpacking python3-dnspython (1. 16.0-1ubuntu1) ... \r\nSelecting previously unselected package leee-data. \r\nPreparing to unp ack ... /03-ieee-data_20180805.1_all.deb ... \r\nUnpacking teee-data (20188805.1) ... \r\nSelecting previously unselected package python3-netaddr. \r\nPreparing to unpack ... /04-python3. netaddr_0.7.19-3ubuntu1_all.deb ... \r\nUnpacking python3-netaddr (0.7.19-3ubuntu1) ... \r\nSelecting previously unselected package ansible. \r\nPreparing to unpack .. . /05-ansible_2.9.6 +dfsg-1_all.deb ... \r\nUnpacking ansible (2.9.6+dfsg-1) ... \r\nSelecting previously unselected package python3-argcomplete. \r\nPreparing to unpack .. . /06-python3-argcomplete_1.8.1-1. 3ubuntu1_all.deb ... \r\nUnpacking python3-argcomplete (1.8.1-1.3ubuntu1) ... \r\nSelecting previously unselected package python3- jnespath. \r\nPreparing to unpack ... /07-python3- jnespa th_0.9.4-2ubuntul_all.deb ... \r\nUnpacking python3-jmespath (0.9.4-2ubuntu1) ... \r\nSelecting previously unselected package python3-kerberos. \r\nPreparing to unpack ... /08-python3-ke rberos_1.1.14-3.1build1_and64.deb ... \r\nUnpacking python3-kerberos (1.1.14-3.1build1) ... \r\nSelecting previously unselected package python3-libcloud. \r\nPreparing to unpack .. . /09- python3-libcloud_2.8.0-1_all.deb ... \r\nUnpacking python3-libcloud (2.8.0-1) ... \r\nSelecting previously unselected package python3-ntlm-auth. \r\nPreparing to unpack ... /10-python3-n tim-auth_1.1.0-1_all.deb .. . \r\nUnpacking python3-ntlm-auth (1.1.0-1) ... \r\nSelecting previously unselected package python3-requests-kerberos. \r\nPreparing to unpack ... /11-python3. requests-kerberos 0.12.8-2 all. deb .... \r\nUnpacking python3-requests-kerberos (0.12.0-2) ... \r\nSelecting previously unselected package python3-requests-ntln. \r\nPreparing to unpack By using these ad-hoc commands, you can perform quick, task-specific operations on your managed nodes, saving time and effort in your automation tasks. CommonModules: Ansible offers an extensive library of modules for various tasks, from system administration to application deployment. You can explore the full list of Ansible modules in the official documentation to find the most suitable module for your specific requirements. BestPractices: When working with ad-hoc commands, consider using SSH key-based authentication for secure, passwordless access. It streamlines the authentication process and enhances the security of your automation tasks. Using Ansible Playbooks Introduction: In the world of Ansible, playbooks are the cornerstone of structured and complex automation. While ad- hoc commands are perfect for quick and isolated tasks, playbooks step in when you need a well- orchestrated sequence of operations. They allow you to tackle multi-step automation scenarios where tasks depend on each other, and conditional actions are required. Let's delve deeper into the importance of playbooks: The Role of Playbooks: Playbooks serve as the framework for orchestrating automation tasks that involve a sequence of operations. Whether it's configuring servers, deploying applications, or managing infrastructure, playbooks provide a structured way to define how these tasks are executed. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N33 nirf 1750 Rank

--- Page 143 ---
Unit: V Course Code/Title: CS3V15/Devops Email preferences Send me occasional product updates, announcements, and offers. Verify your account Pick one square that shows two identical objects. wiki How Step 4: Click the green Create account button. It's belowthe form, at the bottom of the page. This will take you to an email verification page. Verify your account Create account By creating an account, you agree to the Terms of Service. For more information about GitHub's privacy practices, see the GitHub Privacy Statement. We'll occasionally send you account-related wikiHow UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 3

--- Page 144 ---
Course Code/Title: CS3V15/Devops Unit: V Carefully review the Terms of Service at https://help.github.com/en/articles/github- terms-of-service and the Privacy Statement at https://help.github.com/en/articles/github- privacy-statement before you continue. You're almost done! We sent a launch code to wikihowneveconcepts@gmail.com + Enter code Didn't get your email? Resend the code or update your email address. wiki How Step 5: Verify your email by entering the code. After clicking Create account, you'll receive an email with a code.Enter this code on the verification page. Entering the code will automatically take you to the welcome page. Step 6: Select your preferences and click Continue. GitHub displays a quick survey that can help you tailor your experience to match what you're looking for. You'll be sent tothe plan selection page after completing the survey. How many team members will be working with you? This will help us guide you to the tools that are best suited for your projects. Just me 2 - 5 5 - 10 10 - 20 20 - 50 50+ Are you a student or teacher? Student Teacher Continue wikiHow CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 4 175° Rank ---

--- Page 37 ---
Course Code/Title:CS3V15/Devops Unit: I GIT INSTALLATION Git for Windows stand-alone installer · Download the latest Git for Windows installer. · When you've successfully started the installer, you should see the Git Setup wizard screen. Follow the Next and Finish prompts to complete the installation. The default options are pretty sensible for most users. · Open a Command Prompt (or Git Bash if during installation you elected not to use Git from the Windows Command Prompt). . Run the following commands to configure your Git username and email using the following commands, replacing Emma's name with your own. These details will be associated with any commits that you create: $ git config -- global user.name "CIT_CHENNAI" $ git config -- global user.email CITCHENNAI@atlassian.com • Optional: Install the Git credential helper on Windows Bitbucket supports pushing and pulling over HTTP to your remote Git repositories on Bitbucket. Every time you interact with the remote repository, you must supply a username/password combination. You can store these credentials, instead of supplying the combination every time, with the Git Credential Manager for Windows. BASIC COMMANDS OF GIT: Some basic Git commands along with their syntax and examples: · Initialize a Repository: Syntax: `git init' Example: `git init' · Clone a Repository: Syntax: `git clone <repository_url> Example: `git clone https://github.com/example/repository.git' · Check Repository Status: Syntax: `git status' Example: `git status' · Add Changes to Staging Area: Syntax: `git add <file(s)>' Example: `git add file.txt' · Commit Changes: Syntax:`git commit -m "Commit message" Example:`git commit -m "Add new feature" · Create a New Branch: Syntax: `git branch <branch_name>' Example: `git CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY (Autonomous] N33 37 nirf 1750 Rank

--- Page 38 ---
Course Code/Title:CS3V15/Devops branch feature-branch' Unit: I • Switch to a Branch: Syntax: `git checkout <branch_name>' Example:`git checkout feature-branch' OR Syntax: `git switch <branch_name>' (Git version 2.23 and later) Example: `git switch feature-branch' · Create and Switch to a New Branch: Syntax: `git checkout -b <new_branch name>' Example: `git checkout -b new-feature' OR Syntax: `git switch -c <new_branch_name>' (Git version 2.23 and later) Example: `git switch -c new-feature' . Merge Changes from One Branch to Another: Syntax: `git merge <branch_name>' Example: `git merge feature-branch' · View the Commit History: Syntax: `git log' Example: `git log' · Push Changes to a Remote Repository: Syntax: `git push <remote_name> <branch_name>' Example:`git push origin master' · Pull Changes from a Remote Repository: Syntax: `git pull <remote_name> <branch_name>' Example:`git pull origin master' . Show the Differences Between Working Directory and Staging Area: Syntax: `git diff Example: `git diff · Show the Differences Between Staging Area and Last Commit: Syntax: `git diff -- cached' Example: `git diff -- cached' . Show the Differences Between Working Directory and Last Commit: Syntax: `git diff HEAD' Example: `git diff HEAD' CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 38 N32 nirf 1750 Rank

--- Page 135 ---
Course Code/Title:CS3V15/Devops Unit: IV Syntax for writing the module on command line While writing the module we use the options such as -m and -a. The -m is for module and -a is for argument, after the -a option we specify the executable command in double quotes. here 'all' stands for all the remote hosts, otherwise we can mention the group name of the hosts 1 $ ansible all -m command -a "echo 'Hello hosts'" The -m option followed by module name 'command' Command to be executed on remote hosts, given in double quotes Commonly used modules 1. Command : Following is a command module that executes a command on remote hosts $ ansible all -m command -a "echo 'Hello hosts'" 2. User : The user module is used to create, modify or delete users. For example - To create a user named 'new_user' on all the hosts in the 'dev' group using ansible, you can use the user module. Here's the ansible command to do that : $ ansible dev -m user -a "name=new_user state=present" Sometimes the above command fails as there is no privilege to create user on remote host. Then we can issue the command as follows $ ansible all -m user -a "name=new_user state=present" -- become -- ask-become-pass The above command asks for the password for the remote host to login. On supplying the password the user will be created. We can verify. if new user is getting created or not on remote host by issuing following command on each remote host $ less /etc/passwd If we want to create new user without asking for password on all the remote hosts then we can issue the command as - $ ansible all -m user -a "name=new_user1 state=present" -- become -b The -b option is used to become superuser. By this the sudo or su privileges can be granted. .3. Ping : The ping module is used to connectivity to remote hosts $ ansible all -m ping 4. Copy : The copy module is used to copy files from control machine to remote hosts $ ansible dev -m copy -a "src=/home/ansible_user/myfile.txt dest=/home/ansible_user/myfile.txt" The above command will copy the myfile.txt file from control node to all the managed nodes of dev group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 21 N32 nirf 1750 Rank

--- Page 136 ---
Course Code/Title:CS3V15/Devops Unit: IV 5. Package : The package module is used to install, remove or upgrade the packages, For example, the following command installs the nginx web server package : $ ansible package -a "name=nginx state=present" Here name indicates the name of the package to be installed, the state=present means install the package. Normally "present" is for installation, "remove" is for removing the package and "latest" is used to upgrade the package. 6. Shell : The shell module is used to run shell commands with more flexibility on remote hosts. For example - Following shell command executes on remote hosts. $ ansible dev -m shell -a "cmd=is" 7. Service : The service module is used to start, stop or restart a specific service on remote hosts. For example - Following service will start the apache web service on all the remote hosts. $ ansible dev -m service -a "name=apache2 state=started" Note that the state can started for starting the service, stopped for stopping the service. o started/stopped are idempotent actions that will not run commands unless necessary. o restarted will always bounce the service. o reloaded will always reload. I Example 1: Get the Uptime of remote servers We have used command module to run the uptime command and we have given both the ad hoc and the playbook form of execution. as AD-HOC Command $ ansible testservers -m command -a uptime -i ansible_hosts as Playbook - name: Check the remote host uptime hosts: testservers tasks: - name: Execute the Uptime command over Command module register: uptimeoutput command: "uptime" - debug: var: uptimeoutput.stdout_lines CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 22 N32 nirf 1750 Rank

--- Page 87 ---
Course Code/Title:CS3V15/Devops Unit: III 2. Go to Plugin Manager: From the dashboard, click on 'Manage Jenkins' on the left-hand side menu. On the Manage Jenkins page, click on 'Manage Plugins'. This will take you to the Plugin Manager, where you can view, install, and update plugins. 3. Exploring Available Plugins 1. Available Tab: The 'Available' tab lists all plugins that are available for installation. This includes thousands of plugins categorized by functionality (e.g., SCM, build tools, user interface enhancements). 2. Search for Plugins: Use the search box to quickly find the plugin you need. For example, if you're looking to integrate Jenkins with Git, type 'Git' in the search box. 3. Popular Plugins: Jenkins highlights popular plugins at the top of the Available tab. These are commonly used plugins that most users find essential. 4. Installing Plugins 1. Selecting Plugins to Install: Check the box next to each plugin you want to install. You can select multiple plugins at once. 2. Install Without Restart: After selecting your plugins, scroll down and click 'Install without restart'. Jenkins will install the plugins immediately, and you can continue using Jenkins during the installation. 3. Install After Restart: If you prefer, you can choose to 'Download now and install after restart'. This will install the plugins after Jenkins is restarted, ensuring that all changes are applied without interference. 4. Installation Progress: The installation process will show a progress bar for each plugin. Once a plugin is installed, it will move to the 'Installed' tab. 5. Configuring Installed Plugins 1. Plugin Configuration: After installation, some plugins may require additional configuration. Navigate to 'Manage Jenkins' > 'Configure System' or 'Configure Global Security' to adjust settings for your new plugins. 2. Global Tool Configuration: For build tools like Maven or JDKs installed via plugins, you can configure them under 'Global Tool Configuration'. Here, you define the paths to the tools or specify installation options. 6. Updating and Managing Plugins 1. Updating Plugins: Regularly update your plugins to ensure compatibility with the latest version of Jenkins and to benefit from new features or security patches. Go to the 'Updates' tab in the Plugin CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N32 nirf 12 1750 Rank

--- Page 88 ---
Unit: III Course Code/Title:CS3V15/Devops Manager. Jenkins will show you all plugins with available updates. Click 'Update' to install the latest versions. 2. Uninstalling Plugins: If a plugin is no longer needed, you can uninstall it from the 'Installed' tab. Select the plugin and click 'Uninstall'. Uninstallation typically requires a Jenkins restart to complete. 7. Commonly Used Plugins Here are a few essential plugins that you might consider adding to your Jenkins setup: 1. Git Plugin: Integrates Jenkins with Git, allowing you to pull code from GitHub, Bitbucket, or other Git repositories. 2. Pipeline Plugin: Enables the creation and management of Jenkins pipelines, a powerful way to define complex build, test, and deployment processes. 3. Maven Integration Plugin: Adds support for Maven projects, allowing you to build and manage Maven-based projects directly within Jenkins. 4. Email Extension Plugin: Provides advanced email notification options for build statuses, including customizable email templates. 5. Blue Ocean: An alternative user interface for Jenkins that simplifies pipeline creation and offers a modern, user-friendly design. 8. Troubleshooting Plugin Issues 1. Compatibility Issues: Sometimes, plugins might conflict with each other or with the Jenkins version. In such cases, refer to the plugin's documentation or Jenkins logs to diagnose the issue. 2. Rollback: If a plugin update causes problems, you can rollback to a previous version from the 'Installed' tab by selecting the desired version. 3. Restart Jenkins: Some plugin changes require a Jenkins restart. Ensure all jobs are completed before restarting to avoid any interruptions. Commonly Used Jenkins Plugins 1. Git Plugin Overview: The Git Plugin integrates Jenkins with Git repositories, allowing Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nårf 1750 Rank

--- Page 55 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Environment Variables Now remove active profile from maven settings.xml and update the test profile mentioned in pom.xml. Add activation element to profile element as shown below. The test profile will trigger when the system property "env" is specified with the value "test". Create an environment variable "env" and set its value as "test". <profile> <id>test</id> <activation> <property> <name>env</name> <value>test</value> </property> </activation> </profile> Let's open command console, go to the folder containing pom.xml and execute the following mvn command. C:\MVN\project>mvn test Profile Activation via Operating System Activation element to include os detail as shown below. This test profile will trigger when the system is windows XP. <profile> <id>test</id> <activation> <os> <name>Windows XP</name> <family>Windows</family> <arch>x86</arch> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 14 N32 narf 1750 Rank -

--- Page 56 ---
Course Code/Title:CS3V15/Devops Unit: II <version>5.1.2600</version> </os> </activation> </profile> Now open command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test Profile Activation via Present/Missing File Now activation element to include OS details as shown below. The test profile will triggerwhen target/generated-sources/axistools/wsdl2java/com/companyname/group is missing. <profile> <id>test</id> <activation> <file> <missing>target/generated-sources/axistools/wsdl2java/ com/companyname/group</missing> </file> </activation> </profile> Now open the command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test 8. Maven create and build artifacts: The first step is to open Eclipse, which comes with the integrated Maven environment. The Eclipse window opens on the screen. Complete the following steps: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 15 N32 nirf 175º Rank -

--- Page 29 ---
Unit: I Course Code/Title:CS3V15/Devops Enterprise services. AWS virtual private cloud. Outsourced cloud services. Salesforce User Controls Operating System, Runtime, Middleware, and Application data Others It is highly scalable and flexible. Microsoft Azure. Force.com, Gigaspaces. Data of the application It is highly scalable to suit the different businesses according to resources. Google cloud services Google offers a seven wide range of Services: · Compute · Networking · Storage and Databases · Big Data · Machine Learning · Identity & Security · Management and Developer Tools IBM cloud analysis. AWS, Terremark Nothing It is highly scalable to suit the small, mid and enterprise level business 1. Compute: GCP provides a scalable range of computing options you can tailor to match your needs. It provides highly customizable virtual machines. and the option to deploy your code directly or via containers. · Google Compute Engine · Google App Engine · Google Kubernetes Engine · Google Cloud Container Registry · Cloud Functions 2. Networking: The Storage domain includes services related to networking, it includes the following services • Google Virtual Private Cloud (VPC) . Google Cloud Load Balancing · Content Delivery Network · What is Google Cloud Connect · Google Cloud DNS · What is Google Cloud Web Hosting 3. Storage and Databases: The Storage domain includes services related to data storage, it includes the following services · Google Cloud Storage . Cloud SQL · Cloud Bigtable · Google Cloud Datastore • Persistent Disk TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 29 N32 nirf 1750 Rank

--- Page 30 ---
Unit: I Course Code/Title:CS3V15/Devops 4. Big Data: The Storage domain includes services related to big data, it includes the following services · Google BigQuery · Google Cloud Dataproc · Google Cloud Datalab · Google Cloud Pub/Sub 5. Cloud AI: The Storage domain includes services related to machine learning, it includes the following services · Cloud Machine Learning · Vision API · Speech API · Natural Language API · Translation API • Jobs API 6. Identity & Security: The Storage domain includes services related to security, it includes the following services · Cloud Resource Manager · Cloud IAM · Cloud Security Scanner · Cloud Platform Security 7. Management Tools: The Storage domain includes services related to monitoring and management, it includes the following services • Stackdriver · Monitoring · Logging · Error Reporting · Trace · Cloud Console 8. Developer Tools: The Storage domain includes services related to development, it includes the following services . Cloud SDK · Deployment Manager · Cloud Source Repositories · Cloud Test Lab AZURE Azure is Microsoft's cloud platform, just like Google has its Google Cloud and Amazon has its Amazon Web Service or AWS.000. Generally, it is a platform through which we can use Microsoft's resources. For example, to set up a huge server, we will require huge investment, effort, physical space, and so on. In such situations, Microsoft Azure comes to our rescue. It will provide us with virtual machines, fast processing of data, analytical and monitoring tools, and so on to make TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 30 N33 narf 1750 Rank

--- Page 111 ---
Course Code/Title:CS3V15/Devops Unit: III Steps: 1. Install the Git Plugin: 0 Go to Manage Jenkins > Manage Plugins. ○ Under the Available tab, search for Git Plugin and install it. 2. Configure Git in Jenkins: ○ Navigate to Manage Jenkins> Global Tool Configuration. ○ Scroll down to the Git section and click Add Git. ○ Specify the path to the Git executable or use the default path. ○ Optionally, configure additional settings such as Git installation locations. 3. Example Configuration: ○ Name: Git ○ Path to Git executable: /usr/bin/ git (or use the default if Git is installed in a standard location). 4. Configure Job to Use Git: ○ Create or edit a Jenkins job and go to Source Code Management. ○ Select Git. ○ Enter the repository URL (e.g., https://github.com/user/repository.git). ○ Configure credentials if the repository is private. ○ Specify the branch to build (e.g., main). 5. Example Repository URL: ○ Repository URL: https://github.com/example/repo.git o Branch Specifier: main 3. Configuring Jenkins to Work with Maven Overview: The Maven Plugin integrates Jenkins with Apache Maven, allowing Jenkins to use Maven to build projects. Steps: 1. Install Maven: ○ Ensure that Apache Maven is installed on your system. You can download it from the Apache Maven website. 2. Configure Maven in Jenkins: ○ Go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to the Maven section and click Add Maven. ○ Enter a name for the Maven installation (e.g., Maven 3.8.6). ○ Check Install automatically to let Jenkins download Maven, or specify the path to an existing Maven installation. ○ CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 36 1750 Rank

--- Page 112 ---
Unit: III Course Code/Title:CS3V15/Devops 3. Example Configuration: ○ Name: Maven 3.8.6 ○ MAVEN_HOME: / usr / share / maven 4. Configure Maven in a Jenkins Job: ○ Create or edit a Jenkins job and go to Build section. 0 Add a build step and select Invoke top-level Maven targets. o Choose the Maven version configured earlier. ○ Specify the goals to run (e.g., clean install). 5. Example Maven Goals: ○ Goals: clean install 6. Configure Build Environment: ○ Ensure that the job's build environment is set up to use Maven. This might involve setting environment variables or configuring build scripts. 7. Example Environment Variables: ○ MAVEN_OPTS: - Xms512m -Xmx2048m (if you need to set specific JVM options for Maven). By following these steps, Jenkins will be properly configured to work with Java, Git, and Maven, allowing you to set up and manage continuous integration and delivery pipelines effectively. If you need further customization or run into issues, consulting the documentation for each tool or plugin can provide additional guidance. Creating a Jenkins Build and Understanding Jenkins Workspace 1. Creating a Jenkins Build Overview: A Jenkins build is a process where Jenkins executes a series of steps defined in a job configuration. This typically involves compiling code, running tests, and generating artifacts. Steps to Create a Jenkins Build: 1. Access Jenkins Dashboard: ○ Open your Jenkins dashboard, usually found at http://localhost: 8080. 2. Create a New Job: ○ Click on New Item on the left sidebar. Enter a name for your job. ○ Choose a job type. Common types include: I Freestyle project: For simple build jobs with basic configuration. · Pipeline: For more complex build processes using a Jenkinsfile. ○ Click OK to proceed. ○ TECHNOLOGY N33 37 nirf 1750 Rank CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous)

--- Page 63 ---
Course Code/Title:CS3V15/Devops Unit: II 10. Introduction of Gradle: The list of features that Gradle provides, · Gradle is available with separate Domain Specific Language (DSL) based on Groovy language. · It provides the declarative language elements. Those elements also provide build-by- convention support for Java, Groovy, OSGI, Web and Scala. Language for dependency based programming The declarative language lies on a top of a general purpose task graph, which can be fully supported in the build. Structure your build Gradle allows you to apply common design principles to your build. It will give you a perfect structure for build, so that, you can design well-structured and easily maintained, comprehensible build. Deep API By using this API, you can monitor and customize its configuration and execution behavior to the core. Gradle scales Gradle can easily increase the productivity, from simple and single project builds to huge enterprise multi-project builds. Multi-project builds Gradle supports the multi-project builds and partial builds. If you build a subproject, Gradle takes care of building all the subprojects, that the subproject depends on. Different ways to manage your builds Gradle supports different strategies to manage your dependencies. Gradle is the first build integration tool Gradle is fully supported for your ANT tasks, Maven and lvy repository infrastructure for publishing and retrieving dependencies. It also provides a converter for turning a Maven pom.xml to Gradle script. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 22 N32 nirf 175º Rank -

--- Page 64 ---
Course Code/Title:CS3V15/Devops Unit: II Ease of migration Gradle can easily adapt to any structure. Therefore, you can always develop your Gradle build in the same branch, where you can build live script. Gradle Wrapper Gradle Wrapper allows you to execute the Gradle builds on machines, where Gradle is not installed. This is useful for continuous integration of servers. Free open source Gradle is an open source project, and licensed under the Apache Software License (ASL). Groovy Gradle's build script are written in Groovy programming language. The whole design of Gradle is oriented towards being used as a language and not as a rigid framework. Groovy allows you to write your own script with some abstractions. The whole Gradle API is fully designed in Groovy language. Installation of Gradle: Prerequisites to install Gradle JDK and Groovy are the prerequisites for Gradle installation. Gradle requires JDK version 6 or later to be installed in the system. It uses the JDK libraries which are installed, and sets to the JAVA_HOME environmental variable. Gradle carries its own Groovy library, therefore, we need not install Groovy explicitly. If it is installed, that is ignored by Gradle. The steps to install Gradle in your system are explained below. Step 1 - Verify JAVA Installation First of all, you need to have Java Software Development Kit (SDK) installed on your system. To verify this, execute Java -version command in any of the platform you are working on. In Windows Execute the following command to verify Java installation. I have installed JDK 1.8 in my system. C:\> java - version CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE . TECHNOLOGY 23 N32 narf 1750 Rank

--- Page 13 ---
Course Code/Title:CS3V15/Devops Unit: I 2024. Terraform provides the agility and scalability required to keep up with the dynamic demands of modern applications. Terraform's importance lies in its ability to bring consistency, version control, and automation to infrastructure operations, thereby reducing manual errors, streamlining DevOps workflows, and facilitating applications' rapid and reliable deployment in an increasingly complex and cloud-centric environment. As organizations adopt cloud-native technologies, Terraform remains essential to ensure efficient and consistent infrastructure management. 25. Phantom Phantom enhances security automation and incident response capabilities. In today's rapidly evolving threat landscape, organizations face a constant barrage of cybersecurity incidents, and the ability to respond swiftly and effectively is necessary. It provides a platform for automating security workflows, from detecting and investigating potential threats to orchestrating responses and mitigating risks. Phantom's importance lies in its capacity to reduce response times, increase consistency in incident handling, and free up manual resources from repetitive tasks. With the growing complexity of cyber threats, Phantom empowers security teams to defend against attacks and safeguard critical assets proactively. 26. Nagios Nagios, an open-source monitoring and alerting system, remains vital due to its enduring significance in maintaining the reliability and performance of IT infrastructure and applications. Organizations increasingly rely on complex systems and services. Nagios plays a crucial role by providing real-time monitoring and alerting capabilities, allowing IT teams to detect and address issues before they impact users or cause system outages. Its versatility, extensibility, and support for both on-premises and cloud environments make Nagios a valuable tool for ensuring critical systems' availability, stability, and security, aligning perfectly with the demands of modern IT operations and DevOps practices. 27. Vagrant Vagrant continues to play a crucial role in software development and DevOps. It is a tool that simplifies creating and managing reproducible development environments. Its importance lies in its ability to provide developers and DevOps teams with a consistent and isolated environment for software development, testing, and deployment. With the ever-evolving complexity of software stacks, dependencies, and infrastructure configurations, Vagrant remains essential in ensuring these environments are easily shareable, scalable, and maintainable. It allows developers to work seamlessly across various operating systems and provides a standardized setup that minimizes compatibility issues. 28. Sentry Sentry plays a critical role in modern software development and DevOps practices. With software applications' increasing complexity and scale, identifying and addressing errors and issues has become crucial. Sentry is vital because it provides real-time error tracking and monitoring, allowing development teams to proactively detect and diagnose issues, whether they occur in production or during development. Its importance is minimizing downtime, improving user experience, and maintaining software systems' overall health and reliability. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nirf 1750 Rank

--- Page 14 ---
Course Code/Title:CS3V15/Devops Unit: I 29. Gradle Gradle continues to be a vital tool in software development and DevOps. Gradle is an advanced build automation system that plays a crucial role in managing dependencies, building projects, and orchestrating complex workflows efficiently. Its importance lies in its versatility and scalability, as it caters to various project sizes and types. Gradle's ability to easily handle multi-language, multi-project builds and its support for plugin-based customization make it indispensable in modern software development. As organizations increasingly adopt microservices architectures and cloud-native technologies, Gradle's capabilities are instrumental in managing the complexity of building, testing, and deploying applications across diverse environments. DevOps Architecture DevOps Architecture Plan Release Code Dep Deploy Development and operations both play essential roles in order to deliver applications. The deployment comprises analyzing the requirements, designing, developing, and testing of the software components or frameworks. The operation consists of the administrative processes, services, and support for the software. When both the development and operations cture is the solution to fix the gap between Test Monitor Build Operate ined with collaborating, then the DevOps al www deployment and operation terms; therefore, delivery can be faster. DevOps architecture is used for the applications hosted on the cloud platform and large distributed applications. Agile Development is used in the DevOps architecture so that integration and delivery can be contiguous. When the development and operations team works separately from each other, then it is time- consuming to design, test, and deploy. And if the terms are not in sync with each other, then it may cause a delay in the delivery. So DevOps enables the teams to change their shortcomings and increases productivity. Below are the various components that are used in the DevOps architecture: 1) Build: Without DevOps, the cost of the consumption of the resources was evaluated based on the pre-defined individual usage with fixed hardware allocation. And with DevOps, the usage of cloud, sharing of resources comes into the picture, and the build is dependent upon the user's need, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Build Code Test Plan DevOps Components Moniter Dev Ops Deploy Operate Release 14 N33 nirf 1750 Rank

--- Page 87 ---
Course Code/Title:CS3V15/Devops Unit: III 2. Go to Plugin Manager: From the dashboard, click on 'Manage Jenkins' on the left-hand side menu. On the Manage Jenkins page, click on 'Manage Plugins'. This will take you to the Plugin Manager, where you can view, install, and update plugins. 3. Exploring Available Plugins 1. Available Tab: The 'Available' tab lists all plugins that are available for installation. This includes thousands of plugins categorized by functionality (e.g., SCM, build tools, user interface enhancements). 2. Search for Plugins: Use the search box to quickly find the plugin you need. For example, if you're looking to integrate Jenkins with Git, type 'Git' in the search box. 3. Popular Plugins: Jenkins highlights popular plugins at the top of the Available tab. These are commonly used plugins that most users find essential. 4. Installing Plugins 1. Selecting Plugins to Install: Check the box next to each plugin you want to install. You can select multiple plugins at once. 2. Install Without Restart: After selecting your plugins, scroll down and click 'Install without restart'. Jenkins will install the plugins immediately, and you can continue using Jenkins during the installation. 3. Install After Restart: If you prefer, you can choose to 'Download now and install after restart'. This will install the plugins after Jenkins is restarted, ensuring that all changes are applied without interference. 4. Installation Progress: The installation process will show a progress bar for each plugin. Once a plugin is installed, it will move to the 'Installed' tab. 5. Configuring Installed Plugins 1. Plugin Configuration: After installation, some plugins may require additional configuration. Navigate to 'Manage Jenkins' > 'Configure System' or 'Configure Global Security' to adjust settings for your new plugins. 2. Global Tool Configuration: For build tools like Maven or JDKs installed via plugins, you can configure them under 'Global Tool Configuration'. Here, you define the paths to the tools or specify installation options. 6. Updating and Managing Plugins 1. Updating Plugins: Regularly update your plugins to ensure compatibility with the latest version of Jenkins and to benefit from new features or security patches. Go to the 'Updates' tab in the Plugin CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N32 nirf 12 1750 Rank

--- Page 88 ---
Unit: III Course Code/Title:CS3V15/Devops Manager. Jenkins will show you all plugins with available updates. Click 'Update' to install the latest versions. 2. Uninstalling Plugins: If a plugin is no longer needed, you can uninstall it from the 'Installed' tab. Select the plugin and click 'Uninstall'. Uninstallation typically requires a Jenkins restart to complete. 7. Commonly Used Plugins Here are a few essential plugins that you might consider adding to your Jenkins setup: 1. Git Plugin: Integrates Jenkins with Git, allowing you to pull code from GitHub, Bitbucket, or other Git repositories. 2. Pipeline Plugin: Enables the creation and management of Jenkins pipelines, a powerful way to define complex build, test, and deployment processes. 3. Maven Integration Plugin: Adds support for Maven projects, allowing you to build and manage Maven-based projects directly within Jenkins. 4. Email Extension Plugin: Provides advanced email notification options for build statuses, including customizable email templates. 5. Blue Ocean: An alternative user interface for Jenkins that simplifies pipeline creation and offers a modern, user-friendly design. 8. Troubleshooting Plugin Issues 1. Compatibility Issues: Sometimes, plugins might conflict with each other or with the Jenkins version. In such cases, refer to the plugin's documentation or Jenkins logs to diagnose the issue. 2. Rollback: If a plugin update causes problems, you can rollback to a previous version from the 'Installed' tab by selecting the desired version. 3. Restart Jenkins: Some plugin changes require a Jenkins restart. Ensure all jobs are completed before restarting to avoid any interruptions. Commonly Used Jenkins Plugins 1. Git Plugin Overview: The Git Plugin integrates Jenkins with Git repositories, allowing Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nårf 1750 Rank

--- Page 61 ---
Course Code/Title:CS3V15/Devops Unit: II Return to the previous screen and undo the deletion. The errors that occurred will disappear. *mavenproject/pom.xml & IApp.java AppTest.java 6 <artifactId>mavenproject</artifactId> 7 <version>0.0.1-SNAPSHOT</version> <packaging>jar</packaging> 8 9 16 <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 138 <properties> 14 15 16 17% <dependencies> 18 19 <groupId>junit</groupId> I 28 21 22 23 24 <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> cdependency> <artifactId>junit</artifactid> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> 25 </project> 26 9. Dependency Management: The dependencyManagement and dependencies are especially useful for multi-module projects. Dependency Management : This tag consists of a dependencies tag which itself might contain multiple dependency tags. Each dependency is supposed to have at least three maintags: groupId, artifactId, and version. For example: <dependencyManagement> <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 20 N33 nirf 1750 Rank -

--- Page 62 ---
Course Code/Title:CS3V15/Devops Unit: II The above code just declares the new artifact commons-lang3, but it doesn't really add it to the project dependency resource list. Dependencies: This tag contains a list of dependency tags. Each dependency is supposed to have at least two main tags, which are groupId and artifactId. For example: <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> The version and scope tags can be inherited implicitly if we have used the dependencyManagement tag before in the POM file. DependencyManagement is just a declaration, and it does not really add a dependency. Dependencies tag adds the actual dependency to the project. An example for adding the JUnit library dependency: <dependencyManagement> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.2</version> <scope>test</scope> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 21 N33 nirf 175º Rank

--- Page 1 ---
Course Code/Title:CS3V15/Devops CHENNAI INSTITUTE OF TECHNOLOGY Transforming Lives CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) NAME OF THE PROGRAMME B. E (CSE) YEAR III SEMESTER V REGULATIONS 2022R COURSE CODE CS3V15 COURSE NAME DEVOPS FACULTY NAME (Prepared by) NAME OF SUBJECT EXPERT (Verified by) M.SUNDHARI CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Unit: I N33 INATIONAL BOARD . ACCREDITATION 100% Accreditation All eligible UG & PG Programs NIRF 151 - 200 Band Engineering 2023 A+ DE NAAC Contact 8681927167 Contact 1 N32 nirf 1750 Rank

--- Page 2 ---
Course Code/Title:CS3V15/Devops Unit: I UNIT - I Introduction to Devops Devops Essentials - Introduction To AWS, GCP, Azure - Version control systems: Git and Github. The word "DevOps" was coined in 2009 by Patrick Debois, who became one of its gurus. The term was formed by combining "development" and "operations," which provides a starting point for understanding exactly what people typically mean when they say "DevOps." Notably, DevOps isn't a process or a technology or a standard. Many devotees refer to DevOps as a "culture"-a viewpoint that New Relic favors. We also use the term "DevOps movement" when talking about topics such as adoption rates and trends for the future, and "DevOps environment" to refer to an IT organization that has adopted a DevOps culture. "DevOps represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile, lean practices in the context of a system-oriented approach. DevOps emphasizes people (and culture), and seeks to improve collaboration between operations and development teams. DevOps implementations utilize technology- especially automation tools that can leverage an increasingly programmable and dynamic infrastructure from a life cycle perspective." DevOps is defined as a combination of processes and tools created to facilitate organizations in delivering services and applications much faster than they can through conventional software development processes. It helps increase customers' confidence in the applications that an organization offers, thereby allowing the company to flourish and achieve its business goals faster. Development 3 3 DevOps 13 51 Operations Quality Assurance (QA) How DevOps Works? A DevOps process can be summarized as an infinite loop that comprises the following stages - build, test, and release through the delivery pipeline and plan and monitor through feedback, which resets the loop again. With such an amazing combination, teams use tech stack and tooling that assists them in reliably developing apps. Moreover, going away from the Under the DevOps model, development and operations teams work in constant cohesion throughout the entire project lifecycle, starting right from development to deployment. When security is the main focus, the quality assurance team is tightly knitted with the DevOps team throughout the app lifecycle. In this situation, some DevOps teams are also referred to as DevSecOps. Close coordination with the QA team ensures that no loopholes are left unchecked in the provided service/app. COMPANY HOW DEVOPS WORKS DELIVERY PIPELINE BUILD TEST RELEASE PLAN MONITOR FEEDBACK LOOP TOOLBOX" CUSTOMER norm, teams use automated processes here. DevOps tools also allow engineers to complete different tasks independently. Be it provisioning infrastructure or deploying code, they can accomplish these tasks without being dependent on one another. As such, the DevOps model accelerates the overall application development process. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 49 ---
Course Code/Title:CS3V15/Devops Unit: II Validate Deploy Compile Install Test Verify Package Integration Test · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. There are always pre and post phases to register goals, which must run prior to, or after a particular phase. When Maven starts building a project, it steps through a defined sequence of phases and executes goals, which are registered with each phase. Maven has the following three standard lifecycles - • default: This is the main lifecycle, as it's responsible for project deployment. CHENNAI CHENNAI INSTITUTE . TECHNOLOGY 8 clean: Handles project cleaning, ensuring that all artifacts generated by previous INSTITUTE OF TECHNOLOGY 175º Rank (Autonomous]

--- Page 50 ---
Course Code/Title:CS3V15/Devops Unit: II builds are removed. · site: Manages the creation of the project's site documentation. Default Lifecycle (default) Most Maven users will be familiar with the default lifecycle. It is a general model of a build process for a software application. The first phase is validate and the last phase is deploy. · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. Clean Lifecycle (clean) The first lifecycle in Maven. Running mvn clean invokes the clean lifecycle which consists of three lifecycle phases: · pre-clean : execute processes needed prior to the actual project cleaning · clean : remove all files generated by the previous build · post-clean : execute processes needed to finalize the project cleaning Site Lifecycle (site) Maven does more than build software artifacts from project, it can also generate project documentation and reports about the project, or a collection of projects. Project documentation and site generation have a dedicated lifecycle which contains four phases: 1. pre-site : execute processes needed prior to the actual project site generation 2. site : generate the project's site documentation 3. post-site : execute processes needed to finalize the site generation, and to prepare for site deployment 4. site-deploy: deploy the generated site documentation to the specified web server CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 73 ---
Course Code/Title:CS3V15/Devops Unit: II init - Initializes a new Gradle build. wrapper - Generates Gradle wrapper files. Help tasks Build Environment - Displays all build script dependencies declared in root project 'eclipse-workspace'. components - Displays the components produced by root project 'eclipse-workspace '. [incubating] dependencies - Displays all dependencies declared in root project 'eclipse-works pace'. Dependency Insight - Displays the insight into a specific dependency in root project 'eclipse-workspace.' Dependent Components : It displays the dependent components of components in the root project 'eclipse-workspace.' [incubating] help - Displays a help message. model - Displays the configuration model of root project 'eclipse-workspace.' [incubating] projects - Displays the sub-projects of root project 'eclipse-workspace.' properties - Displays the properties of root project 'eclipse-workspace.' tasks - Displays the tasks runnable from root project 'eclipse-workspace.' To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> To list all the tasks of the project, run the below command: 1. gradle tasks -all CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 32 N33 nirf 1750 Rank

--- Page 74 ---
Course Code/Title:CS3V15/Devops Unit: II To display more details about a task, run the below command: 1. gradle help -- task Listing Dependencies In Gradle, we can list the dependencies which are broken down by the configuration. To list the dependencies, run the below command: 1. gradle -q dependencies Difference between Gradle and Maven Basis Based on Gradle is based on developing Maven is based on developing pure Java language-based software. Configuration It uses a Groovy-based Domain-specific language (DSL) for creating project structure. Focuses on Performance It performs better than maven as it optimized for tracking only current running task. It is necessary to compile. Java Compilation Usability It is a new tool , which requires users to spend a lot This tool is a known tool for many users and is easily Gradle domain-specific language projects. It uses Extensible Markup Language (XML) for creating project structure. Developing applications by adding new features to them . It avoids compilation. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] Maven Developing applications in a given time limit. It does not create local temporary files during software creation and is hence - slower . 33 N33 nirf 1750 Rank -

--- Page 83 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Click "OK" to proceed. 4. Configure the Job General Settings 1. Project Description: ○ Enter a description for your job. 2. Discard Old Builds: ○ Set up a policy to manage the number of builds Jenkins keeps. Source Code Management (SCM) 1. Select SCM: ○ Choose your source code management system (e.g., Git). 2. Configure Repository: ○ Enter the repository URL and provide credentials if needed. 3. Branch Specification: ○ Specify the branch to build (e.g., main, develop). Build Triggers 1. Choose Build Triggers: ○ Poll SCM: Check for changes at intervals. ○ Build Periodically: Schedule builds. ○ GitHub hook trigger: Trigger builds on GitHub push. ○ Build after other projects: Trigger builds based on other jobs. Build Environment 1. Configure Build Environment: ○ Clean workspace: Delete workspace before build starts. ○ Manage secrets: Use secret texts or files. ○ Node selection: Specify where to run the job. Build Steps 1. Add Build Steps: ○ Click "Add build step". ○ Choose the build step type (e.g., Execute shell, Invoke Gradle script). ○ Enter the commands or scripts needed for the build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 nirf 8 1750 Rank

--- Page 84 ---
Course Code/Title:CS3V15/Devops Unit: III Post-build Actions 1. Add Post-build Actions: Click "Add post-build action". ○ Common actions include: Archive artifacts: Store build artifacts. Email notification: Send build status emails. I Publish test results: Aggregate test results. Trigger other jobs: Trigger other jobs based on results. 5. Save and Build 1. Save the Configuration: ○ Click "Save". 2. Build the Job: ○ Click "Build Now" to manually trigger a build. ○ View progress and logs by clicking on the build number. 6. Monitor and Manage Builds 1. Check Build History: ○ View past builds and their statuses. 2. Console Output: ○ Click on a build number to see detailed logs. 3. Build Artifacts: ○ Access any artifacts produced by the build. 7. Advanced Configuration (Optional) 1. Pipeline as Code: 0 For complex workflows, use Jenkins Pipelines. Create a Jenkinsfile in your repository. 2. Plugins: ○ Extend Jenkins functionality with plugins from the Jenkins Plugin Manager. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) 9 N32 nirf 1750 Rank

--- Page 21 ---
Unit: I Course Code/Title:CS3V15/Devops · Gaming: AWS has been serving many gaming studios. Combining Amazon EC2 and S3 services with CloudFront enables gaming websites to deliver high-quality gaming experiences to their customers regardless of location. Use Cases of AWS · Netflix Netflix is an entertainment platform that started in the United States, but eventually, it expanded to many countries and soon became popular. However, once Netflix confronted the scalability problem because of the sudden increase in viewers. That made Netflix choose AWS services. Netflix reports that when it started using AWS services like DynamoDB and Cassandra for its distributed databases, it could handle the data easily. So, scalability is a great advantage of AWS. Netflix has adapted around 100,000 server instances from AWS for computing and storage databases, analytics, recommendation engines, and video transcoding as well. . McDonald's McDonald's is the world's largest fast-food company that serves around 64 million people per day. The growth of this company has gone to another level when it started home deliveries. By utilizing AWS services, McDonald's created a platform that integrates local restaurants with delivery partners such as Uber Eats. Scalability is also a reason for the company to choose AWS services. Moreover, with AWS Microservices Architecture, McDonald's platform can scale 20,000 orders per second and integrate with the global partners easily. · Airbnb Airbnb is an international online marketplace for rental homes. This platform connects people who are looking for rental accommodation with those who want to rent out their houses. Quite soon, Airbnb became unable to handle the constant streaming of data on the website from its customers. That is when it started using Amazon EC2 service and Elastic Load Balancing, which distributes incoming traffic to multiple Amazon EC2 instances. In this way, Airbnb could avoid traffic, and customers could use the online platform without any disruption. • Novartis Novartis is the best example for AWS use cases in healthcare. Novartis is one of the world's largest healthcare companies that provides solutions for patients' well-being. It adapted Amazon EC2 services and built a platform using other services such as Amazon Simple Storage Service, Amazon Elastic Block Store, and four availability zones. Data Analysts of Novartis are taking advantage of the AWS services and still implementing new solutions for the patients. · Expedia Expedia is a worldwide online travel agency that has always focused on the constant development and innovation of its platform to offer an extraordinary user experience for its clients. Since 2010, Expedia has been using AWS services to build a standard deployment model for better infrastructure as AWS offers the best data security through different availability zones. • Samsung If you are using Samsung mobile phones, then you may know about the Samsung app store. For setting up the apps stacked in its store, the company started using AWS services. Using AWS app development services, Samsung wanted to provide its customers with the facility to download the apps anywhere without any network traffic. • NASA NASA (National Aeronautics and Space Administration) has always wondered about creating a library to present people with all its achievements through pictures and videos of space. Later on, it created such platforms, but because it had 10 different NASA centers, it couldn't provide the best experience for viewers. So, all it wanted was to create an easy-access platform for TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 21 N32 nirf 1750 Rank

--- Page 22 ---
Unit: I Course Code/Title:CS3V15/Devops people to search for and view images and videos. Then, NASA started adopting many services from AWS to solve this problem, which included Amazon Elastic Compute Cloud, Elastic Load Balancing, Amazon Simple Storage Service, Amazon Simple Queue Service, etc. Among these, Amazon S3 helped the company store all the incoming data such as photos, videos, and audio files without any hassle. • Facebook Facebook, without a doubt, is a widespread social media platform. To build a scalable application, Facebook used services such as Amazon Elastic Compute Cloud, Amazon Simple Storage Service, Amazon Relational Database Service, Amazon SimpleDB, Amazon CloudFront, Amazon Simple Queue Service, etc. Amazon RDS helps the platform to make it easy to set up, operate, and scale the database in the cloud. Various Services offered by AWS · Amazon EC2 (Elastic Cloud computing) · Amazon RDS (Relational Database Services) · Bonus Service: Amazon Connect · Amazon S3 (Simple Storage Service) · Amazon Lambda · Amazon Cognito · Amazon Glacier · Amazon SNS (Simple Notification Service) · Bonus Service: Amazon Lex · Amazon Lightsail · Amazon VPC (Virtual Private Cloud) · Amazon Kinesis · Amazon Inspector · Amazon Auto-scaling · Amazon IAM (Identity and Access Management) · Dynamo DB · Amazon SQS (Simple Queue Service) · Amazon ElastiCache · Amazon Chime · AWS Athena · Code Catalyst · Web Application Firewall · AWS Amplify · AWS Rekognition · AWS QuickSight · AWS CloudFormation · AWS Management Console The Important Cloud Services according to various categories that are provided by AWS are given below : 1. Compute • Amazon EC2: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It allows organizations to obtain and configure virtual compute capacity in the cloud. You can select from a variety of operating systems and resource configurations like memory, CPU, and storage that are required for your TECHNOLOGY (Autonomous) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY 22 N33 narf 1750 Rank

--- Page 53 ---
Course Code/Title:CS3V15/Devops Unit: II <execution> <phase>test</phase> <goals> <goal>run</goal> </goals> <configuration> <tasks> <echo>Using env.test.properties</echo> <copy file="src/main/resources/env.test.properties" tofile="${project.build.outputDirectory}/env.properties"/> </tasks> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> Now open the command console, go to the folder containing pom.xml and execute the following mvn command. Pass the profile name as argument using -P option. C:\MVN\project>mvn test -Ptest Maven will start processing and displaying the result of test build profile. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 12 N32 nirf 1750 Rank

--- Page 54 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Maven Settings Open Maven settings.xml file available in %USER_HOME%/.m2 directory where %USER_HOME% represents the user home directory. If settings.xml file is not there, then create a new one. Add test profile as an active profile using active Profiles node as shown below in example. <settings xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"> <mirrors> <mirror> <id>maven.dev.snaponglobal.com</id> <name>Internal Artifactory Maven repository</name> <url>http://repo1.maven.org/maven2/</url> <mirrorOf> *< /mirrorOf> /mirror </mirrors> <activeProfiles> <activeProfile>test</activeProfile> </activeProfiles> </settings> Now open command console, go to the folder containing pom.xml and execute the following mvn command. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 13 N33 nirf 1750 Rank

--- Page 39 ---
Course Code/Title:CS3V15/Devops Unit: I GITHUB GitHub is an increasingly popular programming resource used for code sharing. It's a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in October 2020, it is the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 56 million total developers. GitHub is a Git repository hosting service that provides a web-based graphical interface. It is the world's largest coding community. Putting a code or a project into GitHub brings it increased, widespread exposure. Programmers can find source codes in many different languages and use the command-line interface, Git, to make and keep track of any changes. GitHub helps every team member work together on a project from any location while facilitating collaboration. You can also review previous versions created at an earlier point in time. GitHub's Features? 1. Easy Project Management: GitHub is a place where project managers and developers come together to coordinate, track, and update their work so that projects are transparent and stay on schedule. 2. Increased Safety With Packages Packages can be published privately, within the team, or publicly to the open-source community. The packages can be used or reused by downloading them from GitHub. 3. Effective Team Management GitHub helps all the team members stay on the same page and organized. Moderation tools like Issue and Pull Request Locking help the team to focus on the code. 4. Improved Code Writing Pull requests help the organizations to review, develop, and propose new code. Team members can discuss any implementations and proposals through these before changing the source code. 5. Increased Code Safety GitHub uses dedicated tools to identify and analyze vulnerabilities to the code that other tools tend to miss. Development teams everywhere work together to secure the software supply chain, from start to finish. 6. Easy Code Hosting All the code and documentation are in one place. There are millions of repositories on GitHub, and each repository has its own tools to help you host and release code. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 39 N32 nirf 1750 Rank

--- Page 40 ---
Course Code/Title:CS3V15/Devops Unit: I HOSTING SERVICE FOR GIT REPOSITORY When it comes to hosting Git repositories, various platforms provide a robust infrastructure for collaborative development, version control, and project management. Choosing the right hosting service depends on factors like ease of use, collaboration features, and integration capabilities. Here are some popular Git hosting services widely utilized in the software development community: 1. GitHub: GitHub stands out as one of the most prevalent Git hosting platforms, offering a user- friendly interface, powerful collaboration features, and seamless integration with various tools. It serves as an ideal choice for open-source projects, private repositories, and team collaboration. 2. GitLab: GitLab is a comprehensive web-based Git repository manager that not only provides source code management but also includes features like continuous integration. It caters to both cloud-based and self-hosted solutions, giving users flexibility in hosting their repositories. 3. Bitbucket: Owned by Atlassian, Bitbucket is another popular Git repository hosting service. Supporting both Git and Mercurial repositories, it offers features like code collaboration, issue tracking, and continuous integration. Bitbucket is often preferred by teams using other Atlassian tools such as Jira and Confluence. 4. GitKraken Glo Boards: GitKraken Glo Boards is an integrated task and issue tracking service linked with GitKraken, a Git client. This platform allows teams to manage tasks directly associated with their Git repositories and provides a visual approach to monitoring project progress. 5. SourceForge: SourceForge, with a long history, hosts open-source software projects and offers version control, bug tracking, and project management tools. While not as prominent as some other options, it remains a viable choice for numerous projects. 6. AWS CodeCommit: As part of Amazon Web Services (AWS), AWS CodeCommit is a fully managed source control service. It seamlessly integrates with other AWS services and provides a secure and scalable environment for hosting Git repositories. Selecting the most suitable Git hosting service depends on your team's requirements, project size, and preferences for cloud-based or self-hosted solutions. Each platform has its strengths, catering to specific use cases within the software development landscape. Difference between GIT and GITHUB Git and GitHub are related concepts but serve different purposes in the context of version control and collaborative software development. Git: Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows. GitHub: GitHub is a web-based Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 40 N32 nirf 1750 Rank

--- Page 57 ---
Course Code/Title:CS3V15/Devops · Go to the File option · In the drop-down menu, select New · Select the Project option Unit: II If you want to create a Java project, you can select the "Java Project" option. Since we are not creating a Java project specifically, we have chosen the "Project" option. eclipse-workspace - Eclipse File Edit Source Refactor Navigate Search Project Run Window Help New Open File ... G) Open Projects from File System ... Shift+Alt+N Close Close All Shift + Cb1+W Save Save As. Save All Shift+Ctrl+S Revert Move .:. Rename 2 Refresh ES Convert Line Delimiters To Print Import ... Export ... Ctrl+P de java Project ui Project ... El Package G Class Interface G Enum @ Annotation 69 Source Folder 18 Java Working Set Folder File Untitled Text File Task JUnit Test Case Example ... Other ... Ctrl+N The dialog box that appears on the screen will display different types of projects. · Select the Maven Project option . Click on Next New Project + Select a wizard Create a Maven Project Wizards: type filter text irroject Gradle Java 2º Java Project #Java Project from Existing Ant Buildfile - @Maven Lu Check out Maven Projects from SCM M& Maven Module Maven Project Examples ? < Back Next > X Cancel Finish A dialog box will appear. Select the default workspace. . Click on "Next" Several Group IDs, Artifact IDs, and Versions will then appear. · Select a plugin there and click on "Next" CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 16 N33 narf 1750 Rank

--- Page 58 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Select an Archetype M Catalog: All Catalogs Configure .. Biter: × Group Id Artifact id Version org.apache.maven.archetypes maven-archetype-archetype 1.0 org.apache.maven.archetypes maven-archetype-|2ee-simple 1.0 org.apache.maven archetypes maven-archetype-plugin 1.2 org.apache.maven.archetypes maven-archetype-plugin-site 1.1 org.apache.maven.archetypes maven-archetype-portlet 1.0.1 org.apache.maven.archetypes maven-archetype-profiles 1.0-alpha-4 org.apache.maven.archetypes maven-archetype-quickstart 1.1 org.apache.maven.archetypes maven-archetype-site 11 org anacha mauan archabenes masan.archchina.cito.cimnia 11 An archetype which contains a sample Maven project. Show the last version of Archetype only Include snapshot archetypes Add Archetype ... > Advanced ? < Back Next > Cancel Finish In the next dialog box that appears, you'll complete the following steps: · Enter the Group ID “com.xyz" · Enter the Artifact ID "mavenproject" · The version will appear on the screen These items can all be modified at a later time if needed. · Click on "Finish" CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 17 N33 nirf 1750 Rank

--- Page 71 ---
Unit: II Course Code/Title:CS3V15/Devops build.gradle &3 I 1 /* 2 * This file was generated by the Gradle 'init' task. 3 * 4 * This generated file contains a sample Java Library project to get you started. 5 * For more details take a look at the Java Libraries chapter in the Gradle 6 * User Manual available at https://docs.gradle.org/6.0.1/userguide/java_library_plugin.html 7 */ 8 9 plugins { 10 // Apply the java-library plugin to add support for Java Library 11 id 'java-library' 12 } 13 14 repositories { 15 // Use jcenter for resolving dependencies. 16 // You can declare any Maven/Ivy/file repository here. 17 jcenter() 18 } 19 20 dependencies { 21 // This dependency is exported to consumers, that is to say found on their compile classpath. 22 api 'org. apache. commons : commons-math3: 3.6.1' 23 24 // This dependency is used internally, and not exposed to consumers on their own compile classpath. 25 implementation 'com. google. guava: guava: 28.0-jre' 26 27 // Use JUnit test framework 30 28 testImplementation 'junit : junit: 4.12' 29 } 30 The build.gradle file contains three default sections. They are as follows: o plugins: In this section, we can apply the java-library plugin to add support for java library. o Repositories: In this section, we can declare internal and external repository for resolving dependencies. We can declare the different types of repository supported by Gradle like Maven, Ant, and Ivy. · Dependencies: In this section, we can declare dependencies that are necessary for a particular subject. Additionally, we can declare other project-related modules like a task in this file. Display the Information of the Gradle project To understand the structure, dependencies and debugging problems of a build, Gradle provides many built-in features that display information on a project. Following are some basic commands to display the information of the project: Listing projects In Gradle, all the sub-projects of a project in the workspace can be listed in a hierarchy. To do so, run the below command from the root directory of the project. 1. gradle -q projects CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 72 ---
Course Code/Title:CS3V15/Devops Unit: II To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> C:\Users \HiMaNshU\eclipse-workspace >gradle -q projects Root project Root project 'eclipse-workspace' No sub-projects To see a list of the tasks of a project, run gradle <project-path> : tasks For example, try running gradle :tasks C:\Users\HiMaNshU\eclipse-workspace> Listing Tasks Gradle allows us to list all the essential tasks of the project. To list the task, run the below command: 1. gradle -q tasks Output: C:\Users\HiMaNshUleclipse-workspace>gradle -q tasks Tasks runnable from root project Build Setup tasks CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 31 N33 nirf 1750 Rank

--- Page 77 ---
Unit: III Course Code/Title:CS3V15/Devops 5. Customize Jenkins: ○ Install suggested plugins or select the required plugins manually. ○ Create the first admin user. Configuration 1. Global Configuration: ○ Navigate to Manage Jenkins > Configure System. ○ Set up global environment variables, email notifications, and other system-wide settings. 2. Security Configuration: ○ Configure security settings under Manage Jenkins > Configure Global Security. o Use the built-in user database or integrate with external authentication mechanisms like LDAP. 3. Node Configuration: ○ Jenkins can distribute build loads to multiple nodes. ○ Configure nodes by navigating to Manage Jenkins > Manage Nodes and Clouds. 4. Job Configuration: ○ Create new jobs by clicking New Item on the Jenkins dashboard. o Configure job-specific settings such as source code management, build triggers, and post-build actions. 5. Pipeline Configuration: ○ Jenkins supports pipeline as code using Jenkinsfile. ○ Define your pipeline stages in a Jenkinsfile placed in the root of your project repository. 6. Plugins: ○ Enhance Jenkins functionality by installing plugins from the Manage Jenkins > Manage Plugins section. 7. Backup and Restore: ○ Regularly back up the jenkins_home directory, which contains Jenkins configurations, job configurations, and build history. ○ Use plugins like ThinBackup for automated backups. Ref: https://youtube.com/playlist?list=PL6flErFppaj35spJjPy41-IruDjw2kRV-&si=Vzd9AAMv53xwWB S Ref: https://bit.ly/youtube_jenkins (click here) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 78 ---
Course Code/Title:CS3V15/Devops Unit: III Jenkins Architecture Overview Jenkins is a robust automation server widely used for continuous integration and continuous delivery (CI/CD) processes. Here's a detailed breakdown of the Jenkins architecture and its core components. Core Components of Jenkins Architecture 1. Jenkins Master: ○ Role: The Jenkins master is the central control unit. 0 Functions: Scheduling Jobs: Assigns build jobs to the appropriate agents. Monitoring Agents: Keeps track of the state and health of agents. I Job Execution: Executes build jobs directly if no agents are available. I User Interface: Provides a web-based interface for users to configure jobs, view build results, and manage the Jenkins environment. 2. Build Agents (Nodes): ○ Role: Agents are responsible for executing the build jobs assigned by the master. ○ Types: Agents can run on different operating systems and hardware configurations. ○ Configuration: Static Agents: Permanently assigned to the Jenkins master. . Dynamic Agents: Created and destroyed as needed, often used in cloud environments. ○ Communication: Agents communicate with the master using the Jenkins Remoting protocol. 3. Job Configuration: ○ Types of Jobs: I Freestyle Projects: Basic job type with simple configurations. Pipeline Jobs: Scripted or declarative pipelines that define the entire build process as code. I Multi-Configuration (Matrix) Projects: Allows testing across different environments and configurations. ○ Components: I Source Code Management: Integration with version control systems like Git, SVN, etc. . Build Triggers: Conditions that start the job, such as code commits, scheduled times, or manual triggers. Build Steps: Actions performed during the build, such as compiling code, running tests, and packaging artifacts. . Post-Build Actions: Steps executed after the build, like deploying artifacts, sending notifications, or archiving results. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 9 ---
Course Code/Title:CS3V15/Devops Unit: I 9. Slack Slack is a crucial tool for businesses and organizations worldwide. Its significance lies in facilitating seamless communication and collaboration among teams, whether working in the same office or remotely. Slack's real-time messaging, file sharing, and integration capabilities streamline workflow, enhance productivity and keep teams connected across different time zones and locations. As the work landscape evolves, with more companies embracing hybrid and remote work models, Slack is a vital hub for quick decision-making, project coordination, and knowledge sharing. With an ever- expanding ecosystem of integrations and features, Slack remains at the forefront of modern workplace communication, making it essential for businesses to stay agile, efficient, and competitive. 10. AWS Cloud Computing and Storage in DevOps AWS (Amazon Web Services) Cloud Computing and Storage are crucial in DevOps because they provide scalable, flexible, and cost-effective infrastructure for DevOps practices. AWS offers many services, including compute resources, databases, container orchestration, and serverless computing, which align perfectly with modern software development and deployment demands. Organizations adopt DevOps to accelerate software delivery. AWS provides the foundation for rapidly deploying and scaling applications, supporting continuous integration and continuous delivery (CI/CD) pipelines, and automating infrastructure provisioning through tools like AWS CloudFormation. Furthermore, AWS's storage solutions enable efficient data management, backup, and recovery, ensuring the resilience and reliability required for DevOps operations. As cloud technology evolves, AWS remains at the forefront, enabling DevOps teams to focus on innovation and efficiency. 11. Azure Cloud Computing and Storage in DevOps Azure Cloud Computing and Storage will be pivotal in DevOps practices in 2024 and beyond. Azure offers a comprehensive cloud ecosystem that enables organizations to scale their infrastructure, deploy applications, and store data efficiently. Azure provides essential services for continuous integration and continuous deployment (CI/CD), automation, monitoring, and security. Its cloud computing capabilities facilitate the provisioning of resources on demand, ensuring that development and testing environments are readily available. Azure's storage solutions, including Azure Blob Storage, Azure Files, and Azure SQL Database, enable secure data storage and retrieval, supporting the data-driven aspects of DevOps. Besides, Azure's integration with DevOps tools like Azure DevOps Services streamlines the software development lifecycle, enhancing collaboration and automation. 12. GCP Cloud Computing and Storage in DevOps Google Cloud Platform (GCP) offers robust cloud computing and storage solutions. GCP provides a scalable, reliable, and highly available infrastructure essential for modern DevOps practices. With its comprehensive set of services, including Google Compute Engine, Google Kubernetes Engine, Cloud Storage, and BigQuery, GCP empowers DevOps teams to build, deploy, and manage applications easily. Its emphasis on automation, infrastructure as code, and container orchestration aligns seamlessly with DevOps principles. Moreover, GCP's cutting-edge technologies, such as AI and machine learning capabilities, provide DevOps practitioners with advanced tools for monitoring, analytics, and automation, making it a powerful choice for organizations seeking to optimize their software development and delivery processes. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 10 ---
Course Code/Title:CS3V15/Devops Unit: I 13. Monitoring, Alerting, and Incident Response Tools: SignalFx Monitoring, alerting, and incident response tools like SignalFx are pivotal in DevOps and software development. As software systems become complex and distributed, the need for real-time visibility into performance and the ability to respond swiftly to incidents is significant. SignalFx excels in this regard by providing advanced monitoring and observability solutions that enable organizations to detect anomalies, trace issues across microservices proactively, and set up intelligent alerts. As applications scale, cloud-native architectures become the norm, and user expectations for reliability grow, SignalFx's capabilities are crucial. It empowers DevOps teams to ensure high availability, optimize resource utilization, and maintain a seamless user experience by identifying and addressing performance issues before they impact end-users. It is one of the most essential tools for modern software operations. 14. Appdynamics AppDynamics, a leading application performance management and monitoring platform, remains critically important as it ensures the optimal performance of modern digital businesses. As organizations rely on complex and distributed software systems, proactively monitoring, troubleshooting, and optimizing these applications becomes essential. AppDynamics provides real-time visibility into application performance, allowing businesses to swiftly identify bottlenecks, latency issues, and errors. With the ever-growing complexity of applications, the importance of AppDynamics lies in its ability to empower organizations to deliver exceptional user experiences, maintain application reliability, and swiftly respond to performance issues, thereby ensuring the continued success and competitiveness of digital businesses. 15. Raygun It is a crucial tool in software development and DevOps because it ensures application reliability and performance. Raygun is an application monitoring and error-tracking platform that empowers development teams to identify, diagnose, and resolve real-time issues. With software systems growing in complexity and the increased demand for seamless user experiences, Raygun's importance lies in providing actionable insights into application errors and performance bottlenecks. It enables organizations to proactively address issues, reduce downtime, and enhance user satisfaction, leading to higher software quality and improved customer experiences. Software is central to businesses across industries. Raygun's role in maintaining application health and facilitating rapid issue resolution makes it a fundamental tool for DevOps professionals and software developers. 16. Splunk Cloud Splunk Cloud helps organizations gain critical insights from the ever-expanding volume of data generated in today's digital landscape. As businesses increasingly rely on data- driven decision-making, Splunk Cloud stands out as a robust and scalable platform for monitoring, searching, analyzing, and visualizing machine-generated data. Its importance lies in providing real-time visibility into the health and performance of complex systems, applications, and infrastructures, enabling rapid incident detection and response. As cybersecurity threats evolve, Splunk Cloud's advanced security analytics and threat detection capabilities remain indispensable for safeguarding against cyberattacks and ensuring data integrity. In a world where data is a strategic asset, Splunk Cloud's role in harnessing the power of data for operational excellence and security cannot be overstated. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 10 N32 nirf 1750 Rank

--- Page 89 ---
Course Code/Title:CS3V15/Devops Unit: III Key Features: · Source Code Management: Configures Jenkins to use Git repositories as the source code for jobs. Example Demo Step 1 : Create a simple Java program. I have created a folder named MyJavaPrograms and inside it created a simple Java program as follows - e.g., SSH test.java public class test { public static void main(String args[I) { for(int i=1;i <= 5;1++) System.out.printin("Welcome Anuradha"); } Plugin," Step 2 : Open the command prompt, switch to that folder and execute the above Java program. It is illustrated by following screenshot. 1 C:\Windows\System32\cmd.e X + Microsoft Windows [Version 10.0.22621.2134] (c) Microsoft Corporation. All rights reserved. 0 X before E:\MyJavaPrograms>javac test. java E: \MyJavaPrograms>java test Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha E: \MyJavaPrograms> Step 3 : Now we will create a Git repository and push this repository on GitHub. First of all we will initialise the Git repository my using git init command. CHENNAI INSTITUTE TECHNOLOGY 14 - N32 nirf 1750 Rank CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt)

--- Page 90 ---
Unit: III Course Code/Title:CS3V15/Devops C:\Windows\System32\cmd.e x X E:\MyJavaPrograms>git init Initialized empty Git repository in E:/MyJavaPrograms/.git/ E:\MyJavaPrograms>git status On branch master No commits yet Untracked files: (use "git add <file> ... " to include in what will be committed) test.class test. java nothing added to commit but untracked files present (use "git add" to track) E:\MyJavaPrograms> then add the java and class files to git repository - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git add . E:\MyJavaPrograms>git status On branch master No commits yet Changes to be committed: (use "git rm -- cached <file> ... " to unstage) new file: test.class new file: test. java E: \MyJavaPrograms> Now we will commit the changes - C:\Windows\System32\cmd.e X E:\MyJavaPrograms>git commit -m "First Commit for Java program" [master (root-commit) 4cb04db] First Commit for Java program 2 files changed, 6 insertions(+) create mode 100644 test.class create mode 100644 test. java E:\MyJavaPrograms> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) - INSTITUTE . TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 85 ---
Course Code/Title:CS3V15/Devops Unit: III By following these steps, you can set up a Jenkins job to automate builds, integrate with your source code management system, and perform various actions based on build results. Introduction to Jenkins Plugins What Are Jenkins Plugins? Jenkins plugins are extensions that add extra functionality to Jenkins. They allow you to customize and extend Jenkins to better fit your specific needs. Plugins can provide integrations with other tools, add new features, and improve existing functionalities. Why Use Plugins? . Extend Functionality: Add features not available in the core Jenkins. . Integrate Tools: Seamlessly integrate with other tools and platforms (e.g., GitHub, Docker). . Improve Productivity: Automate more tasks and streamline your workflow. · Customize UI: Tailor the Jenkins interface to better suit your preferences and needs. How to Manage Plugins 1. Accessing the Plugin Manager 1. Go to Jenkins Dashboard: ○ Open your Jenkins dashboard. 2. Navigate to Plugin Manager: ○ Click on "Manage Jenkins". ○ Select "Manage Plugins". 2. Installing Plugins 1. Available Plugins: ○ Go to the "Available" tab to see a list of plugins that can be installed. 2. Search for Plugins: ○ Use the search bar to find specific plugins. 3. Select Plugins: ○ Check the box next to the plugins you want to install. 4. Install Plugins: ○ Click "Install without restart" or "Download now and install after restart". 3. Updating Plugins CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 narf 10 1750 Rank

--- Page 86 ---
Course Code/Title:CS3V15/Devops Unit: III 1. Go to Updates: ○ Navigate to the "Updates" tab to see plugins with available updates. 2. Select Plugins: ○ Check the box next to the plugins you want to update. 3. Update Plugins: ○ Click "Download now and install after restart". 4. Managing Installed Plugins 1. Installed Plugins: ○ Go to the "Installed" tab to see all installed plugins. 2. Uninstall Plugins: ○ Click the "Uninstall" button next to the plugin you want to remove. 3. Check Plugin Versions: ○ View the current version of each installed plugin. Adding Plugins to Jenkins Plugins are a crucial part of Jenkins, allowing you to extend its functionality to suit your project's specific needs. Whether you're integrating Jenkins with other tools, customizing your job configuration, or adding new build steps, plugins provide the flexibility you need. Here's a detailed guide on how to add plugins to Jenkins. 1. Why Plugins Are Important - Extending Jenkins Functionality: Plugins allow Jenkins to integrate with various tools and technologies, such as Git, Maven, Docker, and more. - Customizing Jobs: With plugins, you can add specific steps, triggers, and post-build actions to your jobs, making Jenkins adaptable to any workflow. - Automation: Plugins help automate processes like code quality checks, notifications, and deployments. 2. Accessing the Plugin Manager 1. Navigate to Jenkins Dashboard: Open your Jenkins dashboard in a web browser (e.g., http://localhost:8080). CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 175º Rank

--- Page 109 ---
Course Code/Title:CS3V15/Devops Unit: III Note that the git repository and the files from that repository are copied from the workspace of App1 to App2. Also note that test.class and test.java files are copied to the workspace of App2. Started by user Anuradha R Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\. jenkins\workspace\App2 Copied 16 artifacts from "Appl" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621189248227895.bat C:\ProgramData\Jenkins\.jenkins\workspace\App2>dir Volume in drive C is OS Volume Serial Number is C6C2-8A9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 .git 474 test. class 144 test.java 2 File(s) 618 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ Finished: SUCCESS 5. Extended Choice Parameter Overview: The Extended Choice Parameter Plugin provides advanced parameter types for Jenkins jobs, such as multi-select lists, checkboxes, and more complex formats. Key Features: · Parameter Types: Includes multi-select, checkboxes, and other advanced input options. . Dynamic Choices: Allows dynamic generation of choices from scripts or external sources. Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Extended Choice Parameter Plugin," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add an Extended Choice Parameter. ○ Configure the parameter type, choices, and other settings. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 110 ---
Unit: III Course Code/Title:CS3V15/Devops Usage: · Useful for scenarios where complex user input is needed or where multiple selection options are required. Configuring Jenkins to Work with Java, Git, and Maven 1. Configuring Jenkins to Work with Java Overview: Jenkins requires Java to run. Configuring Java in Jenkins involves specifying the Java Development Kit (JDK) installations Jenkins should use. Steps: 1. Install Java Development Kit (JDK): ○ Ensure that JDK is installed on your system. You can download it from the Oracle website or use OpenJDK. 2. Configure JDK in Jenkins: ○ Open Jenkins and go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to JDK section and click Add JDK. ○ Enter a name for the JDK installation (e.g., JDK 11). Check Install automatically to let Jenkins download and install the JDK, or specify the path to an existing JDK installation. ○ If specifying the path manually, provide the JAVA_HOME directory. 3. Example Configuration: ○ Name: JDK 11 ○ JAVA_HOME: /usr/lib/jvm/java-11-openjdk 4. Verify JDK Configuration: You can verify the JDK configuration by creating a simple Jenkins job and adding a build step that prints the Java version using: bash Copy code java -version 2. Configuring Jenkins to Work with Git Overview: The Git Plugin integrates Jenkins with Git repositories, enabling Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 35 1750 Rank

--- Page 17 ---
Course Code/Title:CS3V15/Devops Unit: I that failed in a test suite gets simpler. Also, we can schedule the execution of the test cases at predefined times. After testing, the code is continuously integrated with the existing code. 4) Continuous Monitoring Monitoring is a phase that involves all the operational factors of the entire DevOps process, where important information about the use of the software is recorded and carefully processed to find out trends and identify problem areas. Usually, the monitoring is integrated within the operational capabilities of the software application. 5) Continuous Feedback The application development is consistently improved by analyzing the results from the operations of the software. This is carried out by placing the critical phase of constant feedback between the operations and the development of the next version of the current software application. The continuity is the essential factor in the DevOps as it removes the unnecessary steps which are required to take a software application from development, using it to find out its issues and then producing a better version. It kills the efficiency that may be possible with the app and reduce the number of interested customers. 6) Continuous Deployment In this phase, the code is deployed to the production servers. Also, it is essential to ensure that the code is correctly used on all the servers. The new code is deployed continuously, and configuration management tools play an essential role in executing tasks frequently and quickly. Here are some popular tools which are used in this phase, such as Chef, Puppet, Ansible, and SaltStack. Containerization tools are also playing an essential role in the deployment phase. Vagrant and Docker are popular tools that are used for this purpose. These tools help to produce consistency across development, staging, testing, and production environment. They also help in scaling up and scaling down instances softly. Containerization tools help to maintain consistency across the environments where the application is tested, developed, and deployed. There is no chance of errors or failure in the production environment as they package and replicate the same dependencies and packages used in the testing, development, and staging environment. It makes the application easy to run on different computers. 7) Continuous Operations All DevOps operations are based on the continuity with complete automation of the release process and allow the organization to accelerate the overall time to market continuingly. It is clear from the discussion that continuity is the critical factor in the DevOps in removing steps that often distract the development, take it longer to detect issues and produce a better version of the product after several months. With DevOps, we can make any software product more efficient and increase the overall count of interested customers in your product. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 17 N32 nirf 1750 Rank

--- Page 18 ---
Course Code/Title:CS3V15/Devops Unit: I AWS AWS stands for Amazon Web Services, It is an expanded cloud computing platform provided by Amazon Company. AWS provides a wide range of services with a pay-as-per-use pricing model over the Internet such as Storage, Computing power, Databases, Machine Learning services, and much more. AWS facilitates for both businesses and individual users with effectively hosting the applications, storing the data securely, and making use of a wide variety of tools and services improving management flexibility for IT resources. Advantages & Features of AWS: 1. Cost savings: One of the biggest benefits of AWS is that it can help businesses save money. As mentioned previously, businesses can avoid the high upfront costs of traditional infrastructure with AWS and pay only for the resources they use. Traditionally, businesses had to invest in hardware and software upfront, which often led to overspending. Let's look at this for example - if a business needs to run a website that gets 1000 visitors per day, they would need to purchase and maintain enough servers to support this traffic. With AWS, the business only pays for the compute resources they use when someone visits their website. This can result in significant cost savings. 2. Flexibility: Another key benefit of AWS is its flexibility. Businesses are able to customize their virtual environment - whether the operating system, database, programming language, or something else - to meet their specific needs. Especially in today's climate, the migration process to the cloud should be as frictionless as possible - and AWS makes that possible. Regardless of your use case or industry, AWS can be tailored to fit your needs, whether you're looking for a single cloud- hosted application or an entire suite of integrated solutions. 3. Reliability: AWS is known for being reliable, with an uptime of 99.9%. This makes it a great platform for mission-critical applications that need to be available 24/7. AWS also offers the ability to deploy resources across multiple availability zones for even greater reliability. The cloud platform also has a number of features that make it easier to ensure reliability, such as autoscaling and auto-healing. Autoscaling allows businesses to automatically scale their resources up or down based on demand, while auto-healing enables them to quickly identify and replace any faulty components. 4. Security: Businesses can take advantage of advanced security features, such as identity and access management, to help protect their data. Their tough infrastructure with an end-to-end approach is designed to withstand attacks and AWS provides customers with tools to help them monitor and respond to threats. When it comes to storage, Amazon S3 provides customers with a secure and reliable way to store and access data. The service is designed to be highly scalable and resilient, with built-in redundancy. Fine-grain identity and access controls can be applied to S3 buckets and objects, giving customers control over who has access to their data. Security tasks can be automated with AWS CloudFormation, making it easier for businesses to manage their security policies. And, you can rest easy knowing that AWS takes privacy seriously, with comprehensive customer data protection and compliance measures. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 18 N32 nirf 1750 Rank

--- Page 115 ---
Unit: IV Course Code/Title:CS3V15/Devops UNIT IV CONFIGURATION MANAGEMENT USING ANSIBLE Ansible Introduction, Installation, Ansible master/slave configuration, YAML basics, Ansible modules, Ansible Inventory files, Ansible playbooks, Ansible Roles, adhoc commands in ansible. Ansible A SSH ANSIBLE TARGET NODE HOSTS PLAYBOOK Ansible is a powerful DevOps tool designed for automating tasks on remote servers or nodes. In simple terms, it allows you to automate commands and functions on multiple remote machines from a central 'master' node. To illustrate its usefulness, consider a scenario where you need to reboot dozens or even hundreds of remote hosts. You could manually SSH into each one and initiate the reboot, or you can use Ansible to streamline the process, making it efficient and offering a wide range of additional functionalities. In essence, Ansible operates much like the second method mentioned, using SSH (Secure Shell), a secure communication protocol, to control remote nodes in a secure and optimized manner. Ansible Components Ansible Core Components Control node Ansible Inventory Managed node 1 Managed node 2 Managed node 3 1. Control Node - · The central or main node where Ansible is installed. · Used to trigger commands like ansible and ansible-inventory on other nodes. · Acts as the orchestrator for Ansible operations. • TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 1 N33 nirf 1750 Rank

--- Page 116 ---
Course Code/Title:CS3V15/Devops Unit: IV 2. Manage Node- · A remote or slave node where tasks are executed or controlled by Ansible. . These are the servers or devices you want to manage or automate. 3. Inventory - · A list of managed node IPs and configurations. · Logically organized, typically using file formats like YAML or INI. · Created on the control node to describe the deployment of hosts to Ansible. Ansible Additional Components Ansible Architecture CMDB Users Public / Private Cloud Hosts ... ... Inventory .. Modules 000 Ansible Playbook API Plugins Networking www.educba.com 1. Ad-Hoc Commands - . These are one-off commands that you can execute using the ansible command. · Useful for quick tasks or tests on remote nodes. 2. Plugins - · Plugins are pieces of code that extend Ansible's core functionality. · Ansible uses a plugin architecture for flexibility and expandability. · Examples include connectivity plugins for establishing connections and cache plugins. · Learn more about Ansible plugins. 3. Module- · Modules are built-in functions that can be used to perform various tasks. . They eliminate the need to write custom code for common operations. · Examples include modules for package management (apt, yum), service management, and more. CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) CHENNAI INSTITUTE . TECHNOLOGY 2 N33 nirf 1750 Rank

--- Page 25 ---
Course Code/Title:CS3V15/Devops Unit: I · backups · automated multi-region replication in-memory caching · data export tools. Global Infrastructure of AWS · AWS is a cloud computing platform which is globally available. . Global infrastructure is a region around the world in which AWS is based. Global infrastructure is a bunch of high-level IT services which is shown below: AWS is available in 19 regions, and 57 availability zones in December 2018 and 5 more regions 15 more availability zones for 2019. The following are the components that make up the AWS infrastructure: o Availability Zones o Region o Edge locations o Regional Edge Caches Compnents of Global Infrastructure Availability zones Region Edge Locations Regional Edge Caches Availability zone as a Data Center An availability zone is a facility that can be somewhere in a country or in a city. Inside this facility, i.e., Data Centre, we can have multiple servers, switches, load balancing, firewalls. The things which interact with the cloud sits inside the data centers. o An availability zone can be a several data centers, but if they are close together, they are counted as 1 availability zone. Region o A region is a geographical area. Each region consists of 2 more availability zones. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 26 ---
Course Code/Title:CS3V15/Devops Unit: I o A region is a collection of data centers which are completely isolated from other regions. · A region consists of more than two availability zones connected to each other through links. AZ-a AZ-b AZ-c Availability zones are connected through redundant and isolated metro fibers. Edge Locations o Edge locations are the endpoints for AWS used for caching content. o Edge locations consist of CloudFront, Amazon's Content Delivery Network (CDN). o Edge locations are more than regions. Currently, there are over 150 edge locations. Edge location is not a region but a small location that AWS have. It is used for caching the content. o Edge locations are mainly located in most of the major cities to distribute the content to end users with reduced latency. o For example, some user accesses your website from Singapore; then this request would be redirected to the edge location closest to Singapore where cached data can be read. Regional Edge Cache o AWS announced a new type of edge location in November 2016, known as a Regional Edge Cache. o Regional Edge cache lies between CloudFront Origin servers and the edge locations. o A regional edge cache has a large cache than an individual edge location. o Data is removed from the cache at the edge location while the data is retained at the Regional Edge Caches. o When the user requests the data, then data is no longer available at the edge location. Therefore, the edge location retrieves the cached data from the Regional edge cache instead of the Origin servers that have high latency. What Is Cloud Computing? Cloud computing is the use of hardware and software components in an off-premises location to deliver a service to a network. Users can access files and applications from any device that can access the internet. Some features and capabilities include: · Cloud providers can pull the computing resources to provide services to multiple customers with the help of a multi-tenant model · Cloud computing proves an on-demand self-service, which helps administrators monitor performance CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 26 N32 nirf 1750 Rank -

--- Page 93 ---
Course Code/Title:CS3V15/Devops Unit: III And configure the Jenkins job as follows - General Description In this job we have used GitHub plugin Plain text Preview Discard old builds ? GitHub project This project is parameterized ? Throttle builds Execute concurrent builds if necessary Advanced V Source Code Management None Git ? Repositories ? Repository URL https://github.com/AnurdhaP/MyJavaPrograms.gil Please enter Git repository. Credentials ? - none - Add Advanced CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 18 N3 nirf 1750 Rank

--- Page 94 ---
Course Code/Title:CS3V15/Devops Unit: III Build Triggers Trigger builds remotely (e.g. from scripts) ? Build after other projects are built ? Build periodically ? GitHub hook trigger for GITSem polling ? Poll SCM ? Schedule ? A Do you really mean "every minute" when you say "" "" " ""? Perhaps you meant "H" "" "" to poll once per hour Would last have run at Wednesday, 30 August, 2023 at 4:11:02 pm india Standard Time; would'nest run at Wednesday, 30.A Ignore post-commit hooks ? Build Steps Execute Windows batch command ? Command See the list of available environment variables javas test.java java test. java ----- Advanced V Add build sten Save this configuration. Step 5 : Now if we build this job the output that we see is as follows on Console Output. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 19 N3 nirf 1750 Rank

--- Page 43 ---
Course Code/Title:CS3V15/Devops Unit: II o Generates source code (if auto-generated code is used) o Generates documentation from source code o Compiles source code o Packages compiled code into JAR of ZIP file o Installs the packaged code in local repository, server repository, or central repository 2. Installation of Maven: To install maven on windows, you need to perform following steps: 1. Download maven and extract it 2. Add JAVA_HOME and MAVEN_HOME in environment variable 3. Add maven path in environment variable 4. Verify Maven 1) Download Maven To install maven on windows, you need to download apache maven first. Download Maven latest Maven software from Download latest version of Maven For example: apache-maven-3.1.1-bin.zip 2) Add MAVEN_HOME in environment variable Right click on MyComputer -> properties -> Advanced > Environment variables -> click new button System Settings - Now add MAVEN_HOME in variable name and path of maven in variable value. It must be the home directory of maven i.e. outer directory of bin. For example: E:\apache-maven-3.1.1 3) Add Maven Path in environment variable Click on new tab if path is not set, then set the path of maven. If it is set, edit the path and append the path of maven.Here, we have installed JDK and its path is set by default, so we are going to append the path of maven. The path of maven should be %maven home%/bin. For example, E:\apache-maven- 3.1.1\bin . 4) Verify maven To verify whether maven is installed or not, open the command prompt and write: mvn -version Now it will display the version of maven and jdk including the maven home and java home. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 2 N32 nirf 175º Rank

--- Page 44 ---
Unit: II Course Code/Title:CS3V15/Devops 3. POM FILES: POM is an acronym for Project Object Model. The pom.xml file contains information of project and configuration information for the maven to build the project such as dependencies, build directory, source directory, test source directory, plugin, goals etc. Maven reads the pom.xml file, then executes the goal. Before maven 2, it was named as project.xml file. But, since maven 2 (also in maven 3), it is renamed as pom.xml. Elements of maven pom.xml file For creating the simple pom.xml file, you need to have following elements: Element Description project It is the root element of pom.xml file. modelVersion It is the sub element of project. It specifies the modelVersion. It should be set to 4.0.0. groupId It is the sub element of project. It specifies the id for the project group. artifactId It is the sub element of project. It specifies the id for the artifact (project). An artifact is something that is either produced or used by a project. Examples of artifacts produced by Maven for a project include: JARs, source and binary distributions, and WARs. version It is the sub element of project. It specifies the version of the artifact under given group. Maven pom.xml file with additional elements Here, we are going to add other elements in pom.xml file such as: Element Description packaging defines packaging type such as jar, war etc. name defines name of the maven project. url defines url of the project. dependencies defines dependencies for this project. dependency defines a dependency. It is used inside dependencies. scope defines scope for this maven project. It can be compile, provided, runtime, test and system. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 3 N32 narf 1750 Rank -

--- Page 51 ---
Course Code/Title:CS3V15/Devops Unit: II 7. Maven Profiles: A Build profile is a set of configuration values, which can be used to set or override default values of Maven build. Using a build profile, you can customize build for different environments such as Production v/s Development environments. Types of Build Profile Build profiles are majorly of three types. Type Where it is defined Per Project Defined in the project POM file, pom.xml Per User Defined in Maven settings xml file (%USER_HOME%/.m2/settings.xml) Global Defined in Maven global settings xml file (%M2_HOME%/conf/settings.xml) Profile Activation A Maven Build Profile can be activated in various ways. · Explicitly using command console input. · Through maven settings. · Based on environment variables (User/System variables). · OS Settings (for example, Windows family). · Present/missing files. Now, under src/main/resources, there are three environment specific files - Sl.No. File Name & Description env.properties 1 default configuration used if no profile is mentioned. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 10 N33 nirf 1750 Rank

--- Page 52 ---
Course Code/Title:CS3V15/Devops Unit: II env.test.properties 2 test configuration when test profile is used. 3 env.prod.properties production configuration when prod profile is used. Explicit Profile Activation In the following example, we will attach maven-antrun-plugin:run goal to test the phase. This will allow us to echo text messages for different profiles. We will be using pom.xml to define different profiles and will activate profile at command console using maven command. Assume, we've created the following pom.xml in C:\MVN\project folder. <project xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation = "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.companyname.projectgroup</groupId> <artifactId>project</artifactId> <version>1.0</version> <profiles> <profile> <id>test</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <version>1.1</version> <executions> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 11 N33 nirf 1750 Rank

--- Page 119 ---
Course Code/Title:CS3V15/Devops Unit: IV · Groupings in inventories enable you to organize your infrastructure, allowing you to target specific subsets of nodes based on your needs. For instance, when you need to perform tasks in a particular location, groupings become essential. Creating Inventories There are two primary methods to create an inventory on the control node: using a YAML file or an INI file. 1. INI Inventory Example (inventory.ini): 1[virtualmachines] 2host1 ansible_host:192.168.0.1 3192.168.0.2 2. In this INI file, it captures the IP addresses of managed nodes. The default username used for SSH connections is typically the working user of your control node. 3. ansible_host: This parameter is used to specify the IP address, although you can write the IP directly. 4. Inventory.yaml 1 --- 2virtualmachines: # Define a class name to represent a particular group of devices. 1 hosts: # Define the hosts. 2 vm01: # Define a parent name to indicate a class of parent type. 3 ansible_host: 192.168.0.1 # Specify the IP address. 4 http_port: 80 # (optional) Define the HTTP port. 5 ansible_user: auriga # (optional) Provide the username if it's different from the control node's user. o ansible_user: Use this to specify a different user if the managed node's user is not the same as the control node's user. o http_port: Specify a port if necessary. In real-world scenarios, inventory files can be more complex, reflecting the diverse and extensive infrastructure of organizations. You can explore more configurations here. Verifying Your Inventory After creating your inventory, it's a good practice to verify its correctness. This step ensures that the inventory is correctly structured and accessible to Ansible. To do this, use the following command: 1ansible-inventory -i inventory.yaml -list · - i: Indicates the inventory file you want to use. . - list: Requests the listing of the inventory content. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 5 N33 nirf 1750 Rank

--- Page 120 ---
Course Code/Title:CS3V15/Devops Unit:IV output: auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-inventory ·i inventory.yaml -- list "_meta": { "hostvars": { "vm01": "ansible_host": "192.168.0.208", "ansible_user"; "au" "children": [ "ungrouped" "ungrouped": { "hosts": "vn01 1 By following these steps, you have effectively set up and verified your inventory, ensuring a solid foundation for your Ansible automation. Using Ansible Ad-Hoc Commands Introduction: In Ansible, ad-hoc commands are your go-to solution for executing quick, one-off tasks on remote nodes. They provide a straightforward and efficient way to interact with managed nodes without the need for creating full-fledged playbooks. Ad-hoc commands are particularly useful when you need immediate results without the overhead of playbook development. When to Use Ad-Hoc Commands: Ad-hoc commands are best suited for scenarios where the task at hand is simple and doesn't require the complexity of a playbook. They are perfect for tasks like system health checks, package installation, service management, or any other single-operation job. Ad-Hoc Command Syntax: Ad-hoc commands follow a specific syntax that comprises various components, each serving a unique role: . Target Group: This is the group of hosts you intend to target with the ad-hoc command. · Module (-m): Specifies the module to execute. Modules are Ansible's building blocks for performing tasks, and they can range from basic operations like "ping" to more advanced tasks such as package management. · Inventory File (-i): Indicates the location of your inventory file, which defines the list of target devices. Example: Ping All Inventory Devices To illustrate the use of ad-hoc commands, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: 1ansible virtualmachines -m ping -i inventory.yaml virtualmachines is the target group. In this case, it could be any group, or you can use all to target all devices in your inventory. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 45 ---
Course Code/Title:CS3V15/Devops Unit: II Example for pom.xml file <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> 4. Maven Repository A maven repository is a directory of packaged JAR file with pom.xml file. Maven searches for dependencies in the repositories. There are 3 types of maven repository: 1. Local Repository 2. Central Repository 3. Remote Repository Maven searches for the dependencies in the following order: Local repository then Central repository then Remote repository. Local Central Remote Repository Repository Repository CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 4 N32 nirf 1750 Rank -

--- Page 46 ---
Course Code/Title:CS3V15/Devops Unit: II If dependency is not found in these repositories, maven stops processing and throws an error. 1) Maven Local Repository Maven local repository is located in your local system. It is created by the maven when you run any maven command. By default, maven local repository is %USER_HOME%/.m2 directory. For example: C:\Users\SSS IT\.m2. Update location of Local Repository We can change the location of maven local repository by changing the settings.xml file. It is located in MAVEN_HOME/conf/settings.xml, for example: E:\apache-maven- 3.1.1\conf\settings.xml. 2) Maven Central Repository Maven central repository is located on the web. It has been created by the apache maven community itself. The path of central repository is: http://repo1.maven.org/maven2/. The central repository contains a lot of common libraries that can be viewed by this url http://search.maven.org/#browse. 3) Maven Remote Repository Maven remote repository is located on the web. Most of libraries can be missing from the central repository such as JBoss library etc, so we need to define remote repository in pom.xml file. Let's see the code to add the jUnit library in pom.xml file. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 5 N32 narf 1750 Rank -

--- Page 31 ---
Course Code/Title:CS3V15/Devops Unit: I our work simpler. The pricing of Azure is also simpler and cost-effective. Popularly termed as "Pay As You Go", which means how much you use, pay only for that. Microsoft Azure Used for · Deployment Of applications: You can develop and deploy the application in the azure cloud by using the service called Azure App Service and Azure Functions after deploying the applications end users can access it. · Identity and Access Managment: The application and data which is deployed and stored in the Microsoft Azure can be secured with the help of Identity and Access Managment. It's commonly used for single sign-on, multi-factor authentication, and identity governance. · Data Storage and Databases: You can store the data in Microsoft azure in service like blob storage for unstructured data, table storage for NoSQL data, file storage, and Azure SQL Database for relational databases. The service can be scaled depending on the amount of data we are getting. · DevOps and Continuous Integration/Continuous Deployment (CI/CD): Azure DevOps will provide some tools like ncluding version control, build automation, release management, and application monitoring Following are some of the services Microsoft Azure offers: 1. Compute: Includes Virtual Machines, Virtual Machine Scale Sets, Functions for serverless computing, Batch for containerized batch workloads, Service Fabric for microservices and container orchestration, and Cloud Services for building cloud-based apps and APIs. 2. Networking: With Azure, you can use a variety of networking tools, like the Virtual Network, which can connect to on-premise data centers; Load Balancer; Application Gateway; VPN Gateway; Azure DNS for domain hosting, Content Delivery Network, Traffic Manager, ExpressRoute dedicated private network fiber connections; and Network Watcher monitoring and diagnostics 3. Storage: Includes Blob, Queue, File, and Disk Storage, as well as a Data Lake Store, Backup, and Site Recovery, among others. 4. Web + Mobile: Creating Web + Mobile applications is very easy as it includes several services for building and deploying applications. 5. Containers: Azure has a property that includes Container Service, which supports Kubernetes, DC/OS or Docker Swarm, and Container Registry, as well as tools for microservices. 6. Databases: Azure also included several SQL-based databases and related tools. 7. Data + Analytics: Azure has some big data tools like HDInsight for Hadoop Spark, R Server, HBase, and Storm clusters 8. AI + Cognitive Services: With Azure developing applications with artificial intelligence capabilities, like the Computer Vision API, Face API, Bing Web Search, Video Indexer, and Language Understanding Intelligent. 9. Internet of Things: Includes IoT Hub and IoT Edge services that can be combined with a variety of machine learning, analytics, and communications services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 31 N32 nirf 1750 Rank

--- Page 32 ---
Unit: I Course Code/Title:CS3V15/Devops 10. Security + Identity: Includes Security Center, Azure Active Directory, Key Vault, and Multi-Factor Authentication Services. 11. Developer Tools: Includes cloud development services like Visual Studio Team Services, Azure DevTest Labs, HockeyApp mobile app deployment and monitoring, Xamarin cross- platform mobile development, and more. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 32 N32 nirf 1750 Rank

--- Page 67 ---
Course Code/Title:CS3V15/Devops Unit: II Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013 JVM: 1.7.0_60 (Oracle Corporation 24.60-b09) OS: Windows 8.1 6.3 amd64 Understanding build using Gradle: The Gradle build is a process of creating a Gradle project. When we run a gradle command, it will look for a file called build.gradle in the current directory. This file is also called the Gradle build script. The build configuration, tasks, and plugins are described in this file. The build script describes a project and its tasks. Let's create a small Gradle project, run some of the basic Gradle commands, and understand how Gradle manages the project. Follow the steps below to create and test a Gradle project. Step1: Open the command line and create a directory First, Open the command line and create a directory for the project and change directory to it. Let's create a demo directory. C: \Users \HiMaNshU>mkdir demo C: \Users \HiMaNshU>cd demo Step2: Initialize a Gradle project To generate a Gradle project, run the gradle init command. It will generate a simple project. With this project, we will explore and understand everything that is generated. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 26 N33 nirf 1750 Rank

--- Page 68 ---
Course Code/Title:CS3V15/Devops Unit: II When we run the gradle init command, it will ask for some basic requirements. First, it will ask the type of project that we want to create. It will give four options: 1. 1: basic 2. 2: application 3. 3: library 4. 4: Gradle plugin Select our requirements. Hence, we are just making a demo project so that we will select the basic option. To select basic option, press 1 and Enter key. Consider the below output: C: \Users \HiMaNshU\demo>gradle init Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could ed, use -- status for details Select type of project to generate: 1: basic 2: application 3: library 4: Gradle plugin Enter selection (default: basic) [1 .. 4] 1 Next, it will ask for DSL. There are two options that are available for DSL: 1. 1: Groovy 2. 2: Kotlin Groovy is the default DSL provided by Gradle. Select build script DSL. Select build script DSL: 1: Groovy 2: Kotlin Enter selection (default: Groovy) [1 .. 2] 1 Next, it will ask for the project name. Type the project name and press Enter key. It will take a while to build a project. After the successful execution of the project, we will get a message BUILD SUCCESSFUL. Project name (default: demo): First_Gradle > Task : init Get more help with your project: https: //guides. gradle.org/creating- uilds BUILD SUCCESSFUL in 2m 56s 2 actionable tasks: 2 executed C: \Users\HiMaNshU\demo>mkdir src Now we have successfully created a Gradle project. Now, what will happen to our specified directory? Consider the below structure of the Gradle project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 27 N33 narf 1750 Rank

--- Page 103 ---
Course Code/Title:CS3V15/Devops Unit: III =Publish HTML reports ? Reports HTML directory to archive ? E:\Eclipse_workplace\SeleniumDemo\test-output Index page[s] ? index.html Index page title[s] (Optional) Report title ? HTML Report Publishing options Step 11 : Click on Apply and then Save. Now build this project. Step 12 : On successful build, it creates the HTML Report as follows - x + € > C @ localhost:0000/juby/HTML/\portDemo/HTML_20Report/ Back to HTMLBepsatDemo Test results All suites Default suite 11 D myPackage.myTestPage Info . Repetir output Results . Thus using HTML Publish Report Plugin we can successfully generate HTML report of some testing framework application. · Troubleshooting : Sometimes, the raw file is displayed as HTML report. To get the HTML formatted report we must click on Manage Jenkins section, scroll down and locate Script Console section. Click on it. Tools and Actions C Broad Configuration Sium Disk Discard all the loaded alla lo memory and reloud Jerking CLI Script Console Ewecubes arbitrary suyt lor and run below given command : System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", ") . Then restart the Jenkins Page and Build the project once again. The HTML report can then be displayed as given in above step. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 28 N32 nirf 1750 Rank

--- Page 104 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Copy Artifact Overview: The Copy Artifact Plugin allows Jenkins jobs to copy build artifacts from other jobs, facilitating artifact reuse across different jobs. Key Features: · Artifact Copying: Copies files from one build to another, which can be from a specific build or the latest successful build. · Triggering Builds: Can be used in downstream jobs to fetch artifacts from upstream jobs. Configuration: 1. Install the Plugin: Go to Manage Jenkins > Manage Plugins > Available tab, search for "Copy Artifact Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Build section. ○ Add a Copy artifacts from another project build step. o Specify the project name, build number or criteria, and the target directory for copied artifacts. Usage: · Commonly used in multi-job pipelines where artifacts need to be shared between different jobs or stages. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 29 N3 nirf 1750 Rank

--- Page 91 ---
Course Code/Title:CS3V15/Devops Unit: III Now we will create a repository on GitHub and then push the above committed Git repository on the GitHub. Open a web browser, and login to www.GitHub.com. Create a repository by the name MyJavaPrograms. Start a new repository A repository contains all of your project's files, revision history, and collaborator discussion. AnurdhaP / MyJavaPrograms Public Anyone on the internet can see this repository Private You choose who can see and commit to this repository Create a new repository 1 then copy the URL of this repository. HTTPS SSH https://github.com/AnurdhaP/MyJavaPrograms.git Now open the command prompt window and go to the MyJavaPrograms directory and issue the following command. git remote add origin https://github.com/AnurdhaP/MyJavaPrograms.git then push the Git to GitHub repository using the command push -u origin master CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 16

--- Page 92 ---
Course Code/Title:CS3V15/Devops Unit: III It is illustrated as follows - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git push -u origin master Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 719 bytes | 719.00 KiB/s, done. Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 To https://github.com/AnurdhaP/MyJavaPrograms.git * [new branch] master -> master branch 'master' set up to track 'origin/master'. E: \MyJavaPrograms> · Now it we refresh the repository on GitHub then we can see our test.java and test.class programs are present in that repository. MyJavaPrograms Public !" master - 1º 1 branch 0 tags Go to f AnurdhaP First Commit for Java program .4cb test.class First Commit for Java program test.java First Commit for Java program Help people interested in this repository understand your project by adding a README. Step 4 : Now create a job in Jenkins that makes use of GitHub plugin in order to access the GitHub repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 17 N3 nirf 1750 Rank

--- Page 23 ---
Course Code/Title:CS3V15/Devops Unit: I application. Amazon EC2 enables you to increase or decrease capacity within minutes. You can use one or hundreds or even thousands of server instances simultaneously. Because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs. Amazon EC2 is integrated with most AWS services, such as Amazon Simple Storage Service (Amazon S3), Amazon Relational Database Service (Amazon RDS), and Amazon Virtual Private Cloud (Amazon VPC) to provide a complete, secure solution for computing applications. Amazon EC2 is an example of Infrastructure as a Service(IaaS). EC2 delivers secure, reliable, cost-effective compute and high-performance compute infrastructure so as to meet the needs of demanding businesses. Amazon EC2 is one of the easiest ways of providing servers on AWS Cloud and also the access to Operating system. · AWS Lambda: AWS Lambda is a serverless, event-driven compute service that allows you to run code without managing servers. You pay only for the compute time you consume and there is no charge when your code is not running. With AWS Lambda, you can run code for any type of application with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services, or you can call it directly from any web or mobile app. But triggering Lambda is possible with over 200 AWS services. You can only pay for what you have used. The compute time that you consume, you are needed to pay for it. You just only need to upload your code and everything required to run will take care of by Lambda and it automatically scales your code with high availability. · AWS Elastic Beanstalk: AWS Elastic Beanstalk is a Platform as a Service that facilitates quick deployment of your applications by providing all the application services that you need for your application. Beanstalk is a plug- and-play platform that allows working with multiple programming languages and environments. Elastic Beanstalk supports a large range of platforms like Node js, Java, PHP, Python, and Ruby. So, you can develop your application to meet your requirements and simply deploy it on Elastic Beanstalk. The main aim to use AWS Elastic Beanstalk is to allow you to focus on the deployment and management of your applications. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. 2. Networking · Amazon VPC: Amazon VPC is your network environment in the cloud. It allows you to create a private network within the AWS cloud that uses many of the same concepts and constructs as an on-premises network. Amazon VPC also gives you complete control of the network configuration. Customers can define normal networking configuration items such as IP address ranges, subnet creation, route table creation, network gateways, and security settings. Amazon VPC is an AWS foundational service and integrates with numerous AWS services. For instance, Amazon EC2 instances are deployed into your CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 24 ---
Course Code/Title:CS3V15/Devops Unit: I Amazon VPC. Similarly, Amazon Relational Database Service (Amazon RDS) database instances deploy into your Amazon VPC, where the database is protected by the structure of the network just like your on-premises network. You can easily launch AWS resources into a virtual network by Amazon Virtual Private Cloud. An isolated virtual network environment in the AWS cloud is created by Amazon VPC. . Amazon Route 53: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating human-readable names, such as www.geeksforgeeks.com, into the numeric IP addresses that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. 2. Storage · Amazon S3 (Simple Storage Service): Amazon Simple Storage Service (Amazon S3) is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the web. It is designed to provide an infinite amount of storage and it is delivered with 99.999999999% durability. You can use Amazon S3 as primary storage for cloud-native applications as a target for backup and recovery and disaster recovery. It offers industry-leading scalability, data availability, security, and performance. It's simple to move large volumes of data into or out of Amazon S3 with Amazon's cloud data migration options. Once data is stored in Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like Amazon S3 Standard - Infrequent Access and Amazon Glacier for archiving. · Amazon Glacier: Amazon Glacier is a secure, durable, and extremely low- cost storage service for data archiving and long-term backup. Data stored in Amazon Glacier takes several hours to retrieve, which is why it's ideal for archiving. The fastest access to your archive data is via Amazon Glacier. 3. Databases · Amazon RDS (Relational Database Service): Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost- efficient and resizable capacity while managing time-consuming database administration tasks, freeing you up to focus on your applications and business. You can find Amazon RDS is also available on several database instance types - optimized for memory, performance, or I/O. Amazon RDS provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. · Amazon DynamoDB (Non-Relational Database): Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. When you create a database table that can store and retrieve any amount of data you can simply use Amazon DynamoDB that will serve any level of requested traffic. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, Internet of Things (IoT), and many other applications. DynamoDB provides many features like · built-in security CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 24 N32 nirf 1750 Rank

--- Page 41 ---
Unit: I Course Code/Title:CS3V15/Devops S.No. Git 1 Git is a software. 2 Git is a command-line tool GitHub is a graphical user interface 3 Git is installed locally on the system GitHub is hosted on the web 4 Git is maintained by linux. GitHub is maintained by Microsoft. 5 Git is focused on version control and code sharing. GitHub is a hosting service for Git repositories. 6 Git is a version control system to manage source code history. 7 Git was first released in 2005. GHub was launched in 2008. 8 Git has no user management feature. 9 Git is open-source licensed. GitHub includes a free-tier and pay- for- use tier. 10 Git has minimal external tool configuration. 11 Git provides a Desktop interface named Git Gui. GitHub provides a Desktop interface named GitHub Desktop. 12 Git competes with CVS, Azure DevOps Server, Subversion, Mercurial, etc. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY GitHub GitHub is a service. GitHub is focused on centralized source code hosting. GitHub has a built-in user management feature. GitHub has an active marketplace for tool integration. GitHub competes with GitLab, Bit Bucket, AWS Code Commit, etc. 41 N32 nirf 1750 Rank

--- Page 42 ---
Course Code/Title:CS2V15/Devops Unit: II UNIT - II COMPILE AND BUILD USING MAVEN AND GRADLE Introduction, Installation of Maven, POM files, Maven Build lifecycle, Build phases(compile build, test, package) Maven Profiles, Maven repositories(local, central, global),Maven plugins, Maven create and build Artifacts, Dependency management, Installation of Gradle, Understand build using Gradle. 1. Introduction of Maven: Apache Maven is an automation tool. The tool is written in Java. It was initially released on 13 July 2004. It is developed by the Apache software foundation. It is part of the Jakarta Project. It is working on two aspects: how software is built, and its dependencies. It was created by Jason van Zyl. It is built by using a plugin-based architecture that allows it to make the use of any application controllable by standard input. It dynamically downloads Java libraries. Understanding the problem without Maven There are many problems that we face during the project development. They are discussed below: 1) Adding set of Jars in each project: In case of struts, spring, hibernate frameworks, we need to add set of jar files in each project. It must include all the dependencies of jars also. 2) Creating the right project structure: We must create the right project structure in servlet, struts etc, otherwise it will not be executed. 3) Building and Deploying the project: We must have to build and deploy the project so that it may work. Maven simplifies the above mentioned problems. It does mainly following tasks. 1. It makes a project easy to build 2. It provides uniform build process (maven project can be shared by all the maven projects) 3. It provides project information (log document, cross referenced sources, mailing list, dependency list, unit test reports etc.) 4. It is easy to migrate for new features of Maven Apache Maven helps to manage o Builds Documentation o Reporing o SCMs o Releases o Distribution What is Build Tool? A build tool is used for building a process. It does following: CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE TECHNOLOGY N32 nirf 1 1750 Rank

--- Page 105 ---
Course Code/Title:CS3V15/Devops Unit: III How to install copy artifact plugin ? Step 1 : Start the Jenkins tool by entering the login and password. Step 2 : Click on Manage Jenkins. Click on Plugins. Step 3 : In the search window, type the name copy Artifact. Step 4 : The plugin name will be displayed if it is not already installed. Select it. Name Adde a build atop to copy artifacts from another project. Thủa plugin le up for adoption! We are looking for naw maintainent. Velt our Adopt a Plugin intuitive for mort information. then click on Install. The plugin gets installed. Example Demo · Let us discuss how to use Copy Artifact plugin. Step 1 : Create a new Job by clicking on item New Item. Give the some name to your job. I have given the name App1 and selected the Freestyle Project. Step 2: Give suitable description in the description box. Step 3 : Under the source code management, click on Git option and specify the repository URL of GitHub. Source Code Management None Git ? Repositories 1 Repository URL + ·hong · CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 30 N3 nirf 1750 Rank

--- Page 106 ---
Unit: III Course Code/Title:CS3V15/Devops Step 4 : Under the Build Steps section, choose the Execute Windows batch command option. And type the javac and java commands. Build Steps = Execute Windows batch command Command See the list of available environment variables javac test.java java test Advanced Add bulld stop * Step 5 : Click in Apply and Save. Step 6 : Select the Build Now option. On successful built, the test, class file gets generated in the Jenkins workspace of the App1 job. C:\ProgramData\Jenkins\.jenkins\workspace\Appl>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\Appi>java test Good Morning Pathill Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! C:\ProgramData\Jenkins\.jenkins\workspace\Appl>exit @ Finished: SUCCESS Step 7 : Now we have to copy test.class file from the workspace of Appl to another job. For that' purpose we will create another Job named App2 by clicking New Item. Select the Freestyle Project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 31 N3 nirf 1750 Rank

--- Page 59 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Specify Archetype parameters M Group Id: com.simplileam Artifact Id: mavenproject Version: 0.0.1-SNAPSHOT Package: com,simplilearn.mavenproject . Properties available from archetype: Name Value Add ... Remove Advanced ? < Back Next > Cancel Finish The project is now created. · Open the pom.xml file You can see all the basic information that you have entered on the screen, such as the Artifact ID, Group ID, etc. You can see the junit dependencies have been added. This process takes place by default in Eclipse. There will also be some by default test cases. File Edit Source Navigate Search Project Run Window Help # Package Explorer E Jo JUnit - mavenproject/pom.xml 28 mavenproject > ( src/main/java ( src/test/java > = JRE System Library (|2SE-1:5] i Maven Dependencies target 7 <artifactIdaavenproject</artifactId> <version>0. 0. 1-SNAPSHOT</version>: <packaging>jar</packaging> <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 136 sproperties> 14 <project. build.sourceEncoding>UTF-8</project. build.sourceEncoding> </properties> 15 16 «dependenciess 18- <dependency> 19 <groupId>junit</grouptasI <artifactId=junit</artifactle> sversion>3.8.1</version> <scope>tests/scope> 21 22 223 </dependency> 24 </dependencies> 25 </project> Overview Dependencies Dependency Hierarchy |Effective POM pom.xml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 18 N32 nirf 175º Rank -

--- Page 60 ---
Course Code/Title:CS3V15/Devops Unit: II There you can find AppTest.java to be a default test case. When you click on that, you can see the test cases written in JUnit on your Eclipse screen. # Package Explorer 13 Ju JUnit mavenproject/pom.xml Z App.java AppTest.java 12 1 package com.simplilearn.mavenproject; - fl mavenproject + ( src/main/java - il com.simplileam.mavenproject U App.java = ( src/test/java il com.simplileam.mavenproject , E AppTest.java > > JRE System Library ||2SE-1.5] al Maven Dependencies - target pom.xml 3w import jumit. framework. Test:[] 6 76 / ** . Unit test for simple App. 10 public class AppTest extends TestCase 12 1 14 . Create the test case 15 16 * @param testName name of the test cole 17 18% public AppTest( String testNane ) 19 1 super( testName ); 28 21 1 22 :23% 24 * @return the suite of tests being tested 75 When it comes to adding more test cases, it will depend on the user, but these test cases and commands can easily be added in the workspace. If we try to remove certain dependencies from our file, we will receive error messages. To troubleshoot this, complete the following steps: · Go to another tab: mavenproject/pom.xml · Delete any dependencies · Save the file Immediately, there will be several error messages in the AppTest.java. R mavenproject/pom.xml App.java AppTest.java 83 1 package com.simplilearn.mavenproject; 2 3@ import junit. framework. Test;[] 6 mavenproject/src/test/java/com/simplileam/mavenproject/AppTest java 8 * Unit test for simple App. 9 18 public class AppTest extends TestCase 011 12 { 136 14 . Create the test case 15 16 * @param testName name of the test case 17 188 19 € public AppTest( String testName ) 28 super( testName ); 21 22 - 230 24 * @return the suite of tests being tested Problems E @ Javadoc ) Declaration History 7 errors, 1 warning, 0 others Description + @ Errors (7 items) a Test cannot be resolved to a type Resource Path Location AppTest.java /mavenproject/src/te line 26 Java Problem Type CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 19 N32 nirf 1750 Rank -

--- Page 69 ---
Unit: II Course Code/Title:CS3V15/Devops 1 build.gradle gradle wrapper 2 gradle-wrapper.jar - gradle-wrapper.properties 3 gradlew 4 gradlew.bat 5 settings.gradle 6 It is the default structure of a Gradle project. Gradle will generate the following things for us: 1. The gradle file is build script for configuring the current project. 2. An executable JAR file is used as a Gradle wrapper. 3. Configuration properties for Gradle Wrapper. 4. The gradlew is a Gradle wrapper script for UNIX based OS. 5. The bat is the Gradle Wrapper script for Windows. 6. The settings script for configuring the Gradle build. Step3: Create a task Gradle supports APIs for creating and managing tasks through a Groovy-based DSL or Kotlin-based DSL. Every project contains a collection of tasks for some basic operation. Gradle supports a library of tasks that configure the project. For example, there is a Copy task, which copies files from one location to another. The Copy task is one of the most used tasks In Gradle. To use the Copy task in build script, follow the below process. Step1: Create a directory called src C: \Users \HiMaNshU\demo>mkdir src C: \Users \HiMaNshU\demo>cd src Step2: Add a file called myfile.txt in the src directory. Add the single line "Hello, World!" to it, also, we can leave it empty. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 28 N32 nirf 1750 Rank

--- Page 70 ---
Unit: II Course Code/Title:CS3V15/Devops C:\Users \HiMaNshU\demo\src>echo myfile.txt myfile.txt Define a task called Copy in build.gradle file. It will copy the src directory to a new directory called dest. We don't have to create the dest directory; the Copy task will do it for us. 1. task copy(type: Copy, group: "Custom", description: "The sources are copied to dest directory") { 2. from "src" 3. into "dest" 4. } We can provide anything in a group and description. Also, we can omit them, but doing so will also be omitted from the report of the task used later. Now execute our new copy task: C: \Users \HiMaNshU\demo>gradle copy BUILD SUCCESSFUL in 2s 1 actionable task: 1 executed C: \Users \HiMaNshU\demo>_ After the successful execution of the task, we will get BUILD SUCCESSFUL message. The build.gradle file The build.gradle file is build script of a Gradle project. All the tasks and plugins are defined in this file. When we run a gradle command, it looks for a file called build.gradle in the current directory. Although we have called it a build script, strictly, it is a build configuration script. The build script defines a project and its tasks. The default build.gradle file looks like as follows: CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 29 N33 nirf 1750 Rank

--- Page 95 ---
Course Code/Title:CS3V15/Devops Unit: III C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha C: \ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>exit e Finished: SUCCESS Step 6 : Now go to GitHub repository and make some changes to your test.java file and commit these changes. BE AnurdhaP Changes in the test java Code Blame 6 lines (6 loc) · 158 Bytes 89 Code 55% faster with GitHub Copilot > public class test { 2 public static void main(String args[]) { for(int 1=1;i <= 5;1++) 3 4 System.out.println("Good Morning Path !!! "); 5 3 6 } Step 7 : Now, if you check on the Jenkins page, it automatically executes the above updated test.java file and creates a build. It is as follows .- C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test. java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Good Morning Path !!! Good Morning Pathi !! Good Morning Path !!! Good Morning Path !!! Good Morning Pathill 2. Parameter Plugin Overview: The Parameter Plugin allows Jenkins jobs to accept parameters at build time, enabling dynamic and flexible builds. Key Features: · Parameter Types: Supports various parameter types such as string, choice, boolean, and more. · Default Values: Provides default values for parameters. · Prompt for Parameters: Users are prompted to enter values for parameters when starting a build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 20

--- Page 96 ---
Course Code/Title:CS3V15/Devops Unit: III Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Parameterized Builds," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add parameters of different types (e.g., string, choice) and configure their options and default values. Usage: · Useful for creating jobs that require user input or need to be customized for different build scenarios. A Example Demo Step 1 : Click on New item and create a job. I have created a job by the name Welcome (Refer section 3.5) Step 2 : Click on the existing job and configure it with parameters. Step 3 : Check the option "This project is parameterized". Select the String Parameter. Create some string parameter. I have created a parameter named Username and default value to it. This project is parameterized ? =String Parameter ? Name ? Username Default Value ? Parth Description ? Plain text Preview Trim the string ? Step 4 : Under the Build Steps section, using echo command I tried to display the parameter value CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 21

--- Page 3 ---
Course Code/Title:CS3V15/Devops Unit: I Key Goals and Benefits of DevOps Goals of DevOps The fast-paced growth of the IT industry and continuous advancements in technology make it critical to set DevOps goals that are experimental and challenging for companies to compete and thrive in the market. Here are the key goals and principles that every successful DevOps program has in common. 1. Ensures effective collaboration between teams: Effective collaboration in any process relies on shared ownership. During the development process, all those involved should embrace the fact that everyone is equally responsible for the entire development process. Whether it is development, testing, or deployment, each team member should be involved. They should understand that they have an equal stake in the final outcome. In the DevOps paradigm, passing of work from one team to another is completely defined and broken down. This accelerates the entire process of development since collaboration between all the teams involved is streamlined. 2. Creates scalable infrastructure platforms: The primary focus of DevOps is to create a sustainable infrastructure for applications that make them highly scalable. According to the demands of the modern-day business world, scalable apps have become an absolute necessity. In an ideal situation, the process of scaling should be reliable and fully automated. As a result, the app will have the ability to adapt to any situation when a marketing effort goes viral. With the app being scalable, it can adjust itself to large traffic volumes and provide an immaculate user experience. 3. Builds on-demand release capabilities: Companies must focus on keeping their software in a 'releasable' state. Continuous delivery will allow the software to add new features and go live at any stage. DevOps aims to automate the process of release management because it has a plethora of advantages. Automated release management is predictable, fast, and very consistent. Moreover, through automation, companies can release new versions as per their requirements. Automated release management also has complete and thorough audit trials, as these are essential for compliance purposes. 4. Provides faster feedback: Automating monotonous tasks such as testing and reporting will accelerate the process of rapid feedback. Since the development team will know what has to change, it can roll out the updated version faster. In addition, the team can better understand the impact of the changes that it has done in the software lifecycle. A concrete understanding of changes will assist team members in working efficiently in tandem. With rapid feedback, the operations team and developers can make better decisions collectively and enhance the app's performance. Benefits of DevOps DevOps helps organizations deliver added value to their customers. Here are some compelling benefits of DevOps. 1. Smarter work and faster release: With DevOps, your development team can release the required deliverables quickly. Faster release of deliverables will keep you miles ahead of your competitors, which is very important in today's cut-throat business realm. Businesses should understand that if their review cycle is not automated, it will slow down the release process. Moreover, the inclusion of disparate tools will lead to context switching and higher costs. Thus, DevOps can help rectify this worrisome business situation. 2. Quick resolution of issues: In a business world where speed and accuracy are paramount, a fast feedback loop will help you thrive. With DevOps, the communication process becomes seamless, and, as such, it minimizes the time required to solve issues. Without open communication, key issues can slip out of mind, which will have serious repercussions in the long run. DevOps fosters open communication that helps resolve issues, thus unblocking the release pipeline faster. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 3 N32 nirf 1750 Rank

--- Page 4 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Better collaboration between teams: DevOps paves the way for more dynamic and round-the- clock communication between teams. It renders an environment for mutual collaboration and integration among teams that are distributed globally. Eliminating the traditional departmental barriers between teams forms a new sense of ownership, wherein each team member feels equally responsible for meeting delivery timelines. This collaboration contributes to happier and more engaged employees. 4. Fostering innovative mindsets: With DevOps, deployment phases of the application are more relaxed as compared to traditional methods. This is because it streamlines the entire process, ensures that there are no lapses in quality, and allows on-time and efficient release. Thus, as everything is in order, the development team is more at peace. This allows it to think out of the box and provide additional value to the user. Having a development team with an innovative mindset is a boon for any business organization. An innovative approach, in itself, has immense scope and leads to better quality and resolution of issues at hand. Thus, through DevOps, the process of expanding the horizon of an app becomes much easier. 5. Faster threat detection: Automated and continuous testing of the code will make the process of threat detection faster. As developers can locate problem areas at an early stage, they can then resolve them faster. Thus, DevOps is a vital cog in maintaining and enhancing the quality and performance of an app. As the overall build of the app is in capable hands, teams working together are empowered to share feedback as and when necessary. 6. Increased customer satisfaction: Customer satisfaction is paramount in any day and age, irrespective of the business one is involved in. DevOps is known for enhancing customer experience, which ultimately increases the level of customer satisfaction. Dissatisfied customers are never a good sign for any business. Feedback loops are an important component of DevOps. These loops empower end users to track the progress of app development at various stages. 7. In addition, they can suggest changes (if any) or give their inputs to make the app more customer- centric. Due to their dynamic nature, feedback loops help developers and customers remain on the same page. Moreover, DevOps accelerates the process of app development, which eventually lessens the delivery timer. This has a positive impact on the customer satisfaction ratio. 8. Providing the much-needed edge: Along with staying true to their development process, companies need to ensure that they sustain themselves in the cut-throat competition. Implementing DevOps can be your trump card to provide your organization with that much- needed edge. Competitive advantage is necessary, as it can become the deciding factor in the popularity of an application in many cases. Some factors set expert businesses apart from mediocre ones: · Top-quality features · Quicker and timely software releases · Maximizing return on investments · Listening to constructive feedback Difference between Agile and DevOps Agile: Agile program advancement comprises different approaches to computer program improvement beneath which prerequisites and arrangements advance through the collaborative exertion of self- organizing and cross-functional groups and their customer/end client. DevOps: DevOps could be a set of hones that combines program improvement and information- technology operations which points to abbreviating the framework's advancement life cycle and giving nonstop conveyance with tall program quality. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 4 N32 nirf 1750 Rank

--- Page 113 ---
Course Code/Title:CS3V15/Devops Unit: III 3. Configure Job Details: ○ General: I Enter a description for your job. Configure options such as discarding old builds if needed. ○ Source Code Management: I Choose Git, Subversion, or another source control system. Enter the repository URL and credentials if the repository is private. Specify the branch or tag to build from. ○ Build Triggers: Set up triggers to start the build. Common options include: Poll SCM: Jenkins will periodically check for changes in the source code repository. . Build periodically: Schedule builds at specific intervals. I GitHub hook trigger for GITScm polling: Trigger builds based on GitHub webhooks. 0 Build Environment: ■ Configure the build environment, such as setting up environment variables or cleaning up before the build starts. ○ Build Steps: Add build steps by clicking Add build step. · Common build steps include: Execute shell: Run shell commands or scripts. . Invoke Gradle script: Use Gradle to build the project. I Invoke Ant: Use Apache Ant for building. ○ Post-build Actions: · Configure actions to perform after the build completes, such as: I Archive the artifacts: Save build artifacts for later use. I Publish JUnit test result report: Display test results in Jenkins. I Send build notifications: Notify stakeholders of build status. 4. Save and Build: ○ Click Save to store the job configuration. o To start a build, click Build Now in the job dashboard. Example Build Configuration: . Job Name: MyApp-Build · Source Code Management: Git o Repository URL: https://github.com/user/myapp.git ○ Branch: main · Build Step: Execute shell ○ Command : myn clean install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 38 nirf 1750 Rank N33

--- Page 114 ---
Course Code/Title:CS3V15/Devops Unit: III · Post-build Action: Archive artifacts ○ Files to archive: target/ *. jar 2. Understanding Jenkins Workspace Overview: The Jenkins workspace is a directory where Jenkins stores files and artifacts related to a particular build. Each job has its own workspace, which is used to perform build operations. Key Aspects of Jenkins Workspace: 1. Workspace Location: o By default, the workspace is located in the Jenkins home directory, typically at /var/lib/jenkins/workspace/ on Linux systems or C:\Program Files (x86) \Jenkins\workspace\ on Windows. 2. Workspace Structure: ○ Each job gets its own subdirectory within the workspace. For example, a job named MyApp-Build will have its workspace at /var/lib/jenkins/workspace/MyApp - Build/. ○ The workspace contains: Source Code: The code pulled from the repository. . Build Artifacts: Files generated during the build process. I Logs: Logs related to the build process. 3. Workspace Usage: ○ Building: During the build process, Jenkins checks out the code into the workspace, executes build steps, and generates artifacts. ○ Archiving Artifacts: After the build, files specified in the post-build actions are archived from the workspace. ○ Cleaning Up: Jenkins may clean up workspaces based on job configurations or policies to save disk space. 4. Customizing Workspace: ○ Configure Custom Workspace Location: In the job configuration, under the Advanced Project Options, you can specify a custom workspace directory. ○ Workspace Cleanup: I Use plugins like the Workspace Cleanup Plugin to manage and clean up workspaces automatically. Example of Workspace Structure: · Workspace Directory: /var/lib/jenkins/workspace/MyApp-Build/ o Source Code: /var/lib/jenkins/workspace/MyApp-Build/src/ o Build Artifacts: /var/lib/jenkins/workspace/MyApp-Build/target/ o Logs: /var/lib/jenkins/workspace/MyApp-Build/logs/ This guide should provide a comprehensive understanding of creating a Jenkins build and managing Jenkins workspaces. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY N32 39 nirf 175º Rank

--- Page 5 ---
Unit: I Course Code/Title:CS3V15/Devops Example: Facebook's mobile app which is updated every two weeks effectively tells users you can have what you want and you can have it. Now ever wondered how Facebook was able to do social smoothing? It's the DevOps philosophy that helps facebook and sure that apps aren't outdated and that users get the best experience on Facebook. Facebook accomplishes this true code ownership model that makes its developers responsible that includes testing and supporting through production and delivery for each kernel of code. They write and update their true policies like this but Facebook has developed a DevOps culture and has successfully accelerated its development lifecycle. Difference between Agile and DevOps: S. No. Agile 1 It started in the year 2001. It started in the year 2007. 2 Invented by John Kern, and Martin Fowler. Invented by John Allspaw and Paul Hammond at Flickr, and the Phoenix Project by Gene Kim. 3 Agile is a method for creating software. It is not related to software development. Instead, the software that is used by DevOps is pre-built, dependable, and simple to deploy. 4 An advancement and administration approach. Typically a conclusion of administration related to designing. 5 The agile handle centers on consistent changes. DevOps centers on steady testing and conveyance. 6 A few of the finest steps embraced in Agile are recorded underneath - 1. Backlog Building 2.Sprint advancement DevOps to have a few best hones that ease the method - 1. Focus on specialized greatness. 2. Collaborate straightforwardly with clients and join their feedback 7 Agile relates generally to the way advancement is carried of, any division of the company can be spry in its hones. This may be accomplished through preparation. 8 All the group individuals working in a spry hone have a wide assortment of comparable ability sets. This is often one of the points of interest of having such a group since within the time of requirement any of the group individuals can loan help instead of holding up for the group leads or any pro impedances. 9 Spry accepts "smaller and concise". Littler the group superior it would be to convey with fewer complexities. 10 Since Agile groups are brief, a foreordained sum of time is there which are sprints. Tough, it happens that a sprint has endured longer than a month but regularly a week long. 11 A big team for your project is not required. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY DevOps DevOps centers more on program arrangement choosing the foremost dependable and most secure course. DevOps features a diverse approach and is very viable, most of the time it takes after "Divide and Conquer". Work partitioned among the improvement and operation groups. DevOps, on the other hand, accepts that "bigger is better". DevOps, on the other hand, prioritizes reliabilities. It is since of this behavior that they can center on a long-term plan that minimizes commerce's unsettling influences. It demands collaboration among different teams for the completion of work. 5 N33 nirf 1750 Rank

--- Page 6 ---
Unit: I Course Code/Title:CS3V15/Devops 12 Some of the Tools- · Bugzilla · JIRA · Kanboard and more. 13 It is suitable for managing complex projects in any department. It centers on the complete engineering process. 14 It does not focus on the automation. It focusses on automation. 15 Working system gets more significance in Agile than documentation. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Some of the Tools- · Puppet • Ansible • AWS · Chef · team City OpenStack and more. The process documentation is significant in DevOps. 6 N32 nirf 1750 Rank

--- Page 37 ---
Course Code/Title:CS3V15/Devops Unit: I GIT INSTALLATION Git for Windows stand-alone installer · Download the latest Git for Windows installer. · When you've successfully started the installer, you should see the Git Setup wizard screen. Follow the Next and Finish prompts to complete the installation. The default options are pretty sensible for most users. · Open a Command Prompt (or Git Bash if during installation you elected not to use Git from the Windows Command Prompt). . Run the following commands to configure your Git username and email using the following commands, replacing Emma's name with your own. These details will be associated with any commits that you create: $ git config -- global user.name "CIT_CHENNAI" $ git config -- global user.email CITCHENNAI@atlassian.com • Optional: Install the Git credential helper on Windows Bitbucket supports pushing and pulling over HTTP to your remote Git repositories on Bitbucket. Every time you interact with the remote repository, you must supply a username/password combination. You can store these credentials, instead of supplying the combination every time, with the Git Credential Manager for Windows. BASIC COMMANDS OF GIT: Some basic Git commands along with their syntax and examples: · Initialize a Repository: Syntax: `git init' Example: `git init' · Clone a Repository: Syntax: `git clone <repository_url> Example: `git clone https://github.com/example/repository.git' · Check Repository Status: Syntax: `git status' Example: `git status' · Add Changes to Staging Area: Syntax: `git add <file(s)>' Example: `git add file.txt' · Commit Changes: Syntax:`git commit -m "Commit message" Example:`git commit -m "Add new feature" · Create a New Branch: Syntax: `git branch <branch_name>' Example: `git CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY (Autonomous] N33 37 nirf 1750 Rank

--- Page 38 ---
Course Code/Title:CS3V15/Devops branch feature-branch' Unit: I • Switch to a Branch: Syntax: `git checkout <branch_name>' Example:`git checkout feature-branch' OR Syntax: `git switch <branch_name>' (Git version 2.23 and later) Example: `git switch feature-branch' · Create and Switch to a New Branch: Syntax: `git checkout -b <new_branch name>' Example: `git checkout -b new-feature' OR Syntax: `git switch -c <new_branch_name>' (Git version 2.23 and later) Example: `git switch -c new-feature' . Merge Changes from One Branch to Another: Syntax: `git merge <branch_name>' Example: `git merge feature-branch' · View the Commit History: Syntax: `git log' Example: `git log' · Push Changes to a Remote Repository: Syntax: `git push <remote_name> <branch_name>' Example:`git push origin master' · Pull Changes from a Remote Repository: Syntax: `git pull <remote_name> <branch_name>' Example:`git pull origin master' . Show the Differences Between Working Directory and Staging Area: Syntax: `git diff Example: `git diff · Show the Differences Between Staging Area and Last Commit: Syntax: `git diff -- cached' Example: `git diff -- cached' . Show the Differences Between Working Directory and Last Commit: Syntax: `git diff HEAD' Example: `git diff HEAD' CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 38 N32 nirf 1750 Rank

--- Page 47 ---
Course Code/Title:CS3V15/Devops Unit: II <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> Any repository of maven is available in mvnrepository.com 5. Maven Plugins: The maven plugins are central part of maven framework, it is used to perform specific goal. According to Apache Maven, there are 2 types of maven plugins. 1. Build Plugins 2. Reporting Plugins Build Plugins These plugins are executed at the time of build. These plugins should be declared inside the <build> element. Reporting Plugins These plugins are executed at the time of site generation. These plugins should be declared inside the <reporting> element. Maven Core Plugins A list of maven core plugins are given below: Plugin Description clean clean up after build. compiler compiles java source code. deploy deploys the artifact to the remote repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY N32 nirf 6 175º Rank -

--- Page 48 ---
Course Code/Title:CS3V15/Devops Unit: II failsafe runs the JUnit integration tests in an isolated classloader. install installs the built artifact into the local repository. resources copies the resources to the output directory for including in the JAR. site generates a site for the current project. surefire runs the JUnit unit tests in an isolated classloader. verifier verifies the existence of certain conditions. It is useful for integration tests. Example for maven plugin: The compiler plugin is used to compile the source code of a Maven project. This plugin has two goals, which are already bound to specific phases of the default lifecycle: · compile - compile main source files · testCompile - compile test source files Here's the compiler plugin in the POM: <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.12.1</version> <configuration> <source>1.8</source> <target>1.8</target> </configuration> </plugin> 6. Maven Build Lifecycle: A Build Lifecycle is a well-defined sequence of phases, which define the order in which the goals are to be executed. Here phase represents a stage in life cycle. As an example, a typical Maven Build Lifecycle consists of the following sequence of phases. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 7 N33 nirf 175º Rank -

--- Page 11 ---
Course Code/Title:CS3V15/Devops Unit: I 17. Selenium It remains a vital tool in software testing and automation due to its enduring relevance in ensuring the quality of web applications. As technology evolves, web applications become increasingly complex, requiring thorough testing across various browsers and platforms. With its robust automation capabilities and extensive browser compatibility, Selenium allows developers and QA teams to automate repetitive testing tasks efficiently, conduct cross- browser testing, and ensure that web applications function flawlessly across diverse environments. Its open-source nature, active community support, and integration with other DevOps tools make Selenium a go-to choice for organizations striving for continuous delivery and the rapid deployment of high-quality software, a cornerstone of modern software development practices. 18. Gremlin Gremlin is an essential tool in chaos engineering, which has become increasingly critical for ensuring the resilience and reliability of modern software systems. As technology advances and complex distributed systems become the norm, the potential for unexpected failures and outages also rises. Gremlin allows organizations to proactively identify weaknesses and vulnerabilities in their infrastructure and applications by simulating controlled failures, such as network disruptions, service outages, and resource constraints. By intentionally inducing chaos and monitoring the system's response, teams can uncover weaknesses before they lead to costly downtime or security breaches. Gremlin facilitates organizations to build more robust, fault-tolerant systems that can withstand real-world challenges and deliver uninterrupted services to users. 19. ServiceNow ServiceNow is a vital platform for organizations seeking to streamline their IT service management and beyond. Its significance lies in its ability to provide a unified, cloud-based solution for automating and optimizing various business processes, including ITSM, ITOM, HR, customer service, and more. Due to the rapid digitization of services, remote work, and the growing complexity of technology infrastructures, ServiceNow offers a comprehensive approach to managing workflows, resolving issues, and delivering services efficiently. Its intelligent automation capabilities, analytics, and AI-driven insights empower organizations to enhance productivity, agility, and customer satisfaction while reducing operational costs. ServiceNow's role in orchestrating and integrating diverse systems and processes makes it an indispensable tool for driving digital transformation and ensuring smooth operations in the ever-evolving business landscape of 2024. 20. Status Service Updates: The Status Page "Status Service Updates: The Status Page" is a critical tool for organizations and businesses of all sizes. In today's world, where online services and applications are integral to operations, ensuring their availability and reliability is essential. It provides real-time information to users and stakeholders about the operational status of services, applications, and infrastructure. The Status Page plays a crucial role in transparency, trust-building, and customer satisfaction by promptly communicating service disruptions, planned maintenance, and incident resolutions. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 1750 Rank

--- Page 12 ---
Course Code/Title:CS3V15/Devops Unit: I Downtime can often lead to significant financial losses and damage to a company's reputation, so having a practical Status Page becomes not just a convenience but a necessity. It allows organizations to showcase their commitment to transparency and responsiveness in addressing service-related issues, ultimately fostering stronger customer relationships and trust. 21. ELK (Elasticsearch, Logstash and Kibana) ELK, which stands for Elasticsearch, Logstash, and Kibana, continues to shine in DevOps and IT operations. This powerful trio of tools remains essential for organizations seeking effective log management, monitoring, and data visualization. Elasticsearch is a highly scalable and fast search engine that enables real-time data indexing and search. Logstash facilitates the collection, processing, and transformation of log data from various sources, making it compatible with Elasticsearch. Kibana, on the other hand, provides a user- friendly interface for visualizing and analyzing data, offering customizable dashboards and powerful data exploration capabilities. ELK's significance in 2024 lies in its ability to empower organizations with comprehensive insights into their systems, applications, and infrastructure. It ultimately facilitates quick problem resolution, proactive monitoring, and data-driven decision-making in an increasingly complex and fast-paced technological landscape. 22. GitLab CI/CD GitLab CI/CD's significance lies in its ability to automate the complete software delivery pipeline, from code changes to deployment, in a single integrated environment. GitLab CI/CD ensures rapid and reliable delivery of software updates. It enables continuous integration (CI) by automatically building and testing code changes, allowing teams to catch issues early in the development cycle. Furthermore, the continuous deployment (CD) aspect automates the release and deployment process, reducing the risk of human errors and enabling organizations to deliver features and updates to users swiftly and confidently. GitLab CI/CD's importance is further accentuated as businesses seek to accelerate digital transformation efforts, respond rapidly to changing market demands, and maintain a competitive edge through efficient and automated software delivery practices. 23. Scripting Scripting remains vital due to its pivotal role in automating and streamlining various aspects of software development, system administration, and DevOps practices. Scripting languages like Python, Bash, and PowerShell empower tech professionals to write code that can execute repetitive tasks, manipulate data, and orchestrate complex processes efficiently. Scripting facilitates rapid prototyping, configuration management, and the creation of automated deployment pipelines. It enhances productivity, ensures consistency and reduces human error in tasks ranging from software testing and deployment to infrastructure provisioning and monitoring. As organizations increasingly embrace DevOps and cloud- native technologies, scripting stays competitive and adaptive in the tech landscape. 24. Terraform Terraform plays a crucial role in modern infrastructure provisioning and management. It allows organizations to define and deploy infrastructure as code, enabling the automated creation and configuration of cloud resources, containers, and other infrastructure components. Cloud computing, microservices, and containerization have become the norm in CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 12 N32 nirf 1750 Rank

--- Page 117 ---
Course Code/Title:CS3V15/Devops Unit: IV • Explore Ansible modules. 4. Playbook - · Playbooks are a sequence of plays that define the order of tasks executed by Ansible. . A play consists of a list of tasks that target managed nodes in an inventory. · Tasks, in turn, are composed of one or more modules that specify operations. · Playbooks provide a structured way to define and automate complex workflows. 5. Roles - · Roles provide an organized environment for managing complex tasks. · They include templates, playbooks, inventories, error handlers, vars, and meta information. · Ideal for handling larger, multi-step automation processes. 6. Collections - · Collections are distribution formats for Ansible content. · They encompass playbooks, roles, modules, and plugins. · Find Ansible collections. 7. Galaxy - · Ansible Galaxy is a platform for sharing and downloading collections. . It allows the Ansible community to collaborate and exchange automation content. Setting Up the Ansible Control Node Prerequisite: Before you begin setting up Ansible, it's essential to ensure you meet the following prerequisites: · SSH: Make sure SSH is installed on your Linux system. SSH is crucial for secure communication between the control node and managed nodes. Installation Steps: 1. Open your terminal. 2. Run the following command to install Ansible: 1python3 -m pip install -- user ansible 3. SSH Key: o For secure and direct connections to managed nodes, you'll need to provide a public SSH key. You can generate one using the ssh-keygen command. o Follow the on-screen instructions to create your SSH key pair. This key pair is a crucial element of Ansible's secure communications. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 118 ---
Course Code/Title:CS3V15/Devops Unit: IV 4. For specific installation instructions tailored to your Linux distribution, click here. 5. Try to connect ssh using once your mange node is set up. 1ssh <user>@<ip> Note: If you're using a different operating system, find the appropriate installation instructions here. Setting Up the Ansible Mange Node Steps: 1. Install OpenSSH-Server: o To enable remote management of the managed node, you'll need to install the OpenSSH server. Use the following command (assuming you're using a Debian-based Linux distribution like Ubuntu): 1sudo apt install openssh-server o This command installs the OpenSSH server, allowing secure remote access. 2. Create a User: o For convenience and consistency, it's helpful to create a user on the managed node with the same name as the user on the control node. This makes it easier to manage SSH keys and ensures a smoother experience. 3. Configure SSH Key: o To establish secure and direct connections to the managed node, you need to paste the public SSH key from the control node into the authorized_keys file located in the .ssh directory of the user's home folder on the managed node. o The path is typically /home/<user>/.ssh/authorized_keys, where <user> is the username you created or are using on the managed node. You can use the ssh-copy-id command to automate the process of copying your public key to the managed node. For example: 1ssh-copy-id <user>@<managed_node_ip> o This command securely copies your public key to the authorized_keys file, allowing passwordless SSH authentication. Managing Ansible Inventories In the world of Ansible, inventories are like the backbone of your automation infrastructure. They serve as a vital component that lists and organizes the managed nodes, making automation tasks seamless and organized. Inventories use a parent-child concept that allows you to create groupings, which come in handy during specific tasks or scenarios. Why Are Inventories Important? · Inventories provide a comprehensive list of managed nodes, making it easy to interact with and manage them. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 4 N33 nirf 1750 Rank

--- Page 81 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Credentials: Add credentials if required. ○ Branch Specifier: Specify the branch (e.g., */main). 7. Build Triggers: ○ Build Periodically: Use CRON syntax to schedule builds. ○ Poll SCM: Jenkins will check the repository for changes at specified intervals. ○ Other Triggers: Configure other triggers such as GitHub hooks, upstream projects, etc. 8. Build Environment: o Configure the build environment settings like setting environment variables, running scripts before the build, etc. 9. Build Steps: ○ Click on "Add Build Step" and choose the appropriate build step (e.g., "Execute Shell", "Invoke Ant", "Invoke Gradle script"). ○ Example: For executing a shell script, add the necessary shell commands. 10. Post-build Actions: ○ Click on "Add post-build action" and choose the appropriate action (e.g., "Archive the artifacts", "Publish JUnit test result report"). ○ Configure the settings for each post-build action. 11. Save the Configuration: ○ Once you have configured all the necessary settings, click "Save" at the bottom of the page. 12. Run the Job: ○ On the job's main page, click "Build Now" to run the job immediately. 13. Monitor the Job: ○ Click on the build number in the "Build History" to view the detailed output and logs of the build process. ○ Check the console output for any errors or warnings. Practical Example Setting up a Freestyle Project for a Maven Build: 1. Create New Item: ○ Name: MyMavenProject o Type: Freestyle project 2. Source Code Management: ○ Git: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 82 ---
Unit: III Course Code/Title:CS3V15/Devops I Repository URL: https://github.com/example/my-maven-project.git · Branch Specifier: * /main 3. Build Triggers: ○ Poll SCM: H/15 * (poll every 15 minutes) 4. Build Steps: ○ Invoke Top-Level Maven Targets: Goals: clean install 5. Post-build Actions: ○ Publish JUnit test result report: I Test report XMLs: * /target/surefire-reports/ *. xml 6. Save and Build: ○ Click "Save". ○ Click "Build Now" on the job's main page. By following these steps, you can set up a Jenkins job to automate your build process for various types of projects. Jenkins provides extensive customization options, allowing you to tailor the job to meet your specific requirements. Configuring a Jenkins Job 1. Install Jenkins First, make sure Jenkins is installed. You can download it from the official Jenkins website. 2. Access the Jenkins Dashboard Open your web browser and go to http://your-server-ip: 8080 to access the Jenkins dashboard. 3. Create a New Job 1. Start a New Job: ○ Click on "New Item" on the left-hand menu. 2. Name the Job: ○ Enter a name for your job in the "Enter an item name" field. 3. Select Job Type: ○ Choose the type of job (e.g., Freestyle project, Pipeline). CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) - TECHNOLOGY N32 nirf 7 1750 Rank

--- Page 75 ---
Course Code/Title:CS3V15/Devops Customization This tool is highly customizable as it supports a variety of IDE's . Languages supported Project Configuration Based on Goal of time to get used to it. available. It supports software development in Java, C, C++, and Groovy. For declaring the project configuration, it does not use the XML files. On the phases of the fixed and linear model. Graph of task dependencies that do the work. To add functionality in the project is the main goal of the Gradle. To finish the project in the given timeline is the main goal of the Maven. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY Unit: II This tool serves a limited number of developers and is not that customizable. It supports software development in Java, Scala, C# , and Ruby and it does not natively support C and C+ but can support through plugins like "maven-native- plugin" or we can integrate other build systems like CMake or Makefile. For declaring the project configuration, it uses the XML files. 34 N33 nirf 1750 Rank

--- Page 76 ---
Course Code/Title:CS3V15/Devops Unit: III Unit - III CONTINUOUS INTEGRATION USING JENKINS Install & Configure Jenkins, Jenkins Architecture Overview, creating a Jenkins Job, configuring a Jenkins job, Introduction to Plugins, Adding Plugins to Jenkins, commonly used plugins (Git Plugin, Parameter Plugin, HTML Publisher, Copy Artifact and Extended choice parameters). Configuring Jenkins to work with java, Git and Maven, creating a Jenkins Build and Jenkins workspace. Installing and Configuring Jenkins Overview Jenkins is a widely-used open-source automation server that helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration and continuous delivery (CI/CD). Below are detailed steps and considerations for installing and configuring Jenkins. Installation 1. System Requirements: ○ Operating System: Jenkins can run on any major operating system. ○ Java: Jenkins requires Java Runtime Environment (JRE) 8 or later. 2. Download Jenkins: ○ Download the latest version of Jenkins from the official Jenkins website. 3. Installation Steps: ○ Windows: I Run the .msi installer and follow the installation wizard. ■ After installation, Jenkins will start automatically as a Windows service. ○ Linux: Add Jenkins repository and import the GPG key. I Use package manager to install Jenkins (e.g., apt-get install jenkins). I Start Jenkins using systemctl (systemctl start jenkins). 4. Initial Setup: ○ After installation, Jenkins can be accessed through a web browser at http : //< your _server _ip_or_domain>: 8080. o Unlock Jenkins by entering the initial admin password, which can be found in the jenkins_home directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 1 N32 nirf 1750 Rank

--- Page 35 ---
Unit: I Course Code/Title:CS3V15/Devops Push: Push is the operation that sends your committed changes to a remote repository, making them accessible to others. Git workflow v0.1 v0.2 v1.0 Master Hotfix Release Develop Feature Feature Git Flow is a structured branching model designed for projects with well- defined release cycles and a need for strict quality control. Branches: The branching model described is commonly known as the Gitflow Workflow. It's a branching strategy that defines a strict branching model designed to facilitate collaboration and streamline the release process. Let's go into detail about each branch: clone operation Working Copy edit, add, move files update operation Modify working copy status and diff operation Review changes commit and push operation push operation Commit changes push operation amend and push operation Review changes 1. Master Branch: The `master'branch represents the main codebase and contains production- ready code. This branch is typically stable and should only include thoroughly tested and approved changes. Each commit on the 'master' branch represents a new version or release of the software. 2. Develop Branch: The 'develop' branch is an integration branch where various feature branches are merged. It serves as a staging area for testing new features and ensuring they work well together before merging into the 'master' branch. This branch may have ongoing development work and is not necessarily always in a production- ready state. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 35 nirf 1750 Rank N33

--- Page 36 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Feature Branches: Feature branches are created for developing new features or implementing changes. These branches are typically based on the 'develop'branch. Once a feature is complete, the branch is merged back into the 'develop' branch. Feature branches allow developers to work on specific tasks without affecting the main codebase. 4. Release Branch: The 'release' branch is created when the 'develop' branch reaches a point where it is ready for a production release. This branch is used for final testing, bug fixes, and preparing the code for deployment. No new features should be added to the release branch. Once the release is deemed stable, it is merged into both the 'master' branch and the 'develop' branch. 5. Hotfix Branch: The 'hotfix' branch is used to quickly address critical issues or bugs in the production code. It is created directly from the 'master' branch. Hotfixes are intended to be small and focused on resolving the specific issue at hand. Once the hotfix is complete, it is merged into both the 'master'branch and the `develop'branch to ensure that the fix is applied to future releases. Here is the typical flow: · Developers work on feature branches based on the 'develop' branch. · Completed features are merged into the 'develop' branch. · When ready for a release, a 'release' branch is created from 'develop". · The release branch undergoes testing and bug fixes. · The release branch is merged into both 'master' and 'develop' once it's stable. · If a critical issue arises in production, a 'hotfix' branch is created from 'master'. · The hotfix is merged into both 'master' and 'develop to keep both branches in sync. · This Gitflow Workflow helps maintain a structured development process, ensuring that features are developed, tested, and released in a controlled manner. Example Scenario: · Imagine you're working on a large software project with a team of 10 developers. You have a major release planned for every six months. · You create a "feature/Ticket-Id" branch to develop a new login system. · Once the feature is complete, it's merged into the "develop" branch for integration and testing. · As the release date approaches, you create a "release/v1.0" branch to freeze code for the upcoming release. · Any critical issues discovered in the production environment are fixed in "hotfix" branch and merged into "master" and "develop." CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 36 N33 nirf 1750 Rank

--- Page 19 ---
Course Code/Title:CS3V15/Devops Unit: I 5. Compliance: By compliance, we mean that certain businesses are required to follow specific regulations. Financial services companies in the United States, for example, must comply with the Sarbanes-Oxley Act, while healthcare, education, and energy companies must comply with HIPAA and other regulations. AWS provides a number of compliance-related features and services, such as data encryption and identity and access management, to help businesses meet these requirements. 6. High-Performance: Interested in delivering your applications quickly and efficiently? Taking advantage of AWS features such as auto-scaling and load balancing will help ensure your applications are always available and running optimally. AWS can help businesses improve their performance by offering a variety of cloud-based services, including Amazon Elastic Compute Cloud (EC2), which provides high-performance computing resources, and Amazon CloudFront, which delivers content quickly and securely to users around the world. Others include machine learning (ML) and analytics services, such as Amazon SageMaker and Amazon Athena. These services provide the tools businesses need to quickly and easily analyze their data for insights. Fast networking in the cloud is also possible with AWS, thanks to its Elastic Load Balancing (ELB) and Amazon Virtual Private Cloud (VPC). With ELB, businesses can balance their workloads across multiple instances for increased performance, while VPC allows businesses to create isolated private networks in the cloud. 7. Developer Tools: Developer tools are designed to make it easier for developers to create, deploy, and manage applications - and AWS provides developers with what they need to build applications quickly and easily. By leveraging developer tools, developers can save time and money by automating tedious tasks. They also benefit from access to AWS's extensive library of pre-built applications that can help them get their projects off the ground quickly. Services such as Amazon Elastic Beanstalk and Amazon CloudFormation can help them automate the process of creating and deploying applications. Other ways developers can improve productivity with AWS include using AWS Code Commit to store and manage source code. 8. Integration: Thanks to its many integrations with other Amazon services, as well as third-party services, AWS makes it easy for businesses to get started with cloud computing. AWS provides a wide range of services that can be easily integrated into existing business infrastructure. This allows businesses to add new features and capabilities without having to make major changes or invest in new hardware or software. For instance, if a business wants to add mobile capabilities to its website, it can take advantage of Amazon's Mobile SDK and Web Services. These tools allow businesses to quickly develop and deploy mobile apps that connect directly with their existing infrastructure. 9. Management Console: The AWS management console is a web-based interface that provides users with a simple way to interact with and manage their AWS resources - essentially a place where you can access and manage everything on the cloud. It provides a graphical view of all the resources associated with an account, as well as tools for creating and configuring new resources. Compared to traditional command- line interfaces, the AWS management console saves time and makes it easier for users to get the most out of their AWS services. Not only that, but your business gets access to 350+ free digital training courses through the AWS Academy, covering topics such as cloud fundamentals, DevOps, security, and big data. This means you can train your employees on how to use AWS, and in turn, help them become more efficient at their jobs. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 19 N32 nirf 1750 Rank

--- Page 20 ---
Course Code/Title:CS3V15/Devops Unit: I 10. Scalability: With an on-demand service, businesses can quickly spin up new servers as needed with just a few clicks. This makes it much easier to scale resources up or down as demand changes, allowing businesses to save costs and maintain performance even during peak periods. For example, if a business is expecting a sudden surge in traffic due to an advertising campaign or seasonal event, they can easily add more capacity to their server infrastructure to handle the increased load. Bru Textiles, a specialty textile company in Belgium, was able to quickly scale its infrastructure by leveraging AWS. Bru Textiles went digital to grow and offer new services. Embracing technology, they brought in digital twin technology to give their customers an idea of the texture and essence of their physical fabrics. AWS Applications · Storage and Backup: Storage and backup are important for any Cloud Computing service. AWS provides you with reliable storage services like Amazon Simple Storage Service to store large- scale data and backup services like AWS Backup to take backups of this data, which is stored in other AWS services. AWS stores the data in three different availability zones so that if one fails, you can still access your data. This makes AWS storage reliable and easily accessible. Therefore, companies with huge application data to store and backup securely can use AWS. · Big Data: One of the biggest challenges faced by companies these days is Big Data. The companies are struggling to store their large amounts of data using traditional methods. With AWS Big Data storage services, they can manage to store their data even if the data limit increases unexpectedly as AWS provides virtually unlimited data storage with scale-in and scale- out options. AWS offers easy access and faster data retrieval as well. For data processing, it offers services like EMR, with which the companies can easily set up, operate, and scale their big data. Therefore, efficiently storing and managing Big Data is among the top AWS applications. · Enterprise IT: AWS is a one-stop solution for any IT business. Many features of it such as secure storage, scalability, flexibility, and elasticity support companies to innovate faster than ever before. Using AWS for IT enterprises makes them profitable in terms of both money and time. As AWS maintains its cloud architecture, it need not waste time and money on professionals to do the same. · Social Networking: Social networking is essential for businesses in the present-day scenario where Digital Marketing is key, and it is easier with AWS. Companies can connect with customers and stakeholders and communicate through social networking sites and develop their business. Services like AWS social networking engine, which is powered by Turn Key GNU/Linux (HVM) AMI stack, are used for performance and scalability to help companies build a suitable social networking site and gain profits. · Mobile Apps: Mobile applications are embedded with day-to-day life. With AWS, you have the facility to create an app in your desired programming language. You can also keep up the applications that are consistently accessible and solid with high compute, storage, database, and application services. You can take advantage of AWS auto-scaling and managed relational database service for the better performance of your apps. · Websites: AWS offers a wide range of website hosting options to create the best website for customers. Its services like Amazon Light sail have everything, such as a virtual machine, SSD- based storage, data transfer, DNS management, and a static IP, to launch a website in such a way that the user can manage the website easily. Amazon EC2, AWS Lambda, Elastic Load Balancing, AWS Amplify, Amazon S3, etc. also help users build reliable and scalable websites. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 33 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between AWS (Amazon Web Services), Google Cloud, and Azure Technology EC2 (Elastic Compute Cloud) AWS Databases Supported Pricing Models Difficulties Many enterprises find it difficult to understand the company cost AWS fully relational and supports NoSQL databases and Big Data. Per hour - rounded up. On demand, reserved spot. structure. Google Cloud Google Engine(GCE) Compute Technologies pioneered by Google, like Big Query, Big Table, and Hadoop, are databases, and Big Data,naturally fully supported. Per minute - rounded up On demand sustained use. — Fewer services. features Azure VHD Disk) (Virtual Hard Azure supports relational and both NoSQL Windows and through AzureTable HDInsight. Per minute - rounded up. Per minute- rounded up commitments(Pre-paid or monthly) and Less "Enterprise-ready. Storage Services Simple Storage Service(S3) Elastic Block Storage. Elastic File storage. Machine Learing Sage maker. Lex. polly.And many more Blob Storage Queue Storage. File Storage Disk Storage. Data Lake Store Machine learning Azure Bot service Cognitive service GIT Cloud storage. Persistent Disk Transfer appliance. Cloud speech AI Cloud Video Intelligence. Cloud Machine learning engine Git is a distributed version control system (DVCS) that helps manage and track changes in source code during software development. It was created by Linus Torvalds in 2005 and has become one of the most widely used version control systems in the software development industry. Some key concepts and features of Git: · Version Control: Git allows developers to keep track of changes made to their code over time. This includes modifications, additions, and deletions of files. · Distributed System: Git is a distributed version control system, meaning that each developer has a complete copy of the entire repository, including its full history. This allows developers to work independently and merge their changes when necessary. · Branching: Git enables developers to create branches, which are essentially separate lines of development. This allows for the parallel development of features or bug fixes without affecting the main codebase. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) LEN 33 nirf 1750 Rank

--- Page 34 ---
Course Code/Title:CS3V15/Devops Unit: I · Merging: Git provides tools for merging changes from one branch into another. This is essential when multiple developers are working on different branches and need to bring their changes together. · History Tracking: Git maintains a complete history of changes made to the codebase. Developers can view, revert, or analyze changes made over time. · Remote Repositories: Git supports remote repositories, allowing developers to collaborate with others by pushing and pulling changes to and from a shared repository. Platforms like GitHub, GitLab, and Bitbucket provide hosting services for Git repositories. · Staging Area: Git uses a staging area (also known as the index) to prepare and review changes before committing them to the repository. This allows developers to selectively include or exclude specific changes. · Open Source: Git is an open-source tool, and its source code is freely available for modification and distribution. Various GIT Components: Git is composed of several key components that work together to enable version control and collaborative development. Here are the main components of Git: · Repository (Repo): A repository is a directory or storage space where your project and its version history are stored. It contains all the files and directories associated with your project, along with the metadata and configuration information. · Working Directory: The working directory is the directory on your local machine where you manipulate files and make changes to your project. It is essentially your local copy of the repository. · Index (Staging Area): The index, also known as the staging area, is a middle ground where changes are prepared before being committed to the repository. It allows you to selectively stage changes, which means you can choose which modifications to include in the next commit. · Commit: A commit is a snapshot of the changes made to the files in the repository. It represents a specific point in the project's history and is accompanied by a commit message that describes the changes. · Branch: A branch is a parallel line of development within a repository. It allows developers to work on different features or bug fixes simultaneously without affecting the main codebase. Branches can be merged to incorporate changes into other branches. . Head: HEAD is a reference to the latest commit in the currently checked-out branch. It essentially points to the tip of the branch you are currently on. · Remote: A remote is a version of the repository stored on a different server. Git supports collaboration by allowing developers to push and pull changes between their local repository and remote repositories. Platforms like GitHub, GitLab, and Bitbucket are examples of remote repositories. · Clone: Cloning is the process of creating a copy of a remote repository on your local machine. This allows you to start working on your own copy of a project. · Fetch: The fetch operation retrieves changes from a remote repository but does not automatically merge them into your working directory. It is useful for reviewing changes before deciding to merge. · Pull: Pull is a combination of fetch and merge. It retrieves changes from a remote repository and automatically merges them into your working directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 7 ---
Course Code/Title:CS3V15/Devops Unit: I DevOps Tools 1. Git (GitLab, GitHub, Bitbucket) Git remains indispensable in software development and DevOps due to its pivotal role in version control, collaborative coding, and efficient project management. As technology has accelerated, the need for streamlined and organized code management has never been greater. Git empowers developers to collaborate on codebases, effortlessly creating and merging branches for new features and bug fixes. Its distributed nature ensures developers can work seamlessly offline, an increasingly valuable feature in today's remote and distributed work environments. Additionally, Git facilitates the tracking of code modifications, making it easier to identify when and why specific changes were made, a critical aspect of maintaining code quality and security. Software development is essential in driving innovation and advancing progress, and Git maintains its prominent position as the bedrock of efficient, cooperative, and secure coding methodologies. 2. Maven Due to its enduring significance in managing project dependencies, building, and project lifecycle management, Maven remains a pivotal tool in SD and DevOps. As a robust build automation and project management tool, Maven simplifies the complexities of Java-based project development by streamlining the compilation, testing, packaging, and distribution processes. It ensures consistent and reproducible builds, making it easier for development teams to collaborate efficiently and deliver high-quality software. Maven's role in managing dependencies and facilitating continuous integration and deployment remains crucial. Its ability to handle complex build scenarios and integrate seamlessly with modern DevOps practices makes it indispensable for ensuring software projects' reliability, maintainability, and scalability in 2024 and beyond. 3. Jenkins Its importance lies in its role as a powerful automation server that enables continuous integration and continuous delivery (CI/CD) pipelines. Jenkins streamlines software development by automating tasks such as building, testing, and deploying code changes, ensuring that software is delivered quickly and highly. With the growing complexity of modern applications, the need for efficient CI/CD processes has become even more paramount. Jenkins provides flexibility, extensibility, and a vast library of plugins that cater to a wide range of technologies and tools, making it adaptable to diverse development environments. As organizations prioritize speed, reliability, and collaboration in their software development practices, Jenkins stands as a cornerstone tool, enabling teams to achieve seamless automation and efficient delivery of software solutions. 4. Chef Chef, a powerful automation platform, is crucial in managing infrastructure as code. Chef empowers organizations to achieve scalability, reliability, and speed seamlessly. By allowing the automation of server provisioning, configuration, and maintenance, Chef enhances efficiency and consistency across the entire infrastructure, reducing manual errors and ensuring that infrastructure remains desired. Moreover, Chef integrates smoothly with various cloud providers, containerization technologies, and other DevOps tools, making it adaptable to the ever-evolving tech TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 7 N32 nirf 1750 Rank

--- Page 8 ---
Course Code/Title:CS3V15/Devops Unit: I landscape. As organizations prioritize agility and scalability, Chef remains a vital tool in automating complex infrastructure tasks and enabling DevOps teams to focus on innovation and delivery. 5. Puppet Puppet is essential because it simplifies the management and orchestration of complex IT infrastructures by allowing administrators to define infrastructure as code. It ensures consistency and repeatability in configuration across servers, cloud instances, and containers. Businesses increasingly rely on diverse, dynamic, and hybrid infrastructures. Puppet's importance lies in its ability to streamline provisioning, configuration, and continuous compliance, thus reducing operational complexity, minimizing errors, and accelerating software delivery. Puppet continues to empower organizations to efficiently manage and scale their infrastructure while maintaining high levels of security and compliance, making it a crucial tool for DevOps teams. 6. Ansible Ansible is a powerful and widely adopted automation and configuration management tool important in 2024 for several reasons. This tool stands out for its simplicity and versatility. It empowers organizations to automate repetitive tasks, provisioning of infrastructure, and configuration management across diverse environments, making it an invaluable asset for DevOps and IT teams. Furthermore, Ansible's agentless architecture, declarative language, and a vast library of pre- built modules make it accessible to both beginners and seasoned professionals. As organizations prioritize efficiency, scalability, and the rapid deployment of applications and services, Ansible remains an indispensable DevOps toolkit, helping teams streamline operations, enhance security, and maintain infrastructure at scale, all while reducing manual errors and increasing agility in a fast-paced technological landscape. 7. Docker Docker is crucial in modern software development and DevOps practices. It can simplify and streamline the management of applications across various environments. Docker containers encapsulate an app and its dependencies, ensuring consistent and reproducible deployments from development to production. This technology enhances portability and scalability, accelerates development cycles, and reduces the "it works on my machine" problem. In a rapidly evolving software landscape, Docker's containerization approach remains crucial for achieving efficient, isolated, and highly flexible application deployment, making it an essential component of DevOps and continuous delivery pipelines. 8. Kubernetes Kubernetes, often abbreviated as K8s, play a central role in modern software development and operations. Its importance lies in its ability to orchestrate, manage, and automate containerized applications at scale. As organizations increasingly embrace microservices architectures and containerization for their applications, Kubernetes provides the essential infrastructure for deploying, scaling, and maintaining these containers efficiently. The tool's resilience, self-healing capabilities, and support for hybrid and multi-cloud environments make it vital for achieving agility, reliability, and cost-effectiveness in application deployment. It serves as the backbone of cloud-native ecosystems, enabling organizations to accelerate software delivery, improve resource utilization, and respond effectively to the evolving demands of the digital landscape. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N32 nirf 1750 Rank

--- Page 27 ---
Course Code/Title:CS3V15/Devops Unit: I · Servers are maintained easily and there is nearly zero downtime · Users can access cloud data and upload it on the cloud from any device with a solid internet connection · Cloud environments can be modified according to the user's requirements and is easily accessible · Clouds are highly secure, making data breaches more unlikely · Migrating to the cloud eliminates the need to buy on-premises infrastructure . It offers pay-as-you-go pricing, meaning you only pay for the resources you use 1. Infrastructure as a Service: IaaS delivers virtualized computing resources over the Internet. Users can rent virtual machines, storage, and networking infrastructure, allowing for easy scalability without investing in physical hardware. Examples include AWS EC2 and Azure Virtual Machines. 2. Platform as a Service: PaaS offers a robust platform for developers to build, deploy, and manage apps without worrying about the underlying infrastructure. It simplifies application development and deployment, with services like Google App Engine and Heroku leading the way. 3. Software as a Service: SaaS offers software applications on a subscription basis, accessible via a web browser. Users don't need to install or maintain software locally, making it ideal for collaboration tools (e.g., Microsoft 365, Google Workspace) and CRM systems (e.g., Salesforce). 4. Function as a Service: FaaS allows developers to execute code responding to events without managing servers. It's highly scalable and cost-efficient, exemplified by AWS Lambda and Azure Functions. FaaS is also known as serverless computing. 5. Container as a Service: CaaS enables the deployment and management of containerized applications using orchestration tools like Kubernetes. It provides portability and scalability for applications across different cloud environments. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 27 N33 nif 1750 Rank

--- Page 28 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between main cloud computing services Terms Stands for Uses Access Model Technical understanding. Popularity Percentage rise Usage Cloud services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY IAAS Infrastructure as a service. IAAS is used by network architects. IAAS gives access to the resources like virtual machines and virtual storage. It is a service model that provides virtualized computing resources over the internet. It requires technical knowledge. It is popular among developers and researchers. It has around a 12% increment. Used by the skilled developer to develop unique applications. Amazon Web Services, sun, vCloud Express. PAAS Platform as a service. PAAS is used by developers. PAAS gives access to run time environment to deployment and development tools for application. It is a cloud computing model that delivers tools that are used for the development of applications. Some knowledge is required for the basic setup. It is popular among developers who focus on the development of apps and scripts. It has around 32% increment. Used by mid-level developers to build applications. Facebook, and Google search engine. SAAS Software as a service. SAAS is used by the end user. SAAS gives access to the end user. It is a service model in cloud computing that hosts software to make it available to clients. There is no requirement about technicalities company handles everything. It is popular among consumers and companies, such as file sharing, email, and networking. It has about a 27 % rise in the cloud computing model. Used among the users of entertainment. MS Office web, Facebook and Google Apps. 28 N32 nirf 1750 Rank

--- Page 107 ---
Course Code/Title:CS3V15/Devops Unit: III Step 8 : Then configure the Build Steps by selecting the option Copy artifacts from another project as follows - . . First of all give the name of the project from which you want to copy the files. In my case it is App1. . Then select the option "copy from WORKSPACE of latest completed build" · Give the regular expression ** / *.*. Here's a breakdown of what ** / *.* means : o ** : The double asterisk ( ** ) is a wildcard that matches zero or more directories and subdirectories. It's often used to recursively search through all directories and subdirectories. o / : The forward slash (/) is used as a directory separator, typically on Unix-like systems. o *.* : This part of the regular expression is used to match files with an extension. In regular expressions, the * symbol represents "zero or more" of the preceding character or pattern, and the . character is used to match any character. So, *.* essentially matches any file with any extension. So, when you use the regular expression ** / *.* in a Jenkins build configuration, you are instructing Jenkins to include all files with any extension ( *.* ) located in any directory and subdirectory ( ** ) within the workspace or specified location. Build Steps E Copy artifacts from another project Project name ? App: Which build ? Copy from WORKSPACE of latest completed buld Limitation Note ? Artifacts to copy ? Artifacts not to copy ? : Toget directory CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 32

--- Page 108 ---
Course Code/Title:CS3V15/Devops Unit: III Add one more Build Step and select Execute Windows batch Command. Give the command dir so that the name of files and directories that are copied from App1 to App2 will be displayed on the Console. = Execute Windows batch command ? Command See the list of available environment variables ---- dir ----- Advanced v Add build step * Click Apply and Save. Step 8 : Now Build this project (App2). The output will be as follows - Started by user Anuradha P Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\.jenkins\workspace\App2 Copied 16 artifacts from "App1" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621109248227895.bat C:\ProgramData\Jenkins\,jenkins\workspace\App2>dir Volume in drive C Is OS Volume Serial Number is CGC2-BA9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023* 15:07 <DIR> 01-09-2023 15:07 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 474 test.class 144 test. java 2 File(s) 616 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 33 N32 narf 1750 Rank

--- Page 153 ---
Create a pipeline in azure What is pipeline Azure Pipelines supports continuous integration (CI)and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline. The latest way to build pipelines is with the YAML pipeline editor. You can also use Classic pipelines with the Classic editor. code release build CI plan continuous testing CD deploy monitor operate Create your first pipeline This is a step-by-step guide to using Azure Pipelines to build a sample application from a Git repository. This guide uses YAML pipelines configured with the YAML pipeline editor. If you'd like to use Classic pipelines instead, see Define your Classic pipeline. For guidance on using TFVC, see Build TFVC repositories. Prerequisites - Azure DevOps A GitHub account where you can create are pository. An Azure DevOps organization. Create one for free. If your team already has one, then make sure you're an administrator of the Azure Devops. 12

--- Page 154 ---
Get the Java sample code To get started, fork the following repository into your GitHub account. https://github.com/MicrosoftDocs/pipelines-java Create your first Java pipeline 1. Sign-in to your Azure DevOps organization and go to your project. 2. Go to Pipelines, and then select New pipeline. 3. Do the steps of the wizard by first selecting GitHub as the location of your source code. 4. You might be redirected to GitHub to sign in. If so, enter your GitHub credentials. 5. When you see the list of repositories, select your repository. 6. You might be redirected to GitHub to install the Azure Pipelines app. If so,select Approve & install. 7. Azure Pipelines will analyze your repository and recommend the Maven pipeline template. 8. When your new pipeline appears, take a look at the YAML to see what it does. When you're ready, select Save and run. 9. You're prompted to commit a new azure pipelines.yml file to your repository. After you're happy with the message, select Save and run again. If you want to watch your pipeline in action, select the build job. You just created and ran a pipeline that we automatically created for you, because your code appeared to be a good match for the Maven template. You now have a working YAML pipeline (azure-pipelines.yml) in your repository that's ready for you to customize! 14

--- Page 159 ---
Choose a job to see the steps for that job From the < Jobs in run #20191 ... FabrikamFiber Build O Build Build 40s Initialize job 1s 2 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 Duration: 40s 6 Pool: Azure Pipelines Image: Ubuntu-16.04 7 > Job preparation parameters Checkout 3s CmdLine 25 Component Detect 32s Post-job: Checkout <1s Finalize Job <1s Deploy DeployWeb 10s Finalize build Report build status <1s steps view, you can review the status and details of eachstep. From the Moreactions you can toggle timestamps or view a raw log of all steps in the pipeline. Build 9 Pool: Azure Pipelines 2 Image: Ubuntu-22.04 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 View job raw log Toggle timestamps 6 Duration: 40s Job preparation parameters 10

--- Page 160 ---
Cancel and re-run a pipeline If the pipeline is running, you can cancel it by choosing Cancel. If the run has completed, you can re-run the pipeline by choosing Run new. Pipeline run more actions menu: #20191210.3 Update azure-pipelines.yml for Azure Pip ... on FabrikamFiber Cancel Stages Jobs € Build C Deploy 0/1 compléted 48s Not started Build 48s Cancel From the More actions menu you can download logs, add tags, edit the pipeline, delete the run, and configure retention for the run. cure Pipe ... Run new Download logs Add tags Edit pipeline View retention leases Retain Delete 20

--- Page 155 ---
10. When you're ready to make changes to your pipeline, select it in the Pipelines page, and then Edit the azure-pipelines.yml file. View and manage your pipelines You can view and manage your pipelines by choosing Pipelines from the left-hand menu to go to the pipelines landing page. Azure DevOps : FabrikamFiber 1 Pipelines 0 Search F FabrikamFiber + Pipelines New pipeline Overview Recent All Runs Y Filter pipelines Boards Repos Pipelines Pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... La 32m ago & Manually triggered 8º main & 42s Environments Releases FabrikamFiber #20191209.3 · Set up CI with Az ... La 1h ago & Manually triggered & main 1m 13s Library Task groups *** Deployment groups From the pipelines landing page you can view pipelines and pipeline runs, create and import pipelines, manage security, and drill down into pipeline and run details. Choose Recent to view recently run pipelines (the default view), or choose All to view all pipelines. Pipelines New pipeline : Recent All Runs Filter pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... & Manually triggered & main Là 36m ago ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Az ... Là 1h ago 1m 13s & Manually triggered & main 15

--- Page 156 ---
Select a pipeline to manage that pipeline and view the runs. Select the build number for the last run to view the results of that build, select the branch name toview the branch for that run, or select the context menu to run the pipeline and perform other management actions. Recently run pipelines Pipeline Last run #20191209.2 · Set up CI with Azure Pip ... > pipelines-dotnet-core 8 Manually triggered 12º main Là 1h ago * ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Azure Pip ... 試 2h 8 Manually triggered 8 main Edit in Run pipeline Manage security Rename/move Delete Select Runs to view all pipeline runs. You canoptionally filter the displayed runs. Pipelines Recent All Runs Filter by keywords State V Repository V New pipeline Requested forV Tags X All pipeline runs Description Stages Set up CI with Azure Pipelines & #20191209.2 on pipelines-dotnet-core & main d4964 .. Set up CI with Azure Pipelines #20191209.1 on pipelines-dotnet-core åº main d4964 ... Set up CI with Azure Pipelines & #20191209.3 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines & #20191209.2 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines #20191209.1 on FabrikamFiber & main 2b4b23c La 44m ago 42s La 53m ago ₾ 46s Là 1h ago 0 1m 13s Ld 1h ago 1m 555 Lo 1h ago 1m 6s 16

--- Page 141 ---
Course Code/Title: CS3V15/Devops Unit: V UNIT V - BUILDING DEVOPS PIPELINES USING AZURE Create Github Account, Create Repository, Create Azure Organization, Create a new pipeline, Build a sample code, Modify azure-pipelines.yaml file Github Account Creation What is Github? GitHub is a code hosting platform for collaboration and version control. GitHub lets you and others work together on projects fromanywhere. Github is owned by Microsoft, provides access to public(free) and private(paid) repositories. Stepsto create Github Account: Join GitHub · GitHub × + < C https://github.com/join f Facebook - Log In o ... Twitter G Google f Facebook YouTub Product v Solu wiki How Step 1:Go to https://github.com/join in a web browser. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 1 175" Rank

--- Page 142 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: Enter your personal details. In addition to creating a username and entering an email address, you'll also have to create a password. Your password must be at least 15 characters in length or at least 8 characters with at least one number and lowercase letter. Join GitHub First, let's create your user account Username * wikihowneveconcepts Email address * Password * .......... Make sure it's at least 15 characters OR at least 8 characters including a number and a lowercase letter. Learn more. Email preferences Send me occasional product updates, announcements, and offers. Verify your account wiki How Step 3: Click Verify to start the verification puzzle. The instructions vary by puzzle, so just follow the on-screen instructions to confirm that you are a human. A green checkmark will appear after completing the puzzle. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 2

--- Page 143 ---
Unit: V Course Code/Title: CS3V15/Devops Email preferences Send me occasional product updates, announcements, and offers. Verify your account Pick one square that shows two identical objects. wiki How Step 4: Click the green Create account button. It's belowthe form, at the bottom of the page. This will take you to an email verification page. Verify your account Create account By creating an account, you agree to the Terms of Service. For more information about GitHub's privacy practices, see the GitHub Privacy Statement. We'll occasionally send you account-related wikiHow UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 3

--- Page 144 ---
Course Code/Title: CS3V15/Devops Unit: V Carefully review the Terms of Service at https://help.github.com/en/articles/github- terms-of-service and the Privacy Statement at https://help.github.com/en/articles/github- privacy-statement before you continue. You're almost done! We sent a launch code to wikihowneveconcepts@gmail.com + Enter code Didn't get your email? Resend the code or update your email address. wiki How Step 5: Verify your email by entering the code. After clicking Create account, you'll receive an email with a code.Enter this code on the verification page. Entering the code will automatically take you to the welcome page. Step 6: Select your preferences and click Continue. GitHub displays a quick survey that can help you tailor your experience to match what you're looking for. You'll be sent tothe plan selection page after completing the survey. How many team members will be working with you? This will help us guide you to the tools that are best suited for your projects. Just me 2 - 5 5 - 10 10 - 20 20 - 50 50+ Are you a student or teacher? Student Teacher Continue wikiHow CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 4 175° Rank ---

--- Page 139 ---
Course Code/Title:CS3V15/Devops Unit: IV Create the file, if it does NOT exist (or) absent $ ansible testservers -a "touch /tmp/testfile creates=/tmp/testfile" -i ansible_hosts As Playbook - name: "Validate if a file is present or not present using Ansible Command module" hosts: testservers tasks: - name: "Create a file if it does not exist" command: "touch /tmp/latestfile" args: creates: "/tmp/latestfile" register: createif - name: "Display the file to make sure its created" command: "ls -lrt /tmp/latestfile" register: displayif when: createif is changed - debug: var=displayif.stdout - name: "Remove the file if it exist" command: "rm -rf /tmp/latestfile" args: removes: "/tmp/latestfile" register: removeif In the playbook we execute three tasks, • First one is a create a file if it does not exist • Second to display if the creation is successful • Third one is to delete the file if it exists Example 5: Execute or Run the Script when a file exists or not exists Now for this example let us take something relatable to real world scenario. Like Start the Server instance (or) program if the PID file or LOCK file does not exist Here is the sample playbook for you to get started. I am giving a general playbook here as I do not want to pick some server/technology of my own which you cannot relate to. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 140 ---
Course Code/Title:CS3V15/Devops Unit:IV you can feel free to modify the script to your needs as this is just a template. - name: Start of Stop Server instance based on PID/LOCK file availability hosts: appservers tasks: - name: Start the instance when the PID file is not present become: yes become_user: appuser command: "startserver.sh" args: creates: "/path/to/pid/instance.pid" register: startinst - name: Stop the instance when the lock file is present become: yes become_user: appuser command: "stopserver.sh" args: removes: "/path/to/lockfile/instance.lck" register: stopinst CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 26 N33 nirf 1750 Rank

--- Page 127 ---
Course Code/Title:CS3V15/Devops Unit:IV Abbreviation You can also use abbreviation to represent lists. Example Countries: ['America', 'China', 'Canada', 'Iceland'] List inside Dictionaries We can use list inside dictionaries, i.e., value of key is list. Example james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english … List of Dictionaries We can also make list of dictionaries. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english - robert: name: robert richardson rollNo: 53 div: B sex: male CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 13 N32 nirf 1750 Rank

--- Page 128 ---
Course Code/Title:CS3V15/Devops Unit:IV likes: - biology - chemistry … YAML uses "|" to include newlines while showing multiple lines and ">" to suppress newlines while showing multiple lines. Due to this we can read and edit large lines. In both the cases intendentation will be ignored. We can also represent Boolean (True/false) values in YAML. where boolean values can be case insensitive. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english result: maths: 87 chemistry: 45 biology: 56 physics: 70 english: 80 passed: TRUE messageIncludeNewLines: | Congratulation !! You passed with 79% messageExcludeNewLines: > Congratulation !! You passed with 79% CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 14 N33 nirf 1750 Rank

--- Page 15 ---
Course Code/Title:CS3V15/Devops Unit: I which is a mechanism to control the usage of resources or capacity. 2) Code: Many good practices such as Git enables the code to be used, which ensures writing the code for business, helps to track changes, getting notified about the reason behind the difference in the actual and the expected output, and if necessary reverting to the original code developed. The code can be appropriately arranged in files, folders, etc. And they can be reused. 3) Test: The application will be ready for production after testing. In the case of manual testing, it consumes more time in testing and moving the code to the output. The testing can be automated, which decreases the time for testing so that the time to deploy the code to production can be reduced as automating the running of the scripts will remove many manual steps. 4) Plan: DevOps use Agile methodology to plan the development. With the operations and development team in sync, it helps in organizing the work to plan accordingly to increase productivity. 5) Monitor: Continuous monitoring is used to identify any risk of failure. Also, it helps in tracking the system accurately so that the health of the application can be checked. The monitoring becomes more comfortable with services where the log data may get monitored through many third-party tools such as Splunk. 6) Deploy: Many systems can support the scheduler for automated deployment. The cloud management platform enables users to capture accurate insights and view the optimization scenario, analytics on trends by the deployment of dashboards. 7) Operate: DevOps changes the way traditional approach of developing and testing separately. The teams operate in a collaborative way where both the teams actively participate throughout the service lifecycle. The operation team interacts with developers, and they come up with a monitoring plan which serves the IT and business requirements. 8) Release: Deployment to an environment can be done by automation. But when the deployment is made to the production environment, it is done by manual triggering. Many processes involved in release management commonly used to do the deployment in the production environment manually to lessen the impact on the customers. DevOps Lifecycle DevOps defines an agile relationship between operations and Development. It is a process that is practiced by the development team and operational engineers Learning DevOps is not complete without understanding the DevOps lifecycle phases. The DevOps lifecycle includes seven phases as given below: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 16 ---
Unit: I Course Code/Title:CS3V15/Devops together from beginning to the final stage of the product. Development Operations Integration Deployment DevOps Lidecycle Feedback 2) Continuous Integration Testing Monitering 1) Continuous Development This phase involves the planning and coding of the software. The vision of the project is decided during the planning phase. And the developers begin developing the code for the application. There are no DevOps tools that are required for planning, but there are several tools for maintaining the code. This stage is the heart of the entire DevOps lifecycle. It is a software development practice in which the developers require to commit changes to the source code more frequently. This may be on a daily or weekly basis. Then every commit is built, and this allows early detection of problems if they are present. Building code is not only involved compilation, but it also includes unit testing, integration testing, code review, and packaging. The code supporting new functionality is continuously integrated with the existing code. Therefore, there is continuous development of software. The updated code needs to be integrated continuously and smoothly with the systems to reflect changes to the end-users. Commit Build O O Test O Stage Deploy Dev/QA O O > Continuous Integration/Delivery Development </> git Code Commit = production server. 3) Continuous Testing Production Jenkins is a popular tool used in this phase. Whenever there is a change in the Git repository, then Jenkins fetches the updated code and prepares a build of that code, which is an executable file in the form of war or jar. Then this build is forwarded to the test server or the This phase, where the developed software is continuously testing for bugs. For constant testing, automation testing tools such as TestNG, JUnit, Selenium, etc are used. These tools allow QAs to test multiple code- bases thoroughly in parallel to ensure that there is no flaw in the functionality. In this phase, Docker Containers can be used for simulating the test environment. It may occur in the form of documentation files or maybe produce large-scale data about the application parameters when it is in a continuous use position. The system errors such as server not reachable, low memory, etc are resolved in this phase. It maintains the security and availability of the service. Automation testing saves a lot of time and effort for executing the tests instead of doing this manually. Apart from that, report generation is a big plus. The task of evaluating the test cases INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 narf 1750 Rank

--- Page 145 ---
Course Code/Title: CS3V15/Devops Unit: V Step 7:Note the types of plans offered by GitHub. There are a few different plans to choose from, varying in the amount of features provided. Learn to ship software like a pro. GitHub gives students free access to the best developer tools so they can learn by doing. Free Get additional student benefits GitHub Pro Unlimited public/private repositories 2,000 CI/CD minutes/month Free for public repositories 500MB of Packages storage Free for public repositories Protect your branches Ensure that collaborators on your repository cannot make irrevocable chang branches. O Draft pull requests O 120 core-hours of Codespaces compute O Pages and Wikis 15GB of Codespaces storage O Community support ® 3,000 CI/CD minutes/month Free for public repositories 2GB of Packages storage Free for public repositories O 180 core-hours of Codespaces compute 20GB of Codespaces storage Web-based support wikiHow Step 8: Select the free plan. On the plan selection page, scrolldown to click the button for choosing a free plan. This will immediately take you to your GitHub dashboard. Continue for free wiki How UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 5 N33 In 175º Rank ---

--- Page 146 ---
Course Code/Title: CS3V15/Devops Unit: V · If you choose a paid plan, you'll have to enter yourpayment information as requested before you can continue. . If you want to upgrade your Github account in the future, click the menu at the top- right corner, select Settings, andchoose Billing and plans to view your options. GitHub essentials are: · Repositories · Branches · Commits · Pull Requests · Git (the version control software GitHub is built on) Repository: A GitHub repository can be used to store a developmentproject. It can contain folders and any type of files (HTML, CSS,Javascript, Documents, Data, Images). A GitHub repository should also include a licence file and a README file about the project. A GitHub repository can also be used to store ideas, or any resources that you want to share. Github Repository Creation Step 1: Click on the new repository option Marketplace Explore New repository and GitHub without any code! Catch Universe Join a community Universe in San Fr ticket pricing-he Import repository × New gist bird New organization New project llo World guide, you'll create a repository, start a , write comments, and open a pull request. GitHub Sponsors Matching Fund × Ready to support open source? GitHub will match your contribution to developers during their first year in GitHub Sponsors. d the guide Start a project × Welcome to the new dashboard. Get closer to the stuff you care about most. Explore repositories atlas-engineer/next Next browser - Be productive. UnENVIVAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 175° Rank 6

--- Page 97 ---
Course Code/Title:CS3V15/Devops Unit: III Build Steps Execute Windows batch command ? Command See the list of available environment variables javac welcome. java java welcome Advanced = Execute Windows batch command 7 Command See the list of avalable chuironment variables acho User name is Nuernane% Advanced Step 5 : Click on Apply and then Save. Step 6 : Select Build with parameters option. Dashboard > welcome > Status Changes Workspace Build with Parameters Configure Delete Project 0 Rename CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 22 N3 nirf 1750 Rank

--- Page 98 ---
Course Code/Title:CS3V15/Devops Unit: III Project welcome This build requires parameters: Username Parth D Build Cancel We can change the username as well. Suppose I changed the Username as Anand then click the Build, we get the console output as follows - C: \ProgramData\Jenkins\.jenkins\workspace\welcome>javac welcome.java C:\ProgramData\Jenkins\.jenkins\workspace\welcomeyjava welcome Welcome to first Job of Jenkins [welcome] $ cmd /c call C:\WINDOWS\TEMP\jenkins16582464065687933173.bat C:\ProgramData\Jenkins\.jenkins\workspace\welcome>echo User name is Anand User name is Anand C:\ProgramData\Jenkins\.jenkins\workspace\welcome>exit 0 Finished: SUCCESS Thus we set variety of parameters to the Jenkins job. We can set the Boolean parameter, choice parameter, credentials, file parameter, password parameter and so on. 3. HTML Publisher Filter Plain Boolean Parameter Choice Parameter Credentials Parameter File Parameter Multi-line String Parameter Password Parameter Run Parameter String Parameter Add Parameter Overview: The HTML Publisher Plugin allows Jenkins to publish HTML reports and artifacts generated during the build process. Key Features: · Report Publishing: Publishes HTML reports, dashboards, or other HTML artifacts. · Report Directory: Allows specifying the directory containing HTML reports. · Index Page: Sets a default HTML file to be displayed when accessing the report. Configuration: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 23 nirf 1750 Rank N33

--- Page 137 ---
Course Code/Title:CS3V15/Devops Unit:IV I Example 2: Get the Hostname and Version of remote servers with UNAME we have used the command module and executing an uname -a command as AD-HOC command $ ansible testservers -m command -a "uname -a" -i ansible hosts As Playbook - name: Check the remote host Hostname, Version, Distribution with UNAME hosts: testservers tasks: - name: Execute the UNAME command register: unameout command: "uname -a" - debug: var: unameout.stdout lines I Example 3: Check the Disk Usage of Remote server To get the disk usage, we are using df -h , here -h is human readable As Ad Hoc command $ ansible testservers -m command -a "df -h" -i ansible hosts As Playbook - name: Check the disk usage of all the file system in the remote servers hosts: testservers tasks: - name: Execute the df command register: dfout command: "df -h" - debug: var: dfout.stdout lines CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 138 ---
Course Code/Title:CS3V15/Devops Unit:IV Example 4: Restart Apache Server using Ansible Command Module So far, In all the sample we have seen we are using one application server and one web server. Totally two servers. But in this example, we need to limit our execution only to the web server as we are going to check the restart the apache web server. this is done using -- limit parameter As Ad hoc command $ ansible testservers -m command -a "httpd -k restart" -i ansible_hosts -b - limit As Playbook - name: restart apache web server hosts: testservers tasks: - name: restartapache register: httpdresout become: yes command: "httpd -k restart" when: ansible_hostname == "mwiweb02" - debug: var: httpdresout.stdout_lines I Example 5: Execute a command when a file exists or not exists There are two most useful parameters in ansible command module such as removes and creates removes - used to tell ansible to Execute the command only if the file exist creates - used to tell ansible to Execute the specified command only if the file does not exist Here we are going to do a very simple file creation and removal based on the file availability or existence. In General using the ansible command module, This method of creating and removing the file is not recommended as Ansible has a dedicated module named file to do the same effortlessly. As Ad hoc command Remove the file, if it does exist (or) present $ ansible testservers -a "rm -rf /tmp/testfile removes=/tmp/testfile" -i ansible_hosts CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 24 N33 nirf 1750 Rank

--- Page 65 ---
Course Code/Title:CS3V15/Devops Unit: II Output The output is as follows - java version "1.8.0 66" Java(TM) SE Runtime Environment (build 1.8.0_66-b18) Java HotSpot(TM) 64-Bit Server VM (build 25.66-b18, mixed mode) Step 2 - Download Gradle Build File Download the latest https://gradle.org/install/. version of Gradle from the link available at Step 3 - Set Up Environment for Gradle Setting up environment means, we have to extract the distribution file and copy the library files into proper location. Set up GRADLE_HOME and PATH environmental variables. This step is platform dependent. In Windows Extract the downloaded zip file named gradle-2.11-all.zip and copy the distribution files from Downloads\gradle-2.11\ to C:\gradlel location. After that, add the C: \gradle and C: \gradlelbin directories to the GRADLE_HOME and PATH system variables. Follow the given instructions - Right Click On My Computers -> Click On Properties -> Advanced System Settings -> Click On Environmental Variables. There you will find a dialog box for creating and editing system variables. Click on new button for creating GRADLE_HOME variable (follow the left side screenshot). Click on Edit for editing the existing Path system variable (follow the right side screenshot). Follow the below given screenshots. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 24 N33 nirf 1750 Rank

--- Page 66 ---
Course Code/Title:CS3V15/Devops System Properties X Computer Name Hardware Advanced System Protection Remote Environment Variables x Unit: II System Properties × Computer Name |Hardware Advanced System Protection Remote Environment Variables × New System Variable × Variable name: GRADLE_HOME| Variable value: C:\grade Cancel System variables Variable Value ComSpec C:\Windows\system32\and.exe FP_NO_HOST_C ... NO GRADLE_HOME E:\work\22- Orientdb \gradle-2.11 Variable name: Path Variable value: #C:\gradle bin - System variables Variable Value Edit System Variable × OK Cancel < ORIENTDB_HOME E:\work\22- Orientdblorientdb-communi ... " JAVA_HOME C:\Program Files\Java\jdk1.7.0_60 New ... Edt ... Delete OK Cancel Step 4 - Verify the Gradle installation In windows os Windows_NT Path C:\oraclexe \app \prade\product\11.2.0\, .. PATHEXT .COM ;. EXE ;. BAT ;. CMD ;. VBS ;. VBE ;. JS ;.... v New ... Edit ... Delete OK Cancel You can execute the following command in command prompt. C:\> gradle -v Output Here you will find the Gradle version. Gradle 2.11 Build time: 2016-02-08 07:59:16 UTC Build number: none Revision: 584db1c7c90bdd1de1d1c4c51271c665bfcba978 Groovy: 2.4.4 CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 25 N33 nirf 1750 Rank

--- Page 151 ---
Azure DevOps Taking you to your Azure DevOps organization ... Congratulations, you're an organization owner! Sign in to your organization at any time, https://dev.azure.com/{yourorganization}. With your organization, the following aspects are included in the free tier: First five users free (Basic license): Azure Pipelines: o One Microsoft-hosted CI/CD (one concurrent job, upto 30 hours per month) o One self-hosted CI/CD concurrent job Azure Boards: Work item tracking and Kanban boards Azure Repos: Unlimited private Git repos Azure Artifacts: Two GB free per organization Build applications with Azure: Azure DevOps enables you to build, test, and deploy anyapplication to any cloud or on premises To configure build pipelines that continuously build, test, and verify your applications. 11

--- Page 152 ---
Part 1: Get started with Azure DevOps Part 2: Build applications with Azure DevOps Part 3: Deploy applications with Azure DevOps Create a build pipeline with Azure Pipelines: Prerequisites: Familiarity with forking and cloning a GitHub repo Account requirements: An Azure DevOps organization o To use Microsoft-hosted agents, your Azure DevOps organization must have access to Microsoft- hosted parallel jobs. Check your parallel jobs and request a free grant. O You can use GitHub Code spaces to complete the module, even if your Azure DevOps organization doesn't have any parallel jobs. A GitHub account Software requirements: If using GitHub Code spaces to complete the module, there are no software requirements as all software is included in the Codespace If using a local development environment withMicrosoft-hosted agents, you must have the following software installed: o Visual Studio Code o .NET 6.0 SDK o Git 12

--- Page 135 ---
Course Code/Title:CS3V15/Devops Unit: IV Syntax for writing the module on command line While writing the module we use the options such as -m and -a. The -m is for module and -a is for argument, after the -a option we specify the executable command in double quotes. here 'all' stands for all the remote hosts, otherwise we can mention the group name of the hosts 1 $ ansible all -m command -a "echo 'Hello hosts'" The -m option followed by module name 'command' Command to be executed on remote hosts, given in double quotes Commonly used modules 1. Command : Following is a command module that executes a command on remote hosts $ ansible all -m command -a "echo 'Hello hosts'" 2. User : The user module is used to create, modify or delete users. For example - To create a user named 'new_user' on all the hosts in the 'dev' group using ansible, you can use the user module. Here's the ansible command to do that : $ ansible dev -m user -a "name=new_user state=present" Sometimes the above command fails as there is no privilege to create user on remote host. Then we can issue the command as follows $ ansible all -m user -a "name=new_user state=present" -- become -- ask-become-pass The above command asks for the password for the remote host to login. On supplying the password the user will be created. We can verify. if new user is getting created or not on remote host by issuing following command on each remote host $ less /etc/passwd If we want to create new user without asking for password on all the remote hosts then we can issue the command as - $ ansible all -m user -a "name=new_user1 state=present" -- become -b The -b option is used to become superuser. By this the sudo or su privileges can be granted. .3. Ping : The ping module is used to connectivity to remote hosts $ ansible all -m ping 4. Copy : The copy module is used to copy files from control machine to remote hosts $ ansible dev -m copy -a "src=/home/ansible_user/myfile.txt dest=/home/ansible_user/myfile.txt" The above command will copy the myfile.txt file from control node to all the managed nodes of dev group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 21 N32 nirf 1750 Rank

--- Page 136 ---
Course Code/Title:CS3V15/Devops Unit: IV 5. Package : The package module is used to install, remove or upgrade the packages, For example, the following command installs the nginx web server package : $ ansible package -a "name=nginx state=present" Here name indicates the name of the package to be installed, the state=present means install the package. Normally "present" is for installation, "remove" is for removing the package and "latest" is used to upgrade the package. 6. Shell : The shell module is used to run shell commands with more flexibility on remote hosts. For example - Following shell command executes on remote hosts. $ ansible dev -m shell -a "cmd=is" 7. Service : The service module is used to start, stop or restart a specific service on remote hosts. For example - Following service will start the apache web service on all the remote hosts. $ ansible dev -m service -a "name=apache2 state=started" Note that the state can started for starting the service, stopped for stopping the service. o started/stopped are idempotent actions that will not run commands unless necessary. o restarted will always bounce the service. o reloaded will always reload. I Example 1: Get the Uptime of remote servers We have used command module to run the uptime command and we have given both the ad hoc and the playbook form of execution. as AD-HOC Command $ ansible testservers -m command -a uptime -i ansible_hosts as Playbook - name: Check the remote host uptime hosts: testservers tasks: - name: Execute the Uptime command over Command module register: uptimeoutput command: "uptime" - debug: var: uptimeoutput.stdout_lines CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 22 N32 nirf 1750 Rank

--- Page 149 ---
Course Code/Title: CS3V15/Devops Unit: V Step 6: Now you will see that all of our files uploaded in our github. Namanbhatia7 Add initial resume website files Latest commit f47be5b 1 minute ago Display_Picture.png Add initial resume website files 1 minute ago My_Resume.html Add initial resume website files 1 minute ago README.md Initial commit 15 minutes ago Styles.css Add initial resume website files 1 minute ago Branch: · A GitHub branch is used to work with different versions of a repository at the same time. · By default a repository has a master branch (aproduction branch). · Any other branch is a copy of the master branch (as itwas at a point in time). · New Branches are for bug fixes and feature work separate from the master branch. When changes are ready, they can be merged into the master branch. If youmake changes to the master branch while working on a new branch, these updates can be pulled in. Commits: At GitHub, changes are called commits. Each commit (change) has a description explaining why achange was made. Pull Requests : . Pull Requests are the heart of GitHub collaboration. · With a pull request you are proposing that your changesshould be merged (pulled in) with the master. · Pull requests show content differences, changes, additions, and subtractions in colors (green and red). · As soon as you have a commit, you can open a pull request and start a discussion, even before the code isfinished. Git: · Git was created by Linus Torvalds in 2005 to develop Linux Kernel · Git is an open-source distributed version control system. It is designed to handle minor to major projectswith high speed and efficiency. . It is developed to co-ordinate the work among the developers. The version control allows us to track andwork together with our team members at the same workspace. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 9 175" Rank

--- Page 150 ---
CREATE AZURE ORGANIZATION AZURE: Microsoft Azure, often referred to as Azure cloud computing platform run by Microsoft. It offers access, management, and the development of applications and services through global data centers It also provides a range of capabilities, including software as a service (SaaS), platform as a service, and infrastructure as a service (IaaS). It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014 Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Prerequisites: To plan your organizational structure. Microsoft accounts or authenticate users with Microsoft Entra ID. For more information, see Choosing your organization administrator account type. Create an organization: 1. Sign in to Azure DevOps. 2. Select New organization 3. Confirm information, and then select Continue 10

--- Page 163 ---
‹ FabrikamFiber Runs Branches Analytics Description Edit Run pipeline : Stages #20210609.3 Update azure-pipelines.yml for Azure Pipelines Manually triggered for main 9 61bba8c x2 La Wednesday Ø 215 #20210609.2 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for 1º main 9 bc6c105 x La Wednesday ₾ 305 #20210609.1 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for main 9 53eb92b x • #20210520.1 Updated README.md Individual CI for & new-branch 0 1ada387 ‹ azure-cli-example go main V azure-cli-example / azure-pipelines.yml 30 pool: default 31 32 steps: 33 # Specify python version and install if needed Settings 34 - task: UsePythonVersion@0 35 condition: false 36 inputs: 37 -versionSpec: '3.x' 38 architecture: 'x64' 39 40 # Update pip to latest 41 - bash: python -m pip install -- upgrade pip 42 condition: false 43 displayName: 'Upgrade pip' 44 45 container 46 &continueOnError 47 48 88988 name & parameters & resources & schedules 49 50 51 services 52 strategy 53 & variables 54 & workspace La Wednesday .455 Là May 20 ₾ 135 Variables Run : Tasks 7 Search tasks dotnet .NET Core Build, test, package, or publish a dotnet applicatio. Android signing Sign and align Android APK files Ant Build with Apache Ant 80 App Center distribute Distribute app builds to testers and users via Visu .. App Center test Test app packages with Visual Studio App Center Archive files Compress files into .7z, tar.gz on zip ARM template deployment Deploy an Azure Resource Manager (ARM) templ .. Azure App Service deploy Deploy to Azure App Service a web, mobile, or AP. · The YAML pipeline editor provides several keyboard shortcuts, which we show in the following examples. . Choose Ctrl+Space for Intellisense support while you're editing the YAML pipeline.

--- Page 164 ---
· The task assistant provides a method for adding tasksto your YAML pipeline. · To display the task assistant, edit your YAML pipeline and choose Show assistant. Understand the azure-pipelines.yml file · A pipeline is defined using a YAML file in your repo. Usually, this file is named azure-pipelines.yml and is located at the root of your repo. · Navigate to the Pipelines page in Azure Pipelines, select the pipeline you created, and choose Edit in the context menu of the pipeline to open the YAMLeditor for the pipeline. · This pipeline runs whenever your team pushes a change to the main branch of your repo or creates a pull request. It runs on a Microsoft-hosted Linux machine. · The pipeline process has a single step, which is to run the Maven task. YAML:Code trigger: - main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdk Version: "1.10" jdk11_windows: imageName: "windows-latest" jdk Version: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdk VersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: " ** /TEST -*. xml"goals: "package" Change the platform to build · Navigate to the editor for your pipeline by selectingEdit pipeline action on the build, or by selecting Edit from the pipeline's main page. · To choose a different platform like Windows or Mac, change thevmImage value: · pool: · vmImage: "windows-latest" Add steps . You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can usetasks for building, testing, publishing, or deploying your app. For Java, the

--- Page 131 ---
Course Code/Title:CS3V15/Devops Unit: IV Ensure a service is stopped: $ ansible webservers -m service -a "name=httpd state=stopped" 6. Gathering Facts Fact represents the discovered variables about a system. You can use the facts to implement conditional execution of tasks, and also used to get ad-hoc information about your systems. To see all the facts: $ ansible all -m setup Ansible Playbooks Playbooks are the files where the Ansible code is written. Playbooks are written in YAML format. YAML means "Yet Another Markup Language," so there is not much syntax needed. Playbooks are one of the core features of Ansible and tell Ansible what to execute, and it is used in complex scenarios. They offer increased flexibility. Playbooks contain the steps which the user wants to execute on a particular machine. And playbooks are run sequentially. Playbooks are the building blocks for all the use cases of Ansible. Ansible playbooks tend to be more configuration language than a programming language. Through a playbook, you can designate specific roles to some of the hosts and other roles to other hosts. By doing this, you can orchestrate multiple servers in very different scenarios, all in one playbook. Playbook Structure Each playbook is a collection of one or more plays. Playbooks are structured by using Plays. There can be more than one play inside a playbook. Playbook Play Task Module Target Environment O Task Module Play Task Target Environment Play Module O Task CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY 17 N33 nirf 1750 Rank

--- Page 132 ---
Course Code/Title:CS3V15/Devops Unit:IV name: install and configure DB hosts: testServer become: yes vars: oracle_db_port_value : 1521 tasks: -name: Install the Oracle DB yum: < code to install the DB> -name: Ensure the installed service is enabled and running service: name: < your service name> The Different YAML Tags name This tag specifies the name of the Ansible playbook. As in what this playbook will be doing. Any logical name can be given to the playbook. hosts This tag specifies the lists of hosts or host group against which we want to run the task. The hosts field/tag is mandatory. It tells Ansible on which hosts to run the listed tasks. The tasks can be run on the same machine or on a remote machine. One can run the tasks on multiple machines and hence hosts tag can have a group of hosts' entry as well. vars Vars tag lets you define the variables which you can use in your playbook. Usage is similar to variables in any programming language. tasks All playbooks should contain tasks or a list of tasks to be executed. Tasks are a list of actions one needs to perform. A tasks field contains the name of the task. This works as the help text for the user. It is not mandatory but proves useful in debugging the playbook. Each task internally links to a piece of code called a module. A module that should be executed, and arguments that are required for the module you want to execute. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 18 N33 nirf 1750 Rank

--- Page 157 ---
Select a pipeline run to view information about that run. You can choose to Retain or Delete a run from the context menu. For more information on run retention, see Build and release retention policies. Stages La 51m ago > ... 6 42s Retain Delete Ld 2h ago 0 1m 13s - View pipeline details The details page for a pipeline allows you to view and manage that pipeline. Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. Azure DevOps · Search F 4 FabrikamFiber Edit Run pipeline : + Runs Branches Analytics Y Description Stages Ac #20191209.3 Set up CI with Azure Pipelines Manually triggered { main 2b4b23c #20191209.2 Set up CI with Azure Pipelines & Manually triggered & main 2b4b23c Là 54m ago Č 1m 13s Là 55m ago Č 1m 55s #20191209.1 Set up CI with Azure Pipelines Individual CI 1º main 2b4b23c Lô 56m ago 1m 6s 17

--- Page 158 ---
Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. View pipeline run details From the pipeline run summary you can view the status of your run, both while itis running and when it is complete. #20191210.2 Update azure-pipelines.yml for Azure Pipe ... on FabrikamFiber Summary Environments Triggered by Steve Danielson FabrikamFiber & main b2f795e Today at 12:56 PM Duration: 1m 9s Tests: Get started Changes: ¢ 2 commits Work items: ₾ 1 linked Artifacts: 1 published Stages Jobs Build 1 job completed 415 Run new : Deploy 1 job completed 13s 1 artifact From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, test results, and work Items From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, testresults, and work items Jobs and stages The jobs pane displays an overview of the status of your stages and jobs. This pane may have multiple tabs depending on whether your pipeline has stages and jobs, or just jobs. In this example, the pipeline has two stages named Build and Deploy. You can drill down into the pipeline steps by choosing the job from either the Stages or Jobs pane. 10

--- Page 99 ---
Unit: III Course Code/Title:CS3V15/Devops 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "HTML Publisher Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Post-build Actions section. ○ Select Publish HTML reports. ○ Specify the directory containing HTML reports and the index page. Usage: . Ideal for displaying build reports such as test results or code coverage metrics in a user-friendly HTML format. DEMO: Step 1 : We will create a Maven project using Selenium and TestNG framework. Open Eclipse : IDE and Create a Maven Project. Select the Create Simple Project (Skip archetype selection) and enter the Group Id and Artifact Id as per your choice. My project name is SeleniumDemo. Step 2 : Add the Selenium dependency in your pom.xml file as follows - <dependency> <groupId>org.seleniumhq.selenium</groupId> <artifactId> selenium-java</artifactId> <version>3.141.59</version> </dependency > Save the pom.xml file. Step 3 : Now install testNG plugin. For that, from Menu select Help-> Eclipse Marketplace Help Welcome 7 Help Contents Search Show Context Help Show Active Keybindings ... Ctrl+Shift+L Tip of the Day Tips and Tricks ... Cheat Sheets ... @ Eclipse User Storage Open the Eclipse Marketpla ** Perform Setup Tasks ... % Check for Updates Install New Software ... Eclipse Marketplace ... = About Eclipse IDE · Donate Contribute Type TestNG in the search window. The testNG installation option will be available. click Install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 24 N32 nirf 1750 Rank ---

--- Page 100 ---
Course Code/Title:CS3V15/Devops Unit: III C Eclipse Meketplace × Eclipse Marketplace Select solutions to initiit, Prese leuralt Now to proceed with lutallation. Press the "more info" link to learn more about a solution Search Recent Popular Favorites itwled - Research at the Eclipse × All Categories Go TestNG for Eclipse . This plug-in late you run your TextNG wits from Erfiore. You can run sulter, groupe er individuel methods. Errors are reported in a pepwane tab that lets you. more info by Chat Bent Apache 20 tantog zielt testingunit integration functional selection * 737 ** * stal: 1,83M (25,021 last month) Install MoreUnit 3.3.0 Morelinit is an Solipoe plugin that should mult you in wilting mory unk with. It supports ali programming languages (witching between teds and clanset under. mone.info Install Infinitest 5.4.0.3 Infinitest is a continuont tout runner for tova,'and is valuable to developers valing a Marketplaces (7) Cancel Then confirm the selected features by clicking on Confirm button. x Confirá Selected Features These Confirm to gorthus with the induction. Cs go back te chocie bis sules it CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 25 N3 nirf 1750 Rank

--- Page 101 ---
Course Code/Title:CS3V15/Devops Unit: III v => SeleniumDemo src/main/java > src/main/resources > src/test/java > 15 src/test/resources > al JRE System Library [J2SE-1.5] › al Maven Dependencies » drivers v src > @ main v fs test v java v 2> myPackage 10 myTestPage.java 23 resources › › target > > test-output 2 pom.xml > Servers myTestPage.java package myPackage; import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver; import org.testng.annotations.Test; public class myTestPage { @Test public void myFun() { System.setProperty("webdriver.chrome.driver","./drivers/chromedriver.exe"); WebDriver driver = new ChromeDriver(); driver.get("https://google.com"); String title = driver.getTitle(); System.out.println("Hey Anuradha, the tile of this page is: "+title); driver.quit(); } Program explanation : In above program, we have written one test case by the function myFun. In this function, using the Chromedriver which we have installed, we open the web page google.com Then we try to get the tile of this page and display it on the console using System.out statement. Finally we quit the driver. Step 7 : Right click on the Project name in Project Explorer window, Run As-> Maven Test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 26 N3 nirf 1750 Rank

--- Page 102 ---
Unit: III Course Code/Title:CS3V15/Devops Step 8 : Now, locate the Java file myTestPage.java and right click Run As->TestNG Test. It is as follows - 0 Run As # Debug As Profile As Web Services > > 1 TestNG Test > Run Configurations ... > Alt+Shift+X, N Step 9 : We will get the output as follows - Hey Anuradha, the tile of this page is: Google PASSED: myFun Default test Tests run: 1, Failures: 0, Skips: 0 Default suite Total tests run: 1, Failures: 0, Skips: 0 Step 10 : Now, we will open the Jenkin's page and create the HTML Publisher report for the above SeleniumDemo project. The prerequisite to this activity is that the HTML Publisher report plugin must be installed in Jenkins. · Click on New Item, Enter the Item Name and select as Freestyle project, I have given the name HTMLReportDemo. · Then click on configure. Write the suitable description in General section. · The in Build Steps and Add Build steps as Invoke top-level Maven targets provide the Maven version, and under goals write clean test. . Under Post Build Actions, click on Add Post Build Action and select Publish HTML Report. Now give the path of index.html file which is created by Selenium testNG framework in Maven. It is the path of test-output folder. Also give some suitable HTML report title. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 27 N3 nirf 1750 Rank

--- Page 123 ---
Course Code/Title:CS3V15/Devops Unit: IV Ad-Hoc Limitations: While ad-hoc commands are ideal for single, immediate tasks, they are not designed for complex, multi- step automation. Playbooks come to the rescue when tasks have interdependencies and need to be executed in a coordinated manner. Readability and Reusability: Playbooks are authored in YAML, a human-readable and straightforward format. This not only makes them easy to write but also facilitates sharing and collaboration. You can reuse playbooks across various scenarios, saving time and effort. Conditional and Looping Logic: Playbooks offer advanced features, including conditional statements and looping, that allow you to adapt automation to different situations. This flexibility makes playbooks versatile and capable of handling a wide range of automation needs. In essence, playbooks are your tool of choice when automation tasks become multi-faceted and require a structured and logical approach. They provide the power to streamline and automate complex workflows with precision and efficiency. Example: Ping All Inventory Devices To illustrate the use of a playbook, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: playbook.yaml 1 --- 2 -name : My First Play #Name of play 3 hosts: virtualmachines #defining host can be all 4 tasks: 5 -name:Ping My Hosts #Name of task 6 ansible.builtin.ping: # can also write ping To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 124 ---
Course Code/Title:CS3V15/Devops Unit: IV Output and Result: Activities Terminal - Nov 2 12:29 . O auriga@auriga-Latitude-E7470: - /Desktop/ansible X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook.eg1.yaml PLAY [My First Play] TASK [Gathering Facts] ok: [ vri01] TASK [Ping Devices] ok: [vm01] PLAY RECAP vn01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$[] rescued=0 ignored=0 X As you can see there is gathering facts task which we didn't create is running it is the default task which ping the connection. Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: playbook.yaml 1 --- 2- name: Install Nginx 3 hosts: all 4 become: yes 5 tasks: 6 - name: Install Nginx using apt 7 apt: 8 name: nginx 9 state: present To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml -- ask-become-pass CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 10 N33 nirf 1750 Rank

--- Page 125 ---
Course Code/Title:CS3V15/Devops Unit: IV This command executes the specified playbook while prompting for the necessary privilege escalation password. Output and Result: Activities Terminal - Nov 2 13:00 . auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook-eg2.yaml PLAY [Install Nginx] TASK [Gathering Facts] Total: [vnet]: FAILEDT .. ['msg": "Missing sudo password') PLAY RECAP : ok=0 changed=0 unreachable=0 Fatled =! skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory. yaml playbook-eg2.yaml - - ask-becone-pass BECOME password: PLAY [Install Nginx] TASK [Gathering Facts] ok: [vri01] TASK [Install Nginx using apt] ok: [vri01] PLAY RECAP * VMD1 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ [] As shown in the example image above, the first command encountered an error due to the absence of the -ask-become-pass flag. This flag is essential when your task requires elevated privileges, as it prompts Ansible to request the sudo password for authentication. The corrected command includes the -ask-become-pass flag, ensuring that the necessary privileges are obtained before executing the task. This is particularly important when working with tasks that require administrative access, such as package installations or system configurations. Note: In this blog, we primarily focus on the core and fundamental components of Ansible, providing an introductory overview and understanding of its key concepts. While Ansible offers advanced features such as "Collections," "Ansible Galaxy," "Roles," "Dynamic Inventory," and "Custom Modules and Plugins," we recognize that these topics are extensive and may require separate dedicated discussions. For the purpose of this blog, we aim to establish a strong foundation of Ansible's core components. If you're interested in diving deeper into these advanced topics, we recommend exploring Ansible's official documentation and additional resources dedicated to each subject. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 11 N32 nirf 1750 Rank

--- Page 126 ---
Course Code/Title:CS3V15/Devops Unit:IV Understanding YAML Ansible uses YAML syntax for expressing Ansible playbooks. This chapter provides an overview of YAML. Ansible uses YAML because it is very easy for humans to understand, read and write when compared to other data formats like XML and JSON. Every YAML file optionally starts with " --- " and ends with " ... ". In this section, we will learn the different ways in which the YAML data is represented. key-value pair YAML uses simple key-value pair to represent the data. The dictionary is represented in key: value pair. Note - There should be space between : and value. Example: A student record --- #Optional YAML start syntax james: name: james john rollNo: 34 div: B sex: male ... #Optional YAML end syntax Abbreviation You can also use abbreviation to represent dictionaries. Example James: {name: james john, rollNo: 34, div: B, sex: male} Representing List We can also represent List in YAML. Every element(member) of list should be written in a new line with same indentation starting with "- " (- and space). Example countries: - America - China - Canada - Iceland CHENNAI INSTITUTE OF TECHNOLOGY CHENNAI INSTITUTE . TECHNOLOGY (Autonomous) 12 N33 nirf 1750 Rank

--- Page 165 ---
Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. Customize CI triggers Pipeline triggers cause a pipeline to run. You can use trigger: to cause a pipeline to run whenever you push an update to a branch. YAML pipelines are configured by default with a CI trigger on your default branch (which is usually main). You can set up triggers for specific branches or for pull request validation. For a pull request validation trigger, just replace the trigger: step with pr: as shown in the two examples below. By default, the pipeline runs for each pull request change. . If you'd like to set up triggers, add either of the following snippets at the beginning of your azure-pipelines.yml file. YAML I Copy trigger: - main - releases/* YAML Copy pr: - main - releases/* You can specify the full name of the branch (for example, main) or a prefix-matching · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. Build using multiple versions Add steps You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can use tasks for building, testing, publishing, or deploying your app. For Java, the Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. YAML Copy - task: PublishCodeCoverageResults@1 inputs: codeCoverageTool: "JaCoco" summaryFileLocation: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco/ reportDirectory: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco" failIfCoverageEmpty: true · Select Save and then confirm the changes. . You can view your test and code coverage results by selecting your build and going to the Test and Coverage tabs.

--- Page 166 ---
Build using multiple versions Rename/move pipeline Name ScheduledTriggerTest 1234 Select folder 1 .. . Cancel Save Pipeline settings X Processing of new run requests Enabled Paused Disabled YAML file path azure-pipelines.yml Automatically link work items included in this run Cancel Save YAML trigger: main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdkVersion: "1.10" jdk11_windows: imageName: "windows-latest" jdkversion: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdkVersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: **** /TEST -*. xml" goals: "package"

--- Page 129 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible ad-hoc Commands The Ansible ad-hoc command uses the /usr/bin/ansible command-line tool to automate a single task on one or more managed nodes. The Ad-hoc commands are quick and easy, but they are not re-usable. The Ad-hoc commands demonstrate the simplicity and power of Ansible. Syntax ansible <hosts> [-m <module_name>] -a <"arguments"> -u <username> [ -- become] Explanation Hosts: It can be an entry in the inventory file. For specifying all hosts in the inventory, use all or "*" . module_name: It is an optional parameter. There are hundreds of modules available in the Ansible, such as shell, yum, apt, file, and copy. By default, it is the command. Arguments: We should pass values that are required by the module. It can change according to the module used. Username: It specifies the user account in which Ansible can execute commands. Become: It's an optional parameter specified when we want to run operations that need sudo privilege. By default, it becomes false. 1. Parallelism and shell commands You can reboot your company server in 12 parallel forks at the same time. For this, you need to set up the SSHagent for connection. $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa To run reboot for all your company servers in the group, 'abc', in 12 parallel forks: $ ansible abc -a "/sbin/reboot" -f 12 By default, Ansible will run the above ad-hoc commands from the current user account. If you want to change then pass the username in ad-hoc command as follows: $ ansible abc -a "/sbin/reboot" -f 12 -u username 2. File Transfer You can use ad-hoc commands for doing SCP (secure copy protocol) which means lots of files in parallel on multiple machines or servers. CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 15 N32 nirf 175º Rank

--- Page 130 ---
Course Code/Title:CS3V15/Devops Unit:IV Transferring file on many machines or servers $ ansible abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf" Creating new directory $ ansible abc -m file - a "dest = /path/user1/new mode = 888 owner = user group = user1 state = directory" Deleting all directory and files $ ansible abc -m file -a "dest = /path/user1/new state = absent" 3. Managing Packages Ad-hoc commands are available for apt and yum module. Here are the following ad-hoc commands using yum. Below command checks, if the yum package is installed or not, but not update it. $ ansible abc -m yum -a "name = demo-tomcat-1 state = present" Below command checks the package is not installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = absent" And below command checks the latest version of package is installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = latest" 4. Managing Users and Groups You can manage, create, and remove a user account on your managed nodes with ad-hoc commands. $ ansible all -m user -a "name=foo password =< crypted password here>" $ ansible all -m user -a "name=foo state=absent" 5. Managing Services Ensure a service is started on all the webservers. $ ansible webservers -m service -a "name=httpd state=started" Alternatively, restart a service on all webservers: $ ansible webservers -m service -a "name=httpd state=restarted" CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 nirf 1750 Rank

--- Page 133 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible Roles Roles provide a framework for fully independent or interdependent collections of files, tasks, templates, variables, and modules. The role is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks and makes them easier to reuse. The breaking of the playbook allows you to break the playbook into reusable components. Each role is limited to a particular functionality or desired output, with all the necessary steps to provide that result either within the same role itself or in other roles listed as dependencies. Roles are not playbooks. Roles are small functionality that can be used within the playbooks independently. Roles have no specific setting for which hosts the role will apply. Top-level playbooks are the bridge holding the hosts from your inventory file to roles that should be applied to those hosts. Creating a Role The directory structure for roles is essential to creating a new role, such as: Role Structure The roles have a structured layout on the file system. You can change the default structured of the roles as well. For example, let us stick to the default structure of the roles. Each role is a directory tree in itself. So the role name is the directory name within the /roles directory. $ ansible-galaxy -h Usage ansible-galaxy[delete|import|info|init|install|list|login|remove|search|setup][ -- help] [options] ... Options o -h: (help) it shows this help message and exit. o -v: (verbose) Verbose mode (-vvv for more, -vvvv to enable connection debugging). o -- version: it shows program version number and exit. Roles are stored in separate directories and have a particular directory structure CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 19 N33 nirf 1750 Rank

--- Page 134 ---
Course Code/Title:CS3V15/Devops Unit: IV Typical structure of ansible role myweb defaults L main.yml files · handlers L main.yml meta. L main.yml README.md tasks L_ main.yml templates tests inventory test.yml vars L main.yml · defaults : It stores the default variable for the role. For example the default port number is http is 8080, then it can be stored in defaults. · files : This folder contains the files required to transfer or deploy to the target machines. · handlers : It contains handlers, which may be used by some role. · meta : It defines some data or information about the role. · tasks : It contains the main list of tasks to be executed by the role. These tasks might be defined in separate files as per the functionalities. · templates : It contains the templates which can be deployed via a role. Ansible Module Ansible Command module is used to execute commands on a remote node. The Command module, is used mostly to run simple Linux commands on a remote node/server which is part of a host group or Stand alone server mentioned in the host group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 121 ---
Course Code/Title:CS3V15/Devops Unit: IV · - m ping specifies the "ping" module, a basic module that checks the reachability of the managed nodes. . - i inventory.yaml points to the inventory file containing the list of devices. · The "ping" module sends a test command to the target devices and reports their status, confirming whether they are responsive. Output and Result: auriga@auriga-Latitude-E7470 :- /Desktop/ansibleŞ ansible all -[ inventory.yaml -m ping VOOS | UNREACHABLET => ( "changed's false. "mg's "Failed to connect to the host via s "unreachable": true auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ton denied (publickey_ password) In the above example the ssh host is not reachable. Activities Terminal Nov 2 12:13 . auriga@auriga-Latitude-E7470: - /Desktop/ansible C auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -i inventory.yaml -m ping vn81 | SUCCESS => 'ansible_facts": { "discovered_interpreter_python": "/usr/bin/python3" "changed": false, "ping": "pong" auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ In the above output image it is working fine Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: 1ansible all -i inventory.yaml -m apt -a "name=nginx state=present" -- become -- ask-become-pass · - m apt specifies the "apt" module, which is responsible for package management. · - a "name=nginx state=present" includes variables for the module to act upon, such as specifying that Nginx should be installed (state=present). · - become signifies that the command should run with elevated privileges (sudo). · - ask-become-pass prompts for the sudo password of the managed node to ensure the installation proceeds smoothly. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 7 N32 nirf 1750 Rank

--- Page 122 ---
Course Code/Title:CS3V15/Devops Unit:IV Output and Result: Activities Terminal * Nov 2 12:24 · auriga@auriga-Latitude-E7470: - /Desktop/ansible F X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -[ inventory.yaml -m apt -a "name=ansible state=present" -- become - - ask-become-pass BECOME password: vm01 | CHANGED => { "ansible_facts": { "discovered_interpreter_python": * /usr/bin/python3" "cache_update_time": 1698985056, "cache_updated": false, "changed": true, "stderr": "" 'stderr_Lines": []. stdout": "Reading package lists ... \nBuilding dependency tree ... \nReading state information ... \nThe following packages were automatically installed and are no longer required:\n gtr1.2-goa-1.0 hplip-data libcgt-fast-perl libcgt-pm-perl\n libdouble-conversion3 Libevent-core-2.1-7 libevent-pthreads-2.1-7\n \ibfcgt-perl libfprint-2-todi libfwupdplugin1 libht ml-template-perl\n Libpcre2-16-0 libqtScoresa libqtsdbuss libqtsguis libqt5network5\n libqtSpositionings libqt5printsupports libqt5qm15 Libqtsquick5\n libqtSsensors5 libqt5svg5 Li bqt5webchannel5 libqt5webkit5 libqt5widgets5\n Libxcb-xinerama@ Libxcb-xinpute Libxmlbi printer-driver-postscript-hp\n python3-renderpm python3-reportlab python3-reportlab-accel\n qt5-gtk-platformthene qttranslations5-lien shim\nUse 'sudo apt autoremove' to remove them. InThe following additional packages will be installed:\n teee-data python3-argcomplete pyt hon3-crypto python3-dnspython\n python3-jinja2 python3 . jmespath python3-kerberos python3-libcloud\n python3-netaddr python3-ntin-auth python3-requests-kerberos\n python3-requests- ntlm python3-selinux python3-winrm python3-xmltodict\nSuggested packages: \n cowsay sshpass python- jinja2-doc ipython3 python-netaddr-docs\nThe following NEW packages will be install ed:\n ansible leee-data python3-argcomplete python3-crypto python3-dnspython\n python3- jinja2 python3-jmespath python3-kerberos python3 . Libcloud\n python3-netaddr python3-ntlm-aut h python3-requests-kerberos\n python3-requests-ntIm python3-selinux python3-winrm python3-xmltodict\n0 upgraded, 16 newly installed, 0 to remove and 50 not upgraded. InNeed to get 97 25 kb of archives. \nAfter this operation, 98.6 MB of additional disk space will be used. \nGet:1 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-jinja2 all 2.10.1-2 [95.5 KB]\nGet:2 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-crypto amd64 2.6.1-13ubuntu2 [237 KB]\nGet:3 http://in.archive.ubuntu.com/ubuntu focal-updates/main amd64 pyt hon3-dnspython all 1.16.0-1ubuntu1 [89.2 KB]\nGet:4 http://in.archive.ubuntu.com/ubuntu focal/main amd64 ieee-data all 20180805.1 [1589 KB]\nGet: 5 http://in.archive. ubuntu.com/ubuntu focal-updates/main amd64 python3-netaddr all 0.7.19-3ubuntu1 [236 KB]\nGet:6 http://in.archive.ubuntu.com/ubuntu focal/universe and64 ansible all 2.9.6+dfsg-1 [5794 KB]\nGet:7 http: //\n.archive.ubuntu.com/ubuntu focal/universe amd64 python3-argcomplete all 1.8.1-1.3ubuntu1 [27.2 kB]\nGet:8 http://in.archive.ubuntu.com/ubuntu focal-updates/main and64 python3- jne spath all 0.9.4-2ubuntu1 [21.5 kB]\nGet:9 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-kerberos amd64 1.1.14-3.1build1 [22.6 k8]\nGet: 10 http://in.archive.ubuntu. con/ubuntu focal/universe amd64 python3-libcloud all 2.8.0-1 [1483 KB]\nGet:11 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-ntlm-auth all 1.1.0-1 [19.6 KB]\nGet: 1 2 http://in.archive.ubuntu.com/ubuntu focal/universe and64 python3-requests-kerberos all 0.12.0-2 [11.9 KB]\nGet: 13 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-r equests-ntlm all 1.1.0-1 [6004 B]\nGet: 14 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-selinux amd64 3.0-1build2 [139 KB]\nGet : 15 http://in.archive.ubuntu.com/ubu ntu focal/universe and64 python3-xmltodict all 0.12.0-1 [12.6 KB]\nGet:16 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-winrm all 0.3.0-2 [21.7 KB]\nFetched 9725 k B in 7s (1454 KB/s)\nSelecting previously unselected package python3-jinja2.\r\n(Reading database ... \r(Reading database ... 5*\r (Reading database .. . 10%\r(Reading database ... 15% \r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database . .. 45%\r (Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 68%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading da tabase ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 212542 files and directories currently installed.)\r\nPreparin g to unpack ... /00-python3-jinja2_2.10.1-2_all.deb ... \r\nUnpacking python3-jinja2 (2.10.1-2) ... \r\nSelecting previously unselected package python3-crypto. \r\nPreparing to unpack .. ./01-python3-crypto_2.6.1-13ubuntu2_amd64.deb ... \r\nUnpacking python3-crypto (2.6. 1-13ubuntu2) ... \r\nSelecting previously unselected package python3-dnspython. \r\nPreparing to unpa ck ... /02-python3-dnspython_1.16.0-lubuntu1_all.deb ... \r\nUnpacking python3-dnspython (1. 16.0-1ubuntu1) ... \r\nSelecting previously unselected package leee-data. \r\nPreparing to unp ack ... /03-ieee-data_20180805.1_all.deb ... \r\nUnpacking teee-data (20188805.1) ... \r\nSelecting previously unselected package python3-netaddr. \r\nPreparing to unpack ... /04-python3. netaddr_0.7.19-3ubuntu1_all.deb ... \r\nUnpacking python3-netaddr (0.7.19-3ubuntu1) ... \r\nSelecting previously unselected package ansible. \r\nPreparing to unpack .. . /05-ansible_2.9.6 +dfsg-1_all.deb ... \r\nUnpacking ansible (2.9.6+dfsg-1) ... \r\nSelecting previously unselected package python3-argcomplete. \r\nPreparing to unpack .. . /06-python3-argcomplete_1.8.1-1. 3ubuntu1_all.deb ... \r\nUnpacking python3-argcomplete (1.8.1-1.3ubuntu1) ... \r\nSelecting previously unselected package python3- jnespath. \r\nPreparing to unpack ... /07-python3- jnespa th_0.9.4-2ubuntul_all.deb ... \r\nUnpacking python3-jmespath (0.9.4-2ubuntu1) ... \r\nSelecting previously unselected package python3-kerberos. \r\nPreparing to unpack ... /08-python3-ke rberos_1.1.14-3.1build1_and64.deb ... \r\nUnpacking python3-kerberos (1.1.14-3.1build1) ... \r\nSelecting previously unselected package python3-libcloud. \r\nPreparing to unpack .. . /09- python3-libcloud_2.8.0-1_all.deb ... \r\nUnpacking python3-libcloud (2.8.0-1) ... \r\nSelecting previously unselected package python3-ntlm-auth. \r\nPreparing to unpack ... /10-python3-n tim-auth_1.1.0-1_all.deb .. . \r\nUnpacking python3-ntlm-auth (1.1.0-1) ... \r\nSelecting previously unselected package python3-requests-kerberos. \r\nPreparing to unpack ... /11-python3. requests-kerberos 0.12.8-2 all. deb .... \r\nUnpacking python3-requests-kerberos (0.12.0-2) ... \r\nSelecting previously unselected package python3-requests-ntln. \r\nPreparing to unpack By using these ad-hoc commands, you can perform quick, task-specific operations on your managed nodes, saving time and effort in your automation tasks. CommonModules: Ansible offers an extensive library of modules for various tasks, from system administration to application deployment. You can explore the full list of Ansible modules in the official documentation to find the most suitable module for your specific requirements. BestPractices: When working with ad-hoc commands, consider using SSH key-based authentication for secure, passwordless access. It streamlines the authentication process and enhances the security of your automation tasks. Using Ansible Playbooks Introduction: In the world of Ansible, playbooks are the cornerstone of structured and complex automation. While ad- hoc commands are perfect for quick and isolated tasks, playbooks step in when you need a well- orchestrated sequence of operations. They allow you to tackle multi-step automation scenarios where tasks depend on each other, and conditional actions are required. Let's delve deeper into the importance of playbooks: The Role of Playbooks: Playbooks serve as the framework for orchestrating automation tasks that involve a sequence of operations. Whether it's configuring servers, deploying applications, or managing infrastructure, playbooks provide a structured way to define how these tasks are executed. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N33 nirf 1750 Rank

--- Page 79 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Plugins: ○ Role: Extend Jenkins functionality without modifying the core software. ○ Types: I Source Control Plugins: Git, SVN, Mercurial. Build Tools Plugins: Maven, Gradle, Ant. Notification Plugins: Email, Slack, HipChat. · Reporting Plugins: JUnit, Cobertura, Checkstyle. ○ Management: Plugins can be installed, updated, and configured via the Jenkins web interface. 5. Master-Slave Architecture: ○ Concept: The Jenkins master distributes build tasks to multiple agents, enabling distributed builds. ○ Benefits: Scalability: Handle more builds simultaneously. I Resource Management: Assign builds to agents with the necessary resources. Isolation: Run builds in isolated environments to avoid conflicts. Workflow and Data Flow 1. Job Trigger: ○ Jobs can be triggered by various events: Manual Trigger: Initiated by a user through the Jenkins interface. Scheduled Trigger: Using CRON-like syntax to schedule jobs. · SCM Trigger: Automatically triggered by changes in the source code repository. Upstream/Downstream Trigger: Triggered by the completion of other jobs. 2. Build Execution: ○ Job Assignment: The master assigns the job to an available agent based on labels, availability, and resource requirements. Build Environment Setup: The agent sets up the environment, including checking out the code, installing dependencies, and configuring the workspace. ○ Execution: The agent runs the build steps as defined in the job configuration. 3. Build Results: ○ Logs: Captures console output and logs from the build process. ○ Artifacts: Stores build artifacts like binaries, packages, and reports. ○ Test Results: Collects and displays test results and code coverage reports. ○ Build History: Maintains a history of all builds, including status, duration, and changes. 4. Notification and Reporting: ○ Notifications: Jenkins can send notifications through various channels (email, chat, etc.) upon build completion or failure. ○ Dashboards: Provides dashboards for visualizing build status, trends, and metrics. ○ Reports: Generates and displays reports on test results, code quality, and other metrics. INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 4 1750 Rank

--- Page 80 ---
Unit: III Course Code/Title:CS3V15/Devops Summary Jenkins' architecture is designed to be flexible and scalable. The master-agent model allows for efficient distribution of build tasks, while plugins provide extensive customization options. By leveraging Jenkins' comprehensive job configuration and robust notification and reporting capabilities, teams can streamline their CI/CD pipelines and improve software quality and delivery speed. Creating a Jenkins Job Creating a Jenkins job involves setting up a new project within Jenkins to automate various tasks such as building code, running tests, and deploying applications. Here is a step-by-step guide to creating a Jenkins job. Prerequisites · Jenkins installed and running. · Basic understanding of the Jenkins interface. . Access to the Jenkins dashboard. Step-by-Step Guide 1. Access Jenkins Dashboard: . Open your web browser and navigate to your Jenkins instance URL (e.g., http://localhost:8080). 2. Create a New Job: ○ On the Jenkins dashboard, click on the "New Item" link on the left-hand side menu. 3. Enter Job Name: ○ Enter a name for your new job in the "Enter an item name" field. o Choose the type of job you want to create. For most uses, "Freestyle project" is a good starting point. ○ Click "OK". 4. Configure the Job: ○ After clicking "OK", you will be directed to the job configuration page. 5. General Configuration: ○ Description: Provide a brief description of the job. ○ Discard Old Builds: Optionally set this to limit the number of old builds Jenkins keeps. ○ Select Version Control System: Choose the version control system (e.g., Git, Subversion). 6. Source Code Management: ○ Repository URL: Enter the repository URL. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 5 nirf 175 Rank N33

--- Page 147 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: After clicking new repository option, we will have to initialize some things like, naming our project, choosing the visibility etc. After performing these steps click Create Repository button. Create a new repository A repository contains all project files, including the revision history. Already have a project repository elsewhere? Import a repository. Owner Repository name * / Namanbhatia7 - Resume This is going to be name of our project Great repository names are short and memorable. Need inspiration? How about animated-memory? Description (optional) Public Anyone Keep this as public selected ository. You choose who can commit. see this rer Private You choose who can see and commit to this repository. We can add a project description if we want. Skip this step if you're importing an existing repository. Initialize this repository with a README This will let you immediately clone the repository to your comp Tick the README option Add .gitignore: None - Add a license: None ~ Create repository After performing above steps, Click this button Step 3: After clicking the button, we will be directed to below page. Right now the only file we have is a readme file. Namanbhatia7 / Resume o Unwatch - 1 * Star 0 Y Fork 0 <> Code Issues 0 (*) Pull requests 0 ||| Projects 0 BB Wiki Security the Insights # Settings No description, website, or topics provided. Edit Manage topics T 1 commit įº 1 branch > 0 releases 2 1 contributor Branch: master New pull request Create new file Upload files Find File Clone or download TT Namanbhatia7 Initial commit Latest commit 676ac98 now README.md Initial commit now DO README.md Resume UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 7 175" Rank

--- Page 148 ---
Course Code/Title: CS3V15/Devops Unit: V Step 4: Now click on the "Upload files" button. 4 > C https://github.com/avicnotes/html-css-projects ... Search or jump to ... Click to End Session Total: $80.83 Pull requests Issues Marketplace Explore Google" Custom Search @ avicnotes / html-css-projects Unwatch - 1 Star 0 Fork 0 <> Code Issues 1'] Pull requests Actions Projects Wiki O Security ~ Insights ... go main - ¿º 1 branch 0 tags Go to file Add file - { Code About avicnotes screenshots Create new file Upload files 2 24 commits No description, website, or topics provided. README.md Update README.md 2 days ago Readme Screenshot (10).png screenshots 2 days ago Screenshot (11).png screenshots 2 days ago Releases Screenshot (12).png screenshots 2 days ago No releases published Create a new release 19 Screenshot (6).png screenshots 2 days ago accordion.JPG Add files via upload 4 days ago Packages background-dots.png Add files via upload 2 days ago clone-project-1.html Updated navbar items to my original ones 2 days ago index.html Create index.html 's ago No packages published Publish your first package Activate Windows Go to Settings to activate Windows, Environments 1 O Type here to search O W. 11:32 AM 2/1/2021 Step 5:Follow the steps mentioned and click "commitchanges". Drag additional files here to add them to your repository Or choose your files Display_Picture.png Simply drag or add files to be uploaded × My_Resume.html X Styles.css × Commit changes Add initial resume website files Add an optional ended des Give this particular version a name -o- Commit directly to the master branch. Ensure this option is checked !] Create a new branch for this commit and start a pull request. Learn more about pull requests. Commit changes Cancel UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 8

--- Page 161 ---
Add a status badge to your repository Many developers like to show that they're keeping their code quality high by displaying a status badge in their repo. Azure Pipelines succeeded To copy the status badge to your clipboard: 1. In Azure Pipelines, go to the Pipelines page to view the list of pipelines.Select the pipeline you created in the previous section. 2. Select , and then select Status badge. 3. Select Status badge. 4. Copy the sample Markdown from the Sample markdown section. Now with the badge Markdown in your clipboard, take the following steps in GitHub: 1. Go to the list of files and select Readme.md. Select the pencil icon to edit. 2. Paste the status badge Markdown at the beginning of the file. 3. Commit the change to the main branch. 4. Notice that the status badge appears in the description of your repository. To configure anonymous access to badges for private projects: 1. Navigate to Project Settings in the bottom left corner of the page 2. Open the Settings tab under Pipelines 3. Toggle the Disable anonymous access to badges slider under General 21

--- Page 162 ---
Azure-Pipeline • Azure Pipelineș provides a YAML pipeline editor that you can use to author and edit your pipelines. • The YAML editor is based on the Monaco Editor. • The editor provides tools like Intellisense support anda task assistant to provide guidance while you edit a pipeline. • You can also edit pipelines by modifying the azure pipelines.yml file directly in your pipeline's repository using a text editor of your choice Edit a YAML pipeline To access the YAML pipeline editor, do the following steps. • Sign in to your organization • (https://dev.azure.com/{yourorganization}). • Select your project, choose Pipelines, and then select the pipeline you want to edit. You can browse pipelines by Recent, All, and Runs. • Choose Edit. • Make edits to your pipeline using Intellisense and the task assistant forguidance. Azure DevOps fabrikam-tailspin FabrikamFiber F FabrikamFiber + Pipelines Overview Recent All Runs Boards Pipelines Recently run pipelines Repos 1 Pipeline Pipelines Pipelines FabrikamFiber 2 3 17

--- Page 39 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V The Downside of Dropout Although dropout is clearly a highly effective tool, it comes with certain drawbacks. A network with dropout can take 2 - 3 times longer to train than a standard network. One way to attain the benefits of dropout without slowing down training is by finding a regularizer that is essentially equivalent to a dropout layer. For linear regression, this regularizer has been proven to be a modified form of L2 regularization. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) nirf 175º Rank N33

--- Page 25 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Both the ReLU function and its derivative are monotonic. If the function receives any negative input, it returns 0; however, if the function receives any positive value x, it returns that value. As a result, the output has a range of 0 to infinite. ReLU is the most often used activation function in neural networks, especially CNNs, and is utilized as the default activation function. Implementing ReLU function in Python We can implement a simple ReLU function with Python code using an if-else statement as, def ReLU(x): if x>0: return x else: The positive value is returned as it is and for values less than (negative values) or equal to zero, 0.0 is returned. Now, we'll test out function by giving some input values and plot our result using pyplot from matplotlib library. The input range of values is from - 5 to 10. We apply our defined function on this set of input values. from matplotlib import pyplot def relu(x): return max(0.0, x) input = [x for x in range(-5, 10)] # apply relu on each input output = [relu(x) for x in input] # plot our result pyplot.plot(series_in, series_out) pyplot.show() We see from the plot that all the negative values have been set to zero, and the positive values are returned as it is. Note that we've given a set of consecutively increasing numbers as input, so we've a linear output with an increasing slope. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY nirf 1750 Rank N32

--- Page 26 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V 8 6 4 - 2 0 -4 -2 0 2 4 6 8 Fig. 5.17. Advantages of ReLU: ReLU is used in the hidden layers instead of Sigmoid or tanh as using sigmoid or tanh in the hidden layers leads to the infamous problem of "Vanishing Gradient". The "Vanishing Gradient" prevents the earlier layers from learning important information when the network is backpropagating. The sigmoid which is a logistic function is more preferrable to be used in regression or binary classification related problems and that too only in the output layer, as the output of a sigmoid function ranges from 0 to 1. Also Sigmoid and tanh saturate and have lesser sensitivity. Some of the advantages of ReLU are: # Simpler Computation: Derivative remains constant i.e 1 for a positive input and thus reduces the time taken for the model to learn and in minimizing the errors. # Representational Sparsity: It is capable of outputting a true zero value. * Linearity: Linear activation functions are easier to optimize and allow for a smooth flow. So, it is best suited for supervised tasks on large sets of labelled data. Disadvantages of ReLU: ❖ Exploding Gradient: This occurs when the gradient gets accumulated, this causes a large differences in the subsequent weight updates. This as a INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 37 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Weight decay: incentivize the network to use smaller weights by adding a penalty to the loss function (this ensures that the norms of the weights are relatively evenly distributed amongst all the weights in the networks, which prevents just a few weights from heavily influencing network output) Noise: allow some random fluctuations in the data through augmentation (which makes the network robust to a larger distribution of inputs and hence improves generalization) Model combination: average the outputs of separately trained neural networks (requires a lot of computational power, data, and time) Dropout remains an extremely popular protective measure against overfitting because of its efficiency and effectiveness. How Does Dropout Work? When we apply dropout to a neural network, we're creating a -thinnedl network with unique combinations of the units in the hidden layers being dropped randomly at different points in time during training. Each time the gradient of our model is updated, we generate a new thinned neural network with different units dropped based on a probability hyperparameter p. Training a network using dropout can thus be viewed as training loads of different thinned neural networks and merging them into one network that picks up the key properties of each thinned network. This process allows dropout to reduce the overfitting of models on training data. This graph, taken from the paper -Dropout: A Simple Way to Prevent Neural Networks from Overfittingl by Srivastava et al., compares the change in classification error of models without dropout to the same models with dropout (keeping all other hyperparameters constant). All the models have been trained on the MNIST dataset. It is observed that the models with dropout had a lower classification error than the same models without dropout at any given point in time. A similar trend was observed when the models were used to train other datasets in vision, as well as speech recognition and text analysis. The lower error is because dropout helps prevent overfitting on the training data by reducing the reliance of each unit in the hidden layer on other units in the hidden layers. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY nirf 1750 Rank N32

--- Page 38 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V 2.5 2.0 Without dropout Classification Error % 1.5 With dropout 1.0 0 200000 400000 Number of Weight updates 600000 800000 1000000 Fig. 5.20. (a) Without dropout (b) Dropout with p = 0.5. Fig. 5.21. It can be observed in figure a that the units don't seem to pick up on any meaningful feature, whereas in figure b, the units seem to have picked up on distinct edges and spots in the data provided to them. This indicates that dropout helps break co- adaptations among units, and each unit can act more independently when dropout regularization is used. In other words, without dropout, the network would never be able to catch a unit A compensating for another unit B's flaws. With dropout, at some point unit A would be ignored and the training accuracy would decrease as a result, exposing the inaccuracy of unit B. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 7 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V An activation function is a function that is added to an artificial neural network in order to help the network learn complex patterns in the data. When comparing with a neuron-based model that is in our brains, the activation function is at the end deciding what is to be fired to the next neuron. The neuron doesn't really know how to bound to value and thus is not able to decide the firing pattern. Thus the activation function is an important part of an artificial neural network. They basically decide whether a neuron should be activated or not. Thus it bounds the value of the net input. The activation function is a non-linear transformation that we do over the input before sending it to the next layer of neurons or finalizing it as output. 5.2.1. TYPES OF ACTIVATION FUNCTIONS Several different types of activation functions are used in Deep Learning. Some of them are explained below: 1. Sigmoid Activation Function - Sigmoid Activation Function 1.2 10 0.8 0.6 8 0.4 0.2 0.0 -0.2 -10.0 -7.5 -5.0 -2.5 0.0 2.5 5.0 7.5 10.0 X The Sigmoid Function looks like an S-shaped curve. Formula : f (z) = 1/(1+e^-z) Why and when do we use the Sigmoid Activation Function? 1. The output of a sigmoid function ranges between 0 and 1. Since, output values bound between 0 and 1, it normalizes the output of each neuron. LEN CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank

--- Page 8 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V 2. Specially used for models where we have to predict the probability as an output. Since the probability of anything exists only between the range of 0 and 1, sigmoid is the perfect choice. 3. Smooth gradient, preventing -jumps in output values. 4. The function is differentiable. That means, we can find the slope of the sigmoid curve at any two points. 5. Clear predictions, i.e very close to 1 or 0. 2. Tanh or Hyperbolic Tangent Activation Function - y 1 f(x) = tanh x X -2 -1 1 2 -1. The tanh activation function is also sort of sigmoidal (S-shaped). 2 f(x) = tanh (x) = 1+ e-2x -- 1 Formula of tanh activation function Tanh is a hyperbolic tangent function. The curves of tanh function and sigmoid function are relatively similar. But it has some advantage over the sigmoid function. Let's look at what it is. Why is tanh better compared to sigmoid activation function? 1. First of all, when the input is large or small, the output is almost smooth and the gradient is small, which is not conducive to weight update. The CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N33 nirf 1750 Rank

--- Page 33 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V When a model performs well on the training data and does not perform well on the testing data, then the model is said to have high generalization error. In other words, in such a scenario, the model has low bias and high variance and is too complex. This is called overfitting. Overfitting means that the model is a good fit on the train data compared to the data, as illustrated in the graph above. Overfitting is also a result of the model being too complex. What Is Regularization in Machine Learning? Regularization is one of the key concepts in Machine learning as it helps choose a simple model rather than a complex one. We want our model to perform well both on the train and the new unseen data, meaning the model must have the ability to be generalized. Generalization error is -a measure of how accurately an algorithm can predict outcome values for previously unseen data.l Regularization refers to the modifications that can be made to a learning algorithm that helps to reduce this generalization error and not the training error. It reduces by ignoring the less important features. It also helps prevent overfitting, making the model more robust and decreasing the complexity of a model. How Does Regularization Work? Regularization works by shrinking the beta coefficients of a linear regression model. To understand why we need to shrink the coefficients, let us see the below example: Salary Slope is b1 + b3 if X2 = 1 if x2 = 0 bo + b2 bo Total years of experience y = bo + b2×2 + b1×1 + b3×2×1 Fig. 5.18. In the above graph, the two lines represent the relationship between total years of experience and salary, where salary is the target variable. These are slopes indicating CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 34 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V the change in salary per unit change in total years of experience. As the slope b1 + b3 decreases to slope b1, we see that the salary is less sensitive to the total years of experience. By decreasing the slope, the target variable (salary) became less sensitive to the change in the independent X variables, which increases the bias into the model. Remember, bias is the difference between the predicted and the actual values. With the increase in bias to the model, the variance (which is the difference between the predictions when the model fits different datasets.) decreases. And, by decreasing the variance, the overfitting gets reduced. The models having the higher variance leads to overfitting, and we saw above, we will shrink or reduce the beta coefficients to overcome the overfitting. The beta coefficients or the weights of the features converge towards zero, which is known as shrinkage. What Is the Regularization Parameter? For linear regression, the regularization has two terms in the loss function: The Ordinary Least Squares (OLS) function, and The penalty term It becomes : Loss function regularization = Loss function + Penalty term ols The goal of the linear regression model is to minimize the loss function. Now for Regularization, the goal becomes to minimize the following cost function: n ≥ (yact - ypred)2 + penalty i = 1 Where, the penalty term comprises the regularization parameter and the weights associated with the variables. Hence, the penalty term is: penalty = ^ * w where, 2 = Regularization parameter w = weight associated with the variables; generally considered to be the L - p norms The regularization parameter in machine learning is A : It imposes a higher penalty on the variable having higher values, and hence, it controls the strength of the penalty term. This tuning parameter controls the bias-variance trade-off. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 27 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V result causes instability when converging to the global minima and causes instability in the learning too. ❖ Dying ReLU: The problem of "dead neurons" occurs when the neuron gets stuck in the negative side and constantly outputs zero. Because gradient of 0 is also 0, it's unlikely for the neuron to ever recover. This happens when the learning rate is too high or negative bias is quite large. 5.8. HYPERPARAMETER TUNING A Machine Learning model is defined as a mathematical model with a number of parameters that need to be learned from the data. By training a model with existing data, we are able to fit the model parameters. However, there is another kind of parameter, known as Hyperparameters, that cannot be directly learned from the regular training process. They are usually fixed before the actual training process begins. These parameters express important properties of the model such as its complexity or how fast it should learn. Some examples of model hyperparameters include: The penalty in Logistic Regression Classifier i.e. L1 or L2 regularization The learning rate for training a neural network. The C and sigma hyperparameters for support vector machines. The k in k-nearest neighbors. Models can have many hyperparameters and finding the best combination of parameters can be treated as a search problem. The two best strategies for Hyperparameter tuning are: Grid Search CV In GridSearchCV approach, the machine learning model is evaluated for a range of hyperparameter values. This approach is called GridSearchCV, because it searches for the best set of hyperparameters from a grid of hyperparameters values. For example, if we want to set two hyperparameters C and Alpha of the Logistic Regression Classifier model, with different sets of values. The grid search technique will construct many versions of the model with all possible combinations of hyperparameters and will return the best one. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 28 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V As in the image, for C = [0.1, 0.2, 0.3, 0.4, 0.5] and Alpha = [0.1, 0.2, 0.3, 0.4]. For a combination of C = 0.3 and Alpha = 0.2, the performance score comes out to be 0.726(Highest), therefore it is selected. 0.5 0.701 0.703 0.4 0.699 0.702 0.698 0.3 0.721 0.726 0.713 0.2 0.706 0.705 0.704 0.701 C 0.1 0.698 0.692 0.1 0.2 Alpha 0.697 0.696 0.702 0.703 0.688 0.675 0.3 0.5 The following code illustrates how to use GridSearchCV # Necessary imports from sklearn.linear_model import LogisticRegression from sklearn.model_selection import GridSearchCV # Creating the hyperparameter grid c_space = np.logspace(-5, 8, 15) param_grid = {'C': c_space} # Instantiating logistic regression classifier logreg = LogisticRegression() # Instantiating the GridSearchCV object logreg_cv = GridSearchCV(logreg, param_grid, cv = 5) logreg_cv.fit(X, y) # Print the tuned parameters and score print("Tuned Logistic { }".format(logreg_cv.best_params_)) Regression Parameters: print("Best score is { }".format(logreg_cv.best_score_)) Output: Tuned Logistic Regression Parameters: {_C': 3.7275937203149381} Best score is 0.7708333333333334 CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 9 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V difference is the output interval. The output interval of tanh is 1, and the whole function is 0-centric, which is better than sigmoid. 2. The major advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph. 1.0 - sigmoid - tanh 0.5 1 0.0 -2 -6 -4 0 2 4 6 -0.5- -1.0L Fig. 5.6. 3. ReLU (Rectified Linear Unit) Activation Function- output 5 -5 0 Fig. 5.7. 5 The ReLU is half rectified (from the bottom). f (z) is zero when z is less than zero and f(z) is equal to z when z is above or equal to zero. max(0, x) a(x) = { i 0 x >=0 x<0 The ReLU (Rectified Linear Unit) function is an activation function that is currently more popular compared to other activation functions in deep learning. Compared with the sigmoid function and the tanh function, it has the following advantages: 1. When the input is positive, there is no gradient saturation problem. INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 10 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V 2. The calculation speed is much faster. The ReLU function has only a linear relationship. Whether it is forward or backward, it is much faster than sigmoid and tanh. (Sigmoid and tanh need to calculate the exponent, which will be slower.) 4. Leaky ReLU Activation Function An activation function specifically designed to compensate for the dying ReLU problem. f(y) 4 f(y) f(y) = y f(y) = y f(y) = 0 y y f(y) = ay Fig. 5.8. ReLU vs Leaky ReLU Why Leaky ReLU is better than ReLU? if y; > 0 f(yi) = İyi laiyi ify; ≤ 0 1. The leaky ReLU adjusts the problem of zero gradients for negative value, by giving a very small linear component of x to negative inputs(0.01 x). 2. The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so. 3. Range of the Leaky ReLU is (-infinity to infinity). 5. ELU (Exponential Linear Units) Function- ELU is also proposed to solve the problems of ReLU. In contrast to ReLUs, ELUs have negative values which pushes the mean of the activations closer to zero. Mean activations that are closer to zero enable faster learning as they bring the gradient closer to the natural gradient. xx>0 g(x)=ELU(x) = \a(ex-1) x≤0 Obviously, ELU has all the advantages of ReLU, and: ❖ No Dead ReLU issues, the mean of the output is close to 0, zero-centered. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY nirf 1750 Rank N32

--- Page 31 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Similarly, this transformation will take place for the second layer and go till the last layer L as shown in the following image. X1 X2 X3 X4 W1 W2 W3 WL W4 O Normalize the inputs h1 = 0(W1X) h2 = @(W2h1)=0(W20(W1X)) O = 0(WL hL -1) Although, our input X was normalized with time the output will no longer be on the same scale. As the data go through multiple layers of the neural network and L activation functions are applied, it leads to an internal co-variate shift in the data. How does Batch Normalization work? Since by now we have a clear idea of why we need Batch normalization, let's understand how it works. It is a two-step process. First, the input is normalized, and later rescaling and offsetting is performed. Normalization of the Input Normalization is the process of transforming the data to have a mean zero and standard deviation one. In this step we have our batch input from layer h, first, we need to calculate the mean of this hidden activation. 3 = 2-1 M h m i Here, m is the number of neurons at layer h. Once we have meant at our end, the next step is to calculate the standard deviation of the hidden activations. [1 7 1/2 0 = m E (h; - u)2| Further, as we have the mean and the standard deviation ready. We will normalize the hidden activations using these values. For this, we will subtract the mean from CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 32 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V each input and divide the whole value with the sum of standard deviation and the smoothing term (¿). The smoothing term() assures numerical stability within the operation by stopping a division by a zero value. hi(norm) + (hị - ) Advantages of Batch Normalization Now let's look into the advantages the BN process offers. Speed Up the Training By Normalizing the hidden layer activation the Batch normalization speeds up the training process. Handles internal covariate shift It solves the problem of internal covariate shift. Through this, we ensure that the input for every layer is distributed around the same mean and standard deviation. If you are unaware of what is an internal covariate shift, look at the following example. Internal covariate shift Suppose we are training an image classification model, that classifies the images into Dog or Not Dog. Let's say we have the images of white dogs only, these images will have certain distribution as well. Using these images model will update its parameters. Smoothens the Loss Function Batch normalization smoothens the loss function that in turn by optimizing the model parameters improves the training speed of the model. 5.10. REGULARIZATION The Problem of Overfitting So, before diving into regularization, let's take a step back to understand what bias- variance is and its impact. Bias is the deviation between the values predicted by the model and the actual values whereas, variance is the difference between the predictions when the model fits different datasets. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 35 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V À can take values 0 to infinity. If A = 0, then means there is no difference between a model with and without regularization. Regularization Techniques in Machine Learning Each of the following techniques uses different regularization norms (L-p) based on the mathematical methodology that creates different kinds of regularization. These methodologies have different effects on the beta coefficients of the features. The regularization techniques in machine learning as follows: (a) Ridge Regression The Ridge regression technique is used to analyze the model where the variables may be having multicollinearity. It reduces the insignificant independent variables though it does not remove them completely. This type of regularization uses the L2 norm for regularization. Cost function n = E (yact - Ypred) 2 + 2 . | w 12 i= 1 (b) Lasso Regression Least Absolute Shrinkage and Selection Operator (or LASSO) Regression penalizes the coefficients to the extent that it becomes zero. It eliminates the insignificant independent variables. This regularization technique uses the L1 norm for regularization. i = 1 Cost function = > (yact - ypred )2 + 2. | w Il1 n (c) Elastic Net Regression The Elastic Net Regression technique is a combination of the Ridge and Lasso regression technique. It is the linear combination of penalties for both the L1 -norm and L2 -norm regularization. The model using elastic net regression allows the learning of the sparse model where some of the points are zero, similar to Lasso regularization, and yet maintains the Ridge regression properties. Therefore, the model is trained on both the L1 and L2 norms. The cost function of Elastic Net Regression is: n Cost function i= 1 = E (Vact - Y pred)2 + bridge . Il w 11/2 + Masso . | | CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY N32 nirf 1750 Rank

--- Page 36 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V When to Use Which Regularization Technique? The regularization in machine learning is used in following scenarios: Ridge regression is used when it is important to consider all the independent variables in the model or when many interactions are present. That is where collinearity or codependency is present amongst the variables. Lasso regression is applied when there are many predictors available and would want the model to make feature selection as well for us. When many variables are present, and we can't determine whether to use Ridge or Lasso regression, then the Elastic-Net regression is your safe bet. 5.11. DROPOUT -Dropoutl in machine learning refers to the process of randomly ignoring certain nodes in a layer during training. In the figure below, the neural network on the left represents a typical neural network where all units are activated. On the right, the red units have been dropped out of the model - the values of their weights and biases are not considered during training. (a) Standard Neural Net (b) After applying dropout Fig. 5.19. Dropout is used as a regularization technique - it prevents overfitting by ensuring that no units are codependent (more on this later). Common Regularization Methods Common regularization techniques include: Early stopping: stop training automatically when a specific performance measure (eg. Validation loss, accuracy) stops improving INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 13 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V When x > 0, our derivative greater than zero and we need to go in negative direction, when x < 0, the derivative less than zero, we need to go in positive direction. We always need to take a step in the direction which is opposite of derivative. Let's apply the same idea to gradient. Gradient is vector which points to some direction in space. It actually point to the direction of the steepest increase of the function. Since we want minimize our function, we'll take a step in the opposite direction of gradient. In neural network we think of inputs x, and outputs y as fixed numbers. The variable with respect to which we're going to be taking our derivatives are weights w, since these are the values we want to change to improve our network. If we compute the gradient of the loss function w.r.t our weights and take small steps in the opposite direction of gradient our loss will gradually decrease until it converges to some local minima. This algorithms is called Gradient Descent. The rule for updating weights on each iteration of Gradient Descent is the following: Wj = w; - Irô L a wi Ir in the notation above means learning rate. It's there to control how big of a step we're taking each iteration. It is the most important hyper-parameter to tune when training neural networks. 5.3.1. THE ARTIFICIAL NEURAL NETWORK To build a good Artificial Neural Network (ANN) you will need the following ingredients Ingredients: ❖ Artificial Neurons (processing node) composed of: ✓ (many) input neuron(s) connection(s) (dendrites) ✓ a computation unit (nucleus) composed of: a linear function (ax + b) " an activation function (equivalent to the the synapse) ✓ an output (axon) Preparation to get an ANN for image classification training: 1. Decide on the number of output classes (meaning the number of image classes - for example two for cat vs dog). CHENNAI CHENNAI INSTITUTE . TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 14 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V 2. Draw as many computation units as the number of output classes (congrats you just create the Output Layer of the ANN). 3. Add as many Hidden Layers as needed within the defined architecture. Hidden Layers are just a set of neighboured Compute Units, they are not linked together. 4. Stack those Hidden Layers to the Output Layer using Neural Connections 5. It is important to understand that the Input Layer is basically a layer of data ingestion 6. Add an Input Layer that is adapted to ingest your data (or you will adapt your data format to the pre-defined architecture) 7. Assemble many Artificial Neurons together in a way where the output (axon) an Neuron on a given Layer is (one) of the input of another Neuron on a subsequent Layer. As a consequence, the Input Layer is linked to the Hidden Layers which are then linked to the Output Layer (as shown in the picture below) using Neural Connections Training an Artificial Neural Network (ANN) requires just a few steps: 1. First an ANN will require a random weight initialization 2. Split the dataset in batches (batch size) 3. Send the batches 1 by 1 to the GPU 4. Calculate the forward pass (what would be the output with the current weights) 5. Compare the calculated output to the expected output (loss) 6. Adjust the weights (using the learning rate increment or decrement) according to the backward pass (backward gradient propagation). 5.4. STOCHASTIC GRADIENT DESCENT What is Gradient Descent? Gradient Descent is a generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea is to tweak parameters iteratively in order to minimize the cost function. An important parameter of CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 15 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Gradient Descent (GD) is the size of the steps, determined by the learning rate hyperparameters. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time, and if it is too high we may jump the optimal value. What is the objective of Gradient Descent? Gradient, in plain terms means slope or slant of a surface. So gradient descent literally means descending a slope to reach the lowest point on that surface. Let us imagine a two dimensional graph, such as a parabola in the figure below. y y = x2 - 2x - 3 6 5 2 16 + 5 -4 - 3 - 2 -1 1 2 3 4 5 6 7 8 -x -3 -4 -5. Fig. 5.11. A parabolic function with two dimensions (x , y) In the above graph, the lowest point on the parabola occurs at x = 1. The objective of gradient descent algorithm is to find the value of -xl such that -yl is minimum. -yl here is termed as the objective function that the gradient descent algorithm operates upon, to descend to the lowest point. 5.4.1. TYPES OF GRADIENT DESCENT: Typically, there are three types of Gradient Descent: ❖ Batch Gradient Descent Stochastic Gradient Descent CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY nirf 1750 Rank N32

--- Page 16 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V ❖ Mini-batch Gradient Descent Stochastic Gradient Descent (SGD): In Stochastic Gradient Descent, a few samples are selected randomly instead of the whole data set for each iteration. In Gradient Descent, there is a term called -batchl which denotes the total number of samples from a dataset that is used for calculating the gradient for each iteration. In typical Gradient Descent optimization, like Batch Gradient Descent, the batch is taken to be the whole dataset. Suppose, you have a million samples in your dataset, so if you use a typical Gradient Descent optimization technique, you will have to use all of the one million samples for completing one iteration while performing the Gradient Descent, and it has to be done for every iteration until the minima are reached. Hence, it becomes computationally very expensive to perform. This problem is solved by Stochastic Gradient Descent. In SGD, it uses only a single sample, i.e., a batch size of one, to perform each iteration. Steps in SGD: 1. Find the slope of the objective function with respect to each parameter / feature. In other words, compute the gradient of the function. 2. Pick a random initial value for the parameters. (To clarify, in the parabola example, differentiate -yl with respect to -xl. If we had more features like X1, x2 etc., we take the partial derivative of -yl with respect to each of the features.) 3. Update the gradient function by plugging in the parameter values. 4. Calculate the step sizes for each feature as : step size = gradient * learning rate. 5. Calculate the new parameters as : new params = old params -step size 6. Repeat steps 3 to 5 until gradient is almost 0. Stochastic Gradient Descent using Python The SGD algorithm is used in several loss functions. Simply put, it is used to minimize a cost function by iterating a gradient-based weight update. Instead of looking at the full dataset, the weight update is applied to batches randomly extracted from it, which is why it is also known as mini-batch gradient descent. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 3 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V Bias, net sum, and an activation function. The perceptron model begins with the multiplication of all input values and their weights, then adds these values together to create the weighted sum. Then this weighted sum is applied to the activation function 'f to obtain the desired output. This activation function is also known as the step function and is represented by 'f'. 1 Wo Error X1 W1 output X2 W2 Xm : Wm Net input function Activation function Perceptron rule Fig. 5.3. Perceptron model works in two important steps as follows: Step-1 In the first step first, multiply all input values with corresponding weight values and then add them to determine the weighted sum. Mathematically, we can calculate the weighted sum as follows: Ew; * xi = X1 * W1 + X2 * W2 + ... Wn * xn Add a special term called bias 'b' to this weighted sum to improve the model's performance. Zwi * x; + b Step-2 In the second step, an activation function is applied with the above-mentioned weighted sum, which gives us output either in binary form or a continuous value as follows: Y = f(> wi * x¡ + b) Perceptron Function Perceptron function "f (x)" can be achieved as output by multiplying the input 'x' with the learned weight coefficient 'w'. CHENNAI INSTITUTE . TECHNOLOGY LEN CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank

--- Page 4 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Mathematically, we can express it as follows: f(x) = 1; if w . x + b > 0 otherwise, f (x) = 0 * 'w' represents real-valued weights vector 'b' represents the bias 'x' represents a vector of input x values. 5.1. MULTI - LAYERED PERCEPTRON MODEL Multi-Layer perceptron defines the most complex architecture of artificial neural networks. It is substantially formed from multiple layers of the perceptron. Tensor Flow is a very popular deep learning framework released by, and this notebook will guide to build a neural network with this library. If we want to understand what is a Multi-layer perceptron, we have to develop a multi-layer perceptron from scratch using Numpy. The pictorial representation of multi-layer perceptron learning is as shown below- Input Layer Hidden Layer Output Layer · Fig. 5.4. MLP networks are used for supervised learning format. A typical learning algorithm for MLP networks is also called back propagation's algorithm. A multilayer perceptron (MLP) is a feed forward artificial neural network that generates a set of outputs from a set of inputs. An MLP is characterized by several layers of input nodes connected as a directed graph between the input nodes CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N33 nirf 1750 Rank

--- Page 11 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V ❖ In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. * ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. 4 - ELU -LReLU - ReLU ₹ 2 0 -10 -5 0 X 5 Fig. 5.9. ELU vs Leaky ReLU vs ReLU 5.3. NETWORK TRAINING In the process of training, we want to start with a bad performing neural network and wind up with network with high accuracy. In terms of loss function, we want our loss function to much lower in the end of training. Improving the network is possible, because we can change its function by adjusting weights. We want to find another function that performs better than the initial one. The problem of training is equivalent to the problem of minimizing the loss function. Why minimize loss instead of maximizing? Turns out loss is much easier function to optimize. There are a lot of algorithms that optimize functions. These algorithms can gradient-based or not, in sense that they are not only using the information provided by the function, but also by its gradient. First, we need to remember what a derivative is with respect to some variable. Let's take some easy function f (x) = x. If we remember the rules of calculus from high school we know, that the derivative of that is one at every value of x. The CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE . TECHNOLOGY (Autonomous] LEN nirf 1750 Rank

--- Page 12 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V derivative is the rate of how fast our function is changing when we take infinitely small step in the positive direction. Mathematically it can be written as the following: VL ~ 8 L V x à xi i Which means: how much our function changes(left term) approximately equals to derivative of that function with respect to some variable x multiplied with how much we changed that variable. That approximation is going to be exact when we step we take is infinitely small and this is very important concept of the derivative. 8 6 4 2 0 -3 -2 -1 0 y = x2 1 2 3 Fig. 5.10. Let's go back to our function f(x) = x2. Obviously, the minimum of that function is at point x = 0, but how would a computer know it ? Suppose, we start off with some random value of x and this value is 2. The derivative of the function in that in x = 2 equals 4. Which means that is we take a step in positive direction our function will change proportionally to 4. Our derivative only guarantees that the function will decrease if take infinitely small step. Generally, you want to control how big of step you make with some kind of hyper-parameter. This hyper-parameter is called learning rate and I'll talk about it later. Let's now see what happens if we start at a point x = - 2. The derivative is now equals - 4, which means, that if take a small step in positive direction our function will change proportionally to - 4, thus it will decrease. That's exactly what we want. INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 19 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V 2 Hidden layer(s) Input layer x W output layer 3 W x W Difference in desired values X W 5 Backprop output layer Fig. 5.12. The General Algorithm The backpropagation algorithm proceeds in the following steps, assuming a suitable learning rate \ alpha a and random initialization of the parameters w_{ij}^k:wijk: Definition → 1. Calculate the forward phase for each input-output pair ( xa, ya ) and store the results y^ , ak and ok for each node j in layer k by proceeding from d j j layer 0, the input layer, to layer m, the output layer. → 2. Calculate the backward phase for each input-output pair ( Xa, Ya ) and wk for each weight wk connecting node in layer k - 1 store the results O Ed aw j to node j in layer k by proceeding from layer m, the output layer, to layer1, the input layer. (a) Evaluate the error term for the final layer 8'" by using the second equation. (b) Backpropogate the error terms for the hidden layers 8k , working backwards from the final hidden layer k = m - 1, by repeatedly using the third equation. (c) Evaluate the partial derivatives of the individual error Ed with respect to wk by using the first equation. INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 20 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V 3. Combine the individual gradients for each input-output pair aw j ,k O Ed to get OE(X, 0) the total gradient O wk ? for the entire set of input-output pairs X = { ( ij → → X1, y1 ), ... , (XN, YN )}by using the fourth equation (a simple average of the individual gradients). 4. Update the weights according to the learning rate « and total gradient 0E(X, 0) k by using the fifth equation (moving in the direction of the aw j negative gradient). How Backpropagation Algorithm Works Inputs X, arrive through the preconnected path 1. Input is modeled using real weights W. The weights are usually randomly selected. 2. Calculate the output for every neuron from the input layer, to the hidden layers, to the output layer. 3. Calculate the error in the outputs Error B = Actual Output - Desired Output 4. Travel back from the output layer to the hidden layer to adjust the weights such that the error is decreased. 5. Keep repeating the process until the desired output is achieved Why We Need Backpropagation? Most prominent advantages of Backpropagation are: ❖ Backpropagation is fast, simple and easy to program * It has no parameters to tune apart from the numbers of input It is a flexible method as it does not require prior knowledge about the network It is a standard method that generally works well It does not need any special mention of the features of the function to be learned. INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 1 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V UNIT V NEURAL NETWORKS Perceptron - Multilayer perceptron, activation functions, network training - gradient descent optimization - stochastic gradient descent, error backpropagation, from shallow networks to deep networks - Unit saturation (aka the vanishing gradient problem) - ReLU, hyperparameter tuning, batch normalization, regularization, dropout. What is the Perceptron Model in Machine Learning? Perceptron is Machine Learning algorithm for supervised learning of various binary classification tasks. Further, Perceptron is also understood as an Artificial Neuron or neural network unit that helps to detect certain input data computations in business intelligence. Perceptron model is also treated as one of the best and simplest types of Artificial Neural networks. However, it is a supervised learning algorithm of binary classifiers. Hence, we can consider it as a single-layer neural network with four main parameters, i.e., input values, weights and Bias, net sum, and an activation function. Basic Components of Perceptron Mr. Frank Rosenblatt invented the perceptron model as a binary classifier which contains three main components. These are as follows: Inputs Weights 1 Net input function Activation function Wo X1 W1 X2 W2 IT 5 IT - output ... Xm Wm Fig. 5.1. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N39 nirf 175° Rank

--- Page 2 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Input Nodes or Input Layer: This is the primary component of Perceptron which accepts the initial data into the system for further processing. Each input node contains a real numerical value. Wight and Bias: Weight parameter represents the strength of the connection between units. This is another most important parameter of Perceptron components. Weight is directly proportional to the strength of the associated input neuron in deciding the output. Further, Bias can be considered as the line of intercept in a linear equation. Activation Function: These are the final and important components that help to determine whether the neuron will fire or not. Activation Function can be considered primarily as a step function. Types of Activation functions: # Sign function # Step function, and * Sigmoid function a ai ai +1 +1 +1 t in; in; in -1 Step Function Sign Function Sigmoid Function Fig. 5.2. The data scientist uses the activation function to take a subjective decision based on various problem statements and forms the desired outputs. Activation function may differ (e.g., Sign, Step, and Sigmoid) in perceptron models by checking whether the learning process is slow or has vanishing or exploding gradients. How does Perceptron work? In Machine Learning, Perceptron is considered as a single-layer neural network that consists of four main parameters named input values (Input nodes), weights and CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY LEN nirf 175° Rank

--- Page 23 ---
Introduction to Artificial Intelligent and Machine Learning - Unit - V exponentially as we propagate down to the initial layers. A small gradient means that the weights and biases of the initial layers will not be updated effectively with each training session. Since these initial layers are often crucial to recognizing the core elements of the input data, it can lead to overall inaccuracy of the whole network. Solution: The simplest solution is to use other activation functions, such as ReLU, which doesn't cause a small derivative. Residual networks are another solution, as they provide residual connections straight to earlier layers. The residual connection directly adds the value at the beginning of the block, x, to the end of the block (F(x) + x). This residual connection doesn't go through activation functions that -squashesl the derivatives, resulting in a higher overall derivative of the block. X Weight layer F(x) Weight layer X identity F(x) + x + relu Fig. 5.14. A residual block 0.9 - Sigmoid --- Derivative Sigmoid 0.8 0.7 0.6 0.5 0.4 Ø3 02 0.1 -10 -8 -6 -4 -2 0 2 4 6 8 10 Fig. 5.15. Sigmoid function with restricted inputs INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 24 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Finally, batch normalization layers can also resolve the issue. As stated before, the problem arises when a large input space is mapped to a small one, causing the derivatives to disappear. In Image 1, this is most clearly seen at when |x | is big. Batch normalization reduces this problem by simply normalizing the input so |x | doesn't reach the outer edges of the sigmoid function. As seen in Image 3, it normalizes the input so that most of it falls in the green region, where the derivative isn't too small. What is an activation function? Activation function is a simple mathematical function that transforms the given input to the required output that has a certain range. From their name they activate the neuron when output reaches the set threshold value of the function. Basically they are responsible for switching the neuron ON/OFF. The neuron receives the sum of the product of inputs and randomly initialized weights along with a static bias for each layer. The activation function is applied on to this sum, and an output is generated. Activation functions introduce a non-linearity, so as to make the network learn complex patterns in the data such as in the case of images, text, videos or sounds. Without an activation function our model is going to behave like a linear regression model that has limited learning capacity. 5.7. RELU The rectified linear activation unit, or ReLU, is one of the few landmarks in the deep learning revolution. It's simple, yet it's far superior to previous activation functions like sigmoid or tanh. ReLU formula is : f (x) = max(0, x) y = x 3 2 1 y = 0 -3 -2 -1 0 1 2 3 Fig. 5.16. INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 21 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Types of Backpropagation There are two types of Backpropagation which are as follows - Static Back Propagation - In this type of backpropagation, the static output is created because of the mapping of static input. It is used to resolve static classification problems like optical character recognition. Recurrent Backpropagation - The Recurrent Propagation is directed forward or directed until a specific determined value or threshold value is acquired. After the certain value, the error is evaluated and propagated backward. Key Points ❖ Simplifies the network structure by elements weighted links that have the least effect on the trained network * You need to study a group of input and activation values to develop the relationship between the input and hidden unit layers. * It helps to assess the impact that a given input variable has on a network output. The knowledge gained from this analysis should be represented in rules. * Backpropagation is especially useful for deep neural networks working on error-prone projects, such as image or speech recognition. * Backpropagation takes advantage of the chain and power rules allows backpropagation to function with any number of outputs. 5.6. UNIT SATURATION (AKA THE VANISHING GRADIENT PROBLEM) The vanishing gradient problem is an issue that sometimes arises when training machine learning algorithms through gradient descent. This most often occurs in neural networks that have several neuronal layers such as in a deep learning system, but also occurs in recurrent neural networks. The key point is that the calculated partial derivatives used to compute the gradient as one goes deeper into the network. Since the gradients control how much the network learns during training, if the gradients are very small or zero, then little to no training can take place, leading to poor predictive performance. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 22 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V The problem: As more layers using certain activation functions are added to neural networks, the gradients of the loss function approaches zero, making the network hard to train. Why: Certain activation functions, like the sigmoid function, squishes a large input space into a small input space between 0 and 1. Therefore, a large change in the input of the sigmoid function will cause a small change in the output. Hence, the derivative becomes small. 0.9 - Sigmoid 0.8 --- Derivative Sigmoid 0.7 0.6 0.5 0.4 Ø3 02 0.1 -10 -8 -6 -4 -2 0 2 4 6 8 10 Fig. 5.13. The sigmoid function and its derivative As an example, Image 1 is the sigmoid function and its derivative. Note how when the inputs of the sigmoid function becomes larger or smaller (when |x | becomes bigger), the derivative becomes close to zero. Why it's significant: For shallow network with only a few layers that use these activations, this isn't a big problem. However, when more layers are used, it can cause the gradient to be too small for training to work effectively. Gradients of neural networks are found using backpropagation. Simply put, backpropagation finds the derivatives of the network by moving layer by layer from the final layer to the initial one. By the chain rule, the derivatives of each layer are multiplied down the network (from the final layer to the initial) to compute the derivatives of the initial layers. However, when n hidden layers use an activation like the sigmoid function, n small derivatives are multiplied together. Thus, the gradient decreases INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 5 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V connected as a directed graph between the input and output layers. MLP uses backpropagation for training the network. MLP is a deep learning method. The algorithm for the MLP is as follows: 1. Just as with the perceptron, the inputs are pushed forward through the MLP by taking the dot product of the input with the weights that exist between the input layer and the hidden layer (WH). This dot product yields a value at the hidden layer. We do not push this value forward as we would with a perceptron though. 2. MLPs utilize activation functions at each of their calculated layers. There are many activation functions to discuss: rectified linear units (ReLU), sigmoid function, tanh. Push the calculated output at the current layer through any of these activation functions. 3. Once the calculated output at the hidden layer has been pushed through the activation function, push it to the next layer in the MLP by taking the dot product with the corresponding weights. 4. Repeat steps two and three until the output layer is reached. 5. At the output layer, the calculations will either be used for a back propagation algorithm that corresponds to the activation function that was selected for the MLP (in the case of training) or a decision will be made based on the output (in the case of testing). Like a single-layer perceptron model, a multi-layer perceptron model also has the same model structure but has a greater number of hidden layers. The multi-layer perceptron model is also known as the Back propagation algorithm, which executes in two stages as follows: ❖ Forward Stage: Activation functions start from the input layer in the forward stage and terminate on the output layer. ❖ Backward Stage: In the backward stage, weight and bias values are modified as per the model's requirement. In this stage, the error between actual output and demanded originated backward on the output layer and ended on the input layer. Hence, a multi-layered perceptron model has considered as multiple artificial neural networks having various layers in which activation function does not remain CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N39 nirf 175° Rank

--- Page 6 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V linear, similar to a single layer perceptron model. Instead of linear, activation function can be executed as sigmoid, TanH, ReLU, etc., for deployment. A multi-layer perceptron model has greater processing power and can process linear and non-linear patterns. Further, it can also implement logic gates such as AND, OR, XOR, NAND, NOT, XNOR, NOR. Advantages of Multi-Layer Perceptron: ❖ A multi-layered perceptron model can be used to solve complex non-linear problems. * It works well with both small and large input data. It helps us to obtain quick predictions after the training. # It helps to obtain the same accuracy ratio with large as well as small data. Disadvantages of Multi-Layer Perceptron: * In Multi-layer perceptron, computations are difficult and time-consuming. * In multi-layer Perceptron, it is difficult to predict how much the dependent variable affects each independent variable. ❖ The model functioning depends on the quality of the training. 5.2. ACTIVATION FUNCTIONS Inputs Weights ×1 W1 ×2 W2 ×3 W3j · . . . . xn Wnj CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt] INSTITUTE . TECHNOLOGY N39 nirf 175" Rank net input netj Σ transfer function Fig. 5.5. activation function - Oj activation Oj threshold

--- Page 17 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Below is the process of the stochastic gradient descent algorithm: 1. The algorithm starts at a random point by initializing the weights with random values 2. Then it calculates the gradients at that random point 3. Then it moves in the opposite direction of the gradient 4. The process continues to repeat itself until it finds the point of minimum loss from sklearn.datasets import make_classification from sklearn.linear_model import SGDClassifier from sklearn.model_selection import cross_val_score samples = 500 x, y = make_classification(n_samples=samples, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1) SGD_classifier = SGDClassifier(loss="perceptron", learning_rate="optimal", n_iter_no_change=10) print(cross_val_score(SGD_classifier, x, y, scoring="accuracy", cv=10).mean()) 5.5. ERROR BACKPROPAGATION, FROM SHALLOW NETWORKS TO DEEP NETWORKS Backpropagation is one of the important concepts of a neural network. Our task is to classify our data best. For this, we have to update the weights of parameter and bias, but how can we do that in a deep neural network? In the linear regression model, we use gradient descent to optimize the parameter. Similarly here we also use gradient descent algorithm using Backpropagation. Backpropagation defines the whole process encompassing both the calculation of the gradient and its need in the stochastic gradient descent. Technically, CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 18 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V backpropagation is used to calculate the gradient of the error of the network concerning the network's modifiable weights. The characteristics of Backpropagation are the iterative, recursive and effective approach through which it computes the updated weight to increase the network until it is not able to implement the service for which it is being trained. Derivatives of the activation service to be known at network design time are needed for Backpropagation. Backpropagation is widely used in neural network training and calculates the loss function for the weights of the network. Its service with a multi-layer neural network and discover the internal description of input-output mapping. It is a standard form of artificial network training, which supports computing gradient loss function concerning all weights in the network. The backpropagation algorithm is used to train a neural network more effectively through a chain rule method. This gradient is used in a simple stochastic gradient descent algorithm to find weights that minimize the error. The error propagates backward from the output nodes to the inner nodes. The training algorithm of backpropagation involves four stages which are as follows * Initialization of weights - There are some small random values are assigned. # Feed-forward - Each unit X receives an input signal and transmits this signal to each of the hidden unit Z1, Z2, ... Zn. Each hidden unit calculates the activation function and sends its signal Zi to each output unit. The output unit calculates the activation function to form the response of the given input pattern. ❖ Backpropagation of errors - Each output unit compares activation Yx with the target value TR to determine the associated error for that unit. It is based on the error, the factor 881 (K = 1, m) is computed and is used to distribute the error at the output unit Yk back to all units in the previous layer. Similarly the factor 88;(j = 1, ... . p) is compared for each hidden unit Zj. ❖ It can update the weights and biases. Consider the following Back propagation neural network example diagram to understand: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY N32 nirf 1750 Rank

--- Page 29 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Drawback: GridSearch CV will go through all the intermediate combinations of hyperparameters which makes grid search computationally very expensive. 9. Randomized Search CV Randomized Search CV solves the drawbacks of GridSearch CV, as it goes through only a fixed number of hyperparameter settings. It moves within the grid in a random fashion to find the best set of hyperparameters. This approach reduces unnecessary computation. The following code illustrates how to use RandomizedSearch CV # Necessary imports from scipy.stats import randint from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import RandomizedSearchCV # Creating the hyperparameter grid param_dist = {"max_depth": [3, None], "max_features": randint(1, 9), "min_samples_leaf": randint(1, 9), "criterion": ["gini", "entropy"]} # Instantiating Decision Tree classifier tree = Decision TreeClassifier() # Instantiating RandomizedSearchCV object tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5) tree_cv.fit(X, y) # Print the tuned parameters and score print("Tuned Decision Tree Parameters: { }".format(tree_cv.best_params_)) print("Best score is { }".format(tree_cv.best_score_)) CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] nirf 1750 Rank N32

--- Page 30 ---
Introduction to Artificial Intelligent and Machine Learning - Unit -V Output: Tuned Decision Tree Parameters: {_min_samples_leaf': 5, _max_depth': 3, max features': 5, _criterion': _ gini'} Best score is 0.7265625 5.9. BATCH NORMALIZATION Normalization is a data pre-processing tool used to bring the numerical data to a common scale without distorting its shape. - Generally, when we input the data to a machine or deep learning algorithm we tend to change the values to a balanced scale. The reason we normalize is partly to ensure that our model can generalize appropriately. Now coming back to Batch normalization, it is a process to make neural networks faster and more stable through adding extra layers in a deep neural network. The new layer performs the standardizing and normalizing operations on the input of a layer coming from a previous layer. A typical neural network is trained using a collected set of input data called batch. Similarly, the normalizing process in batch normalization takes place in batches, not as a single input. X1 X2 X3 X4 W1 W2 W3 W4 W o L = Number of layers Bias = 0 Activation Function : Sigmoid Initially, our inputs X1, X2, X3, X4 are in normalized form as they are coming from the pre-processing stage. When the input passes through the first layer, it transforms, as a sigmoid function applied over the dot product of input X and the weight matrix W X1 X2 O X3 X4 W1 W2 W3 WL W4 h1 = o(W1X) INSTITUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 55 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Environment Variables Now remove active profile from maven settings.xml and update the test profile mentioned in pom.xml. Add activation element to profile element as shown below. The test profile will trigger when the system property "env" is specified with the value "test". Create an environment variable "env" and set its value as "test". <profile> <id>test</id> <activation> <property> <name>env</name> <value>test</value> </property> </activation> </profile> Let's open command console, go to the folder containing pom.xml and execute the following mvn command. C:\MVN\project>mvn test Profile Activation via Operating System Activation element to include os detail as shown below. This test profile will trigger when the system is windows XP. <profile> <id>test</id> <activation> <os> <name>Windows XP</name> <family>Windows</family> <arch>x86</arch> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 14 N32 narf 1750 Rank -

--- Page 56 ---
Course Code/Title:CS3V15/Devops Unit: II <version>5.1.2600</version> </os> </activation> </profile> Now open command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test Profile Activation via Present/Missing File Now activation element to include OS details as shown below. The test profile will triggerwhen target/generated-sources/axistools/wsdl2java/com/companyname/group is missing. <profile> <id>test</id> <activation> <file> <missing>target/generated-sources/axistools/wsdl2java/ com/companyname/group</missing> </file> </activation> </profile> Now open the command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test 8. Maven create and build artifacts: The first step is to open Eclipse, which comes with the integrated Maven environment. The Eclipse window opens on the screen. Complete the following steps: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 15 N32 nirf 175º Rank -

--- Page 65 ---
Course Code/Title:CS3V15/Devops Unit: II Output The output is as follows - java version "1.8.0 66" Java(TM) SE Runtime Environment (build 1.8.0_66-b18) Java HotSpot(TM) 64-Bit Server VM (build 25.66-b18, mixed mode) Step 2 - Download Gradle Build File Download the latest https://gradle.org/install/. version of Gradle from the link available at Step 3 - Set Up Environment for Gradle Setting up environment means, we have to extract the distribution file and copy the library files into proper location. Set up GRADLE_HOME and PATH environmental variables. This step is platform dependent. In Windows Extract the downloaded zip file named gradle-2.11-all.zip and copy the distribution files from Downloads\gradle-2.11\ to C:\gradlel location. After that, add the C: \gradle and C: \gradlelbin directories to the GRADLE_HOME and PATH system variables. Follow the given instructions - Right Click On My Computers -> Click On Properties -> Advanced System Settings -> Click On Environmental Variables. There you will find a dialog box for creating and editing system variables. Click on new button for creating GRADLE_HOME variable (follow the left side screenshot). Click on Edit for editing the existing Path system variable (follow the right side screenshot). Follow the below given screenshots. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 24 N33 nirf 1750 Rank

--- Page 66 ---
Course Code/Title:CS3V15/Devops System Properties X Computer Name Hardware Advanced System Protection Remote Environment Variables x Unit: II System Properties × Computer Name |Hardware Advanced System Protection Remote Environment Variables × New System Variable × Variable name: GRADLE_HOME| Variable value: C:\grade Cancel System variables Variable Value ComSpec C:\Windows\system32\and.exe FP_NO_HOST_C ... NO GRADLE_HOME E:\work\22- Orientdb \gradle-2.11 Variable name: Path Variable value: #C:\gradle bin - System variables Variable Value Edit System Variable × OK Cancel < ORIENTDB_HOME E:\work\22- Orientdblorientdb-communi ... " JAVA_HOME C:\Program Files\Java\jdk1.7.0_60 New ... Edt ... Delete OK Cancel Step 4 - Verify the Gradle installation In windows os Windows_NT Path C:\oraclexe \app \prade\product\11.2.0\, .. PATHEXT .COM ;. EXE ;. BAT ;. CMD ;. VBS ;. VBE ;. JS ;.... v New ... Edit ... Delete OK Cancel You can execute the following command in command prompt. C:\> gradle -v Output Here you will find the Gradle version. Gradle 2.11 Build time: 2016-02-08 07:59:16 UTC Build number: none Revision: 584db1c7c90bdd1de1d1c4c51271c665bfcba978 Groovy: 2.4.4 CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 25 N33 nirf 1750 Rank

--- Page 5 ---
Unit: I Course Code/Title:CS3V15/Devops Example: Facebook's mobile app which is updated every two weeks effectively tells users you can have what you want and you can have it. Now ever wondered how Facebook was able to do social smoothing? It's the DevOps philosophy that helps facebook and sure that apps aren't outdated and that users get the best experience on Facebook. Facebook accomplishes this true code ownership model that makes its developers responsible that includes testing and supporting through production and delivery for each kernel of code. They write and update their true policies like this but Facebook has developed a DevOps culture and has successfully accelerated its development lifecycle. Difference between Agile and DevOps: S. No. Agile 1 It started in the year 2001. It started in the year 2007. 2 Invented by John Kern, and Martin Fowler. Invented by John Allspaw and Paul Hammond at Flickr, and the Phoenix Project by Gene Kim. 3 Agile is a method for creating software. It is not related to software development. Instead, the software that is used by DevOps is pre-built, dependable, and simple to deploy. 4 An advancement and administration approach. Typically a conclusion of administration related to designing. 5 The agile handle centers on consistent changes. DevOps centers on steady testing and conveyance. 6 A few of the finest steps embraced in Agile are recorded underneath - 1. Backlog Building 2.Sprint advancement DevOps to have a few best hones that ease the method - 1. Focus on specialized greatness. 2. Collaborate straightforwardly with clients and join their feedback 7 Agile relates generally to the way advancement is carried of, any division of the company can be spry in its hones. This may be accomplished through preparation. 8 All the group individuals working in a spry hone have a wide assortment of comparable ability sets. This is often one of the points of interest of having such a group since within the time of requirement any of the group individuals can loan help instead of holding up for the group leads or any pro impedances. 9 Spry accepts "smaller and concise". Littler the group superior it would be to convey with fewer complexities. 10 Since Agile groups are brief, a foreordained sum of time is there which are sprints. Tough, it happens that a sprint has endured longer than a month but regularly a week long. 11 A big team for your project is not required. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY DevOps DevOps centers more on program arrangement choosing the foremost dependable and most secure course. DevOps features a diverse approach and is very viable, most of the time it takes after "Divide and Conquer". Work partitioned among the improvement and operation groups. DevOps, on the other hand, accepts that "bigger is better". DevOps, on the other hand, prioritizes reliabilities. It is since of this behavior that they can center on a long-term plan that minimizes commerce's unsettling influences. It demands collaboration among different teams for the completion of work. 5 N33 nirf 1750 Rank

--- Page 6 ---
Unit: I Course Code/Title:CS3V15/Devops 12 Some of the Tools- · Bugzilla · JIRA · Kanboard and more. 13 It is suitable for managing complex projects in any department. It centers on the complete engineering process. 14 It does not focus on the automation. It focusses on automation. 15 Working system gets more significance in Agile than documentation. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Some of the Tools- · Puppet • Ansible • AWS · Chef · team City OpenStack and more. The process documentation is significant in DevOps. 6 N32 nirf 1750 Rank

--- Page 73 ---
Course Code/Title:CS3V15/Devops Unit: II init - Initializes a new Gradle build. wrapper - Generates Gradle wrapper files. Help tasks Build Environment - Displays all build script dependencies declared in root project 'eclipse-workspace'. components - Displays the components produced by root project 'eclipse-workspace '. [incubating] dependencies - Displays all dependencies declared in root project 'eclipse-works pace'. Dependency Insight - Displays the insight into a specific dependency in root project 'eclipse-workspace.' Dependent Components : It displays the dependent components of components in the root project 'eclipse-workspace.' [incubating] help - Displays a help message. model - Displays the configuration model of root project 'eclipse-workspace.' [incubating] projects - Displays the sub-projects of root project 'eclipse-workspace.' properties - Displays the properties of root project 'eclipse-workspace.' tasks - Displays the tasks runnable from root project 'eclipse-workspace.' To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> To list all the tasks of the project, run the below command: 1. gradle tasks -all CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 32 N33 nirf 1750 Rank

--- Page 74 ---
Course Code/Title:CS3V15/Devops Unit: II To display more details about a task, run the below command: 1. gradle help -- task Listing Dependencies In Gradle, we can list the dependencies which are broken down by the configuration. To list the dependencies, run the below command: 1. gradle -q dependencies Difference between Gradle and Maven Basis Based on Gradle is based on developing Maven is based on developing pure Java language-based software. Configuration It uses a Groovy-based Domain-specific language (DSL) for creating project structure. Focuses on Performance It performs better than maven as it optimized for tracking only current running task. It is necessary to compile. Java Compilation Usability It is a new tool , which requires users to spend a lot This tool is a known tool for many users and is easily Gradle domain-specific language projects. It uses Extensible Markup Language (XML) for creating project structure. Developing applications by adding new features to them . It avoids compilation. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] Maven Developing applications in a given time limit. It does not create local temporary files during software creation and is hence - slower . 33 N33 nirf 1750 Rank -

--- Page 63 ---
Course Code/Title:CS3V15/Devops Unit: II 10. Introduction of Gradle: The list of features that Gradle provides, · Gradle is available with separate Domain Specific Language (DSL) based on Groovy language. · It provides the declarative language elements. Those elements also provide build-by- convention support for Java, Groovy, OSGI, Web and Scala. Language for dependency based programming The declarative language lies on a top of a general purpose task graph, which can be fully supported in the build. Structure your build Gradle allows you to apply common design principles to your build. It will give you a perfect structure for build, so that, you can design well-structured and easily maintained, comprehensible build. Deep API By using this API, you can monitor and customize its configuration and execution behavior to the core. Gradle scales Gradle can easily increase the productivity, from simple and single project builds to huge enterprise multi-project builds. Multi-project builds Gradle supports the multi-project builds and partial builds. If you build a subproject, Gradle takes care of building all the subprojects, that the subproject depends on. Different ways to manage your builds Gradle supports different strategies to manage your dependencies. Gradle is the first build integration tool Gradle is fully supported for your ANT tasks, Maven and lvy repository infrastructure for publishing and retrieving dependencies. It also provides a converter for turning a Maven pom.xml to Gradle script. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 22 N32 nirf 175º Rank -

--- Page 64 ---
Course Code/Title:CS3V15/Devops Unit: II Ease of migration Gradle can easily adapt to any structure. Therefore, you can always develop your Gradle build in the same branch, where you can build live script. Gradle Wrapper Gradle Wrapper allows you to execute the Gradle builds on machines, where Gradle is not installed. This is useful for continuous integration of servers. Free open source Gradle is an open source project, and licensed under the Apache Software License (ASL). Groovy Gradle's build script are written in Groovy programming language. The whole design of Gradle is oriented towards being used as a language and not as a rigid framework. Groovy allows you to write your own script with some abstractions. The whole Gradle API is fully designed in Groovy language. Installation of Gradle: Prerequisites to install Gradle JDK and Groovy are the prerequisites for Gradle installation. Gradle requires JDK version 6 or later to be installed in the system. It uses the JDK libraries which are installed, and sets to the JAVA_HOME environmental variable. Gradle carries its own Groovy library, therefore, we need not install Groovy explicitly. If it is installed, that is ignored by Gradle. The steps to install Gradle in your system are explained below. Step 1 - Verify JAVA Installation First of all, you need to have Java Software Development Kit (SDK) installed on your system. To verify this, execute Java -version command in any of the platform you are working on. In Windows Execute the following command to verify Java installation. I have installed JDK 1.8 in my system. C:\> java - version CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE . TECHNOLOGY 23 N32 narf 1750 Rank

--- Page 31 ---
Course Code/Title:CS3V15/Devops Unit: I our work simpler. The pricing of Azure is also simpler and cost-effective. Popularly termed as "Pay As You Go", which means how much you use, pay only for that. Microsoft Azure Used for · Deployment Of applications: You can develop and deploy the application in the azure cloud by using the service called Azure App Service and Azure Functions after deploying the applications end users can access it. · Identity and Access Managment: The application and data which is deployed and stored in the Microsoft Azure can be secured with the help of Identity and Access Managment. It's commonly used for single sign-on, multi-factor authentication, and identity governance. · Data Storage and Databases: You can store the data in Microsoft azure in service like blob storage for unstructured data, table storage for NoSQL data, file storage, and Azure SQL Database for relational databases. The service can be scaled depending on the amount of data we are getting. · DevOps and Continuous Integration/Continuous Deployment (CI/CD): Azure DevOps will provide some tools like ncluding version control, build automation, release management, and application monitoring Following are some of the services Microsoft Azure offers: 1. Compute: Includes Virtual Machines, Virtual Machine Scale Sets, Functions for serverless computing, Batch for containerized batch workloads, Service Fabric for microservices and container orchestration, and Cloud Services for building cloud-based apps and APIs. 2. Networking: With Azure, you can use a variety of networking tools, like the Virtual Network, which can connect to on-premise data centers; Load Balancer; Application Gateway; VPN Gateway; Azure DNS for domain hosting, Content Delivery Network, Traffic Manager, ExpressRoute dedicated private network fiber connections; and Network Watcher monitoring and diagnostics 3. Storage: Includes Blob, Queue, File, and Disk Storage, as well as a Data Lake Store, Backup, and Site Recovery, among others. 4. Web + Mobile: Creating Web + Mobile applications is very easy as it includes several services for building and deploying applications. 5. Containers: Azure has a property that includes Container Service, which supports Kubernetes, DC/OS or Docker Swarm, and Container Registry, as well as tools for microservices. 6. Databases: Azure also included several SQL-based databases and related tools. 7. Data + Analytics: Azure has some big data tools like HDInsight for Hadoop Spark, R Server, HBase, and Storm clusters 8. AI + Cognitive Services: With Azure developing applications with artificial intelligence capabilities, like the Computer Vision API, Face API, Bing Web Search, Video Indexer, and Language Understanding Intelligent. 9. Internet of Things: Includes IoT Hub and IoT Edge services that can be combined with a variety of machine learning, analytics, and communications services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 31 N32 nirf 1750 Rank

--- Page 32 ---
Unit: I Course Code/Title:CS3V15/Devops 10. Security + Identity: Includes Security Center, Azure Active Directory, Key Vault, and Multi-Factor Authentication Services. 11. Developer Tools: Includes cloud development services like Visual Studio Team Services, Azure DevTest Labs, HockeyApp mobile app deployment and monitoring, Xamarin cross- platform mobile development, and more. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 32 N32 nirf 1750 Rank

--- Page 69 ---
Unit: II Course Code/Title:CS3V15/Devops 1 build.gradle gradle wrapper 2 gradle-wrapper.jar - gradle-wrapper.properties 3 gradlew 4 gradlew.bat 5 settings.gradle 6 It is the default structure of a Gradle project. Gradle will generate the following things for us: 1. The gradle file is build script for configuring the current project. 2. An executable JAR file is used as a Gradle wrapper. 3. Configuration properties for Gradle Wrapper. 4. The gradlew is a Gradle wrapper script for UNIX based OS. 5. The bat is the Gradle Wrapper script for Windows. 6. The settings script for configuring the Gradle build. Step3: Create a task Gradle supports APIs for creating and managing tasks through a Groovy-based DSL or Kotlin-based DSL. Every project contains a collection of tasks for some basic operation. Gradle supports a library of tasks that configure the project. For example, there is a Copy task, which copies files from one location to another. The Copy task is one of the most used tasks In Gradle. To use the Copy task in build script, follow the below process. Step1: Create a directory called src C: \Users \HiMaNshU\demo>mkdir src C: \Users \HiMaNshU\demo>cd src Step2: Add a file called myfile.txt in the src directory. Add the single line "Hello, World!" to it, also, we can leave it empty. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 28 N32 nirf 1750 Rank

--- Page 70 ---
Unit: II Course Code/Title:CS3V15/Devops C:\Users \HiMaNshU\demo\src>echo myfile.txt myfile.txt Define a task called Copy in build.gradle file. It will copy the src directory to a new directory called dest. We don't have to create the dest directory; the Copy task will do it for us. 1. task copy(type: Copy, group: "Custom", description: "The sources are copied to dest directory") { 2. from "src" 3. into "dest" 4. } We can provide anything in a group and description. Also, we can omit them, but doing so will also be omitted from the report of the task used later. Now execute our new copy task: C: \Users \HiMaNshU\demo>gradle copy BUILD SUCCESSFUL in 2s 1 actionable task: 1 executed C: \Users \HiMaNshU\demo>_ After the successful execution of the task, we will get BUILD SUCCESSFUL message. The build.gradle file The build.gradle file is build script of a Gradle project. All the tasks and plugins are defined in this file. When we run a gradle command, it looks for a file called build.gradle in the current directory. Although we have called it a build script, strictly, it is a build configuration script. The build script defines a project and its tasks. The default build.gradle file looks like as follows: CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 29 N33 nirf 1750 Rank

--- Page 1 ---
Course Code/Title:CS3V15/Devops CHENNAI INSTITUTE OF TECHNOLOGY Transforming Lives CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) NAME OF THE PROGRAMME B. E (CSE) YEAR III SEMESTER V REGULATIONS 2022R COURSE CODE CS3V15 COURSE NAME DEVOPS FACULTY NAME (Prepared by) NAME OF SUBJECT EXPERT (Verified by) M.SUNDHARI CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Unit: I N33 INATIONAL BOARD . ACCREDITATION 100% Accreditation All eligible UG & PG Programs NIRF 151 - 200 Band Engineering 2023 A+ DE NAAC Contact 8681927167 Contact 1 N32 nirf 1750 Rank

--- Page 2 ---
Course Code/Title:CS3V15/Devops Unit: I UNIT - I Introduction to Devops Devops Essentials - Introduction To AWS, GCP, Azure - Version control systems: Git and Github. The word "DevOps" was coined in 2009 by Patrick Debois, who became one of its gurus. The term was formed by combining "development" and "operations," which provides a starting point for understanding exactly what people typically mean when they say "DevOps." Notably, DevOps isn't a process or a technology or a standard. Many devotees refer to DevOps as a "culture"-a viewpoint that New Relic favors. We also use the term "DevOps movement" when talking about topics such as adoption rates and trends for the future, and "DevOps environment" to refer to an IT organization that has adopted a DevOps culture. "DevOps represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile, lean practices in the context of a system-oriented approach. DevOps emphasizes people (and culture), and seeks to improve collaboration between operations and development teams. DevOps implementations utilize technology- especially automation tools that can leverage an increasingly programmable and dynamic infrastructure from a life cycle perspective." DevOps is defined as a combination of processes and tools created to facilitate organizations in delivering services and applications much faster than they can through conventional software development processes. It helps increase customers' confidence in the applications that an organization offers, thereby allowing the company to flourish and achieve its business goals faster. Development 3 3 DevOps 13 51 Operations Quality Assurance (QA) How DevOps Works? A DevOps process can be summarized as an infinite loop that comprises the following stages - build, test, and release through the delivery pipeline and plan and monitor through feedback, which resets the loop again. With such an amazing combination, teams use tech stack and tooling that assists them in reliably developing apps. Moreover, going away from the Under the DevOps model, development and operations teams work in constant cohesion throughout the entire project lifecycle, starting right from development to deployment. When security is the main focus, the quality assurance team is tightly knitted with the DevOps team throughout the app lifecycle. In this situation, some DevOps teams are also referred to as DevSecOps. Close coordination with the QA team ensures that no loopholes are left unchecked in the provided service/app. COMPANY HOW DEVOPS WORKS DELIVERY PIPELINE BUILD TEST RELEASE PLAN MONITOR FEEDBACK LOOP TOOLBOX" CUSTOMER norm, teams use automated processes here. DevOps tools also allow engineers to complete different tasks independently. Be it provisioning infrastructure or deploying code, they can accomplish these tasks without being dependent on one another. As such, the DevOps model accelerates the overall application development process. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 29 ---
Unit: I Course Code/Title:CS3V15/Devops Enterprise services. AWS virtual private cloud. Outsourced cloud services. Salesforce User Controls Operating System, Runtime, Middleware, and Application data Others It is highly scalable and flexible. Microsoft Azure. Force.com, Gigaspaces. Data of the application It is highly scalable to suit the different businesses according to resources. Google cloud services Google offers a seven wide range of Services: · Compute · Networking · Storage and Databases · Big Data · Machine Learning · Identity & Security · Management and Developer Tools IBM cloud analysis. AWS, Terremark Nothing It is highly scalable to suit the small, mid and enterprise level business 1. Compute: GCP provides a scalable range of computing options you can tailor to match your needs. It provides highly customizable virtual machines. and the option to deploy your code directly or via containers. · Google Compute Engine · Google App Engine · Google Kubernetes Engine · Google Cloud Container Registry · Cloud Functions 2. Networking: The Storage domain includes services related to networking, it includes the following services • Google Virtual Private Cloud (VPC) . Google Cloud Load Balancing · Content Delivery Network · What is Google Cloud Connect · Google Cloud DNS · What is Google Cloud Web Hosting 3. Storage and Databases: The Storage domain includes services related to data storage, it includes the following services · Google Cloud Storage . Cloud SQL · Cloud Bigtable · Google Cloud Datastore • Persistent Disk TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 29 N32 nirf 1750 Rank

--- Page 30 ---
Unit: I Course Code/Title:CS3V15/Devops 4. Big Data: The Storage domain includes services related to big data, it includes the following services · Google BigQuery · Google Cloud Dataproc · Google Cloud Datalab · Google Cloud Pub/Sub 5. Cloud AI: The Storage domain includes services related to machine learning, it includes the following services · Cloud Machine Learning · Vision API · Speech API · Natural Language API · Translation API • Jobs API 6. Identity & Security: The Storage domain includes services related to security, it includes the following services · Cloud Resource Manager · Cloud IAM · Cloud Security Scanner · Cloud Platform Security 7. Management Tools: The Storage domain includes services related to monitoring and management, it includes the following services • Stackdriver · Monitoring · Logging · Error Reporting · Trace · Cloud Console 8. Developer Tools: The Storage domain includes services related to development, it includes the following services . Cloud SDK · Deployment Manager · Cloud Source Repositories · Cloud Test Lab AZURE Azure is Microsoft's cloud platform, just like Google has its Google Cloud and Amazon has its Amazon Web Service or AWS.000. Generally, it is a platform through which we can use Microsoft's resources. For example, to set up a huge server, we will require huge investment, effort, physical space, and so on. In such situations, Microsoft Azure comes to our rescue. It will provide us with virtual machines, fast processing of data, analytical and monitoring tools, and so on to make TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 30 N33 narf 1750 Rank

--- Page 71 ---
Unit: II Course Code/Title:CS3V15/Devops build.gradle &3 I 1 /* 2 * This file was generated by the Gradle 'init' task. 3 * 4 * This generated file contains a sample Java Library project to get you started. 5 * For more details take a look at the Java Libraries chapter in the Gradle 6 * User Manual available at https://docs.gradle.org/6.0.1/userguide/java_library_plugin.html 7 */ 8 9 plugins { 10 // Apply the java-library plugin to add support for Java Library 11 id 'java-library' 12 } 13 14 repositories { 15 // Use jcenter for resolving dependencies. 16 // You can declare any Maven/Ivy/file repository here. 17 jcenter() 18 } 19 20 dependencies { 21 // This dependency is exported to consumers, that is to say found on their compile classpath. 22 api 'org. apache. commons : commons-math3: 3.6.1' 23 24 // This dependency is used internally, and not exposed to consumers on their own compile classpath. 25 implementation 'com. google. guava: guava: 28.0-jre' 26 27 // Use JUnit test framework 30 28 testImplementation 'junit : junit: 4.12' 29 } 30 The build.gradle file contains three default sections. They are as follows: o plugins: In this section, we can apply the java-library plugin to add support for java library. o Repositories: In this section, we can declare internal and external repository for resolving dependencies. We can declare the different types of repository supported by Gradle like Maven, Ant, and Ivy. · Dependencies: In this section, we can declare dependencies that are necessary for a particular subject. Additionally, we can declare other project-related modules like a task in this file. Display the Information of the Gradle project To understand the structure, dependencies and debugging problems of a build, Gradle provides many built-in features that display information on a project. Following are some basic commands to display the information of the project: Listing projects In Gradle, all the sub-projects of a project in the workspace can be listed in a hierarchy. To do so, run the below command from the root directory of the project. 1. gradle -q projects CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 72 ---
Course Code/Title:CS3V15/Devops Unit: II To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> C:\Users \HiMaNshU\eclipse-workspace >gradle -q projects Root project Root project 'eclipse-workspace' No sub-projects To see a list of the tasks of a project, run gradle <project-path> : tasks For example, try running gradle :tasks C:\Users\HiMaNshU\eclipse-workspace> Listing Tasks Gradle allows us to list all the essential tasks of the project. To list the task, run the below command: 1. gradle -q tasks Output: C:\Users\HiMaNshUleclipse-workspace>gradle -q tasks Tasks runnable from root project Build Setup tasks CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 31 N33 nirf 1750 Rank

--- Page 67 ---
Course Code/Title:CS3V15/Devops Unit: II Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013 JVM: 1.7.0_60 (Oracle Corporation 24.60-b09) OS: Windows 8.1 6.3 amd64 Understanding build using Gradle: The Gradle build is a process of creating a Gradle project. When we run a gradle command, it will look for a file called build.gradle in the current directory. This file is also called the Gradle build script. The build configuration, tasks, and plugins are described in this file. The build script describes a project and its tasks. Let's create a small Gradle project, run some of the basic Gradle commands, and understand how Gradle manages the project. Follow the steps below to create and test a Gradle project. Step1: Open the command line and create a directory First, Open the command line and create a directory for the project and change directory to it. Let's create a demo directory. C: \Users \HiMaNshU>mkdir demo C: \Users \HiMaNshU>cd demo Step2: Initialize a Gradle project To generate a Gradle project, run the gradle init command. It will generate a simple project. With this project, we will explore and understand everything that is generated. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 26 N33 nirf 1750 Rank

--- Page 68 ---
Course Code/Title:CS3V15/Devops Unit: II When we run the gradle init command, it will ask for some basic requirements. First, it will ask the type of project that we want to create. It will give four options: 1. 1: basic 2. 2: application 3. 3: library 4. 4: Gradle plugin Select our requirements. Hence, we are just making a demo project so that we will select the basic option. To select basic option, press 1 and Enter key. Consider the below output: C: \Users \HiMaNshU\demo>gradle init Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could ed, use -- status for details Select type of project to generate: 1: basic 2: application 3: library 4: Gradle plugin Enter selection (default: basic) [1 .. 4] 1 Next, it will ask for DSL. There are two options that are available for DSL: 1. 1: Groovy 2. 2: Kotlin Groovy is the default DSL provided by Gradle. Select build script DSL. Select build script DSL: 1: Groovy 2: Kotlin Enter selection (default: Groovy) [1 .. 2] 1 Next, it will ask for the project name. Type the project name and press Enter key. It will take a while to build a project. After the successful execution of the project, we will get a message BUILD SUCCESSFUL. Project name (default: demo): First_Gradle > Task : init Get more help with your project: https: //guides. gradle.org/creating- uilds BUILD SUCCESSFUL in 2m 56s 2 actionable tasks: 2 executed C: \Users\HiMaNshU\demo>mkdir src Now we have successfully created a Gradle project. Now, what will happen to our specified directory? Consider the below structure of the Gradle project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 27 N33 narf 1750 Rank

--- Page 37 ---
Course Code/Title:CS3V15/Devops Unit: I GIT INSTALLATION Git for Windows stand-alone installer · Download the latest Git for Windows installer. · When you've successfully started the installer, you should see the Git Setup wizard screen. Follow the Next and Finish prompts to complete the installation. The default options are pretty sensible for most users. · Open a Command Prompt (or Git Bash if during installation you elected not to use Git from the Windows Command Prompt). . Run the following commands to configure your Git username and email using the following commands, replacing Emma's name with your own. These details will be associated with any commits that you create: $ git config -- global user.name "CIT_CHENNAI" $ git config -- global user.email CITCHENNAI@atlassian.com • Optional: Install the Git credential helper on Windows Bitbucket supports pushing and pulling over HTTP to your remote Git repositories on Bitbucket. Every time you interact with the remote repository, you must supply a username/password combination. You can store these credentials, instead of supplying the combination every time, with the Git Credential Manager for Windows. BASIC COMMANDS OF GIT: Some basic Git commands along with their syntax and examples: · Initialize a Repository: Syntax: `git init' Example: `git init' · Clone a Repository: Syntax: `git clone <repository_url> Example: `git clone https://github.com/example/repository.git' · Check Repository Status: Syntax: `git status' Example: `git status' · Add Changes to Staging Area: Syntax: `git add <file(s)>' Example: `git add file.txt' · Commit Changes: Syntax:`git commit -m "Commit message" Example:`git commit -m "Add new feature" · Create a New Branch: Syntax: `git branch <branch_name>' Example: `git CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY (Autonomous] N33 37 nirf 1750 Rank

--- Page 38 ---
Course Code/Title:CS3V15/Devops branch feature-branch' Unit: I • Switch to a Branch: Syntax: `git checkout <branch_name>' Example:`git checkout feature-branch' OR Syntax: `git switch <branch_name>' (Git version 2.23 and later) Example: `git switch feature-branch' · Create and Switch to a New Branch: Syntax: `git checkout -b <new_branch name>' Example: `git checkout -b new-feature' OR Syntax: `git switch -c <new_branch_name>' (Git version 2.23 and later) Example: `git switch -c new-feature' . Merge Changes from One Branch to Another: Syntax: `git merge <branch_name>' Example: `git merge feature-branch' · View the Commit History: Syntax: `git log' Example: `git log' · Push Changes to a Remote Repository: Syntax: `git push <remote_name> <branch_name>' Example:`git push origin master' · Pull Changes from a Remote Repository: Syntax: `git pull <remote_name> <branch_name>' Example:`git pull origin master' . Show the Differences Between Working Directory and Staging Area: Syntax: `git diff Example: `git diff · Show the Differences Between Staging Area and Last Commit: Syntax: `git diff -- cached' Example: `git diff -- cached' . Show the Differences Between Working Directory and Last Commit: Syntax: `git diff HEAD' Example: `git diff HEAD' CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 38 N32 nirf 1750 Rank

--- Page 25 ---
Course Code/Title:CS3V15/Devops Unit: I · backups · automated multi-region replication in-memory caching · data export tools. Global Infrastructure of AWS · AWS is a cloud computing platform which is globally available. . Global infrastructure is a region around the world in which AWS is based. Global infrastructure is a bunch of high-level IT services which is shown below: AWS is available in 19 regions, and 57 availability zones in December 2018 and 5 more regions 15 more availability zones for 2019. The following are the components that make up the AWS infrastructure: o Availability Zones o Region o Edge locations o Regional Edge Caches Compnents of Global Infrastructure Availability zones Region Edge Locations Regional Edge Caches Availability zone as a Data Center An availability zone is a facility that can be somewhere in a country or in a city. Inside this facility, i.e., Data Centre, we can have multiple servers, switches, load balancing, firewalls. The things which interact with the cloud sits inside the data centers. o An availability zone can be a several data centers, but if they are close together, they are counted as 1 availability zone. Region o A region is a geographical area. Each region consists of 2 more availability zones. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 26 ---
Course Code/Title:CS3V15/Devops Unit: I o A region is a collection of data centers which are completely isolated from other regions. · A region consists of more than two availability zones connected to each other through links. AZ-a AZ-b AZ-c Availability zones are connected through redundant and isolated metro fibers. Edge Locations o Edge locations are the endpoints for AWS used for caching content. o Edge locations consist of CloudFront, Amazon's Content Delivery Network (CDN). o Edge locations are more than regions. Currently, there are over 150 edge locations. Edge location is not a region but a small location that AWS have. It is used for caching the content. o Edge locations are mainly located in most of the major cities to distribute the content to end users with reduced latency. o For example, some user accesses your website from Singapore; then this request would be redirected to the edge location closest to Singapore where cached data can be read. Regional Edge Cache o AWS announced a new type of edge location in November 2016, known as a Regional Edge Cache. o Regional Edge cache lies between CloudFront Origin servers and the edge locations. o A regional edge cache has a large cache than an individual edge location. o Data is removed from the cache at the edge location while the data is retained at the Regional Edge Caches. o When the user requests the data, then data is no longer available at the edge location. Therefore, the edge location retrieves the cached data from the Regional edge cache instead of the Origin servers that have high latency. What Is Cloud Computing? Cloud computing is the use of hardware and software components in an off-premises location to deliver a service to a network. Users can access files and applications from any device that can access the internet. Some features and capabilities include: · Cloud providers can pull the computing resources to provide services to multiple customers with the help of a multi-tenant model · Cloud computing proves an on-demand self-service, which helps administrators monitor performance CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 26 N32 nirf 1750 Rank -

--- Page 7 ---
Course Code/Title:CS3V15/Devops Unit: I DevOps Tools 1. Git (GitLab, GitHub, Bitbucket) Git remains indispensable in software development and DevOps due to its pivotal role in version control, collaborative coding, and efficient project management. As technology has accelerated, the need for streamlined and organized code management has never been greater. Git empowers developers to collaborate on codebases, effortlessly creating and merging branches for new features and bug fixes. Its distributed nature ensures developers can work seamlessly offline, an increasingly valuable feature in today's remote and distributed work environments. Additionally, Git facilitates the tracking of code modifications, making it easier to identify when and why specific changes were made, a critical aspect of maintaining code quality and security. Software development is essential in driving innovation and advancing progress, and Git maintains its prominent position as the bedrock of efficient, cooperative, and secure coding methodologies. 2. Maven Due to its enduring significance in managing project dependencies, building, and project lifecycle management, Maven remains a pivotal tool in SD and DevOps. As a robust build automation and project management tool, Maven simplifies the complexities of Java-based project development by streamlining the compilation, testing, packaging, and distribution processes. It ensures consistent and reproducible builds, making it easier for development teams to collaborate efficiently and deliver high-quality software. Maven's role in managing dependencies and facilitating continuous integration and deployment remains crucial. Its ability to handle complex build scenarios and integrate seamlessly with modern DevOps practices makes it indispensable for ensuring software projects' reliability, maintainability, and scalability in 2024 and beyond. 3. Jenkins Its importance lies in its role as a powerful automation server that enables continuous integration and continuous delivery (CI/CD) pipelines. Jenkins streamlines software development by automating tasks such as building, testing, and deploying code changes, ensuring that software is delivered quickly and highly. With the growing complexity of modern applications, the need for efficient CI/CD processes has become even more paramount. Jenkins provides flexibility, extensibility, and a vast library of plugins that cater to a wide range of technologies and tools, making it adaptable to diverse development environments. As organizations prioritize speed, reliability, and collaboration in their software development practices, Jenkins stands as a cornerstone tool, enabling teams to achieve seamless automation and efficient delivery of software solutions. 4. Chef Chef, a powerful automation platform, is crucial in managing infrastructure as code. Chef empowers organizations to achieve scalability, reliability, and speed seamlessly. By allowing the automation of server provisioning, configuration, and maintenance, Chef enhances efficiency and consistency across the entire infrastructure, reducing manual errors and ensuring that infrastructure remains desired. Moreover, Chef integrates smoothly with various cloud providers, containerization technologies, and other DevOps tools, making it adaptable to the ever-evolving tech TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 7 N32 nirf 1750 Rank

--- Page 8 ---
Course Code/Title:CS3V15/Devops Unit: I landscape. As organizations prioritize agility and scalability, Chef remains a vital tool in automating complex infrastructure tasks and enabling DevOps teams to focus on innovation and delivery. 5. Puppet Puppet is essential because it simplifies the management and orchestration of complex IT infrastructures by allowing administrators to define infrastructure as code. It ensures consistency and repeatability in configuration across servers, cloud instances, and containers. Businesses increasingly rely on diverse, dynamic, and hybrid infrastructures. Puppet's importance lies in its ability to streamline provisioning, configuration, and continuous compliance, thus reducing operational complexity, minimizing errors, and accelerating software delivery. Puppet continues to empower organizations to efficiently manage and scale their infrastructure while maintaining high levels of security and compliance, making it a crucial tool for DevOps teams. 6. Ansible Ansible is a powerful and widely adopted automation and configuration management tool important in 2024 for several reasons. This tool stands out for its simplicity and versatility. It empowers organizations to automate repetitive tasks, provisioning of infrastructure, and configuration management across diverse environments, making it an invaluable asset for DevOps and IT teams. Furthermore, Ansible's agentless architecture, declarative language, and a vast library of pre- built modules make it accessible to both beginners and seasoned professionals. As organizations prioritize efficiency, scalability, and the rapid deployment of applications and services, Ansible remains an indispensable DevOps toolkit, helping teams streamline operations, enhance security, and maintain infrastructure at scale, all while reducing manual errors and increasing agility in a fast-paced technological landscape. 7. Docker Docker is crucial in modern software development and DevOps practices. It can simplify and streamline the management of applications across various environments. Docker containers encapsulate an app and its dependencies, ensuring consistent and reproducible deployments from development to production. This technology enhances portability and scalability, accelerates development cycles, and reduces the "it works on my machine" problem. In a rapidly evolving software landscape, Docker's containerization approach remains crucial for achieving efficient, isolated, and highly flexible application deployment, making it an essential component of DevOps and continuous delivery pipelines. 8. Kubernetes Kubernetes, often abbreviated as K8s, play a central role in modern software development and operations. Its importance lies in its ability to orchestrate, manage, and automate containerized applications at scale. As organizations increasingly embrace microservices architectures and containerization for their applications, Kubernetes provides the essential infrastructure for deploying, scaling, and maintaining these containers efficiently. The tool's resilience, self-healing capabilities, and support for hybrid and multi-cloud environments make it vital for achieving agility, reliability, and cost-effectiveness in application deployment. It serves as the backbone of cloud-native ecosystems, enabling organizations to accelerate software delivery, improve resource utilization, and respond effectively to the evolving demands of the digital landscape. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N32 nirf 1750 Rank

--- Page 51 ---
Course Code/Title:CS3V15/Devops Unit: II 7. Maven Profiles: A Build profile is a set of configuration values, which can be used to set or override default values of Maven build. Using a build profile, you can customize build for different environments such as Production v/s Development environments. Types of Build Profile Build profiles are majorly of three types. Type Where it is defined Per Project Defined in the project POM file, pom.xml Per User Defined in Maven settings xml file (%USER_HOME%/.m2/settings.xml) Global Defined in Maven global settings xml file (%M2_HOME%/conf/settings.xml) Profile Activation A Maven Build Profile can be activated in various ways. · Explicitly using command console input. · Through maven settings. · Based on environment variables (User/System variables). · OS Settings (for example, Windows family). · Present/missing files. Now, under src/main/resources, there are three environment specific files - Sl.No. File Name & Description env.properties 1 default configuration used if no profile is mentioned. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 10 N33 nirf 1750 Rank

--- Page 52 ---
Course Code/Title:CS3V15/Devops Unit: II env.test.properties 2 test configuration when test profile is used. 3 env.prod.properties production configuration when prod profile is used. Explicit Profile Activation In the following example, we will attach maven-antrun-plugin:run goal to test the phase. This will allow us to echo text messages for different profiles. We will be using pom.xml to define different profiles and will activate profile at command console using maven command. Assume, we've created the following pom.xml in C:\MVN\project folder. <project xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation = "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.companyname.projectgroup</groupId> <artifactId>project</artifactId> <version>1.0</version> <profiles> <profile> <id>test</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <version>1.1</version> <executions> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 11 N33 nirf 1750 Rank

--- Page 45 ---
Course Code/Title:CS3V15/Devops Unit: II Example for pom.xml file <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> 4. Maven Repository A maven repository is a directory of packaged JAR file with pom.xml file. Maven searches for dependencies in the repositories. There are 3 types of maven repository: 1. Local Repository 2. Central Repository 3. Remote Repository Maven searches for the dependencies in the following order: Local repository then Central repository then Remote repository. Local Central Remote Repository Repository Repository CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 4 N32 nirf 1750 Rank -

--- Page 46 ---
Course Code/Title:CS3V15/Devops Unit: II If dependency is not found in these repositories, maven stops processing and throws an error. 1) Maven Local Repository Maven local repository is located in your local system. It is created by the maven when you run any maven command. By default, maven local repository is %USER_HOME%/.m2 directory. For example: C:\Users\SSS IT\.m2. Update location of Local Repository We can change the location of maven local repository by changing the settings.xml file. It is located in MAVEN_HOME/conf/settings.xml, for example: E:\apache-maven- 3.1.1\conf\settings.xml. 2) Maven Central Repository Maven central repository is located on the web. It has been created by the apache maven community itself. The path of central repository is: http://repo1.maven.org/maven2/. The central repository contains a lot of common libraries that can be viewed by this url http://search.maven.org/#browse. 3) Maven Remote Repository Maven remote repository is located on the web. Most of libraries can be missing from the central repository such as JBoss library etc, so we need to define remote repository in pom.xml file. Let's see the code to add the jUnit library in pom.xml file. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 5 N32 narf 1750 Rank -

--- Page 3 ---
Course Code/Title:CS3V15/Devops Unit: I Key Goals and Benefits of DevOps Goals of DevOps The fast-paced growth of the IT industry and continuous advancements in technology make it critical to set DevOps goals that are experimental and challenging for companies to compete and thrive in the market. Here are the key goals and principles that every successful DevOps program has in common. 1. Ensures effective collaboration between teams: Effective collaboration in any process relies on shared ownership. During the development process, all those involved should embrace the fact that everyone is equally responsible for the entire development process. Whether it is development, testing, or deployment, each team member should be involved. They should understand that they have an equal stake in the final outcome. In the DevOps paradigm, passing of work from one team to another is completely defined and broken down. This accelerates the entire process of development since collaboration between all the teams involved is streamlined. 2. Creates scalable infrastructure platforms: The primary focus of DevOps is to create a sustainable infrastructure for applications that make them highly scalable. According to the demands of the modern-day business world, scalable apps have become an absolute necessity. In an ideal situation, the process of scaling should be reliable and fully automated. As a result, the app will have the ability to adapt to any situation when a marketing effort goes viral. With the app being scalable, it can adjust itself to large traffic volumes and provide an immaculate user experience. 3. Builds on-demand release capabilities: Companies must focus on keeping their software in a 'releasable' state. Continuous delivery will allow the software to add new features and go live at any stage. DevOps aims to automate the process of release management because it has a plethora of advantages. Automated release management is predictable, fast, and very consistent. Moreover, through automation, companies can release new versions as per their requirements. Automated release management also has complete and thorough audit trials, as these are essential for compliance purposes. 4. Provides faster feedback: Automating monotonous tasks such as testing and reporting will accelerate the process of rapid feedback. Since the development team will know what has to change, it can roll out the updated version faster. In addition, the team can better understand the impact of the changes that it has done in the software lifecycle. A concrete understanding of changes will assist team members in working efficiently in tandem. With rapid feedback, the operations team and developers can make better decisions collectively and enhance the app's performance. Benefits of DevOps DevOps helps organizations deliver added value to their customers. Here are some compelling benefits of DevOps. 1. Smarter work and faster release: With DevOps, your development team can release the required deliverables quickly. Faster release of deliverables will keep you miles ahead of your competitors, which is very important in today's cut-throat business realm. Businesses should understand that if their review cycle is not automated, it will slow down the release process. Moreover, the inclusion of disparate tools will lead to context switching and higher costs. Thus, DevOps can help rectify this worrisome business situation. 2. Quick resolution of issues: In a business world where speed and accuracy are paramount, a fast feedback loop will help you thrive. With DevOps, the communication process becomes seamless, and, as such, it minimizes the time required to solve issues. Without open communication, key issues can slip out of mind, which will have serious repercussions in the long run. DevOps fosters open communication that helps resolve issues, thus unblocking the release pipeline faster. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 3 N32 nirf 1750 Rank

--- Page 4 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Better collaboration between teams: DevOps paves the way for more dynamic and round-the- clock communication between teams. It renders an environment for mutual collaboration and integration among teams that are distributed globally. Eliminating the traditional departmental barriers between teams forms a new sense of ownership, wherein each team member feels equally responsible for meeting delivery timelines. This collaboration contributes to happier and more engaged employees. 4. Fostering innovative mindsets: With DevOps, deployment phases of the application are more relaxed as compared to traditional methods. This is because it streamlines the entire process, ensures that there are no lapses in quality, and allows on-time and efficient release. Thus, as everything is in order, the development team is more at peace. This allows it to think out of the box and provide additional value to the user. Having a development team with an innovative mindset is a boon for any business organization. An innovative approach, in itself, has immense scope and leads to better quality and resolution of issues at hand. Thus, through DevOps, the process of expanding the horizon of an app becomes much easier. 5. Faster threat detection: Automated and continuous testing of the code will make the process of threat detection faster. As developers can locate problem areas at an early stage, they can then resolve them faster. Thus, DevOps is a vital cog in maintaining and enhancing the quality and performance of an app. As the overall build of the app is in capable hands, teams working together are empowered to share feedback as and when necessary. 6. Increased customer satisfaction: Customer satisfaction is paramount in any day and age, irrespective of the business one is involved in. DevOps is known for enhancing customer experience, which ultimately increases the level of customer satisfaction. Dissatisfied customers are never a good sign for any business. Feedback loops are an important component of DevOps. These loops empower end users to track the progress of app development at various stages. 7. In addition, they can suggest changes (if any) or give their inputs to make the app more customer- centric. Due to their dynamic nature, feedback loops help developers and customers remain on the same page. Moreover, DevOps accelerates the process of app development, which eventually lessens the delivery timer. This has a positive impact on the customer satisfaction ratio. 8. Providing the much-needed edge: Along with staying true to their development process, companies need to ensure that they sustain themselves in the cut-throat competition. Implementing DevOps can be your trump card to provide your organization with that much- needed edge. Competitive advantage is necessary, as it can become the deciding factor in the popularity of an application in many cases. Some factors set expert businesses apart from mediocre ones: · Top-quality features · Quicker and timely software releases · Maximizing return on investments · Listening to constructive feedback Difference between Agile and DevOps Agile: Agile program advancement comprises different approaches to computer program improvement beneath which prerequisites and arrangements advance through the collaborative exertion of self- organizing and cross-functional groups and their customer/end client. DevOps: DevOps could be a set of hones that combines program improvement and information- technology operations which points to abbreviating the framework's advancement life cycle and giving nonstop conveyance with tall program quality. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 4 N32 nirf 1750 Rank

--- Page 59 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Specify Archetype parameters M Group Id: com.simplileam Artifact Id: mavenproject Version: 0.0.1-SNAPSHOT Package: com,simplilearn.mavenproject . Properties available from archetype: Name Value Add ... Remove Advanced ? < Back Next > Cancel Finish The project is now created. · Open the pom.xml file You can see all the basic information that you have entered on the screen, such as the Artifact ID, Group ID, etc. You can see the junit dependencies have been added. This process takes place by default in Eclipse. There will also be some by default test cases. File Edit Source Navigate Search Project Run Window Help # Package Explorer E Jo JUnit - mavenproject/pom.xml 28 mavenproject > ( src/main/java ( src/test/java > = JRE System Library (|2SE-1:5] i Maven Dependencies target 7 <artifactIdaavenproject</artifactId> <version>0. 0. 1-SNAPSHOT</version>: <packaging>jar</packaging> <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 136 sproperties> 14 <project. build.sourceEncoding>UTF-8</project. build.sourceEncoding> </properties> 15 16 «dependenciess 18- <dependency> 19 <groupId>junit</grouptasI <artifactId=junit</artifactle> sversion>3.8.1</version> <scope>tests/scope> 21 22 223 </dependency> 24 </dependencies> 25 </project> Overview Dependencies Dependency Hierarchy |Effective POM pom.xml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 18 N32 nirf 175º Rank -

--- Page 60 ---
Course Code/Title:CS3V15/Devops Unit: II There you can find AppTest.java to be a default test case. When you click on that, you can see the test cases written in JUnit on your Eclipse screen. # Package Explorer 13 Ju JUnit mavenproject/pom.xml Z App.java AppTest.java 12 1 package com.simplilearn.mavenproject; - fl mavenproject + ( src/main/java - il com.simplileam.mavenproject U App.java = ( src/test/java il com.simplileam.mavenproject , E AppTest.java > > JRE System Library ||2SE-1.5] al Maven Dependencies - target pom.xml 3w import jumit. framework. Test:[] 6 76 / ** . Unit test for simple App. 10 public class AppTest extends TestCase 12 1 14 . Create the test case 15 16 * @param testName name of the test cole 17 18% public AppTest( String testNane ) 19 1 super( testName ); 28 21 1 22 :23% 24 * @return the suite of tests being tested 75 When it comes to adding more test cases, it will depend on the user, but these test cases and commands can easily be added in the workspace. If we try to remove certain dependencies from our file, we will receive error messages. To troubleshoot this, complete the following steps: · Go to another tab: mavenproject/pom.xml · Delete any dependencies · Save the file Immediately, there will be several error messages in the AppTest.java. R mavenproject/pom.xml App.java AppTest.java 83 1 package com.simplilearn.mavenproject; 2 3@ import junit. framework. Test;[] 6 mavenproject/src/test/java/com/simplileam/mavenproject/AppTest java 8 * Unit test for simple App. 9 18 public class AppTest extends TestCase 011 12 { 136 14 . Create the test case 15 16 * @param testName name of the test case 17 188 19 € public AppTest( String testName ) 28 super( testName ); 21 22 - 230 24 * @return the suite of tests being tested Problems E @ Javadoc ) Declaration History 7 errors, 1 warning, 0 others Description + @ Errors (7 items) a Test cannot be resolved to a type Resource Path Location AppTest.java /mavenproject/src/te line 26 Java Problem Type CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 19 N32 nirf 1750 Rank -

--- Page 15 ---
Course Code/Title:CS3V15/Devops Unit: I which is a mechanism to control the usage of resources or capacity. 2) Code: Many good practices such as Git enables the code to be used, which ensures writing the code for business, helps to track changes, getting notified about the reason behind the difference in the actual and the expected output, and if necessary reverting to the original code developed. The code can be appropriately arranged in files, folders, etc. And they can be reused. 3) Test: The application will be ready for production after testing. In the case of manual testing, it consumes more time in testing and moving the code to the output. The testing can be automated, which decreases the time for testing so that the time to deploy the code to production can be reduced as automating the running of the scripts will remove many manual steps. 4) Plan: DevOps use Agile methodology to plan the development. With the operations and development team in sync, it helps in organizing the work to plan accordingly to increase productivity. 5) Monitor: Continuous monitoring is used to identify any risk of failure. Also, it helps in tracking the system accurately so that the health of the application can be checked. The monitoring becomes more comfortable with services where the log data may get monitored through many third-party tools such as Splunk. 6) Deploy: Many systems can support the scheduler for automated deployment. The cloud management platform enables users to capture accurate insights and view the optimization scenario, analytics on trends by the deployment of dashboards. 7) Operate: DevOps changes the way traditional approach of developing and testing separately. The teams operate in a collaborative way where both the teams actively participate throughout the service lifecycle. The operation team interacts with developers, and they come up with a monitoring plan which serves the IT and business requirements. 8) Release: Deployment to an environment can be done by automation. But when the deployment is made to the production environment, it is done by manual triggering. Many processes involved in release management commonly used to do the deployment in the production environment manually to lessen the impact on the customers. DevOps Lifecycle DevOps defines an agile relationship between operations and Development. It is a process that is practiced by the development team and operational engineers Learning DevOps is not complete without understanding the DevOps lifecycle phases. The DevOps lifecycle includes seven phases as given below: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 16 ---
Unit: I Course Code/Title:CS3V15/Devops together from beginning to the final stage of the product. Development Operations Integration Deployment DevOps Lidecycle Feedback 2) Continuous Integration Testing Monitering 1) Continuous Development This phase involves the planning and coding of the software. The vision of the project is decided during the planning phase. And the developers begin developing the code for the application. There are no DevOps tools that are required for planning, but there are several tools for maintaining the code. This stage is the heart of the entire DevOps lifecycle. It is a software development practice in which the developers require to commit changes to the source code more frequently. This may be on a daily or weekly basis. Then every commit is built, and this allows early detection of problems if they are present. Building code is not only involved compilation, but it also includes unit testing, integration testing, code review, and packaging. The code supporting new functionality is continuously integrated with the existing code. Therefore, there is continuous development of software. The updated code needs to be integrated continuously and smoothly with the systems to reflect changes to the end-users. Commit Build O O Test O Stage Deploy Dev/QA O O > Continuous Integration/Delivery Development </> git Code Commit = production server. 3) Continuous Testing Production Jenkins is a popular tool used in this phase. Whenever there is a change in the Git repository, then Jenkins fetches the updated code and prepares a build of that code, which is an executable file in the form of war or jar. Then this build is forwarded to the test server or the This phase, where the developed software is continuously testing for bugs. For constant testing, automation testing tools such as TestNG, JUnit, Selenium, etc are used. These tools allow QAs to test multiple code- bases thoroughly in parallel to ensure that there is no flaw in the functionality. In this phase, Docker Containers can be used for simulating the test environment. It may occur in the form of documentation files or maybe produce large-scale data about the application parameters when it is in a continuous use position. The system errors such as server not reachable, low memory, etc are resolved in this phase. It maintains the security and availability of the service. Automation testing saves a lot of time and effort for executing the tests instead of doing this manually. Apart from that, report generation is a big plus. The task of evaluating the test cases INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 narf 1750 Rank

--- Page 33 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between AWS (Amazon Web Services), Google Cloud, and Azure Technology EC2 (Elastic Compute Cloud) AWS Databases Supported Pricing Models Difficulties Many enterprises find it difficult to understand the company cost AWS fully relational and supports NoSQL databases and Big Data. Per hour - rounded up. On demand, reserved spot. structure. Google Cloud Google Engine(GCE) Compute Technologies pioneered by Google, like Big Query, Big Table, and Hadoop, are databases, and Big Data,naturally fully supported. Per minute - rounded up On demand sustained use. — Fewer services. features Azure VHD Disk) (Virtual Hard Azure supports relational and both NoSQL Windows and through AzureTable HDInsight. Per minute - rounded up. Per minute- rounded up commitments(Pre-paid or monthly) and Less "Enterprise-ready. Storage Services Simple Storage Service(S3) Elastic Block Storage. Elastic File storage. Machine Learing Sage maker. Lex. polly.And many more Blob Storage Queue Storage. File Storage Disk Storage. Data Lake Store Machine learning Azure Bot service Cognitive service GIT Cloud storage. Persistent Disk Transfer appliance. Cloud speech AI Cloud Video Intelligence. Cloud Machine learning engine Git is a distributed version control system (DVCS) that helps manage and track changes in source code during software development. It was created by Linus Torvalds in 2005 and has become one of the most widely used version control systems in the software development industry. Some key concepts and features of Git: · Version Control: Git allows developers to keep track of changes made to their code over time. This includes modifications, additions, and deletions of files. · Distributed System: Git is a distributed version control system, meaning that each developer has a complete copy of the entire repository, including its full history. This allows developers to work independently and merge their changes when necessary. · Branching: Git enables developers to create branches, which are essentially separate lines of development. This allows for the parallel development of features or bug fixes without affecting the main codebase. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) LEN 33 nirf 1750 Rank

--- Page 34 ---
Course Code/Title:CS3V15/Devops Unit: I · Merging: Git provides tools for merging changes from one branch into another. This is essential when multiple developers are working on different branches and need to bring their changes together. · History Tracking: Git maintains a complete history of changes made to the codebase. Developers can view, revert, or analyze changes made over time. · Remote Repositories: Git supports remote repositories, allowing developers to collaborate with others by pushing and pulling changes to and from a shared repository. Platforms like GitHub, GitLab, and Bitbucket provide hosting services for Git repositories. · Staging Area: Git uses a staging area (also known as the index) to prepare and review changes before committing them to the repository. This allows developers to selectively include or exclude specific changes. · Open Source: Git is an open-source tool, and its source code is freely available for modification and distribution. Various GIT Components: Git is composed of several key components that work together to enable version control and collaborative development. Here are the main components of Git: · Repository (Repo): A repository is a directory or storage space where your project and its version history are stored. It contains all the files and directories associated with your project, along with the metadata and configuration information. · Working Directory: The working directory is the directory on your local machine where you manipulate files and make changes to your project. It is essentially your local copy of the repository. · Index (Staging Area): The index, also known as the staging area, is a middle ground where changes are prepared before being committed to the repository. It allows you to selectively stage changes, which means you can choose which modifications to include in the next commit. · Commit: A commit is a snapshot of the changes made to the files in the repository. It represents a specific point in the project's history and is accompanied by a commit message that describes the changes. · Branch: A branch is a parallel line of development within a repository. It allows developers to work on different features or bug fixes simultaneously without affecting the main codebase. Branches can be merged to incorporate changes into other branches. . Head: HEAD is a reference to the latest commit in the currently checked-out branch. It essentially points to the tip of the branch you are currently on. · Remote: A remote is a version of the repository stored on a different server. Git supports collaboration by allowing developers to push and pull changes between their local repository and remote repositories. Platforms like GitHub, GitLab, and Bitbucket are examples of remote repositories. · Clone: Cloning is the process of creating a copy of a remote repository on your local machine. This allows you to start working on your own copy of a project. · Fetch: The fetch operation retrieves changes from a remote repository but does not automatically merge them into your working directory. It is useful for reviewing changes before deciding to merge. · Pull: Pull is a combination of fetch and merge. It retrieves changes from a remote repository and automatically merges them into your working directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 21 ---
Unit: I Course Code/Title:CS3V15/Devops · Gaming: AWS has been serving many gaming studios. Combining Amazon EC2 and S3 services with CloudFront enables gaming websites to deliver high-quality gaming experiences to their customers regardless of location. Use Cases of AWS · Netflix Netflix is an entertainment platform that started in the United States, but eventually, it expanded to many countries and soon became popular. However, once Netflix confronted the scalability problem because of the sudden increase in viewers. That made Netflix choose AWS services. Netflix reports that when it started using AWS services like DynamoDB and Cassandra for its distributed databases, it could handle the data easily. So, scalability is a great advantage of AWS. Netflix has adapted around 100,000 server instances from AWS for computing and storage databases, analytics, recommendation engines, and video transcoding as well. . McDonald's McDonald's is the world's largest fast-food company that serves around 64 million people per day. The growth of this company has gone to another level when it started home deliveries. By utilizing AWS services, McDonald's created a platform that integrates local restaurants with delivery partners such as Uber Eats. Scalability is also a reason for the company to choose AWS services. Moreover, with AWS Microservices Architecture, McDonald's platform can scale 20,000 orders per second and integrate with the global partners easily. · Airbnb Airbnb is an international online marketplace for rental homes. This platform connects people who are looking for rental accommodation with those who want to rent out their houses. Quite soon, Airbnb became unable to handle the constant streaming of data on the website from its customers. That is when it started using Amazon EC2 service and Elastic Load Balancing, which distributes incoming traffic to multiple Amazon EC2 instances. In this way, Airbnb could avoid traffic, and customers could use the online platform without any disruption. • Novartis Novartis is the best example for AWS use cases in healthcare. Novartis is one of the world's largest healthcare companies that provides solutions for patients' well-being. It adapted Amazon EC2 services and built a platform using other services such as Amazon Simple Storage Service, Amazon Elastic Block Store, and four availability zones. Data Analysts of Novartis are taking advantage of the AWS services and still implementing new solutions for the patients. · Expedia Expedia is a worldwide online travel agency that has always focused on the constant development and innovation of its platform to offer an extraordinary user experience for its clients. Since 2010, Expedia has been using AWS services to build a standard deployment model for better infrastructure as AWS offers the best data security through different availability zones. • Samsung If you are using Samsung mobile phones, then you may know about the Samsung app store. For setting up the apps stacked in its store, the company started using AWS services. Using AWS app development services, Samsung wanted to provide its customers with the facility to download the apps anywhere without any network traffic. • NASA NASA (National Aeronautics and Space Administration) has always wondered about creating a library to present people with all its achievements through pictures and videos of space. Later on, it created such platforms, but because it had 10 different NASA centers, it couldn't provide the best experience for viewers. So, all it wanted was to create an easy-access platform for TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 21 N32 nirf 1750 Rank

--- Page 22 ---
Unit: I Course Code/Title:CS3V15/Devops people to search for and view images and videos. Then, NASA started adopting many services from AWS to solve this problem, which included Amazon Elastic Compute Cloud, Elastic Load Balancing, Amazon Simple Storage Service, Amazon Simple Queue Service, etc. Among these, Amazon S3 helped the company store all the incoming data such as photos, videos, and audio files without any hassle. • Facebook Facebook, without a doubt, is a widespread social media platform. To build a scalable application, Facebook used services such as Amazon Elastic Compute Cloud, Amazon Simple Storage Service, Amazon Relational Database Service, Amazon SimpleDB, Amazon CloudFront, Amazon Simple Queue Service, etc. Amazon RDS helps the platform to make it easy to set up, operate, and scale the database in the cloud. Various Services offered by AWS · Amazon EC2 (Elastic Cloud computing) · Amazon RDS (Relational Database Services) · Bonus Service: Amazon Connect · Amazon S3 (Simple Storage Service) · Amazon Lambda · Amazon Cognito · Amazon Glacier · Amazon SNS (Simple Notification Service) · Bonus Service: Amazon Lex · Amazon Lightsail · Amazon VPC (Virtual Private Cloud) · Amazon Kinesis · Amazon Inspector · Amazon Auto-scaling · Amazon IAM (Identity and Access Management) · Dynamo DB · Amazon SQS (Simple Queue Service) · Amazon ElastiCache · Amazon Chime · AWS Athena · Code Catalyst · Web Application Firewall · AWS Amplify · AWS Rekognition · AWS QuickSight · AWS CloudFormation · AWS Management Console The Important Cloud Services according to various categories that are provided by AWS are given below : 1. Compute • Amazon EC2: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It allows organizations to obtain and configure virtual compute capacity in the cloud. You can select from a variety of operating systems and resource configurations like memory, CPU, and storage that are required for your TECHNOLOGY (Autonomous) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY 22 N33 narf 1750 Rank

--- Page 47 ---
Course Code/Title:CS3V15/Devops Unit: II <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> Any repository of maven is available in mvnrepository.com 5. Maven Plugins: The maven plugins are central part of maven framework, it is used to perform specific goal. According to Apache Maven, there are 2 types of maven plugins. 1. Build Plugins 2. Reporting Plugins Build Plugins These plugins are executed at the time of build. These plugins should be declared inside the <build> element. Reporting Plugins These plugins are executed at the time of site generation. These plugins should be declared inside the <reporting> element. Maven Core Plugins A list of maven core plugins are given below: Plugin Description clean clean up after build. compiler compiles java source code. deploy deploys the artifact to the remote repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY N32 nirf 6 175º Rank -

--- Page 48 ---
Course Code/Title:CS3V15/Devops Unit: II failsafe runs the JUnit integration tests in an isolated classloader. install installs the built artifact into the local repository. resources copies the resources to the output directory for including in the JAR. site generates a site for the current project. surefire runs the JUnit unit tests in an isolated classloader. verifier verifies the existence of certain conditions. It is useful for integration tests. Example for maven plugin: The compiler plugin is used to compile the source code of a Maven project. This plugin has two goals, which are already bound to specific phases of the default lifecycle: · compile - compile main source files · testCompile - compile test source files Here's the compiler plugin in the POM: <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.12.1</version> <configuration> <source>1.8</source> <target>1.8</target> </configuration> </plugin> 6. Maven Build Lifecycle: A Build Lifecycle is a well-defined sequence of phases, which define the order in which the goals are to be executed. Here phase represents a stage in life cycle. As an example, a typical Maven Build Lifecycle consists of the following sequence of phases. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 7 N33 nirf 175º Rank -

--- Page 11 ---
Course Code/Title:CS3V15/Devops Unit: I 17. Selenium It remains a vital tool in software testing and automation due to its enduring relevance in ensuring the quality of web applications. As technology evolves, web applications become increasingly complex, requiring thorough testing across various browsers and platforms. With its robust automation capabilities and extensive browser compatibility, Selenium allows developers and QA teams to automate repetitive testing tasks efficiently, conduct cross- browser testing, and ensure that web applications function flawlessly across diverse environments. Its open-source nature, active community support, and integration with other DevOps tools make Selenium a go-to choice for organizations striving for continuous delivery and the rapid deployment of high-quality software, a cornerstone of modern software development practices. 18. Gremlin Gremlin is an essential tool in chaos engineering, which has become increasingly critical for ensuring the resilience and reliability of modern software systems. As technology advances and complex distributed systems become the norm, the potential for unexpected failures and outages also rises. Gremlin allows organizations to proactively identify weaknesses and vulnerabilities in their infrastructure and applications by simulating controlled failures, such as network disruptions, service outages, and resource constraints. By intentionally inducing chaos and monitoring the system's response, teams can uncover weaknesses before they lead to costly downtime or security breaches. Gremlin facilitates organizations to build more robust, fault-tolerant systems that can withstand real-world challenges and deliver uninterrupted services to users. 19. ServiceNow ServiceNow is a vital platform for organizations seeking to streamline their IT service management and beyond. Its significance lies in its ability to provide a unified, cloud-based solution for automating and optimizing various business processes, including ITSM, ITOM, HR, customer service, and more. Due to the rapid digitization of services, remote work, and the growing complexity of technology infrastructures, ServiceNow offers a comprehensive approach to managing workflows, resolving issues, and delivering services efficiently. Its intelligent automation capabilities, analytics, and AI-driven insights empower organizations to enhance productivity, agility, and customer satisfaction while reducing operational costs. ServiceNow's role in orchestrating and integrating diverse systems and processes makes it an indispensable tool for driving digital transformation and ensuring smooth operations in the ever-evolving business landscape of 2024. 20. Status Service Updates: The Status Page "Status Service Updates: The Status Page" is a critical tool for organizations and businesses of all sizes. In today's world, where online services and applications are integral to operations, ensuring their availability and reliability is essential. It provides real-time information to users and stakeholders about the operational status of services, applications, and infrastructure. The Status Page plays a crucial role in transparency, trust-building, and customer satisfaction by promptly communicating service disruptions, planned maintenance, and incident resolutions. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 1750 Rank

--- Page 12 ---
Course Code/Title:CS3V15/Devops Unit: I Downtime can often lead to significant financial losses and damage to a company's reputation, so having a practical Status Page becomes not just a convenience but a necessity. It allows organizations to showcase their commitment to transparency and responsiveness in addressing service-related issues, ultimately fostering stronger customer relationships and trust. 21. ELK (Elasticsearch, Logstash and Kibana) ELK, which stands for Elasticsearch, Logstash, and Kibana, continues to shine in DevOps and IT operations. This powerful trio of tools remains essential for organizations seeking effective log management, monitoring, and data visualization. Elasticsearch is a highly scalable and fast search engine that enables real-time data indexing and search. Logstash facilitates the collection, processing, and transformation of log data from various sources, making it compatible with Elasticsearch. Kibana, on the other hand, provides a user- friendly interface for visualizing and analyzing data, offering customizable dashboards and powerful data exploration capabilities. ELK's significance in 2024 lies in its ability to empower organizations with comprehensive insights into their systems, applications, and infrastructure. It ultimately facilitates quick problem resolution, proactive monitoring, and data-driven decision-making in an increasingly complex and fast-paced technological landscape. 22. GitLab CI/CD GitLab CI/CD's significance lies in its ability to automate the complete software delivery pipeline, from code changes to deployment, in a single integrated environment. GitLab CI/CD ensures rapid and reliable delivery of software updates. It enables continuous integration (CI) by automatically building and testing code changes, allowing teams to catch issues early in the development cycle. Furthermore, the continuous deployment (CD) aspect automates the release and deployment process, reducing the risk of human errors and enabling organizations to deliver features and updates to users swiftly and confidently. GitLab CI/CD's importance is further accentuated as businesses seek to accelerate digital transformation efforts, respond rapidly to changing market demands, and maintain a competitive edge through efficient and automated software delivery practices. 23. Scripting Scripting remains vital due to its pivotal role in automating and streamlining various aspects of software development, system administration, and DevOps practices. Scripting languages like Python, Bash, and PowerShell empower tech professionals to write code that can execute repetitive tasks, manipulate data, and orchestrate complex processes efficiently. Scripting facilitates rapid prototyping, configuration management, and the creation of automated deployment pipelines. It enhances productivity, ensures consistency and reduces human error in tasks ranging from software testing and deployment to infrastructure provisioning and monitoring. As organizations increasingly embrace DevOps and cloud- native technologies, scripting stays competitive and adaptive in the tech landscape. 24. Terraform Terraform plays a crucial role in modern infrastructure provisioning and management. It allows organizations to define and deploy infrastructure as code, enabling the automated creation and configuration of cloud resources, containers, and other infrastructure components. Cloud computing, microservices, and containerization have become the norm in CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 12 N32 nirf 1750 Rank

--- Page 61 ---
Course Code/Title:CS3V15/Devops Unit: II Return to the previous screen and undo the deletion. The errors that occurred will disappear. *mavenproject/pom.xml & IApp.java AppTest.java 6 <artifactId>mavenproject</artifactId> 7 <version>0.0.1-SNAPSHOT</version> <packaging>jar</packaging> 8 9 16 <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 138 <properties> 14 15 16 17% <dependencies> 18 19 <groupId>junit</groupId> I 28 21 22 23 24 <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> cdependency> <artifactId>junit</artifactid> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> 25 </project> 26 9. Dependency Management: The dependencyManagement and dependencies are especially useful for multi-module projects. Dependency Management : This tag consists of a dependencies tag which itself might contain multiple dependency tags. Each dependency is supposed to have at least three maintags: groupId, artifactId, and version. For example: <dependencyManagement> <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 20 N33 nirf 1750 Rank -

--- Page 62 ---
Course Code/Title:CS3V15/Devops Unit: II The above code just declares the new artifact commons-lang3, but it doesn't really add it to the project dependency resource list. Dependencies: This tag contains a list of dependency tags. Each dependency is supposed to have at least two main tags, which are groupId and artifactId. For example: <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> The version and scope tags can be inherited implicitly if we have used the dependencyManagement tag before in the POM file. DependencyManagement is just a declaration, and it does not really add a dependency. Dependencies tag adds the actual dependency to the project. An example for adding the JUnit library dependency: <dependencyManagement> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.2</version> <scope>test</scope> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 21 N33 nirf 175º Rank

--- Page 39 ---
Course Code/Title:CS3V15/Devops Unit: I GITHUB GitHub is an increasingly popular programming resource used for code sharing. It's a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in October 2020, it is the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 56 million total developers. GitHub is a Git repository hosting service that provides a web-based graphical interface. It is the world's largest coding community. Putting a code or a project into GitHub brings it increased, widespread exposure. Programmers can find source codes in many different languages and use the command-line interface, Git, to make and keep track of any changes. GitHub helps every team member work together on a project from any location while facilitating collaboration. You can also review previous versions created at an earlier point in time. GitHub's Features? 1. Easy Project Management: GitHub is a place where project managers and developers come together to coordinate, track, and update their work so that projects are transparent and stay on schedule. 2. Increased Safety With Packages Packages can be published privately, within the team, or publicly to the open-source community. The packages can be used or reused by downloading them from GitHub. 3. Effective Team Management GitHub helps all the team members stay on the same page and organized. Moderation tools like Issue and Pull Request Locking help the team to focus on the code. 4. Improved Code Writing Pull requests help the organizations to review, develop, and propose new code. Team members can discuss any implementations and proposals through these before changing the source code. 5. Increased Code Safety GitHub uses dedicated tools to identify and analyze vulnerabilities to the code that other tools tend to miss. Development teams everywhere work together to secure the software supply chain, from start to finish. 6. Easy Code Hosting All the code and documentation are in one place. There are millions of repositories on GitHub, and each repository has its own tools to help you host and release code. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 39 N32 nirf 1750 Rank

--- Page 40 ---
Course Code/Title:CS3V15/Devops Unit: I HOSTING SERVICE FOR GIT REPOSITORY When it comes to hosting Git repositories, various platforms provide a robust infrastructure for collaborative development, version control, and project management. Choosing the right hosting service depends on factors like ease of use, collaboration features, and integration capabilities. Here are some popular Git hosting services widely utilized in the software development community: 1. GitHub: GitHub stands out as one of the most prevalent Git hosting platforms, offering a user- friendly interface, powerful collaboration features, and seamless integration with various tools. It serves as an ideal choice for open-source projects, private repositories, and team collaboration. 2. GitLab: GitLab is a comprehensive web-based Git repository manager that not only provides source code management but also includes features like continuous integration. It caters to both cloud-based and self-hosted solutions, giving users flexibility in hosting their repositories. 3. Bitbucket: Owned by Atlassian, Bitbucket is another popular Git repository hosting service. Supporting both Git and Mercurial repositories, it offers features like code collaboration, issue tracking, and continuous integration. Bitbucket is often preferred by teams using other Atlassian tools such as Jira and Confluence. 4. GitKraken Glo Boards: GitKraken Glo Boards is an integrated task and issue tracking service linked with GitKraken, a Git client. This platform allows teams to manage tasks directly associated with their Git repositories and provides a visual approach to monitoring project progress. 5. SourceForge: SourceForge, with a long history, hosts open-source software projects and offers version control, bug tracking, and project management tools. While not as prominent as some other options, it remains a viable choice for numerous projects. 6. AWS CodeCommit: As part of Amazon Web Services (AWS), AWS CodeCommit is a fully managed source control service. It seamlessly integrates with other AWS services and provides a secure and scalable environment for hosting Git repositories. Selecting the most suitable Git hosting service depends on your team's requirements, project size, and preferences for cloud-based or self-hosted solutions. Each platform has its strengths, catering to specific use cases within the software development landscape. Difference between GIT and GITHUB Git and GitHub are related concepts but serve different purposes in the context of version control and collaborative software development. Git: Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows. GitHub: GitHub is a web-based Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 40 N32 nirf 1750 Rank

--- Page 13 ---
Course Code/Title:CS3V15/Devops Unit: I 2024. Terraform provides the agility and scalability required to keep up with the dynamic demands of modern applications. Terraform's importance lies in its ability to bring consistency, version control, and automation to infrastructure operations, thereby reducing manual errors, streamlining DevOps workflows, and facilitating applications' rapid and reliable deployment in an increasingly complex and cloud-centric environment. As organizations adopt cloud-native technologies, Terraform remains essential to ensure efficient and consistent infrastructure management. 25. Phantom Phantom enhances security automation and incident response capabilities. In today's rapidly evolving threat landscape, organizations face a constant barrage of cybersecurity incidents, and the ability to respond swiftly and effectively is necessary. It provides a platform for automating security workflows, from detecting and investigating potential threats to orchestrating responses and mitigating risks. Phantom's importance lies in its capacity to reduce response times, increase consistency in incident handling, and free up manual resources from repetitive tasks. With the growing complexity of cyber threats, Phantom empowers security teams to defend against attacks and safeguard critical assets proactively. 26. Nagios Nagios, an open-source monitoring and alerting system, remains vital due to its enduring significance in maintaining the reliability and performance of IT infrastructure and applications. Organizations increasingly rely on complex systems and services. Nagios plays a crucial role by providing real-time monitoring and alerting capabilities, allowing IT teams to detect and address issues before they impact users or cause system outages. Its versatility, extensibility, and support for both on-premises and cloud environments make Nagios a valuable tool for ensuring critical systems' availability, stability, and security, aligning perfectly with the demands of modern IT operations and DevOps practices. 27. Vagrant Vagrant continues to play a crucial role in software development and DevOps. It is a tool that simplifies creating and managing reproducible development environments. Its importance lies in its ability to provide developers and DevOps teams with a consistent and isolated environment for software development, testing, and deployment. With the ever-evolving complexity of software stacks, dependencies, and infrastructure configurations, Vagrant remains essential in ensuring these environments are easily shareable, scalable, and maintainable. It allows developers to work seamlessly across various operating systems and provides a standardized setup that minimizes compatibility issues. 28. Sentry Sentry plays a critical role in modern software development and DevOps practices. With software applications' increasing complexity and scale, identifying and addressing errors and issues has become crucial. Sentry is vital because it provides real-time error tracking and monitoring, allowing development teams to proactively detect and diagnose issues, whether they occur in production or during development. Its importance is minimizing downtime, improving user experience, and maintaining software systems' overall health and reliability. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nirf 1750 Rank

--- Page 14 ---
Course Code/Title:CS3V15/Devops Unit: I 29. Gradle Gradle continues to be a vital tool in software development and DevOps. Gradle is an advanced build automation system that plays a crucial role in managing dependencies, building projects, and orchestrating complex workflows efficiently. Its importance lies in its versatility and scalability, as it caters to various project sizes and types. Gradle's ability to easily handle multi-language, multi-project builds and its support for plugin-based customization make it indispensable in modern software development. As organizations increasingly adopt microservices architectures and cloud-native technologies, Gradle's capabilities are instrumental in managing the complexity of building, testing, and deploying applications across diverse environments. DevOps Architecture DevOps Architecture Plan Release Code Dep Deploy Development and operations both play essential roles in order to deliver applications. The deployment comprises analyzing the requirements, designing, developing, and testing of the software components or frameworks. The operation consists of the administrative processes, services, and support for the software. When both the development and operations cture is the solution to fix the gap between Test Monitor Build Operate ined with collaborating, then the DevOps al www deployment and operation terms; therefore, delivery can be faster. DevOps architecture is used for the applications hosted on the cloud platform and large distributed applications. Agile Development is used in the DevOps architecture so that integration and delivery can be contiguous. When the development and operations team works separately from each other, then it is time- consuming to design, test, and deploy. And if the terms are not in sync with each other, then it may cause a delay in the delivery. So DevOps enables the teams to change their shortcomings and increases productivity. Below are the various components that are used in the DevOps architecture: 1) Build: Without DevOps, the cost of the consumption of the resources was evaluated based on the pre-defined individual usage with fixed hardware allocation. And with DevOps, the usage of cloud, sharing of resources comes into the picture, and the build is dependent upon the user's need, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Build Code Test Plan DevOps Components Moniter Dev Ops Deploy Operate Release 14 N33 nirf 1750 Rank

--- Page 9 ---
Course Code/Title:CS3V15/Devops Unit: I 9. Slack Slack is a crucial tool for businesses and organizations worldwide. Its significance lies in facilitating seamless communication and collaboration among teams, whether working in the same office or remotely. Slack's real-time messaging, file sharing, and integration capabilities streamline workflow, enhance productivity and keep teams connected across different time zones and locations. As the work landscape evolves, with more companies embracing hybrid and remote work models, Slack is a vital hub for quick decision-making, project coordination, and knowledge sharing. With an ever- expanding ecosystem of integrations and features, Slack remains at the forefront of modern workplace communication, making it essential for businesses to stay agile, efficient, and competitive. 10. AWS Cloud Computing and Storage in DevOps AWS (Amazon Web Services) Cloud Computing and Storage are crucial in DevOps because they provide scalable, flexible, and cost-effective infrastructure for DevOps practices. AWS offers many services, including compute resources, databases, container orchestration, and serverless computing, which align perfectly with modern software development and deployment demands. Organizations adopt DevOps to accelerate software delivery. AWS provides the foundation for rapidly deploying and scaling applications, supporting continuous integration and continuous delivery (CI/CD) pipelines, and automating infrastructure provisioning through tools like AWS CloudFormation. Furthermore, AWS's storage solutions enable efficient data management, backup, and recovery, ensuring the resilience and reliability required for DevOps operations. As cloud technology evolves, AWS remains at the forefront, enabling DevOps teams to focus on innovation and efficiency. 11. Azure Cloud Computing and Storage in DevOps Azure Cloud Computing and Storage will be pivotal in DevOps practices in 2024 and beyond. Azure offers a comprehensive cloud ecosystem that enables organizations to scale their infrastructure, deploy applications, and store data efficiently. Azure provides essential services for continuous integration and continuous deployment (CI/CD), automation, monitoring, and security. Its cloud computing capabilities facilitate the provisioning of resources on demand, ensuring that development and testing environments are readily available. Azure's storage solutions, including Azure Blob Storage, Azure Files, and Azure SQL Database, enable secure data storage and retrieval, supporting the data-driven aspects of DevOps. Besides, Azure's integration with DevOps tools like Azure DevOps Services streamlines the software development lifecycle, enhancing collaboration and automation. 12. GCP Cloud Computing and Storage in DevOps Google Cloud Platform (GCP) offers robust cloud computing and storage solutions. GCP provides a scalable, reliable, and highly available infrastructure essential for modern DevOps practices. With its comprehensive set of services, including Google Compute Engine, Google Kubernetes Engine, Cloud Storage, and BigQuery, GCP empowers DevOps teams to build, deploy, and manage applications easily. Its emphasis on automation, infrastructure as code, and container orchestration aligns seamlessly with DevOps principles. Moreover, GCP's cutting-edge technologies, such as AI and machine learning capabilities, provide DevOps practitioners with advanced tools for monitoring, analytics, and automation, making it a powerful choice for organizations seeking to optimize their software development and delivery processes. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 10 ---
Course Code/Title:CS3V15/Devops Unit: I 13. Monitoring, Alerting, and Incident Response Tools: SignalFx Monitoring, alerting, and incident response tools like SignalFx are pivotal in DevOps and software development. As software systems become complex and distributed, the need for real-time visibility into performance and the ability to respond swiftly to incidents is significant. SignalFx excels in this regard by providing advanced monitoring and observability solutions that enable organizations to detect anomalies, trace issues across microservices proactively, and set up intelligent alerts. As applications scale, cloud-native architectures become the norm, and user expectations for reliability grow, SignalFx's capabilities are crucial. It empowers DevOps teams to ensure high availability, optimize resource utilization, and maintain a seamless user experience by identifying and addressing performance issues before they impact end-users. It is one of the most essential tools for modern software operations. 14. Appdynamics AppDynamics, a leading application performance management and monitoring platform, remains critically important as it ensures the optimal performance of modern digital businesses. As organizations rely on complex and distributed software systems, proactively monitoring, troubleshooting, and optimizing these applications becomes essential. AppDynamics provides real-time visibility into application performance, allowing businesses to swiftly identify bottlenecks, latency issues, and errors. With the ever-growing complexity of applications, the importance of AppDynamics lies in its ability to empower organizations to deliver exceptional user experiences, maintain application reliability, and swiftly respond to performance issues, thereby ensuring the continued success and competitiveness of digital businesses. 15. Raygun It is a crucial tool in software development and DevOps because it ensures application reliability and performance. Raygun is an application monitoring and error-tracking platform that empowers development teams to identify, diagnose, and resolve real-time issues. With software systems growing in complexity and the increased demand for seamless user experiences, Raygun's importance lies in providing actionable insights into application errors and performance bottlenecks. It enables organizations to proactively address issues, reduce downtime, and enhance user satisfaction, leading to higher software quality and improved customer experiences. Software is central to businesses across industries. Raygun's role in maintaining application health and facilitating rapid issue resolution makes it a fundamental tool for DevOps professionals and software developers. 16. Splunk Cloud Splunk Cloud helps organizations gain critical insights from the ever-expanding volume of data generated in today's digital landscape. As businesses increasingly rely on data- driven decision-making, Splunk Cloud stands out as a robust and scalable platform for monitoring, searching, analyzing, and visualizing machine-generated data. Its importance lies in providing real-time visibility into the health and performance of complex systems, applications, and infrastructures, enabling rapid incident detection and response. As cybersecurity threats evolve, Splunk Cloud's advanced security analytics and threat detection capabilities remain indispensable for safeguarding against cyberattacks and ensuring data integrity. In a world where data is a strategic asset, Splunk Cloud's role in harnessing the power of data for operational excellence and security cannot be overstated. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 10 N32 nirf 1750 Rank

--- Page 75 ---
Course Code/Title:CS3V15/Devops Customization This tool is highly customizable as it supports a variety of IDE's . Languages supported Project Configuration Based on Goal of time to get used to it. available. It supports software development in Java, C, C++, and Groovy. For declaring the project configuration, it does not use the XML files. On the phases of the fixed and linear model. Graph of task dependencies that do the work. To add functionality in the project is the main goal of the Gradle. To finish the project in the given timeline is the main goal of the Maven. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY Unit: II This tool serves a limited number of developers and is not that customizable. It supports software development in Java, Scala, C# , and Ruby and it does not natively support C and C+ but can support through plugins like "maven-native- plugin" or we can integrate other build systems like CMake or Makefile. For declaring the project configuration, it uses the XML files. 34 N33 nirf 1750 Rank

--- Page 76 ---
Course Code/Title:CS3V15/Devops Unit: III Unit - III CONTINUOUS INTEGRATION USING JENKINS Install & Configure Jenkins, Jenkins Architecture Overview, creating a Jenkins Job, configuring a Jenkins job, Introduction to Plugins, Adding Plugins to Jenkins, commonly used plugins (Git Plugin, Parameter Plugin, HTML Publisher, Copy Artifact and Extended choice parameters). Configuring Jenkins to work with java, Git and Maven, creating a Jenkins Build and Jenkins workspace. Installing and Configuring Jenkins Overview Jenkins is a widely-used open-source automation server that helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration and continuous delivery (CI/CD). Below are detailed steps and considerations for installing and configuring Jenkins. Installation 1. System Requirements: ○ Operating System: Jenkins can run on any major operating system. ○ Java: Jenkins requires Java Runtime Environment (JRE) 8 or later. 2. Download Jenkins: ○ Download the latest version of Jenkins from the official Jenkins website. 3. Installation Steps: ○ Windows: I Run the .msi installer and follow the installation wizard. ■ After installation, Jenkins will start automatically as a Windows service. ○ Linux: Add Jenkins repository and import the GPG key. I Use package manager to install Jenkins (e.g., apt-get install jenkins). I Start Jenkins using systemctl (systemctl start jenkins). 4. Initial Setup: ○ After installation, Jenkins can be accessed through a web browser at http : //< your _server _ip_or_domain>: 8080. o Unlock Jenkins by entering the initial admin password, which can be found in the jenkins_home directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 1 N32 nirf 1750 Rank

--- Page 19 ---
Course Code/Title:CS3V15/Devops Unit: I 5. Compliance: By compliance, we mean that certain businesses are required to follow specific regulations. Financial services companies in the United States, for example, must comply with the Sarbanes-Oxley Act, while healthcare, education, and energy companies must comply with HIPAA and other regulations. AWS provides a number of compliance-related features and services, such as data encryption and identity and access management, to help businesses meet these requirements. 6. High-Performance: Interested in delivering your applications quickly and efficiently? Taking advantage of AWS features such as auto-scaling and load balancing will help ensure your applications are always available and running optimally. AWS can help businesses improve their performance by offering a variety of cloud-based services, including Amazon Elastic Compute Cloud (EC2), which provides high-performance computing resources, and Amazon CloudFront, which delivers content quickly and securely to users around the world. Others include machine learning (ML) and analytics services, such as Amazon SageMaker and Amazon Athena. These services provide the tools businesses need to quickly and easily analyze their data for insights. Fast networking in the cloud is also possible with AWS, thanks to its Elastic Load Balancing (ELB) and Amazon Virtual Private Cloud (VPC). With ELB, businesses can balance their workloads across multiple instances for increased performance, while VPC allows businesses to create isolated private networks in the cloud. 7. Developer Tools: Developer tools are designed to make it easier for developers to create, deploy, and manage applications - and AWS provides developers with what they need to build applications quickly and easily. By leveraging developer tools, developers can save time and money by automating tedious tasks. They also benefit from access to AWS's extensive library of pre-built applications that can help them get their projects off the ground quickly. Services such as Amazon Elastic Beanstalk and Amazon CloudFormation can help them automate the process of creating and deploying applications. Other ways developers can improve productivity with AWS include using AWS Code Commit to store and manage source code. 8. Integration: Thanks to its many integrations with other Amazon services, as well as third-party services, AWS makes it easy for businesses to get started with cloud computing. AWS provides a wide range of services that can be easily integrated into existing business infrastructure. This allows businesses to add new features and capabilities without having to make major changes or invest in new hardware or software. For instance, if a business wants to add mobile capabilities to its website, it can take advantage of Amazon's Mobile SDK and Web Services. These tools allow businesses to quickly develop and deploy mobile apps that connect directly with their existing infrastructure. 9. Management Console: The AWS management console is a web-based interface that provides users with a simple way to interact with and manage their AWS resources - essentially a place where you can access and manage everything on the cloud. It provides a graphical view of all the resources associated with an account, as well as tools for creating and configuring new resources. Compared to traditional command- line interfaces, the AWS management console saves time and makes it easier for users to get the most out of their AWS services. Not only that, but your business gets access to 350+ free digital training courses through the AWS Academy, covering topics such as cloud fundamentals, DevOps, security, and big data. This means you can train your employees on how to use AWS, and in turn, help them become more efficient at their jobs. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 19 N32 nirf 1750 Rank

--- Page 20 ---
Course Code/Title:CS3V15/Devops Unit: I 10. Scalability: With an on-demand service, businesses can quickly spin up new servers as needed with just a few clicks. This makes it much easier to scale resources up or down as demand changes, allowing businesses to save costs and maintain performance even during peak periods. For example, if a business is expecting a sudden surge in traffic due to an advertising campaign or seasonal event, they can easily add more capacity to their server infrastructure to handle the increased load. Bru Textiles, a specialty textile company in Belgium, was able to quickly scale its infrastructure by leveraging AWS. Bru Textiles went digital to grow and offer new services. Embracing technology, they brought in digital twin technology to give their customers an idea of the texture and essence of their physical fabrics. AWS Applications · Storage and Backup: Storage and backup are important for any Cloud Computing service. AWS provides you with reliable storage services like Amazon Simple Storage Service to store large- scale data and backup services like AWS Backup to take backups of this data, which is stored in other AWS services. AWS stores the data in three different availability zones so that if one fails, you can still access your data. This makes AWS storage reliable and easily accessible. Therefore, companies with huge application data to store and backup securely can use AWS. · Big Data: One of the biggest challenges faced by companies these days is Big Data. The companies are struggling to store their large amounts of data using traditional methods. With AWS Big Data storage services, they can manage to store their data even if the data limit increases unexpectedly as AWS provides virtually unlimited data storage with scale-in and scale- out options. AWS offers easy access and faster data retrieval as well. For data processing, it offers services like EMR, with which the companies can easily set up, operate, and scale their big data. Therefore, efficiently storing and managing Big Data is among the top AWS applications. · Enterprise IT: AWS is a one-stop solution for any IT business. Many features of it such as secure storage, scalability, flexibility, and elasticity support companies to innovate faster than ever before. Using AWS for IT enterprises makes them profitable in terms of both money and time. As AWS maintains its cloud architecture, it need not waste time and money on professionals to do the same. · Social Networking: Social networking is essential for businesses in the present-day scenario where Digital Marketing is key, and it is easier with AWS. Companies can connect with customers and stakeholders and communicate through social networking sites and develop their business. Services like AWS social networking engine, which is powered by Turn Key GNU/Linux (HVM) AMI stack, are used for performance and scalability to help companies build a suitable social networking site and gain profits. · Mobile Apps: Mobile applications are embedded with day-to-day life. With AWS, you have the facility to create an app in your desired programming language. You can also keep up the applications that are consistently accessible and solid with high compute, storage, database, and application services. You can take advantage of AWS auto-scaling and managed relational database service for the better performance of your apps. · Websites: AWS offers a wide range of website hosting options to create the best website for customers. Its services like Amazon Light sail have everything, such as a virtual machine, SSD- based storage, data transfer, DNS management, and a static IP, to launch a website in such a way that the user can manage the website easily. Amazon EC2, AWS Lambda, Elastic Load Balancing, AWS Amplify, Amazon S3, etc. also help users build reliable and scalable websites. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 53 ---
Course Code/Title:CS3V15/Devops Unit: II <execution> <phase>test</phase> <goals> <goal>run</goal> </goals> <configuration> <tasks> <echo>Using env.test.properties</echo> <copy file="src/main/resources/env.test.properties" tofile="${project.build.outputDirectory}/env.properties"/> </tasks> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> Now open the command console, go to the folder containing pom.xml and execute the following mvn command. Pass the profile name as argument using -P option. C:\MVN\project>mvn test -Ptest Maven will start processing and displaying the result of test build profile. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 12 N32 nirf 1750 Rank

--- Page 54 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Maven Settings Open Maven settings.xml file available in %USER_HOME%/.m2 directory where %USER_HOME% represents the user home directory. If settings.xml file is not there, then create a new one. Add test profile as an active profile using active Profiles node as shown below in example. <settings xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"> <mirrors> <mirror> <id>maven.dev.snaponglobal.com</id> <name>Internal Artifactory Maven repository</name> <url>http://repo1.maven.org/maven2/</url> <mirrorOf> *< /mirrorOf> /mirror </mirrors> <activeProfiles> <activeProfile>test</activeProfile> </activeProfiles> </settings> Now open command console, go to the folder containing pom.xml and execute the following mvn command. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 13 N33 nirf 1750 Rank

--- Page 79 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Plugins: ○ Role: Extend Jenkins functionality without modifying the core software. ○ Types: I Source Control Plugins: Git, SVN, Mercurial. Build Tools Plugins: Maven, Gradle, Ant. Notification Plugins: Email, Slack, HipChat. · Reporting Plugins: JUnit, Cobertura, Checkstyle. ○ Management: Plugins can be installed, updated, and configured via the Jenkins web interface. 5. Master-Slave Architecture: ○ Concept: The Jenkins master distributes build tasks to multiple agents, enabling distributed builds. ○ Benefits: Scalability: Handle more builds simultaneously. I Resource Management: Assign builds to agents with the necessary resources. Isolation: Run builds in isolated environments to avoid conflicts. Workflow and Data Flow 1. Job Trigger: ○ Jobs can be triggered by various events: Manual Trigger: Initiated by a user through the Jenkins interface. Scheduled Trigger: Using CRON-like syntax to schedule jobs. · SCM Trigger: Automatically triggered by changes in the source code repository. Upstream/Downstream Trigger: Triggered by the completion of other jobs. 2. Build Execution: ○ Job Assignment: The master assigns the job to an available agent based on labels, availability, and resource requirements. Build Environment Setup: The agent sets up the environment, including checking out the code, installing dependencies, and configuring the workspace. ○ Execution: The agent runs the build steps as defined in the job configuration. 3. Build Results: ○ Logs: Captures console output and logs from the build process. ○ Artifacts: Stores build artifacts like binaries, packages, and reports. ○ Test Results: Collects and displays test results and code coverage reports. ○ Build History: Maintains a history of all builds, including status, duration, and changes. 4. Notification and Reporting: ○ Notifications: Jenkins can send notifications through various channels (email, chat, etc.) upon build completion or failure. ○ Dashboards: Provides dashboards for visualizing build status, trends, and metrics. ○ Reports: Generates and displays reports on test results, code quality, and other metrics. INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 4 1750 Rank

--- Page 80 ---
Unit: III Course Code/Title:CS3V15/Devops Summary Jenkins' architecture is designed to be flexible and scalable. The master-agent model allows for efficient distribution of build tasks, while plugins provide extensive customization options. By leveraging Jenkins' comprehensive job configuration and robust notification and reporting capabilities, teams can streamline their CI/CD pipelines and improve software quality and delivery speed. Creating a Jenkins Job Creating a Jenkins job involves setting up a new project within Jenkins to automate various tasks such as building code, running tests, and deploying applications. Here is a step-by-step guide to creating a Jenkins job. Prerequisites · Jenkins installed and running. · Basic understanding of the Jenkins interface. . Access to the Jenkins dashboard. Step-by-Step Guide 1. Access Jenkins Dashboard: . Open your web browser and navigate to your Jenkins instance URL (e.g., http://localhost:8080). 2. Create a New Job: ○ On the Jenkins dashboard, click on the "New Item" link on the left-hand side menu. 3. Enter Job Name: ○ Enter a name for your new job in the "Enter an item name" field. o Choose the type of job you want to create. For most uses, "Freestyle project" is a good starting point. ○ Click "OK". 4. Configure the Job: ○ After clicking "OK", you will be directed to the job configuration page. 5. General Configuration: ○ Description: Provide a brief description of the job. ○ Discard Old Builds: Optionally set this to limit the number of old builds Jenkins keeps. ○ Select Version Control System: Choose the version control system (e.g., Git, Subversion). 6. Source Code Management: ○ Repository URL: Enter the repository URL. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 5 nirf 175 Rank N33

--- Page 57 ---
Course Code/Title:CS3V15/Devops · Go to the File option · In the drop-down menu, select New · Select the Project option Unit: II If you want to create a Java project, you can select the "Java Project" option. Since we are not creating a Java project specifically, we have chosen the "Project" option. eclipse-workspace - Eclipse File Edit Source Refactor Navigate Search Project Run Window Help New Open File ... G) Open Projects from File System ... Shift+Alt+N Close Close All Shift + Cb1+W Save Save As. Save All Shift+Ctrl+S Revert Move .:. Rename 2 Refresh ES Convert Line Delimiters To Print Import ... Export ... Ctrl+P de java Project ui Project ... El Package G Class Interface G Enum @ Annotation 69 Source Folder 18 Java Working Set Folder File Untitled Text File Task JUnit Test Case Example ... Other ... Ctrl+N The dialog box that appears on the screen will display different types of projects. · Select the Maven Project option . Click on Next New Project + Select a wizard Create a Maven Project Wizards: type filter text irroject Gradle Java 2º Java Project # Java Project from Existing Ant Buildfile - @Maven Lu Check out Maven Projects from SCM M& Maven Module Maven Project Examples ? < Back Next > X Cancel Finish A dialog box will appear. Select the default workspace. . Click on "Next" Several Group IDs, Artifact IDs, and Versions will then appear. · Select a plugin there and click on "Next" CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 16 N33 narf 1750 Rank

--- Page 58 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Select an Archetype M Catalog: All Catalogs Configure .. Biter: × Group Id Artifact id Version org.apache.maven.archetypes maven-archetype-archetype 1.0 org.apache.maven.archetypes maven-archetype-|2ee-simple 1.0 org.apache.maven archetypes maven-archetype-plugin 1.2 org.apache.maven.archetypes maven-archetype-plugin-site 1.1 org.apache.maven.archetypes maven-archetype-portlet 1.0.1 org.apache.maven.archetypes maven-archetype-profiles 1.0-alpha-4 org.apache.maven.archetypes maven-archetype-quickstart 1.1 org.apache.maven.archetypes maven-archetype-site 11 org anacha mauan archabenes masan.archchina.cito.cimnia 11 An archetype which contains a sample Maven project. Show the last version of Archetype only Include snapshot archetypes Add Archetype ... > Advanced ? < Back Next > Cancel Finish In the next dialog box that appears, you'll complete the following steps: · Enter the Group ID “com.xyz" · Enter the Artifact ID "mavenproject" · The version will appear on the screen These items can all be modified at a later time if needed. · Click on "Finish" CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 17 N33 nirf 1750 Rank

--- Page 43 ---
Course Code/Title:CS3V15/Devops Unit: II o Generates source code (if auto-generated code is used) o Generates documentation from source code o Compiles source code o Packages compiled code into JAR of ZIP file o Installs the packaged code in local repository, server repository, or central repository 2. Installation of Maven: To install maven on windows, you need to perform following steps: 1. Download maven and extract it 2. Add JAVA_HOME and MAVEN_HOME in environment variable 3. Add maven path in environment variable 4. Verify Maven 1) Download Maven To install maven on windows, you need to download apache maven first. Download Maven latest Maven software from Download latest version of Maven For example: apache-maven-3.1.1-bin.zip 2) Add MAVEN_HOME in environment variable Right click on MyComputer -> properties -> Advanced > Environment variables -> click new button System Settings - Now add MAVEN_HOME in variable name and path of maven in variable value. It must be the home directory of maven i.e. outer directory of bin. For example: E:\apache-maven-3.1.1 3) Add Maven Path in environment variable Click on new tab if path is not set, then set the path of maven. If it is set, edit the path and append the path of maven.Here, we have installed JDK and its path is set by default, so we are going to append the path of maven. The path of maven should be %maven home%/bin. For example, E:\apache-maven- 3.1.1\bin . 4) Verify maven To verify whether maven is installed or not, open the command prompt and write: mvn -version Now it will display the version of maven and jdk including the maven home and java home. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 2 N32 nirf 175º Rank

--- Page 44 ---
Unit: II Course Code/Title:CS3V15/Devops 3. POM FILES: POM is an acronym for Project Object Model. The pom.xml file contains information of project and configuration information for the maven to build the project such as dependencies, build directory, source directory, test source directory, plugin, goals etc. Maven reads the pom.xml file, then executes the goal. Before maven 2, it was named as project.xml file. But, since maven 2 (also in maven 3), it is renamed as pom.xml. Elements of maven pom.xml file For creating the simple pom.xml file, you need to have following elements: Element Description project It is the root element of pom.xml file. modelVersion It is the sub element of project. It specifies the modelVersion. It should be set to 4.0.0. groupId It is the sub element of project. It specifies the id for the project group. artifactId It is the sub element of project. It specifies the id for the artifact (project). An artifact is something that is either produced or used by a project. Examples of artifacts produced by Maven for a project include: JARs, source and binary distributions, and WARs. version It is the sub element of project. It specifies the version of the artifact under given group. Maven pom.xml file with additional elements Here, we are going to add other elements in pom.xml file such as: Element Description packaging defines packaging type such as jar, war etc. name defines name of the maven project. url defines url of the project. dependencies defines dependencies for this project. dependency defines a dependency. It is used inside dependencies. scope defines scope for this maven project. It can be compile, provided, runtime, test and system. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 3 N32 narf 1750 Rank -

--- Page 27 ---
Course Code/Title:CS3V15/Devops Unit: I · Servers are maintained easily and there is nearly zero downtime · Users can access cloud data and upload it on the cloud from any device with a solid internet connection · Cloud environments can be modified according to the user's requirements and is easily accessible · Clouds are highly secure, making data breaches more unlikely · Migrating to the cloud eliminates the need to buy on-premises infrastructure . It offers pay-as-you-go pricing, meaning you only pay for the resources you use 1. Infrastructure as a Service: IaaS delivers virtualized computing resources over the Internet. Users can rent virtual machines, storage, and networking infrastructure, allowing for easy scalability without investing in physical hardware. Examples include AWS EC2 and Azure Virtual Machines. 2. Platform as a Service: PaaS offers a robust platform for developers to build, deploy, and manage apps without worrying about the underlying infrastructure. It simplifies application development and deployment, with services like Google App Engine and Heroku leading the way. 3. Software as a Service: SaaS offers software applications on a subscription basis, accessible via a web browser. Users don't need to install or maintain software locally, making it ideal for collaboration tools (e.g., Microsoft 365, Google Workspace) and CRM systems (e.g., Salesforce). 4. Function as a Service: FaaS allows developers to execute code responding to events without managing servers. It's highly scalable and cost-efficient, exemplified by AWS Lambda and Azure Functions. FaaS is also known as serverless computing. 5. Container as a Service: CaaS enables the deployment and management of containerized applications using orchestration tools like Kubernetes. It provides portability and scalability for applications across different cloud environments. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 27 N33 nif 1750 Rank

--- Page 28 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between main cloud computing services Terms Stands for Uses Access Model Technical understanding. Popularity Percentage rise Usage Cloud services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY IAAS Infrastructure as a service. IAAS is used by network architects. IAAS gives access to the resources like virtual machines and virtual storage. It is a service model that provides virtualized computing resources over the internet. It requires technical knowledge. It is popular among developers and researchers. It has around a 12% increment. Used by the skilled developer to develop unique applications. Amazon Web Services, sun, vCloud Express. PAAS Platform as a service. PAAS is used by developers. PAAS gives access to run time environment to deployment and development tools for application. It is a cloud computing model that delivers tools that are used for the development of applications. Some knowledge is required for the basic setup. It is popular among developers who focus on the development of apps and scripts. It has around 32% increment. Used by mid-level developers to build applications. Facebook, and Google search engine. SAAS Software as a service. SAAS is used by the end user. SAAS gives access to the end user. It is a service model in cloud computing that hosts software to make it available to clients. There is no requirement about technicalities company handles everything. It is popular among consumers and companies, such as file sharing, email, and networking. It has about a 27 % rise in the cloud computing model. Used among the users of entertainment. MS Office web, Facebook and Google Apps. 28 N32 nirf 1750 Rank

--- Page 23 ---
Course Code/Title:CS3V15/Devops Unit: I application. Amazon EC2 enables you to increase or decrease capacity within minutes. You can use one or hundreds or even thousands of server instances simultaneously. Because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs. Amazon EC2 is integrated with most AWS services, such as Amazon Simple Storage Service (Amazon S3), Amazon Relational Database Service (Amazon RDS), and Amazon Virtual Private Cloud (Amazon VPC) to provide a complete, secure solution for computing applications. Amazon EC2 is an example of Infrastructure as a Service(IaaS). EC2 delivers secure, reliable, cost-effective compute and high-performance compute infrastructure so as to meet the needs of demanding businesses. Amazon EC2 is one of the easiest ways of providing servers on AWS Cloud and also the access to Operating system. · AWS Lambda: AWS Lambda is a serverless, event-driven compute service that allows you to run code without managing servers. You pay only for the compute time you consume and there is no charge when your code is not running. With AWS Lambda, you can run code for any type of application with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services, or you can call it directly from any web or mobile app. But triggering Lambda is possible with over 200 AWS services. You can only pay for what you have used. The compute time that you consume, you are needed to pay for it. You just only need to upload your code and everything required to run will take care of by Lambda and it automatically scales your code with high availability. · AWS Elastic Beanstalk: AWS Elastic Beanstalk is a Platform as a Service that facilitates quick deployment of your applications by providing all the application services that you need for your application. Beanstalk is a plug- and-play platform that allows working with multiple programming languages and environments. Elastic Beanstalk supports a large range of platforms like Node js, Java, PHP, Python, and Ruby. So, you can develop your application to meet your requirements and simply deploy it on Elastic Beanstalk. The main aim to use AWS Elastic Beanstalk is to allow you to focus on the deployment and management of your applications. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. 2. Networking · Amazon VPC: Amazon VPC is your network environment in the cloud. It allows you to create a private network within the AWS cloud that uses many of the same concepts and constructs as an on-premises network. Amazon VPC also gives you complete control of the network configuration. Customers can define normal networking configuration items such as IP address ranges, subnet creation, route table creation, network gateways, and security settings. Amazon VPC is an AWS foundational service and integrates with numerous AWS services. For instance, Amazon EC2 instances are deployed into your CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 24 ---
Course Code/Title:CS3V15/Devops Unit: I Amazon VPC. Similarly, Amazon Relational Database Service (Amazon RDS) database instances deploy into your Amazon VPC, where the database is protected by the structure of the network just like your on-premises network. You can easily launch AWS resources into a virtual network by Amazon Virtual Private Cloud. An isolated virtual network environment in the AWS cloud is created by Amazon VPC. . Amazon Route 53: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating human-readable names, such as www.geeksforgeeks.com, into the numeric IP addresses that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. 2. Storage · Amazon S3 (Simple Storage Service): Amazon Simple Storage Service (Amazon S3) is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the web. It is designed to provide an infinite amount of storage and it is delivered with 99.999999999% durability. You can use Amazon S3 as primary storage for cloud-native applications as a target for backup and recovery and disaster recovery. It offers industry-leading scalability, data availability, security, and performance. It's simple to move large volumes of data into or out of Amazon S3 with Amazon's cloud data migration options. Once data is stored in Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like Amazon S3 Standard - Infrequent Access and Amazon Glacier for archiving. · Amazon Glacier: Amazon Glacier is a secure, durable, and extremely low- cost storage service for data archiving and long-term backup. Data stored in Amazon Glacier takes several hours to retrieve, which is why it's ideal for archiving. The fastest access to your archive data is via Amazon Glacier. 3. Databases · Amazon RDS (Relational Database Service): Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost- efficient and resizable capacity while managing time-consuming database administration tasks, freeing you up to focus on your applications and business. You can find Amazon RDS is also available on several database instance types - optimized for memory, performance, or I/O. Amazon RDS provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. · Amazon DynamoDB (Non-Relational Database): Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. When you create a database table that can store and retrieve any amount of data you can simply use Amazon DynamoDB that will serve any level of requested traffic. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, Internet of Things (IoT), and many other applications. DynamoDB provides many features like · built-in security CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 24 N32 nirf 1750 Rank

--- Page 49 ---
Course Code/Title:CS3V15/Devops Unit: II Validate Deploy Compile Install Test Verify Package Integration Test · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. There are always pre and post phases to register goals, which must run prior to, or after a particular phase. When Maven starts building a project, it steps through a defined sequence of phases and executes goals, which are registered with each phase. Maven has the following three standard lifecycles - • default: This is the main lifecycle, as it's responsible for project deployment. CHENNAI CHENNAI INSTITUTE . TECHNOLOGY 8 clean: Handles project cleaning, ensuring that all artifacts generated by previous INSTITUTE OF TECHNOLOGY 175º Rank (Autonomous]

--- Page 50 ---
Course Code/Title:CS3V15/Devops Unit: II builds are removed. · site: Manages the creation of the project's site documentation. Default Lifecycle (default) Most Maven users will be familiar with the default lifecycle. It is a general model of a build process for a software application. The first phase is validate and the last phase is deploy. · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. Clean Lifecycle (clean) The first lifecycle in Maven. Running mvn clean invokes the clean lifecycle which consists of three lifecycle phases: · pre-clean : execute processes needed prior to the actual project cleaning · clean : remove all files generated by the previous build · post-clean : execute processes needed to finalize the project cleaning Site Lifecycle (site) Maven does more than build software artifacts from project, it can also generate project documentation and reports about the project, or a collection of projects. Project documentation and site generation have a dedicated lifecycle which contains four phases: 1. pre-site : execute processes needed prior to the actual project site generation 2. site : generate the project's site documentation 3. post-site : execute processes needed to finalize the site generation, and to prepare for site deployment 4. site-deploy: deploy the generated site documentation to the specified web server CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 41 ---
Unit: I Course Code/Title:CS3V15/Devops S.No. Git 1 Git is a software. 2 Git is a command-line tool GitHub is a graphical user interface 3 Git is installed locally on the system GitHub is hosted on the web 4 Git is maintained by linux. GitHub is maintained by Microsoft. 5 Git is focused on version control and code sharing. GitHub is a hosting service for Git repositories. 6 Git is a version control system to manage source code history. 7 Git was first released in 2005. GHub was launched in 2008. 8 Git has no user management feature. 9 Git is open-source licensed. GitHub includes a free-tier and pay- for- use tier. 10 Git has minimal external tool configuration. 11 Git provides a Desktop interface named Git Gui. GitHub provides a Desktop interface named GitHub Desktop. 12 Git competes with CVS, Azure DevOps Server, Subversion, Mercurial, etc. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY GitHub GitHub is a service. GitHub is focused on centralized source code hosting. GitHub has a built-in user management feature. GitHub has an active marketplace for tool integration. GitHub competes with GitLab, Bit Bucket, AWS Code Commit, etc. 41 N32 nirf 1750 Rank

--- Page 42 ---
Course Code/Title:CS2V15/Devops Unit: II UNIT - II COMPILE AND BUILD USING MAVEN AND GRADLE Introduction, Installation of Maven, POM files, Maven Build lifecycle, Build phases(compile build, test, package) Maven Profiles, Maven repositories(local, central, global),Maven plugins, Maven create and build Artifacts, Dependency management, Installation of Gradle, Understand build using Gradle. 1. Introduction of Maven: Apache Maven is an automation tool. The tool is written in Java. It was initially released on 13 July 2004. It is developed by the Apache software foundation. It is part of the Jakarta Project. It is working on two aspects: how software is built, and its dependencies. It was created by Jason van Zyl. It is built by using a plugin-based architecture that allows it to make the use of any application controllable by standard input. It dynamically downloads Java libraries. Understanding the problem without Maven There are many problems that we face during the project development. They are discussed below: 1) Adding set of Jars in each project: In case of struts, spring, hibernate frameworks, we need to add set of jar files in each project. It must include all the dependencies of jars also. 2) Creating the right project structure: We must create the right project structure in servlet, struts etc, otherwise it will not be executed. 3) Building and Deploying the project: We must have to build and deploy the project so that it may work. Maven simplifies the above mentioned problems. It does mainly following tasks. 1. It makes a project easy to build 2. It provides uniform build process (maven project can be shared by all the maven projects) 3. It provides project information (log document, cross referenced sources, mailing list, dependency list, unit test reports etc.) 4. It is easy to migrate for new features of Maven Apache Maven helps to manage o Builds Documentation o Reporing o SCMs o Releases o Distribution What is Build Tool? A build tool is used for building a process. It does following: CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE TECHNOLOGY N32 nirf 1 1750 Rank

--- Page 35 ---
Unit: I Course Code/Title:CS3V15/Devops Push: Push is the operation that sends your committed changes to a remote repository, making them accessible to others. Git workflow v0.1 v0.2 v1.0 Master Hotfix Release Develop Feature Feature Git Flow is a structured branching model designed for projects with well- defined release cycles and a need for strict quality control. Branches: The branching model described is commonly known as the Gitflow Workflow. It's a branching strategy that defines a strict branching model designed to facilitate collaboration and streamline the release process. Let's go into detail about each branch: clone operation Working Copy edit, add, move files update operation Modify working copy status and diff operation Review changes commit and push operation push operation Commit changes push operation amend and push operation Review changes 1. Master Branch: The `master'branch represents the main codebase and contains production- ready code. This branch is typically stable and should only include thoroughly tested and approved changes. Each commit on the 'master' branch represents a new version or release of the software. 2. Develop Branch: The 'develop' branch is an integration branch where various feature branches are merged. It serves as a staging area for testing new features and ensuring they work well together before merging into the 'master' branch. This branch may have ongoing development work and is not necessarily always in a production- ready state. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 35 nirf 1750 Rank N33

--- Page 36 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Feature Branches: Feature branches are created for developing new features or implementing changes. These branches are typically based on the 'develop'branch. Once a feature is complete, the branch is merged back into the 'develop' branch. Feature branches allow developers to work on specific tasks without affecting the main codebase. 4. Release Branch: The 'release' branch is created when the 'develop' branch reaches a point where it is ready for a production release. This branch is used for final testing, bug fixes, and preparing the code for deployment. No new features should be added to the release branch. Once the release is deemed stable, it is merged into both the 'master' branch and the 'develop' branch. 5. Hotfix Branch: The 'hotfix' branch is used to quickly address critical issues or bugs in the production code. It is created directly from the 'master' branch. Hotfixes are intended to be small and focused on resolving the specific issue at hand. Once the hotfix is complete, it is merged into both the 'master'branch and the `develop'branch to ensure that the fix is applied to future releases. Here is the typical flow: · Developers work on feature branches based on the 'develop' branch. · Completed features are merged into the 'develop' branch. · When ready for a release, a 'release' branch is created from 'develop". · The release branch undergoes testing and bug fixes. · The release branch is merged into both 'master' and 'develop' once it's stable. · If a critical issue arises in production, a 'hotfix' branch is created from 'master'. · The hotfix is merged into both 'master' and 'develop to keep both branches in sync. · This Gitflow Workflow helps maintain a structured development process, ensuring that features are developed, tested, and released in a controlled manner. Example Scenario: · Imagine you're working on a large software project with a team of 10 developers. You have a major release planned for every six months. · You create a "feature/Ticket-Id" branch to develop a new login system. · Once the feature is complete, it's merged into the "develop" branch for integration and testing. · As the release date approaches, you create a "release/v1.0" branch to freeze code for the upcoming release. · Any critical issues discovered in the production environment are fixed in "hotfix" branch and merged into "master" and "develop." CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 36 N33 nirf 1750 Rank

--- Page 17 ---
Course Code/Title:CS3V15/Devops Unit: I that failed in a test suite gets simpler. Also, we can schedule the execution of the test cases at predefined times. After testing, the code is continuously integrated with the existing code. 4) Continuous Monitoring Monitoring is a phase that involves all the operational factors of the entire DevOps process, where important information about the use of the software is recorded and carefully processed to find out trends and identify problem areas. Usually, the monitoring is integrated within the operational capabilities of the software application. 5) Continuous Feedback The application development is consistently improved by analyzing the results from the operations of the software. This is carried out by placing the critical phase of constant feedback between the operations and the development of the next version of the current software application. The continuity is the essential factor in the DevOps as it removes the unnecessary steps which are required to take a software application from development, using it to find out its issues and then producing a better version. It kills the efficiency that may be possible with the app and reduce the number of interested customers. 6) Continuous Deployment In this phase, the code is deployed to the production servers. Also, it is essential to ensure that the code is correctly used on all the servers. The new code is deployed continuously, and configuration management tools play an essential role in executing tasks frequently and quickly. Here are some popular tools which are used in this phase, such as Chef, Puppet, Ansible, and SaltStack. Containerization tools are also playing an essential role in the deployment phase. Vagrant and Docker are popular tools that are used for this purpose. These tools help to produce consistency across development, staging, testing, and production environment. They also help in scaling up and scaling down instances softly. Containerization tools help to maintain consistency across the environments where the application is tested, developed, and deployed. There is no chance of errors or failure in the production environment as they package and replicate the same dependencies and packages used in the testing, development, and staging environment. It makes the application easy to run on different computers. 7) Continuous Operations All DevOps operations are based on the continuity with complete automation of the release process and allow the organization to accelerate the overall time to market continuingly. It is clear from the discussion that continuity is the critical factor in the DevOps in removing steps that often distract the development, take it longer to detect issues and produce a better version of the product after several months. With DevOps, we can make any software product more efficient and increase the overall count of interested customers in your product. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 17 N32 nirf 1750 Rank

--- Page 18 ---
Course Code/Title:CS3V15/Devops Unit: I AWS AWS stands for Amazon Web Services, It is an expanded cloud computing platform provided by Amazon Company. AWS provides a wide range of services with a pay-as-per-use pricing model over the Internet such as Storage, Computing power, Databases, Machine Learning services, and much more. AWS facilitates for both businesses and individual users with effectively hosting the applications, storing the data securely, and making use of a wide variety of tools and services improving management flexibility for IT resources. Advantages & Features of AWS: 1. Cost savings: One of the biggest benefits of AWS is that it can help businesses save money. As mentioned previously, businesses can avoid the high upfront costs of traditional infrastructure with AWS and pay only for the resources they use. Traditionally, businesses had to invest in hardware and software upfront, which often led to overspending. Let's look at this for example - if a business needs to run a website that gets 1000 visitors per day, they would need to purchase and maintain enough servers to support this traffic. With AWS, the business only pays for the compute resources they use when someone visits their website. This can result in significant cost savings. 2. Flexibility: Another key benefit of AWS is its flexibility. Businesses are able to customize their virtual environment - whether the operating system, database, programming language, or something else - to meet their specific needs. Especially in today's climate, the migration process to the cloud should be as frictionless as possible - and AWS makes that possible. Regardless of your use case or industry, AWS can be tailored to fit your needs, whether you're looking for a single cloud- hosted application or an entire suite of integrated solutions. 3. Reliability: AWS is known for being reliable, with an uptime of 99.9%. This makes it a great platform for mission-critical applications that need to be available 24/7. AWS also offers the ability to deploy resources across multiple availability zones for even greater reliability. The cloud platform also has a number of features that make it easier to ensure reliability, such as autoscaling and auto-healing. Autoscaling allows businesses to automatically scale their resources up or down based on demand, while auto-healing enables them to quickly identify and replace any faulty components. 4. Security: Businesses can take advantage of advanced security features, such as identity and access management, to help protect their data. Their tough infrastructure with an end-to-end approach is designed to withstand attacks and AWS provides customers with tools to help them monitor and respond to threats. When it comes to storage, Amazon S3 provides customers with a secure and reliable way to store and access data. The service is designed to be highly scalable and resilient, with built-in redundancy. Fine-grain identity and access controls can be applied to S3 buckets and objects, giving customers control over who has access to their data. Security tasks can be automated with AWS CloudFormation, making it easier for businesses to manage their security policies. And, you can rest easy knowing that AWS takes privacy seriously, with comprehensive customer data protection and compliance measures. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 18 N32 nirf 1750 Rank

--- Page 77 ---
Unit: III Course Code/Title:CS3V15/Devops 5. Customize Jenkins: ○ Install suggested plugins or select the required plugins manually. ○ Create the first admin user. Configuration 1. Global Configuration: ○ Navigate to Manage Jenkins > Configure System. ○ Set up global environment variables, email notifications, and other system-wide settings. 2. Security Configuration: ○ Configure security settings under Manage Jenkins > Configure Global Security. o Use the built-in user database or integrate with external authentication mechanisms like LDAP. 3. Node Configuration: ○ Jenkins can distribute build loads to multiple nodes. ○ Configure nodes by navigating to Manage Jenkins > Manage Nodes and Clouds. 4. Job Configuration: ○ Create new jobs by clicking New Item on the Jenkins dashboard. o Configure job-specific settings such as source code management, build triggers, and post-build actions. 5. Pipeline Configuration: ○ Jenkins supports pipeline as code using Jenkinsfile. ○ Define your pipeline stages in a Jenkinsfile placed in the root of your project repository. 6. Plugins: ○ Enhance Jenkins functionality by installing plugins from the Manage Jenkins > Manage Plugins section. 7. Backup and Restore: ○ Regularly back up the jenkins_home directory, which contains Jenkins configurations, job configurations, and build history. ○ Use plugins like ThinBackup for automated backups. Ref: https://youtube.com/playlist?list=PL6flErFppaj35spJjPy41-IruDjw2kRV-&si=Vzd9AAMv53xwWB S Ref: https://bit.ly/youtube_jenkins (click here) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 78 ---
Course Code/Title:CS3V15/Devops Unit: III Jenkins Architecture Overview Jenkins is a robust automation server widely used for continuous integration and continuous delivery (CI/CD) processes. Here's a detailed breakdown of the Jenkins architecture and its core components. Core Components of Jenkins Architecture 1. Jenkins Master: ○ Role: The Jenkins master is the central control unit. 0 Functions: Scheduling Jobs: Assigns build jobs to the appropriate agents. Monitoring Agents: Keeps track of the state and health of agents. I Job Execution: Executes build jobs directly if no agents are available. I User Interface: Provides a web-based interface for users to configure jobs, view build results, and manage the Jenkins environment. 2. Build Agents (Nodes): ○ Role: Agents are responsible for executing the build jobs assigned by the master. ○ Types: Agents can run on different operating systems and hardware configurations. ○ Configuration: Static Agents: Permanently assigned to the Jenkins master. . Dynamic Agents: Created and destroyed as needed, often used in cloud environments. ○ Communication: Agents communicate with the master using the Jenkins Remoting protocol. 3. Job Configuration: ○ Types of Jobs: I Freestyle Projects: Basic job type with simple configurations. Pipeline Jobs: Scripted or declarative pipelines that define the entire build process as code. I Multi-Configuration (Matrix) Projects: Allows testing across different environments and configurations. ○ Components: I Source Code Management: Integration with version control systems like Git, SVN, etc. . Build Triggers: Conditions that start the job, such as code commits, scheduled times, or manual triggers. Build Steps: Actions performed during the build, such as compiling code, running tests, and packaging artifacts. . Post-Build Actions: Steps executed after the build, like deploying artifacts, sending notifications, or archiving results. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 105 ---
Course Code/Title:CS3V15/Devops Unit: III How to install copy artifact plugin ? Step 1 : Start the Jenkins tool by entering the login and password. Step 2 : Click on Manage Jenkins. Click on Plugins. Step 3 : In the search window, type the name copy Artifact. Step 4 : The plugin name will be displayed if it is not already installed. Select it. Name Adde a build atop to copy artifacts from another project. Thủa plugin le up for adoption! We are looking for naw maintainent. Velt our Adopt a Plugin intuitive for mort information. then click on Install. The plugin gets installed. Example Demo · Let us discuss how to use Copy Artifact plugin. Step 1 : Create a new Job by clicking on item New Item. Give the some name to your job. I have given the name App1 and selected the Freestyle Project. Step 2: Give suitable description in the description box. Step 3 : Under the source code management, click on Git option and specify the repository URL of GitHub. Source Code Management None Git ? Repositories 1 Repository URL + ·hong · CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 30 N3 nirf 1750 Rank

--- Page 106 ---
Unit: III Course Code/Title:CS3V15/Devops Step 4 : Under the Build Steps section, choose the Execute Windows batch command option. And type the javac and java commands. Build Steps = Execute Windows batch command Command See the list of available environment variables javac test.java java test Advanced Add bulld stop * Step 5 : Click in Apply and Save. Step 6 : Select the Build Now option. On successful built, the test, class file gets generated in the Jenkins workspace of the App1 job. C:\ProgramData\Jenkins\.jenkins\workspace\Appl>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\Appi>java test Good Morning Pathill Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! C:\ProgramData\Jenkins\.jenkins\workspace\Appl>exit @ Finished: SUCCESS Step 7 : Now we have to copy test.class file from the workspace of Appl to another job. For that' purpose we will create another Job named App2 by clicking New Item. Select the Freestyle Project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 31 N3 nirf 1750 Rank

--- Page 141 ---
Course Code/Title: CS3V15/Devops Unit: V UNIT V - BUILDING DEVOPS PIPELINES USING AZURE Create Github Account, Create Repository, Create Azure Organization, Create a new pipeline, Build a sample code, Modify azure-pipelines.yaml file Github Account Creation What is Github? GitHub is a code hosting platform for collaboration and version control. GitHub lets you and others work together on projects fromanywhere. Github is owned by Microsoft, provides access to public(free) and private(paid) repositories. Stepsto create Github Account: Join GitHub · GitHub × + < C https://github.com/join f Facebook - Log In o ... Twitter G Google f Facebook YouTub Product v Solu wiki How Step 1:Go to https://github.com/join in a web browser. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 1 175" Rank

--- Page 142 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: Enter your personal details. In addition to creating a username and entering an email address, you'll also have to create a password. Your password must be at least 15 characters in length or at least 8 characters with at least one number and lowercase letter. Join GitHub First, let's create your user account Username * wikihowneveconcepts Email address * Password * .......... Make sure it's at least 15 characters OR at least 8 characters including a number and a lowercase letter. Learn more. Email preferences Send me occasional product updates, announcements, and offers. Verify your account wiki How Step 3: Click Verify to start the verification puzzle. The instructions vary by puzzle, so just follow the on-screen instructions to confirm that you are a human. A green checkmark will appear after completing the puzzle. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 2

--- Page 139 ---
Course Code/Title:CS3V15/Devops Unit: IV Create the file, if it does NOT exist (or) absent $ ansible testservers -a "touch /tmp/testfile creates=/tmp/testfile" -i ansible_hosts As Playbook - name: "Validate if a file is present or not present using Ansible Command module" hosts: testservers tasks: - name: "Create a file if it does not exist" command: "touch /tmp/latestfile" args: creates: "/tmp/latestfile" register: createif - name: "Display the file to make sure its created" command: "ls -lrt /tmp/latestfile" register: displayif when: createif is changed - debug: var=displayif.stdout - name: "Remove the file if it exist" command: "rm -rf /tmp/latestfile" args: removes: "/tmp/latestfile" register: removeif In the playbook we execute three tasks, • First one is a create a file if it does not exist • Second to display if the creation is successful • Third one is to delete the file if it exists Example 5: Execute or Run the Script when a file exists or not exists Now for this example let us take something relatable to real world scenario. Like Start the Server instance (or) program if the PID file or LOCK file does not exist Here is the sample playbook for you to get started. I am giving a general playbook here as I do not want to pick some server/technology of my own which you cannot relate to. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 140 ---
Course Code/Title:CS3V15/Devops Unit:IV you can feel free to modify the script to your needs as this is just a template. - name: Start of Stop Server instance based on PID/LOCK file availability hosts: appservers tasks: - name: Start the instance when the PID file is not present become: yes become_user: appuser command: "startserver.sh" args: creates: "/path/to/pid/instance.pid" register: startinst - name: Stop the instance when the lock file is present become: yes become_user: appuser command: "stopserver.sh" args: removes: "/path/to/lockfile/instance.lck" register: stopinst CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 26 N33 nirf 1750 Rank

--- Page 81 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Credentials: Add credentials if required. ○ Branch Specifier: Specify the branch (e.g., */main). 7. Build Triggers: ○ Build Periodically: Use CRON syntax to schedule builds. ○ Poll SCM: Jenkins will check the repository for changes at specified intervals. ○ Other Triggers: Configure other triggers such as GitHub hooks, upstream projects, etc. 8. Build Environment: o Configure the build environment settings like setting environment variables, running scripts before the build, etc. 9. Build Steps: ○ Click on "Add Build Step" and choose the appropriate build step (e.g., "Execute Shell", "Invoke Ant", "Invoke Gradle script"). ○ Example: For executing a shell script, add the necessary shell commands. 10. Post-build Actions: ○ Click on "Add post-build action" and choose the appropriate action (e.g., "Archive the artifacts", "Publish JUnit test result report"). ○ Configure the settings for each post-build action. 11. Save the Configuration: ○ Once you have configured all the necessary settings, click "Save" at the bottom of the page. 12. Run the Job: ○ On the job's main page, click "Build Now" to run the job immediately. 13. Monitor the Job: ○ Click on the build number in the "Build History" to view the detailed output and logs of the build process. ○ Check the console output for any errors or warnings. Practical Example Setting up a Freestyle Project for a Maven Build: 1. Create New Item: ○ Name: MyMavenProject o Type: Freestyle project 2. Source Code Management: ○ Git: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 82 ---
Unit: III Course Code/Title:CS3V15/Devops I Repository URL: https://github.com/example/my-maven-project.git · Branch Specifier: * /main 3. Build Triggers: ○ Poll SCM: H/15 * (poll every 15 minutes) 4. Build Steps: ○ Invoke Top-Level Maven Targets: Goals: clean install 5. Post-build Actions: ○ Publish JUnit test result report: I Test report XMLs: * /target/surefire-reports/ *. xml 6. Save and Build: ○ Click "Save". ○ Click "Build Now" on the job's main page. By following these steps, you can set up a Jenkins job to automate your build process for various types of projects. Jenkins provides extensive customization options, allowing you to tailor the job to meet your specific requirements. Configuring a Jenkins Job 1. Install Jenkins First, make sure Jenkins is installed. You can download it from the official Jenkins website. 2. Access the Jenkins Dashboard Open your web browser and go to http://your-server-ip: 8080 to access the Jenkins dashboard. 3. Create a New Job 1. Start a New Job: ○ Click on "New Item" on the left-hand menu. 2. Name the Job: ○ Enter a name for your job in the "Enter an item name" field. 3. Select Job Type: ○ Choose the type of job (e.g., Freestyle project, Pipeline). CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) - TECHNOLOGY N32 nirf 7 1750 Rank

--- Page 85 ---
Course Code/Title:CS3V15/Devops Unit: III By following these steps, you can set up a Jenkins job to automate builds, integrate with your source code management system, and perform various actions based on build results. Introduction to Jenkins Plugins What Are Jenkins Plugins? Jenkins plugins are extensions that add extra functionality to Jenkins. They allow you to customize and extend Jenkins to better fit your specific needs. Plugins can provide integrations with other tools, add new features, and improve existing functionalities. Why Use Plugins? . Extend Functionality: Add features not available in the core Jenkins. . Integrate Tools: Seamlessly integrate with other tools and platforms (e.g., GitHub, Docker). . Improve Productivity: Automate more tasks and streamline your workflow. · Customize UI: Tailor the Jenkins interface to better suit your preferences and needs. How to Manage Plugins 1. Accessing the Plugin Manager 1. Go to Jenkins Dashboard: ○ Open your Jenkins dashboard. 2. Navigate to Plugin Manager: ○ Click on "Manage Jenkins". ○ Select "Manage Plugins". 2. Installing Plugins 1. Available Plugins: ○ Go to the "Available" tab to see a list of plugins that can be installed. 2. Search for Plugins: ○ Use the search bar to find specific plugins. 3. Select Plugins: ○ Check the box next to the plugins you want to install. 4. Install Plugins: ○ Click "Install without restart" or "Download now and install after restart". 3. Updating Plugins CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 narf 10 1750 Rank

--- Page 86 ---
Course Code/Title:CS3V15/Devops Unit: III 1. Go to Updates: ○ Navigate to the "Updates" tab to see plugins with available updates. 2. Select Plugins: ○ Check the box next to the plugins you want to update. 3. Update Plugins: ○ Click "Download now and install after restart". 4. Managing Installed Plugins 1. Installed Plugins: ○ Go to the "Installed" tab to see all installed plugins. 2. Uninstall Plugins: ○ Click the "Uninstall" button next to the plugin you want to remove. 3. Check Plugin Versions: ○ View the current version of each installed plugin. Adding Plugins to Jenkins Plugins are a crucial part of Jenkins, allowing you to extend its functionality to suit your project's specific needs. Whether you're integrating Jenkins with other tools, customizing your job configuration, or adding new build steps, plugins provide the flexibility you need. Here's a detailed guide on how to add plugins to Jenkins. 1. Why Plugins Are Important - Extending Jenkins Functionality: Plugins allow Jenkins to integrate with various tools and technologies, such as Git, Maven, Docker, and more. - Customizing Jobs: With plugins, you can add specific steps, triggers, and post-build actions to your jobs, making Jenkins adaptable to any workflow. - Automation: Plugins help automate processes like code quality checks, notifications, and deployments. 2. Accessing the Plugin Manager 1. Navigate to Jenkins Dashboard: Open your Jenkins dashboard in a web browser (e.g., http://localhost:8080). CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 175º Rank

--- Page 107 ---
Course Code/Title:CS3V15/Devops Unit: III Step 8 : Then configure the Build Steps by selecting the option Copy artifacts from another project as follows - . . First of all give the name of the project from which you want to copy the files. In my case it is App1. . Then select the option "copy from WORKSPACE of latest completed build" · Give the regular expression ** / *.*. Here's a breakdown of what ** / *.* means : o ** : The double asterisk ( ** ) is a wildcard that matches zero or more directories and subdirectories. It's often used to recursively search through all directories and subdirectories. o / : The forward slash (/) is used as a directory separator, typically on Unix-like systems. o *.* : This part of the regular expression is used to match files with an extension. In regular expressions, the * symbol represents "zero or more" of the preceding character or pattern, and the . character is used to match any character. So, *.* essentially matches any file with any extension. So, when you use the regular expression ** / *.* in a Jenkins build configuration, you are instructing Jenkins to include all files with any extension ( *.* ) located in any directory and subdirectory ( ** ) within the workspace or specified location. Build Steps E Copy artifacts from another project Project name ? App: Which build ? Copy from WORKSPACE of latest completed buld Limitation Note ? Artifacts to copy ? Artifacts not to copy ? : Toget directory CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 32

--- Page 108 ---
Course Code/Title:CS3V15/Devops Unit: III Add one more Build Step and select Execute Windows batch Command. Give the command dir so that the name of files and directories that are copied from App1 to App2 will be displayed on the Console. = Execute Windows batch command ? Command See the list of available environment variables ---- dir ----- Advanced v Add build step * Click Apply and Save. Step 8 : Now Build this project (App2). The output will be as follows - Started by user Anuradha P Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\.jenkins\workspace\App2 Copied 16 artifacts from "App1" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621109248227895.bat C:\ProgramData\Jenkins\,jenkins\workspace\App2>dir Volume in drive C Is OS Volume Serial Number is CGC2-BA9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023* 15:07 <DIR> 01-09-2023 15:07 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 474 test.class 144 test. java 2 File(s) 616 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 33 N32 narf 1750 Rank

--- Page 93 ---
Course Code/Title:CS3V15/Devops Unit: III And configure the Jenkins job as follows - General Description In this job we have used GitHub plugin Plain text Preview Discard old builds ? GitHub project This project is parameterized ? Throttle builds Execute concurrent builds if necessary Advanced V Source Code Management None Git ? Repositories ? Repository URL https://github.com/AnurdhaP/MyJavaPrograms.gil Please enter Git repository. Credentials ? - none - Add Advanced CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 18 N3 nirf 1750 Rank

--- Page 94 ---
Course Code/Title:CS3V15/Devops Unit: III Build Triggers Trigger builds remotely (e.g. from scripts) ? Build after other projects are built ? Build periodically ? GitHub hook trigger for GITSem polling ? Poll SCM ? Schedule ? A Do you really mean "every minute" when you say "" "" " ""? Perhaps you meant "H" "" "" to poll once per hour Would last have run at Wednesday, 30 August, 2023 at 4:11:02 pm india Standard Time; would'nest run at Wednesday, 30.A Ignore post-commit hooks ? Build Steps Execute Windows batch command ? Command See the list of available environment variables javas test.java java test. java ----- Advanced V Add build sten Save this configuration. Step 5 : Now if we build this job the output that we see is as follows on Console Output. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 19 N3 nirf 1750 Rank

--- Page 83 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Click "OK" to proceed. 4. Configure the Job General Settings 1. Project Description: ○ Enter a description for your job. 2. Discard Old Builds: ○ Set up a policy to manage the number of builds Jenkins keeps. Source Code Management (SCM) 1. Select SCM: ○ Choose your source code management system (e.g., Git). 2. Configure Repository: ○ Enter the repository URL and provide credentials if needed. 3. Branch Specification: ○ Specify the branch to build (e.g., main, develop). Build Triggers 1. Choose Build Triggers: ○ Poll SCM: Check for changes at intervals. ○ Build Periodically: Schedule builds. ○ GitHub hook trigger: Trigger builds on GitHub push. ○ Build after other projects: Trigger builds based on other jobs. Build Environment 1. Configure Build Environment: ○ Clean workspace: Delete workspace before build starts. ○ Manage secrets: Use secret texts or files. ○ Node selection: Specify where to run the job. Build Steps 1. Add Build Steps: ○ Click "Add build step". ○ Choose the build step type (e.g., Execute shell, Invoke Gradle script). ○ Enter the commands or scripts needed for the build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 nirf 8 1750 Rank

--- Page 84 ---
Course Code/Title:CS3V15/Devops Unit: III Post-build Actions 1. Add Post-build Actions: Click "Add post-build action". ○ Common actions include: Archive artifacts: Store build artifacts. Email notification: Send build status emails. I Publish test results: Aggregate test results. Trigger other jobs: Trigger other jobs based on results. 5. Save and Build 1. Save the Configuration: ○ Click "Save". 2. Build the Job: ○ Click "Build Now" to manually trigger a build. ○ View progress and logs by clicking on the build number. 6. Monitor and Manage Builds 1. Check Build History: ○ View past builds and their statuses. 2. Console Output: ○ Click on a build number to see detailed logs. 3. Build Artifacts: ○ Access any artifacts produced by the build. 7. Advanced Configuration (Optional) 1. Pipeline as Code: 0 For complex workflows, use Jenkins Pipelines. Create a Jenkinsfile in your repository. 2. Plugins: ○ Extend Jenkins functionality with plugins from the Jenkins Plugin Manager. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) 9 N32 nirf 1750 Rank

--- Page 145 ---
Course Code/Title: CS3V15/Devops Unit: V Step 7:Note the types of plans offered by GitHub. There are a few different plans to choose from, varying in the amount of features provided. Learn to ship software like a pro. GitHub gives students free access to the best developer tools so they can learn by doing. Free Get additional student benefits GitHub Pro Unlimited public/private repositories 2,000 CI/CD minutes/month Free for public repositories 500MB of Packages storage Free for public repositories Protect your branches Ensure that collaborators on your repository cannot make irrevocable chang branches. O Draft pull requests O 120 core-hours of Codespaces compute O Pages and Wikis 15GB of Codespaces storage O Community support ® 3,000 CI/CD minutes/month Free for public repositories 2GB of Packages storage Free for public repositories O 180 core-hours of Codespaces compute 20GB of Codespaces storage Web-based support wikiHow Step 8: Select the free plan. On the plan selection page, scrolldown to click the button for choosing a free plan. This will immediately take you to your GitHub dashboard. Continue for free wiki How UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 5 N33 In 175º Rank ---

--- Page 146 ---
Course Code/Title: CS3V15/Devops Unit: V · If you choose a paid plan, you'll have to enter yourpayment information as requested before you can continue. . If you want to upgrade your Github account in the future, click the menu at the top- right corner, select Settings, andchoose Billing and plans to view your options. GitHub essentials are: · Repositories · Branches · Commits · Pull Requests · Git (the version control software GitHub is built on) Repository: A GitHub repository can be used to store a developmentproject. It can contain folders and any type of files (HTML, CSS,Javascript, Documents, Data, Images). A GitHub repository should also include a licence file and a README file about the project. A GitHub repository can also be used to store ideas, or any resources that you want to share. Github Repository Creation Step 1: Click on the new repository option Marketplace Explore New repository and GitHub without any code! Catch Universe Join a community Universe in San Fr ticket pricing-he Import repository × New gist bird New organization New project llo World guide, you'll create a repository, start a , write comments, and open a pull request. GitHub Sponsors Matching Fund × Ready to support open source? GitHub will match your contribution to developers during their first year in GitHub Sponsors. d the guide Start a project × Welcome to the new dashboard. Get closer to the stuff you care about most. Explore repositories atlas-engineer/next Next browser - Be productive. UnENVIVAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 175° Rank 6

--- Page 103 ---
Course Code/Title:CS3V15/Devops Unit: III =Publish HTML reports ? Reports HTML directory to archive ? E:\Eclipse_workplace\SeleniumDemo\test-output Index page[s] ? index.html Index page title[s] (Optional) Report title ? HTML Report Publishing options Step 11 : Click on Apply and then Save. Now build this project. Step 12 : On successful build, it creates the HTML Report as follows - x + € > C @ localhost:0000/juby/HTML/\portDemo/HTML_20Report/ Back to HTMLBepsatDemo Test results All suites Default suite 11 D myPackage.myTestPage Info . Repetir output Results . Thus using HTML Publish Report Plugin we can successfully generate HTML report of some testing framework application. · Troubleshooting : Sometimes, the raw file is displayed as HTML report. To get the HTML formatted report we must click on Manage Jenkins section, scroll down and locate Script Console section. Click on it. Tools and Actions C Broad Configuration Sium Disk Discard all the loaded alla lo memory and reloud Jerking CLI Script Console Ewecubes arbitrary suyt lor and run below given command : System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", ") . Then restart the Jenkins Page and Build the project once again. The HTML report can then be displayed as given in above step. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 28 N32 nirf 1750 Rank

--- Page 104 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Copy Artifact Overview: The Copy Artifact Plugin allows Jenkins jobs to copy build artifacts from other jobs, facilitating artifact reuse across different jobs. Key Features: · Artifact Copying: Copies files from one build to another, which can be from a specific build or the latest successful build. · Triggering Builds: Can be used in downstream jobs to fetch artifacts from upstream jobs. Configuration: 1. Install the Plugin: Go to Manage Jenkins > Manage Plugins > Available tab, search for "Copy Artifact Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Build section. ○ Add a Copy artifacts from another project build step. o Specify the project name, build number or criteria, and the target directory for copied artifacts. Usage: · Commonly used in multi-job pipelines where artifacts need to be shared between different jobs or stages. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 29 N3 nirf 1750 Rank

--- Page 87 ---
Course Code/Title:CS3V15/Devops Unit: III 2. Go to Plugin Manager: From the dashboard, click on 'Manage Jenkins' on the left-hand side menu. On the Manage Jenkins page, click on 'Manage Plugins'. This will take you to the Plugin Manager, where you can view, install, and update plugins. 3. Exploring Available Plugins 1. Available Tab: The 'Available' tab lists all plugins that are available for installation. This includes thousands of plugins categorized by functionality (e.g., SCM, build tools, user interface enhancements). 2. Search for Plugins: Use the search box to quickly find the plugin you need. For example, if you're looking to integrate Jenkins with Git, type 'Git' in the search box. 3. Popular Plugins: Jenkins highlights popular plugins at the top of the Available tab. These are commonly used plugins that most users find essential. 4. Installing Plugins 1. Selecting Plugins to Install: Check the box next to each plugin you want to install. You can select multiple plugins at once. 2. Install Without Restart: After selecting your plugins, scroll down and click 'Install without restart'. Jenkins will install the plugins immediately, and you can continue using Jenkins during the installation. 3. Install After Restart: If you prefer, you can choose to 'Download now and install after restart'. This will install the plugins after Jenkins is restarted, ensuring that all changes are applied without interference. 4. Installation Progress: The installation process will show a progress bar for each plugin. Once a plugin is installed, it will move to the 'Installed' tab. 5. Configuring Installed Plugins 1. Plugin Configuration: After installation, some plugins may require additional configuration. Navigate to 'Manage Jenkins' > 'Configure System' or 'Configure Global Security' to adjust settings for your new plugins. 2. Global Tool Configuration: For build tools like Maven or JDKs installed via plugins, you can configure them under 'Global Tool Configuration'. Here, you define the paths to the tools or specify installation options. 6. Updating and Managing Plugins 1. Updating Plugins: Regularly update your plugins to ensure compatibility with the latest version of Jenkins and to benefit from new features or security patches. Go to the 'Updates' tab in the Plugin CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N32 nirf 12 1750 Rank

--- Page 88 ---
Unit: III Course Code/Title:CS3V15/Devops Manager. Jenkins will show you all plugins with available updates. Click 'Update' to install the latest versions. 2. Uninstalling Plugins: If a plugin is no longer needed, you can uninstall it from the 'Installed' tab. Select the plugin and click 'Uninstall'. Uninstallation typically requires a Jenkins restart to complete. 7. Commonly Used Plugins Here are a few essential plugins that you might consider adding to your Jenkins setup: 1. Git Plugin: Integrates Jenkins with Git, allowing you to pull code from GitHub, Bitbucket, or other Git repositories. 2. Pipeline Plugin: Enables the creation and management of Jenkins pipelines, a powerful way to define complex build, test, and deployment processes. 3. Maven Integration Plugin: Adds support for Maven projects, allowing you to build and manage Maven-based projects directly within Jenkins. 4. Email Extension Plugin: Provides advanced email notification options for build statuses, including customizable email templates. 5. Blue Ocean: An alternative user interface for Jenkins that simplifies pipeline creation and offers a modern, user-friendly design. 8. Troubleshooting Plugin Issues 1. Compatibility Issues: Sometimes, plugins might conflict with each other or with the Jenkins version. In such cases, refer to the plugin's documentation or Jenkins logs to diagnose the issue. 2. Rollback: If a plugin update causes problems, you can rollback to a previous version from the 'Installed' tab by selecting the desired version. 3. Restart Jenkins: Some plugin changes require a Jenkins restart. Ensure all jobs are completed before restarting to avoid any interruptions. Commonly Used Jenkins Plugins 1. Git Plugin Overview: The Git Plugin integrates Jenkins with Git repositories, allowing Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nårf 1750 Rank

--- Page 129 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible ad-hoc Commands The Ansible ad-hoc command uses the /usr/bin/ansible command-line tool to automate a single task on one or more managed nodes. The Ad-hoc commands are quick and easy, but they are not re-usable. The Ad-hoc commands demonstrate the simplicity and power of Ansible. Syntax ansible <hosts> [-m <module_name>] -a <"arguments"> -u <username> [ -- become] Explanation Hosts: It can be an entry in the inventory file. For specifying all hosts in the inventory, use all or "*" . module_name: It is an optional parameter. There are hundreds of modules available in the Ansible, such as shell, yum, apt, file, and copy. By default, it is the command. Arguments: We should pass values that are required by the module. It can change according to the module used. Username: It specifies the user account in which Ansible can execute commands. Become: It's an optional parameter specified when we want to run operations that need sudo privilege. By default, it becomes false. 1. Parallelism and shell commands You can reboot your company server in 12 parallel forks at the same time. For this, you need to set up the SSHagent for connection. $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa To run reboot for all your company servers in the group, 'abc', in 12 parallel forks: $ ansible abc -a "/sbin/reboot" -f 12 By default, Ansible will run the above ad-hoc commands from the current user account. If you want to change then pass the username in ad-hoc command as follows: $ ansible abc -a "/sbin/reboot" -f 12 -u username 2. File Transfer You can use ad-hoc commands for doing SCP (secure copy protocol) which means lots of files in parallel on multiple machines or servers. CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 15 N32 nirf 175º Rank

--- Page 130 ---
Course Code/Title:CS3V15/Devops Unit:IV Transferring file on many machines or servers $ ansible abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf" Creating new directory $ ansible abc -m file - a "dest = /path/user1/new mode = 888 owner = user group = user1 state = directory" Deleting all directory and files $ ansible abc -m file -a "dest = /path/user1/new state = absent" 3. Managing Packages Ad-hoc commands are available for apt and yum module. Here are the following ad-hoc commands using yum. Below command checks, if the yum package is installed or not, but not update it. $ ansible abc -m yum -a "name = demo-tomcat-1 state = present" Below command checks the package is not installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = absent" And below command checks the latest version of package is installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = latest" 4. Managing Users and Groups You can manage, create, and remove a user account on your managed nodes with ad-hoc commands. $ ansible all -m user -a "name=foo password =< crypted password here>" $ ansible all -m user -a "name=foo state=absent" 5. Managing Services Ensure a service is started on all the webservers. $ ansible webservers -m service -a "name=httpd state=started" Alternatively, restart a service on all webservers: $ ansible webservers -m service -a "name=httpd state=restarted" CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 nirf 1750 Rank

--- Page 127 ---
Course Code/Title:CS3V15/Devops Unit:IV Abbreviation You can also use abbreviation to represent lists. Example Countries: ['America', 'China', 'Canada', 'Iceland'] List inside Dictionaries We can use list inside dictionaries, i.e., value of key is list. Example james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english … List of Dictionaries We can also make list of dictionaries. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english - robert: name: robert richardson rollNo: 53 div: B sex: male CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 13 N32 nirf 1750 Rank

--- Page 128 ---
Course Code/Title:CS3V15/Devops Unit:IV likes: - biology - chemistry … YAML uses "|" to include newlines while showing multiple lines and ">" to suppress newlines while showing multiple lines. Due to this we can read and edit large lines. In both the cases intendentation will be ignored. We can also represent Boolean (True/false) values in YAML. where boolean values can be case insensitive. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english result: maths: 87 chemistry: 45 biology: 56 physics: 70 english: 80 passed: TRUE messageIncludeNewLines: | Congratulation !! You passed with 79% messageExcludeNewLines: > Congratulation !! You passed with 79% CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 14 N33 nirf 1750 Rank

--- Page 115 ---
Unit: IV Course Code/Title:CS3V15/Devops UNIT IV CONFIGURATION MANAGEMENT USING ANSIBLE Ansible Introduction, Installation, Ansible master/slave configuration, YAML basics, Ansible modules, Ansible Inventory files, Ansible playbooks, Ansible Roles, adhoc commands in ansible. Ansible A SSH ANSIBLE TARGET NODE HOSTS PLAYBOOK Ansible is a powerful DevOps tool designed for automating tasks on remote servers or nodes. In simple terms, it allows you to automate commands and functions on multiple remote machines from a central 'master' node. To illustrate its usefulness, consider a scenario where you need to reboot dozens or even hundreds of remote hosts. You could manually SSH into each one and initiate the reboot, or you can use Ansible to streamline the process, making it efficient and offering a wide range of additional functionalities. In essence, Ansible operates much like the second method mentioned, using SSH (Secure Shell), a secure communication protocol, to control remote nodes in a secure and optimized manner. Ansible Components Ansible Core Components Control node Ansible Inventory Managed node 1 Managed node 2 Managed node 3 1. Control Node - · The central or main node where Ansible is installed. · Used to trigger commands like ansible and ansible-inventory on other nodes. · Acts as the orchestrator for Ansible operations. • TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 1 N33 nirf 1750 Rank

--- Page 116 ---
Course Code/Title:CS3V15/Devops Unit: IV 2. Manage Node- · A remote or slave node where tasks are executed or controlled by Ansible. . These are the servers or devices you want to manage or automate. 3. Inventory - · A list of managed node IPs and configurations. · Logically organized, typically using file formats like YAML or INI. · Created on the control node to describe the deployment of hosts to Ansible. Ansible Additional Components Ansible Architecture CMDB Users Public / Private Cloud Hosts ... ... Inventory .. Modules 000 Ansible Playbook API Plugins Networking www.educba.com 1. Ad-Hoc Commands - . These are one-off commands that you can execute using the ansible command. · Useful for quick tasks or tests on remote nodes. 2. Plugins - · Plugins are pieces of code that extend Ansible's core functionality. · Ansible uses a plugin architecture for flexibility and expandability. · Examples include connectivity plugins for establishing connections and cache plugins. · Learn more about Ansible plugins. 3. Module- · Modules are built-in functions that can be used to perform various tasks. . They eliminate the need to write custom code for common operations. · Examples include modules for package management (apt, yum), service management, and more. CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) CHENNAI INSTITUTE . TECHNOLOGY 2 N33 nirf 1750 Rank

--- Page 95 ---
Course Code/Title:CS3V15/Devops Unit: III C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha C: \ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>exit e Finished: SUCCESS Step 6 : Now go to GitHub repository and make some changes to your test.java file and commit these changes. BE AnurdhaP Changes in the test java Code Blame 6 lines (6 loc) · 158 Bytes 89 Code 55% faster with GitHub Copilot > public class test { 2 public static void main(String args[]) { for(int 1=1;i <= 5;1++) 3 4 System.out.println("Good Morning Path !!! "); 5 3 6 } Step 7 : Now, if you check on the Jenkins page, it automatically executes the above updated test.java file and creates a build. It is as follows .- C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test. java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Good Morning Path !!! Good Morning Pathi !! Good Morning Path !!! Good Morning Path !!! Good Morning Pathill 2. Parameter Plugin Overview: The Parameter Plugin allows Jenkins jobs to accept parameters at build time, enabling dynamic and flexible builds. Key Features: · Parameter Types: Supports various parameter types such as string, choice, boolean, and more. · Default Values: Provides default values for parameters. · Prompt for Parameters: Users are prompted to enter values for parameters when starting a build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 20

--- Page 96 ---
Course Code/Title:CS3V15/Devops Unit: III Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Parameterized Builds," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add parameters of different types (e.g., string, choice) and configure their options and default values. Usage: · Useful for creating jobs that require user input or need to be customized for different build scenarios. A Example Demo Step 1 : Click on New item and create a job. I have created a job by the name Welcome (Refer section 3.5) Step 2 : Click on the existing job and configure it with parameters. Step 3 : Check the option "This project is parameterized". Select the String Parameter. Create some string parameter. I have created a parameter named Username and default value to it. This project is parameterized ? =String Parameter ? Name ? Username Default Value ? Parth Description ? Plain text Preview Trim the string ? Step 4 : Under the Build Steps section, using echo command I tried to display the parameter value CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 21

--- Page 109 ---
Course Code/Title:CS3V15/Devops Unit: III Note that the git repository and the files from that repository are copied from the workspace of App1 to App2. Also note that test.class and test.java files are copied to the workspace of App2. Started by user Anuradha R Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\. jenkins\workspace\App2 Copied 16 artifacts from "Appl" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621189248227895.bat C:\ProgramData\Jenkins\.jenkins\workspace\App2>dir Volume in drive C is OS Volume Serial Number is C6C2-8A9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 .git 474 test. class 144 test.java 2 File(s) 618 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ Finished: SUCCESS 5. Extended Choice Parameter Overview: The Extended Choice Parameter Plugin provides advanced parameter types for Jenkins jobs, such as multi-select lists, checkboxes, and more complex formats. Key Features: · Parameter Types: Includes multi-select, checkboxes, and other advanced input options. . Dynamic Choices: Allows dynamic generation of choices from scripts or external sources. Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Extended Choice Parameter Plugin," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add an Extended Choice Parameter. ○ Configure the parameter type, choices, and other settings. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 110 ---
Unit: III Course Code/Title:CS3V15/Devops Usage: · Useful for scenarios where complex user input is needed or where multiple selection options are required. Configuring Jenkins to Work with Java, Git, and Maven 1. Configuring Jenkins to Work with Java Overview: Jenkins requires Java to run. Configuring Java in Jenkins involves specifying the Java Development Kit (JDK) installations Jenkins should use. Steps: 1. Install Java Development Kit (JDK): ○ Ensure that JDK is installed on your system. You can download it from the Oracle website or use OpenJDK. 2. Configure JDK in Jenkins: ○ Open Jenkins and go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to JDK section and click Add JDK. ○ Enter a name for the JDK installation (e.g., JDK 11). Check Install automatically to let Jenkins download and install the JDK, or specify the path to an existing JDK installation. ○ If specifying the path manually, provide the JAVA_HOME directory. 3. Example Configuration: ○ Name: JDK 11 ○ JAVA_HOME: /usr/lib/jvm/java-11-openjdk 4. Verify JDK Configuration: You can verify the JDK configuration by creating a simple Jenkins job and adding a build step that prints the Java version using: bash Copy code java -version 2. Configuring Jenkins to Work with Git Overview: The Git Plugin integrates Jenkins with Git repositories, enabling Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 35 1750 Rank

--- Page 113 ---
Course Code/Title:CS3V15/Devops Unit: III 3. Configure Job Details: ○ General: I Enter a description for your job. Configure options such as discarding old builds if needed. ○ Source Code Management: I Choose Git, Subversion, or another source control system. Enter the repository URL and credentials if the repository is private. Specify the branch or tag to build from. ○ Build Triggers: Set up triggers to start the build. Common options include: Poll SCM: Jenkins will periodically check for changes in the source code repository. . Build periodically: Schedule builds at specific intervals. I GitHub hook trigger for GITScm polling: Trigger builds based on GitHub webhooks. 0 Build Environment: ■ Configure the build environment, such as setting up environment variables or cleaning up before the build starts. ○ Build Steps: Add build steps by clicking Add build step. · Common build steps include: Execute shell: Run shell commands or scripts. . Invoke Gradle script: Use Gradle to build the project. I Invoke Ant: Use Apache Ant for building. ○ Post-build Actions: · Configure actions to perform after the build completes, such as: I Archive the artifacts: Save build artifacts for later use. I Publish JUnit test result report: Display test results in Jenkins. I Send build notifications: Notify stakeholders of build status. 4. Save and Build: ○ Click Save to store the job configuration. o To start a build, click Build Now in the job dashboard. Example Build Configuration: . Job Name: MyApp-Build · Source Code Management: Git o Repository URL: https://github.com/user/myapp.git ○ Branch: main · Build Step: Execute shell ○ Command : myn clean install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 38 nirf 1750 Rank N33

--- Page 114 ---
Course Code/Title:CS3V15/Devops Unit: III · Post-build Action: Archive artifacts ○ Files to archive: target/ *. jar 2. Understanding Jenkins Workspace Overview: The Jenkins workspace is a directory where Jenkins stores files and artifacts related to a particular build. Each job has its own workspace, which is used to perform build operations. Key Aspects of Jenkins Workspace: 1. Workspace Location: o By default, the workspace is located in the Jenkins home directory, typically at /var/lib/jenkins/workspace/ on Linux systems or C:\Program Files (x86) \Jenkins\workspace\ on Windows. 2. Workspace Structure: ○ Each job gets its own subdirectory within the workspace. For example, a job named MyApp-Build will have its workspace at /var/lib/jenkins/workspace/MyApp - Build/. ○ The workspace contains: Source Code: The code pulled from the repository. . Build Artifacts: Files generated during the build process. I Logs: Logs related to the build process. 3. Workspace Usage: ○ Building: During the build process, Jenkins checks out the code into the workspace, executes build steps, and generates artifacts. ○ Archiving Artifacts: After the build, files specified in the post-build actions are archived from the workspace. ○ Cleaning Up: Jenkins may clean up workspaces based on job configurations or policies to save disk space. 4. Customizing Workspace: ○ Configure Custom Workspace Location: In the job configuration, under the Advanced Project Options, you can specify a custom workspace directory. ○ Workspace Cleanup: I Use plugins like the Workspace Cleanup Plugin to manage and clean up workspaces automatically. Example of Workspace Structure: · Workspace Directory: /var/lib/jenkins/workspace/MyApp-Build/ o Source Code: /var/lib/jenkins/workspace/MyApp-Build/src/ o Build Artifacts: /var/lib/jenkins/workspace/MyApp-Build/target/ o Logs: /var/lib/jenkins/workspace/MyApp-Build/logs/ This guide should provide a comprehensive understanding of creating a Jenkins build and managing Jenkins workspaces. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY N32 39 nirf 175º Rank

--- Page 97 ---
Course Code/Title:CS3V15/Devops Unit: III Build Steps Execute Windows batch command ? Command See the list of available environment variables javac welcome. java java welcome Advanced = Execute Windows batch command 7 Command See the list of avalable chuironment variables acho User name is Nuernane% Advanced Step 5 : Click on Apply and then Save. Step 6 : Select Build with parameters option. Dashboard > welcome > Status Changes Workspace Build with Parameters Configure Delete Project 0 Rename CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 22 N3 nirf 1750 Rank

--- Page 98 ---
Course Code/Title:CS3V15/Devops Unit: III Project welcome This build requires parameters: Username Parth D Build Cancel We can change the username as well. Suppose I changed the Username as Anand then click the Build, we get the console output as follows - C: \ProgramData\Jenkins\.jenkins\workspace\welcome>javac welcome.java C:\ProgramData\Jenkins\.jenkins\workspace\welcomeyjava welcome Welcome to first Job of Jenkins [welcome] $ cmd /c call C:\WINDOWS\TEMP\jenkins16582464065687933173.bat C:\ProgramData\Jenkins\.jenkins\workspace\welcome>echo User name is Anand User name is Anand C:\ProgramData\Jenkins\.jenkins\workspace\welcome>exit 0 Finished: SUCCESS Thus we set variety of parameters to the Jenkins job. We can set the Boolean parameter, choice parameter, credentials, file parameter, password parameter and so on. 3. HTML Publisher Filter Plain Boolean Parameter Choice Parameter Credentials Parameter File Parameter Multi-line String Parameter Password Parameter Run Parameter String Parameter Add Parameter Overview: The HTML Publisher Plugin allows Jenkins to publish HTML reports and artifacts generated during the build process. Key Features: · Report Publishing: Publishes HTML reports, dashboards, or other HTML artifacts. · Report Directory: Allows specifying the directory containing HTML reports. · Index Page: Sets a default HTML file to be displayed when accessing the report. Configuration: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 23 nirf 1750 Rank N33

--- Page 123 ---
Course Code/Title:CS3V15/Devops Unit: IV Ad-Hoc Limitations: While ad-hoc commands are ideal for single, immediate tasks, they are not designed for complex, multi- step automation. Playbooks come to the rescue when tasks have interdependencies and need to be executed in a coordinated manner. Readability and Reusability: Playbooks are authored in YAML, a human-readable and straightforward format. This not only makes them easy to write but also facilitates sharing and collaboration. You can reuse playbooks across various scenarios, saving time and effort. Conditional and Looping Logic: Playbooks offer advanced features, including conditional statements and looping, that allow you to adapt automation to different situations. This flexibility makes playbooks versatile and capable of handling a wide range of automation needs. In essence, playbooks are your tool of choice when automation tasks become multi-faceted and require a structured and logical approach. They provide the power to streamline and automate complex workflows with precision and efficiency. Example: Ping All Inventory Devices To illustrate the use of a playbook, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: playbook.yaml 1 --- 2 -name : My First Play #Name of play 3 hosts: virtualmachines #defining host can be all 4 tasks: 5 -name:Ping My Hosts #Name of task 6 ansible.builtin.ping: # can also write ping To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 124 ---
Course Code/Title:CS3V15/Devops Unit: IV Output and Result: Activities Terminal - Nov 2 12:29 . O auriga@auriga-Latitude-E7470: - /Desktop/ansible X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook.eg1.yaml PLAY [My First Play] TASK [Gathering Facts] ok: [ vri01] TASK [Ping Devices] ok: [vm01] PLAY RECAP vn01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$[] rescued=0 ignored=0 X As you can see there is gathering facts task which we didn't create is running it is the default task which ping the connection. Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: playbook.yaml 1 --- 2- name: Install Nginx 3 hosts: all 4 become: yes 5 tasks: 6 - name: Install Nginx using apt 7 apt: 8 name: nginx 9 state: present To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml -- ask-become-pass CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 10 N33 nirf 1750 Rank

--- Page 119 ---
Course Code/Title:CS3V15/Devops Unit: IV · Groupings in inventories enable you to organize your infrastructure, allowing you to target specific subsets of nodes based on your needs. For instance, when you need to perform tasks in a particular location, groupings become essential. Creating Inventories There are two primary methods to create an inventory on the control node: using a YAML file or an INI file. 1. INI Inventory Example (inventory.ini): 1[virtualmachines] 2host1 ansible_host:192.168.0.1 3192.168.0.2 2. In this INI file, it captures the IP addresses of managed nodes. The default username used for SSH connections is typically the working user of your control node. 3. ansible_host: This parameter is used to specify the IP address, although you can write the IP directly. 4. Inventory.yaml 1 --- 2virtualmachines: # Define a class name to represent a particular group of devices. 1 hosts: # Define the hosts. 2 vm01: # Define a parent name to indicate a class of parent type. 3 ansible_host: 192.168.0.1 # Specify the IP address. 4 http_port: 80 # (optional) Define the HTTP port. 5 ansible_user: auriga # (optional) Provide the username if it's different from the control node's user. o ansible_user: Use this to specify a different user if the managed node's user is not the same as the control node's user. o http_port: Specify a port if necessary. In real-world scenarios, inventory files can be more complex, reflecting the diverse and extensive infrastructure of organizations. You can explore more configurations here. Verifying Your Inventory After creating your inventory, it's a good practice to verify its correctness. This step ensures that the inventory is correctly structured and accessible to Ansible. To do this, use the following command: 1ansible-inventory -i inventory.yaml -list · - i: Indicates the inventory file you want to use. . - list: Requests the listing of the inventory content. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 5 N33 nirf 1750 Rank

--- Page 120 ---
Course Code/Title:CS3V15/Devops Unit:IV output: auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-inventory ·i inventory.yaml -- list "_meta": { "hostvars": { "vm01": "ansible_host": "192.168.0.208", "ansible_user"; "au" "children": [ "ungrouped" "ungrouped": { "hosts": "vn01 1 By following these steps, you have effectively set up and verified your inventory, ensuring a solid foundation for your Ansible automation. Using Ansible Ad-Hoc Commands Introduction: In Ansible, ad-hoc commands are your go-to solution for executing quick, one-off tasks on remote nodes. They provide a straightforward and efficient way to interact with managed nodes without the need for creating full-fledged playbooks. Ad-hoc commands are particularly useful when you need immediate results without the overhead of playbook development. When to Use Ad-Hoc Commands: Ad-hoc commands are best suited for scenarios where the task at hand is simple and doesn't require the complexity of a playbook. They are perfect for tasks like system health checks, package installation, service management, or any other single-operation job. Ad-Hoc Command Syntax: Ad-hoc commands follow a specific syntax that comprises various components, each serving a unique role: . Target Group: This is the group of hosts you intend to target with the ad-hoc command. · Module (-m): Specifies the module to execute. Modules are Ansible's building blocks for performing tasks, and they can range from basic operations like "ping" to more advanced tasks such as package management. · Inventory File (-i): Indicates the location of your inventory file, which defines the list of target devices. Example: Ping All Inventory Devices To illustrate the use of ad-hoc commands, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: 1ansible virtualmachines -m ping -i inventory.yaml virtualmachines is the target group. In this case, it could be any group, or you can use all to target all devices in your inventory. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 111 ---
Course Code/Title:CS3V15/Devops Unit: III Steps: 1. Install the Git Plugin: 0 Go to Manage Jenkins > Manage Plugins. ○ Under the Available tab, search for Git Plugin and install it. 2. Configure Git in Jenkins: ○ Navigate to Manage Jenkins> Global Tool Configuration. ○ Scroll down to the Git section and click Add Git. ○ Specify the path to the Git executable or use the default path. ○ Optionally, configure additional settings such as Git installation locations. 3. Example Configuration: ○ Name: Git ○ Path to Git executable: /usr/bin/ git (or use the default if Git is installed in a standard location). 4. Configure Job to Use Git: ○ Create or edit a Jenkins job and go to Source Code Management. ○ Select Git. ○ Enter the repository URL (e.g., https://github.com/user/repository.git). ○ Configure credentials if the repository is private. ○ Specify the branch to build (e.g., main). 5. Example Repository URL: ○ Repository URL: https://github.com/example/repo.git o Branch Specifier: main 3. Configuring Jenkins to Work with Maven Overview: The Maven Plugin integrates Jenkins with Apache Maven, allowing Jenkins to use Maven to build projects. Steps: 1. Install Maven: ○ Ensure that Apache Maven is installed on your system. You can download it from the Apache Maven website. 2. Configure Maven in Jenkins: ○ Go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to the Maven section and click Add Maven. ○ Enter a name for the Maven installation (e.g., Maven 3.8.6). ○ Check Install automatically to let Jenkins download Maven, or specify the path to an existing Maven installation. ○ CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 36 1750 Rank

--- Page 112 ---
Unit: III Course Code/Title:CS3V15/Devops 3. Example Configuration: ○ Name: Maven 3.8.6 ○ MAVEN_HOME: / usr / share / maven 4. Configure Maven in a Jenkins Job: ○ Create or edit a Jenkins job and go to Build section. 0 Add a build step and select Invoke top-level Maven targets. o Choose the Maven version configured earlier. ○ Specify the goals to run (e.g., clean install). 5. Example Maven Goals: ○ Goals: clean install 6. Configure Build Environment: ○ Ensure that the job's build environment is set up to use Maven. This might involve setting environment variables or configuring build scripts. 7. Example Environment Variables: ○ MAVEN_OPTS: - Xms512m -Xmx2048m (if you need to set specific JVM options for Maven). By following these steps, Jenkins will be properly configured to work with Java, Git, and Maven, allowing you to set up and manage continuous integration and delivery pipelines effectively. If you need further customization or run into issues, consulting the documentation for each tool or plugin can provide additional guidance. Creating a Jenkins Build and Understanding Jenkins Workspace 1. Creating a Jenkins Build Overview: A Jenkins build is a process where Jenkins executes a series of steps defined in a job configuration. This typically involves compiling code, running tests, and generating artifacts. Steps to Create a Jenkins Build: 1. Access Jenkins Dashboard: ○ Open your Jenkins dashboard, usually found at http://localhost: 8080. 2. Create a New Job: ○ Click on New Item on the left sidebar. Enter a name for your job. ○ Choose a job type. Common types include: I Freestyle project: For simple build jobs with basic configuration. · Pipeline: For more complex build processes using a Jenkinsfile. ○ Click OK to proceed. ○ TECHNOLOGY N33 37 nirf 1750 Rank CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous)

--- Page 131 ---
Course Code/Title:CS3V15/Devops Unit: IV Ensure a service is stopped: $ ansible webservers -m service -a "name=httpd state=stopped" 6. Gathering Facts Fact represents the discovered variables about a system. You can use the facts to implement conditional execution of tasks, and also used to get ad-hoc information about your systems. To see all the facts: $ ansible all -m setup Ansible Playbooks Playbooks are the files where the Ansible code is written. Playbooks are written in YAML format. YAML means "Yet Another Markup Language," so there is not much syntax needed. Playbooks are one of the core features of Ansible and tell Ansible what to execute, and it is used in complex scenarios. They offer increased flexibility. Playbooks contain the steps which the user wants to execute on a particular machine. And playbooks are run sequentially. Playbooks are the building blocks for all the use cases of Ansible. Ansible playbooks tend to be more configuration language than a programming language. Through a playbook, you can designate specific roles to some of the hosts and other roles to other hosts. By doing this, you can orchestrate multiple servers in very different scenarios, all in one playbook. Playbook Structure Each playbook is a collection of one or more plays. Playbooks are structured by using Plays. There can be more than one play inside a playbook. Playbook Play Task Module Target Environment O Task Module Play Task Target Environment Play Module O Task CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY 17 N33 nirf 1750 Rank

--- Page 132 ---
Course Code/Title:CS3V15/Devops Unit:IV name: install and configure DB hosts: testServer become: yes vars: oracle_db_port_value : 1521 tasks: -name: Install the Oracle DB yum: < code to install the DB> -name: Ensure the installed service is enabled and running service: name: < your service name> The Different YAML Tags name This tag specifies the name of the Ansible playbook. As in what this playbook will be doing. Any logical name can be given to the playbook. hosts This tag specifies the lists of hosts or host group against which we want to run the task. The hosts field/tag is mandatory. It tells Ansible on which hosts to run the listed tasks. The tasks can be run on the same machine or on a remote machine. One can run the tasks on multiple machines and hence hosts tag can have a group of hosts' entry as well. vars Vars tag lets you define the variables which you can use in your playbook. Usage is similar to variables in any programming language. tasks All playbooks should contain tasks or a list of tasks to be executed. Tasks are a list of actions one needs to perform. A tasks field contains the name of the task. This works as the help text for the user. It is not mandatory but proves useful in debugging the playbook. Each task internally links to a piece of code called a module. A module that should be executed, and arguments that are required for the module you want to execute. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 18 N33 nirf 1750 Rank

--- Page 101 ---
Course Code/Title:CS3V15/Devops Unit: III v => SeleniumDemo src/main/java > src/main/resources > src/test/java > 15 src/test/resources > al JRE System Library [J2SE-1.5] › al Maven Dependencies » drivers v src > @ main v fs test v java v 2> myPackage 10 myTestPage.java 23 resources › › target > > test-output 2 pom.xml > Servers myTestPage.java package myPackage; import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver; import org.testng.annotations.Test; public class myTestPage { @Test public void myFun() { System.setProperty("webdriver.chrome.driver","./drivers/chromedriver.exe"); WebDriver driver = new ChromeDriver(); driver.get("https://google.com"); String title = driver.getTitle(); System.out.println("Hey Anuradha, the tile of this page is: "+title); driver.quit(); } Program explanation : In above program, we have written one test case by the function myFun. In this function, using the Chromedriver which we have installed, we open the web page google.com Then we try to get the tile of this page and display it on the console using System.out statement. Finally we quit the driver. Step 7 : Right click on the Project name in Project Explorer window, Run As-> Maven Test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 26 N3 nirf 1750 Rank

--- Page 102 ---
Unit: III Course Code/Title:CS3V15/Devops Step 8 : Now, locate the Java file myTestPage.java and right click Run As->TestNG Test. It is as follows - 0 Run As # Debug As Profile As Web Services > > 1 TestNG Test > Run Configurations ... > Alt+Shift+X, N Step 9 : We will get the output as follows - Hey Anuradha, the tile of this page is: Google PASSED: myFun Default test Tests run: 1, Failures: 0, Skips: 0 Default suite Total tests run: 1, Failures: 0, Skips: 0 Step 10 : Now, we will open the Jenkin's page and create the HTML Publisher report for the above SeleniumDemo project. The prerequisite to this activity is that the HTML Publisher report plugin must be installed in Jenkins. · Click on New Item, Enter the Item Name and select as Freestyle project, I have given the name HTMLReportDemo. · Then click on configure. Write the suitable description in General section. · The in Build Steps and Add Build steps as Invoke top-level Maven targets provide the Maven version, and under goals write clean test. . Under Post Build Actions, click on Add Post Build Action and select Publish HTML Report. Now give the path of index.html file which is created by Selenium testNG framework in Maven. It is the path of test-output folder. Also give some suitable HTML report title. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 27 N3 nirf 1750 Rank

--- Page 89 ---
Course Code/Title:CS3V15/Devops Unit: III Key Features: · Source Code Management: Configures Jenkins to use Git repositories as the source code for jobs. Example Demo Step 1 : Create a simple Java program. I have created a folder named MyJavaPrograms and inside it created a simple Java program as follows - e.g., SSH test.java public class test { public static void main(String args[I) { for(int i=1;i <= 5;1++) System.out.printin("Welcome Anuradha"); } Plugin," Step 2 : Open the command prompt, switch to that folder and execute the above Java program. It is illustrated by following screenshot. 1 C:\Windows\System32\cmd.e X + Microsoft Windows [Version 10.0.22621.2134] (c) Microsoft Corporation. All rights reserved. 0 X before E:\MyJavaPrograms>javac test. java E: \MyJavaPrograms>java test Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha E: \MyJavaPrograms> Step 3 : Now we will create a Git repository and push this repository on GitHub. First of all we will initialise the Git repository my using git init command. CHENNAI INSTITUTE TECHNOLOGY 14 - N32 nirf 1750 Rank CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt)

--- Page 90 ---
Unit: III Course Code/Title:CS3V15/Devops C:\Windows\System32\cmd.e x X E:\MyJavaPrograms>git init Initialized empty Git repository in E:/MyJavaPrograms/.git/ E:\MyJavaPrograms>git status On branch master No commits yet Untracked files: (use "git add <file> ... " to include in what will be committed) test.class test. java nothing added to commit but untracked files present (use "git add" to track) E:\MyJavaPrograms> then add the java and class files to git repository - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git add . E:\MyJavaPrograms>git status On branch master No commits yet Changes to be committed: (use "git rm -- cached <file> ... " to unstage) new file: test.class new file: test. java E: \MyJavaPrograms> Now we will commit the changes - C:\Windows\System32\cmd.e X E:\MyJavaPrograms>git commit -m "First Commit for Java program" [master (root-commit) 4cb04db] First Commit for Java program 2 files changed, 6 insertions(+) create mode 100644 test.class create mode 100644 test. java E:\MyJavaPrograms> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) - INSTITUTE . TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 133 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible Roles Roles provide a framework for fully independent or interdependent collections of files, tasks, templates, variables, and modules. The role is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks and makes them easier to reuse. The breaking of the playbook allows you to break the playbook into reusable components. Each role is limited to a particular functionality or desired output, with all the necessary steps to provide that result either within the same role itself or in other roles listed as dependencies. Roles are not playbooks. Roles are small functionality that can be used within the playbooks independently. Roles have no specific setting for which hosts the role will apply. Top-level playbooks are the bridge holding the hosts from your inventory file to roles that should be applied to those hosts. Creating a Role The directory structure for roles is essential to creating a new role, such as: Role Structure The roles have a structured layout on the file system. You can change the default structured of the roles as well. For example, let us stick to the default structure of the roles. Each role is a directory tree in itself. So the role name is the directory name within the /roles directory. $ ansible-galaxy -h Usage ansible-galaxy[delete|import|info|init|install|list|login|remove|search|setup][ -- help] [options] ... Options o -h: (help) it shows this help message and exit. o -v: (verbose) Verbose mode (-vvv for more, -vvvv to enable connection debugging). o -- version: it shows program version number and exit. Roles are stored in separate directories and have a particular directory structure CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 19 N33 nirf 1750 Rank

--- Page 134 ---
Course Code/Title:CS3V15/Devops Unit: IV Typical structure of ansible role myweb defaults L main.yml files · handlers L main.yml meta. L main.yml README.md tasks L_ main.yml templates tests inventory test.yml vars L main.yml · defaults : It stores the default variable for the role. For example the default port number is http is 8080, then it can be stored in defaults. · files : This folder contains the files required to transfer or deploy to the target machines. · handlers : It contains handlers, which may be used by some role. · meta : It defines some data or information about the role. · tasks : It contains the main list of tasks to be executed by the role. These tasks might be defined in separate files as per the functionalities. · templates : It contains the templates which can be deployed via a role. Ansible Module Ansible Command module is used to execute commands on a remote node. The Command module, is used mostly to run simple Linux commands on a remote node/server which is part of a host group or Stand alone server mentioned in the host group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 121 ---
Course Code/Title:CS3V15/Devops Unit: IV · - m ping specifies the "ping" module, a basic module that checks the reachability of the managed nodes. . - i inventory.yaml points to the inventory file containing the list of devices. · The "ping" module sends a test command to the target devices and reports their status, confirming whether they are responsive. Output and Result: auriga@auriga-Latitude-E7470 :- /Desktop/ansibleŞ ansible all -[ inventory.yaml -m ping VOOS | UNREACHABLET => ( "changed's false. "mg's "Failed to connect to the host via s "unreachable": true auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ton denied (publickey_ password) In the above example the ssh host is not reachable. Activities Terminal Nov 2 12:13 . auriga@auriga-Latitude-E7470: - /Desktop/ansible C auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -i inventory.yaml -m ping vn81 | SUCCESS => 'ansible_facts": { "discovered_interpreter_python": "/usr/bin/python3" "changed": false, "ping": "pong" auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ In the above output image it is working fine Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: 1ansible all -i inventory.yaml -m apt -a "name=nginx state=present" -- become -- ask-become-pass · - m apt specifies the "apt" module, which is responsible for package management. · - a "name=nginx state=present" includes variables for the module to act upon, such as specifying that Nginx should be installed (state=present). · - become signifies that the command should run with elevated privileges (sudo). · - ask-become-pass prompts for the sudo password of the managed node to ensure the installation proceeds smoothly. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 7 N32 nirf 1750 Rank

--- Page 122 ---
Course Code/Title:CS3V15/Devops Unit:IV Output and Result: Activities Terminal * Nov 2 12:24 · auriga@auriga-Latitude-E7470: - /Desktop/ansible F X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -[ inventory.yaml -m apt -a "name=ansible state=present" -- become - - ask-become-pass BECOME password: vm01 | CHANGED => { "ansible_facts": { "discovered_interpreter_python": * /usr/bin/python3" "cache_update_time": 1698985056, "cache_updated": false, "changed": true, "stderr": "" 'stderr_Lines": []. stdout": "Reading package lists ... \nBuilding dependency tree ... \nReading state information ... \nThe following packages were automatically installed and are no longer required:\n gtr1.2-goa-1.0 hplip-data libcgt-fast-perl libcgt-pm-perl\n libdouble-conversion3 Libevent-core-2.1-7 libevent-pthreads-2.1-7\n \ibfcgt-perl libfprint-2-todi libfwupdplugin1 libht ml-template-perl\n Libpcre2-16-0 libqtScoresa libqtsdbuss libqtsguis libqt5network5\n libqtSpositionings libqt5printsupports libqt5qm15 Libqtsquick5\n libqtSsensors5 libqt5svg5 Li bqt5webchannel5 libqt5webkit5 libqt5widgets5\n Libxcb-xinerama@ Libxcb-xinpute Libxmlbi printer-driver-postscript-hp\n python3-renderpm python3-reportlab python3-reportlab-accel\n qt5-gtk-platformthene qttranslations5-lien shim\nUse 'sudo apt autoremove' to remove them. InThe following additional packages will be installed:\n teee-data python3-argcomplete pyt hon3-crypto python3-dnspython\n python3-jinja2 python3 . jmespath python3-kerberos python3-libcloud\n python3-netaddr python3-ntin-auth python3-requests-kerberos\n python3-requests- ntlm python3-selinux python3-winrm python3-xmltodict\nSuggested packages: \n cowsay sshpass python- jinja2-doc ipython3 python-netaddr-docs\nThe following NEW packages will be install ed:\n ansible leee-data python3-argcomplete python3-crypto python3-dnspython\n python3- jinja2 python3-jmespath python3-kerberos python3 . Libcloud\n python3-netaddr python3-ntlm-aut h python3-requests-kerberos\n python3-requests-ntIm python3-selinux python3-winrm python3-xmltodict\n0 upgraded, 16 newly installed, 0 to remove and 50 not upgraded. InNeed to get 97 25 kb of archives. \nAfter this operation, 98.6 MB of additional disk space will be used. \nGet:1 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-jinja2 all 2.10.1-2 [95.5 KB]\nGet:2 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-crypto amd64 2.6.1-13ubuntu2 [237 KB]\nGet:3 http://in.archive.ubuntu.com/ubuntu focal-updates/main amd64 pyt hon3-dnspython all 1.16.0-1ubuntu1 [89.2 KB]\nGet:4 http://in.archive.ubuntu.com/ubuntu focal/main amd64 ieee-data all 20180805.1 [1589 KB]\nGet: 5 http://in.archive. ubuntu.com/ubuntu focal-updates/main amd64 python3-netaddr all 0.7.19-3ubuntu1 [236 KB]\nGet:6 http://in.archive.ubuntu.com/ubuntu focal/universe and64 ansible all 2.9.6+dfsg-1 [5794 KB]\nGet:7 http: //\n.archive.ubuntu.com/ubuntu focal/universe amd64 python3-argcomplete all 1.8.1-1.3ubuntu1 [27.2 kB]\nGet:8 http://in.archive.ubuntu.com/ubuntu focal-updates/main and64 python3- jne spath all 0.9.4-2ubuntu1 [21.5 kB]\nGet:9 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-kerberos amd64 1.1.14-3.1build1 [22.6 k8]\nGet: 10 http://in.archive.ubuntu. con/ubuntu focal/universe amd64 python3-libcloud all 2.8.0-1 [1483 KB]\nGet:11 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-ntlm-auth all 1.1.0-1 [19.6 KB]\nGet: 1 2 http://in.archive.ubuntu.com/ubuntu focal/universe and64 python3-requests-kerberos all 0.12.0-2 [11.9 KB]\nGet: 13 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-r equests-ntlm all 1.1.0-1 [6004 B]\nGet: 14 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-selinux amd64 3.0-1build2 [139 KB]\nGet : 15 http://in.archive.ubuntu.com/ubu ntu focal/universe and64 python3-xmltodict all 0.12.0-1 [12.6 KB]\nGet:16 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-winrm all 0.3.0-2 [21.7 KB]\nFetched 9725 k B in 7s (1454 KB/s)\nSelecting previously unselected package python3-jinja2.\r\n(Reading database ... \r(Reading database ... 5*\r (Reading database .. . 10%\r(Reading database ... 15% \r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database . .. 45%\r (Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 68%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading da tabase ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 212542 files and directories currently installed.)\r\nPreparin g to unpack ... /00-python3-jinja2_2.10.1-2_all.deb ... \r\nUnpacking python3-jinja2 (2.10.1-2) ... \r\nSelecting previously unselected package python3-crypto. \r\nPreparing to unpack .. ./01-python3-crypto_2.6.1-13ubuntu2_amd64.deb ... \r\nUnpacking python3-crypto (2.6. 1-13ubuntu2) ... \r\nSelecting previously unselected package python3-dnspython. \r\nPreparing to unpa ck ... /02-python3-dnspython_1.16.0-lubuntu1_all.deb ... \r\nUnpacking python3-dnspython (1. 16.0-1ubuntu1) ... \r\nSelecting previously unselected package leee-data. \r\nPreparing to unp ack ... /03-ieee-data_20180805.1_all.deb ... \r\nUnpacking teee-data (20188805.1) ... \r\nSelecting previously unselected package python3-netaddr. \r\nPreparing to unpack ... /04-python3. netaddr_0.7.19-3ubuntu1_all.deb ... \r\nUnpacking python3-netaddr (0.7.19-3ubuntu1) ... \r\nSelecting previously unselected package ansible. \r\nPreparing to unpack .. . /05-ansible_2.9.6 +dfsg-1_all.deb ... \r\nUnpacking ansible (2.9.6+dfsg-1) ... \r\nSelecting previously unselected package python3-argcomplete. \r\nPreparing to unpack .. . /06-python3-argcomplete_1.8.1-1. 3ubuntu1_all.deb ... \r\nUnpacking python3-argcomplete (1.8.1-1.3ubuntu1) ... \r\nSelecting previously unselected package python3- jnespath. \r\nPreparing to unpack ... /07-python3- jnespa th_0.9.4-2ubuntul_all.deb ... \r\nUnpacking python3-jmespath (0.9.4-2ubuntu1) ... \r\nSelecting previously unselected package python3-kerberos. \r\nPreparing to unpack ... /08-python3-ke rberos_1.1.14-3.1build1_and64.deb ... \r\nUnpacking python3-kerberos (1.1.14-3.1build1) ... \r\nSelecting previously unselected package python3-libcloud. \r\nPreparing to unpack .. . /09- python3-libcloud_2.8.0-1_all.deb ... \r\nUnpacking python3-libcloud (2.8.0-1) ... \r\nSelecting previously unselected package python3-ntlm-auth. \r\nPreparing to unpack ... /10-python3-n tim-auth_1.1.0-1_all.deb .. . \r\nUnpacking python3-ntlm-auth (1.1.0-1) ... \r\nSelecting previously unselected package python3-requests-kerberos. \r\nPreparing to unpack ... /11-python3. requests-kerberos 0.12.8-2 all. deb .... \r\nUnpacking python3-requests-kerberos (0.12.0-2) ... \r\nSelecting previously unselected package python3-requests-ntln. \r\nPreparing to unpack By using these ad-hoc commands, you can perform quick, task-specific operations on your managed nodes, saving time and effort in your automation tasks. CommonModules: Ansible offers an extensive library of modules for various tasks, from system administration to application deployment. You can explore the full list of Ansible modules in the official documentation to find the most suitable module for your specific requirements. BestPractices: When working with ad-hoc commands, consider using SSH key-based authentication for secure, passwordless access. It streamlines the authentication process and enhances the security of your automation tasks. Using Ansible Playbooks Introduction: In the world of Ansible, playbooks are the cornerstone of structured and complex automation. While ad- hoc commands are perfect for quick and isolated tasks, playbooks step in when you need a well- orchestrated sequence of operations. They allow you to tackle multi-step automation scenarios where tasks depend on each other, and conditional actions are required. Let's delve deeper into the importance of playbooks: The Role of Playbooks: Playbooks serve as the framework for orchestrating automation tasks that involve a sequence of operations. Whether it's configuring servers, deploying applications, or managing infrastructure, playbooks provide a structured way to define how these tasks are executed. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N33 nirf 1750 Rank

--- Page 99 ---
Unit: III Course Code/Title:CS3V15/Devops 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "HTML Publisher Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Post-build Actions section. ○ Select Publish HTML reports. ○ Specify the directory containing HTML reports and the index page. Usage: . Ideal for displaying build reports such as test results or code coverage metrics in a user-friendly HTML format. DEMO: Step 1 : We will create a Maven project using Selenium and TestNG framework. Open Eclipse : IDE and Create a Maven Project. Select the Create Simple Project (Skip archetype selection) and enter the Group Id and Artifact Id as per your choice. My project name is SeleniumDemo. Step 2 : Add the Selenium dependency in your pom.xml file as follows - <dependency> <groupId>org.seleniumhq.selenium</groupId> <artifactId> selenium-java</artifactId> <version>3.141.59</version> </dependency > Save the pom.xml file. Step 3 : Now install testNG plugin. For that, from Menu select Help-> Eclipse Marketplace Help Welcome 7 Help Contents Search Show Context Help Show Active Keybindings ... Ctrl+Shift+L Tip of the Day Tips and Tricks ... Cheat Sheets ... @ Eclipse User Storage Open the Eclipse Marketpla ** Perform Setup Tasks ... % Check for Updates Install New Software ... Eclipse Marketplace ... = About Eclipse IDE · Donate Contribute Type TestNG in the search window. The testNG installation option will be available. click Install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 24 N32 nirf 1750 Rank ---

--- Page 100 ---
Course Code/Title:CS3V15/Devops Unit: III C Eclipse Meketplace × Eclipse Marketplace Select solutions to initiit, Prese leuralt Now to proceed with lutallation. Press the "more info" link to learn more about a solution Search Recent Popular Favorites itwled - Research at the Eclipse × All Categories Go TestNG for Eclipse . This plug-in late you run your TextNG wits from Erfiore. You can run sulter, groupe er individuel methods. Errors are reported in a pepwane tab that lets you. more info by Chat Bent Apache 20 tantog zielt testingunit integration functional selection * 737 ** * stal: 1,83M (25,021 last month) Install MoreUnit 3.3.0 Morelinit is an Solipoe plugin that should mult you in wilting mory unk with. It supports ali programming languages (witching between teds and clanset under. mone.info Install Infinitest 5.4.0.3 Infinitest is a continuont tout runner for tova,'and is valuable to developers valing a Marketplaces (7) Cancel Then confirm the selected features by clicking on Confirm button. x Confirá Selected Features These Confirm to gorthus with the induction. Cs go back te chocie bis sules it CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 25 N3 nirf 1750 Rank

--- Page 125 ---
Course Code/Title:CS3V15/Devops Unit: IV This command executes the specified playbook while prompting for the necessary privilege escalation password. Output and Result: Activities Terminal - Nov 2 13:00 . auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook-eg2.yaml PLAY [Install Nginx] TASK [Gathering Facts] Total: [vnet]: FAILEDT .. ['msg": "Missing sudo password') PLAY RECAP : ok=0 changed=0 unreachable=0 Fatled =! skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory. yaml playbook-eg2.yaml - - ask-becone-pass BECOME password: PLAY [Install Nginx] TASK [Gathering Facts] ok: [vri01] TASK [Install Nginx using apt] ok: [vri01] PLAY RECAP * VMD1 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ [] As shown in the example image above, the first command encountered an error due to the absence of the -ask-become-pass flag. This flag is essential when your task requires elevated privileges, as it prompts Ansible to request the sudo password for authentication. The corrected command includes the -ask-become-pass flag, ensuring that the necessary privileges are obtained before executing the task. This is particularly important when working with tasks that require administrative access, such as package installations or system configurations. Note: In this blog, we primarily focus on the core and fundamental components of Ansible, providing an introductory overview and understanding of its key concepts. While Ansible offers advanced features such as "Collections," "Ansible Galaxy," "Roles," "Dynamic Inventory," and "Custom Modules and Plugins," we recognize that these topics are extensive and may require separate dedicated discussions. For the purpose of this blog, we aim to establish a strong foundation of Ansible's core components. If you're interested in diving deeper into these advanced topics, we recommend exploring Ansible's official documentation and additional resources dedicated to each subject. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 11 N32 nirf 1750 Rank

--- Page 126 ---
Course Code/Title:CS3V15/Devops Unit:IV Understanding YAML Ansible uses YAML syntax for expressing Ansible playbooks. This chapter provides an overview of YAML. Ansible uses YAML because it is very easy for humans to understand, read and write when compared to other data formats like XML and JSON. Every YAML file optionally starts with " --- " and ends with " ... ". In this section, we will learn the different ways in which the YAML data is represented. key-value pair YAML uses simple key-value pair to represent the data. The dictionary is represented in key: value pair. Note - There should be space between : and value. Example: A student record --- #Optional YAML start syntax james: name: james john rollNo: 34 div: B sex: male ... #Optional YAML end syntax Abbreviation You can also use abbreviation to represent dictionaries. Example James: {name: james john, rollNo: 34, div: B, sex: male} Representing List We can also represent List in YAML. Every element(member) of list should be written in a new line with same indentation starting with "- " (- and space). Example countries: - America - China - Canada - Iceland CHENNAI INSTITUTE OF TECHNOLOGY CHENNAI INSTITUTE . TECHNOLOGY (Autonomous) 12 N33 nirf 1750 Rank

--- Page 117 ---
Course Code/Title:CS3V15/Devops Unit: IV • Explore Ansible modules. 4. Playbook - · Playbooks are a sequence of plays that define the order of tasks executed by Ansible. . A play consists of a list of tasks that target managed nodes in an inventory. · Tasks, in turn, are composed of one or more modules that specify operations. · Playbooks provide a structured way to define and automate complex workflows. 5. Roles - · Roles provide an organized environment for managing complex tasks. · They include templates, playbooks, inventories, error handlers, vars, and meta information. · Ideal for handling larger, multi-step automation processes. 6. Collections - · Collections are distribution formats for Ansible content. · They encompass playbooks, roles, modules, and plugins. · Find Ansible collections. 7. Galaxy - · Ansible Galaxy is a platform for sharing and downloading collections. . It allows the Ansible community to collaborate and exchange automation content. Setting Up the Ansible Control Node Prerequisite: Before you begin setting up Ansible, it's essential to ensure you meet the following prerequisites: · SSH: Make sure SSH is installed on your Linux system. SSH is crucial for secure communication between the control node and managed nodes. Installation Steps: 1. Open your terminal. 2. Run the following command to install Ansible: 1python3 -m pip install -- user ansible 3. SSH Key: o For secure and direct connections to managed nodes, you'll need to provide a public SSH key. You can generate one using the ssh-keygen command. o Follow the on-screen instructions to create your SSH key pair. This key pair is a crucial element of Ansible's secure communications. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 118 ---
Course Code/Title:CS3V15/Devops Unit: IV 4. For specific installation instructions tailored to your Linux distribution, click here. 5. Try to connect ssh using once your mange node is set up. 1ssh <user>@<ip> Note: If you're using a different operating system, find the appropriate installation instructions here. Setting Up the Ansible Mange Node Steps: 1. Install OpenSSH-Server: o To enable remote management of the managed node, you'll need to install the OpenSSH server. Use the following command (assuming you're using a Debian-based Linux distribution like Ubuntu): 1sudo apt install openssh-server o This command installs the OpenSSH server, allowing secure remote access. 2. Create a User: o For convenience and consistency, it's helpful to create a user on the managed node with the same name as the user on the control node. This makes it easier to manage SSH keys and ensures a smoother experience. 3. Configure SSH Key: o To establish secure and direct connections to the managed node, you need to paste the public SSH key from the control node into the authorized_keys file located in the .ssh directory of the user's home folder on the managed node. o The path is typically /home/<user>/.ssh/authorized_keys, where <user> is the username you created or are using on the managed node. You can use the ssh-copy-id command to automate the process of copying your public key to the managed node. For example: 1ssh-copy-id <user>@<managed_node_ip> o This command securely copies your public key to the authorized_keys file, allowing passwordless SSH authentication. Managing Ansible Inventories In the world of Ansible, inventories are like the backbone of your automation infrastructure. They serve as a vital component that lists and organizes the managed nodes, making automation tasks seamless and organized. Inventories use a parent-child concept that allows you to create groupings, which come in handy during specific tasks or scenarios. Why Are Inventories Important? · Inventories provide a comprehensive list of managed nodes, making it easy to interact with and manage them. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 4 N33 nirf 1750 Rank

--- Page 135 ---
Course Code/Title:CS3V15/Devops Unit: IV Syntax for writing the module on command line While writing the module we use the options such as -m and -a. The -m is for module and -a is for argument, after the -a option we specify the executable command in double quotes. here 'all' stands for all the remote hosts, otherwise we can mention the group name of the hosts 1 $ ansible all -m command -a "echo 'Hello hosts'" The -m option followed by module name 'command' Command to be executed on remote hosts, given in double quotes Commonly used modules 1. Command : Following is a command module that executes a command on remote hosts $ ansible all -m command -a "echo 'Hello hosts'" 2. User : The user module is used to create, modify or delete users. For example - To create a user named 'new_user' on all the hosts in the 'dev' group using ansible, you can use the user module. Here's the ansible command to do that : $ ansible dev -m user -a "name=new_user state=present" Sometimes the above command fails as there is no privilege to create user on remote host. Then we can issue the command as follows $ ansible all -m user -a "name=new_user state=present" -- become -- ask-become-pass The above command asks for the password for the remote host to login. On supplying the password the user will be created. We can verify. if new user is getting created or not on remote host by issuing following command on each remote host $ less /etc/passwd If we want to create new user without asking for password on all the remote hosts then we can issue the command as - $ ansible all -m user -a "name=new_user1 state=present" -- become -b The -b option is used to become superuser. By this the sudo or su privileges can be granted. .3. Ping : The ping module is used to connectivity to remote hosts $ ansible all -m ping 4. Copy : The copy module is used to copy files from control machine to remote hosts $ ansible dev -m copy -a "src=/home/ansible_user/myfile.txt dest=/home/ansible_user/myfile.txt" The above command will copy the myfile.txt file from control node to all the managed nodes of dev group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 21 N32 nirf 1750 Rank

--- Page 136 ---
Course Code/Title:CS3V15/Devops Unit: IV 5. Package : The package module is used to install, remove or upgrade the packages, For example, the following command installs the nginx web server package : $ ansible package -a "name=nginx state=present" Here name indicates the name of the package to be installed, the state=present means install the package. Normally "present" is for installation, "remove" is for removing the package and "latest" is used to upgrade the package. 6. Shell : The shell module is used to run shell commands with more flexibility on remote hosts. For example - Following shell command executes on remote hosts. $ ansible dev -m shell -a "cmd=is" 7. Service : The service module is used to start, stop or restart a specific service on remote hosts. For example - Following service will start the apache web service on all the remote hosts. $ ansible dev -m service -a "name=apache2 state=started" Note that the state can started for starting the service, stopped for stopping the service. o started/stopped are idempotent actions that will not run commands unless necessary. o restarted will always bounce the service. o reloaded will always reload. I Example 1: Get the Uptime of remote servers We have used command module to run the uptime command and we have given both the ad hoc and the playbook form of execution. as AD-HOC Command $ ansible testservers -m command -a uptime -i ansible_hosts as Playbook - name: Check the remote host uptime hosts: testservers tasks: - name: Execute the Uptime command over Command module register: uptimeoutput command: "uptime" - debug: var: uptimeoutput.stdout_lines CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 22 N32 nirf 1750 Rank

--- Page 91 ---
Course Code/Title:CS3V15/Devops Unit: III Now we will create a repository on GitHub and then push the above committed Git repository on the GitHub. Open a web browser, and login to www.GitHub.com. Create a repository by the name MyJavaPrograms. Start a new repository A repository contains all of your project's files, revision history, and collaborator discussion. AnurdhaP / MyJavaPrograms Public Anyone on the internet can see this repository Private You choose who can see and commit to this repository Create a new repository 1 then copy the URL of this repository. HTTPS SSH https://github.com/AnurdhaP/MyJavaPrograms.git Now open the command prompt window and go to the MyJavaPrograms directory and issue the following command. git remote add origin https://github.com/AnurdhaP/MyJavaPrograms.git then push the Git to GitHub repository using the command push -u origin master CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 16

--- Page 92 ---
Course Code/Title:CS3V15/Devops Unit: III It is illustrated as follows - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git push -u origin master Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 719 bytes | 719.00 KiB/s, done. Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 To https://github.com/AnurdhaP/MyJavaPrograms.git * [new branch] master -> master branch 'master' set up to track 'origin/master'. E: \MyJavaPrograms> · Now it we refresh the repository on GitHub then we can see our test.java and test.class programs are present in that repository. MyJavaPrograms Public !" master - 1º 1 branch 0 tags Go to f AnurdhaP First Commit for Java program .4cb test.class First Commit for Java program test.java First Commit for Java program Help people interested in this repository understand your project by adding a README. Step 4 : Now create a job in Jenkins that makes use of GitHub plugin in order to access the GitHub repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 17 N3 nirf 1750 Rank

--- Page 161 ---
Add a status badge to your repository Many developers like to show that they're keeping their code quality high by displaying a status badge in their repo. Azure Pipelines succeeded To copy the status badge to your clipboard: 1. In Azure Pipelines, go to the Pipelines page to view the list of pipelines.Select the pipeline you created in the previous section. 2. Select , and then select Status badge. 3. Select Status badge. 4. Copy the sample Markdown from the Sample markdown section. Now with the badge Markdown in your clipboard, take the following steps in GitHub: 1. Go to the list of files and select Readme.md. Select the pencil icon to edit. 2. Paste the status badge Markdown at the beginning of the file. 3. Commit the change to the main branch. 4. Notice that the status badge appears in the description of your repository. To configure anonymous access to badges for private projects: 1. Navigate to Project Settings in the bottom left corner of the page 2. Open the Settings tab under Pipelines 3. Toggle the Disable anonymous access to badges slider under General 21

--- Page 162 ---
Azure-Pipeline • Azure Pipelineș provides a YAML pipeline editor that you can use to author and edit your pipelines. • The YAML editor is based on the Monaco Editor. • The editor provides tools like Intellisense support anda task assistant to provide guidance while you edit a pipeline. • You can also edit pipelines by modifying the azure pipelines.yml file directly in your pipeline's repository using a text editor of your choice Edit a YAML pipeline To access the YAML pipeline editor, do the following steps. • Sign in to your organization • (https://dev.azure.com/{yourorganization}). • Select your project, choose Pipelines, and then select the pipeline you want to edit. You can browse pipelines by Recent, All, and Runs. • Choose Edit. • Make edits to your pipeline using Intellisense and the task assistant forguidance. Azure DevOps fabrikam-tailspin FabrikamFiber F FabrikamFiber + Pipelines Overview Recent All Runs Boards Pipelines Recently run pipelines Repos 1 Pipeline Pipelines Pipelines FabrikamFiber 2 3 17

--- Page 147 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: After clicking new repository option, we will have to initialize some things like, naming our project, choosing the visibility etc. After performing these steps click Create Repository button. Create a new repository A repository contains all project files, including the revision history. Already have a project repository elsewhere? Import a repository. Owner Repository name * / Namanbhatia7 - Resume This is going to be name of our project Great repository names are short and memorable. Need inspiration? How about animated-memory? Description (optional) Public Anyone Keep this as public selected ository. You choose who can commit. see this rer Private You choose who can see and commit to this repository. We can add a project description if we want. Skip this step if you're importing an existing repository. Initialize this repository with a README This will let you immediately clone the repository to your comp Tick the README option Add .gitignore: None - Add a license: None ~ Create repository After performing above steps, Click this button Step 3: After clicking the button, we will be directed to below page. Right now the only file we have is a readme file. Namanbhatia7 / Resume o Unwatch - 1 * Star 0 Y Fork 0 <> Code Issues 0 (*) Pull requests 0 ||| Projects 0 BB Wiki Security the Insights # Settings No description, website, or topics provided. Edit Manage topics T 1 commit įº 1 branch > 0 releases 2 1 contributor Branch: master New pull request Create new file Upload files Find File Clone or download TT Namanbhatia7 Initial commit Latest commit 676ac98 now README.md Initial commit now DO README.md Resume UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 7 175" Rank

--- Page 148 ---
Course Code/Title: CS3V15/Devops Unit: V Step 4: Now click on the "Upload files" button. 4 > C https://github.com/avicnotes/html-css-projects ... Search or jump to ... Click to End Session Total: $80.83 Pull requests Issues Marketplace Explore Google" Custom Search @ avicnotes / html-css-projects Unwatch - 1 Star 0 Fork 0 <> Code Issues 1'] Pull requests Actions Projects Wiki O Security ~ Insights ... go main - ¿º 1 branch 0 tags Go to file Add file - { Code About avicnotes screenshots Create new file Upload files 2 24 commits No description, website, or topics provided. README.md Update README.md 2 days ago Readme Screenshot (10).png screenshots 2 days ago Screenshot (11).png screenshots 2 days ago Releases Screenshot (12).png screenshots 2 days ago No releases published Create a new release 19 Screenshot (6).png screenshots 2 days ago accordion.JPG Add files via upload 4 days ago Packages background-dots.png Add files via upload 2 days ago clone-project-1.html Updated navbar items to my original ones 2 days ago index.html Create index.html 's ago No packages published Publish your first package Activate Windows Go to Settings to activate Windows, Environments 1 O Type here to search O W. 11:32 AM 2/1/2021 Step 5:Follow the steps mentioned and click "commitchanges". Drag additional files here to add them to your repository Or choose your files Display_Picture.png Simply drag or add files to be uploaded × My_Resume.html X Styles.css × Commit changes Add initial resume website files Add an optional ended des Give this particular version a name -o- Commit directly to the master branch. Ensure this option is checked !] Create a new branch for this commit and start a pull request. Learn more about pull requests. Commit changes Cancel UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 8

--- Page 157 ---
Select a pipeline run to view information about that run. You can choose to Retain or Delete a run from the context menu. For more information on run retention, see Build and release retention policies. Stages La 51m ago > ... 6 42s Retain Delete Ld 2h ago 0 1m 13s - View pipeline details The details page for a pipeline allows you to view and manage that pipeline. Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. Azure DevOps · Search F 4 FabrikamFiber Edit Run pipeline : + Runs Branches Analytics Y Description Stages Ac #20191209.3 Set up CI with Azure Pipelines Manually triggered { main 2b4b23c #20191209.2 Set up CI with Azure Pipelines & Manually triggered & main 2b4b23c Là 54m ago Č 1m 13s Là 55m ago Č 1m 55s #20191209.1 Set up CI with Azure Pipelines Individual CI 1º main 2b4b23c Lô 56m ago 1m 6s 17

--- Page 158 ---
Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. View pipeline run details From the pipeline run summary you can view the status of your run, both while itis running and when it is complete. #20191210.2 Update azure-pipelines.yml for Azure Pipe ... on FabrikamFiber Summary Environments Triggered by Steve Danielson FabrikamFiber & main b2f795e Today at 12:56 PM Duration: 1m 9s Tests: Get started Changes: ¢ 2 commits Work items: ₾ 1 linked Artifacts: 1 published Stages Jobs Build 1 job completed 415 Run new : Deploy 1 job completed 13s 1 artifact From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, test results, and work Items From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, testresults, and work items Jobs and stages The jobs pane displays an overview of the status of your stages and jobs. This pane may have multiple tabs depending on whether your pipeline has stages and jobs, or just jobs. In this example, the pipeline has two stages named Build and Deploy. You can drill down into the pipeline steps by choosing the job from either the Stages or Jobs pane. 10

--- Page 137 ---
Course Code/Title:CS3V15/Devops Unit:IV I Example 2: Get the Hostname and Version of remote servers with UNAME we have used the command module and executing an uname -a command as AD-HOC command $ ansible testservers -m command -a "uname -a" -i ansible hosts As Playbook - name: Check the remote host Hostname, Version, Distribution with UNAME hosts: testservers tasks: - name: Execute the UNAME command register: unameout command: "uname -a" - debug: var: unameout.stdout lines I Example 3: Check the Disk Usage of Remote server To get the disk usage, we are using df -h , here -h is human readable As Ad Hoc command $ ansible testservers -m command -a "df -h" -i ansible hosts As Playbook - name: Check the disk usage of all the file system in the remote servers hosts: testservers tasks: - name: Execute the df command register: dfout command: "df -h" - debug: var: dfout.stdout lines CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 138 ---
Course Code/Title:CS3V15/Devops Unit:IV Example 4: Restart Apache Server using Ansible Command Module So far, In all the sample we have seen we are using one application server and one web server. Totally two servers. But in this example, we need to limit our execution only to the web server as we are going to check the restart the apache web server. this is done using -- limit parameter As Ad hoc command $ ansible testservers -m command -a "httpd -k restart" -i ansible_hosts -b - limit As Playbook - name: restart apache web server hosts: testservers tasks: - name: restartapache register: httpdresout become: yes command: "httpd -k restart" when: ansible_hostname == "mwiweb02" - debug: var: httpdresout.stdout_lines I Example 5: Execute a command when a file exists or not exists There are two most useful parameters in ansible command module such as removes and creates removes - used to tell ansible to Execute the command only if the file exist creates - used to tell ansible to Execute the specified command only if the file does not exist Here we are going to do a very simple file creation and removal based on the file availability or existence. In General using the ansible command module, This method of creating and removing the file is not recommended as Ansible has a dedicated module named file to do the same effortlessly. As Ad hoc command Remove the file, if it does exist (or) present $ ansible testservers -a "rm -rf /tmp/testfile removes=/tmp/testfile" -i ansible_hosts CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 24 N33 nirf 1750 Rank

--- Page 159 ---
Choose a job to see the steps for that job From the < Jobs in run #20191 ... FabrikamFiber Build O Build Build 40s Initialize job 1s 2 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 Duration: 40s 6 Pool: Azure Pipelines Image: Ubuntu-16.04 7 > Job preparation parameters Checkout 3s CmdLine 25 Component Detect 32s Post-job: Checkout <1s Finalize Job <1s Deploy DeployWeb 10s Finalize build Report build status <1s steps view, you can review the status and details of eachstep. From the Moreactions you can toggle timestamps or view a raw log of all steps in the pipeline. Build 9 Pool: Azure Pipelines 2 Image: Ubuntu-22.04 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 View job raw log Toggle timestamps 6 Duration: 40s Job preparation parameters 10

--- Page 160 ---
Cancel and re-run a pipeline If the pipeline is running, you can cancel it by choosing Cancel. If the run has completed, you can re-run the pipeline by choosing Run new. Pipeline run more actions menu: #20191210.3 Update azure-pipelines.yml for Azure Pip ... on FabrikamFiber Cancel Stages Jobs € Build C Deploy 0/1 compléted 48s Not started Build 48s Cancel From the More actions menu you can download logs, add tags, edit the pipeline, delete the run, and configure retention for the run. cure Pipe ... Run new Download logs Add tags Edit pipeline View retention leases Retain Delete 20

--- Page 165 ---
Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. Customize CI triggers Pipeline triggers cause a pipeline to run. You can use trigger: to cause a pipeline to run whenever you push an update to a branch. YAML pipelines are configured by default with a CI trigger on your default branch (which is usually main). You can set up triggers for specific branches or for pull request validation. For a pull request validation trigger, just replace the trigger: step with pr: as shown in the two examples below. By default, the pipeline runs for each pull request change. . If you'd like to set up triggers, add either of the following snippets at the beginning of your azure-pipelines.yml file. YAML I Copy trigger: - main - releases/* YAML Copy pr: - main - releases/* You can specify the full name of the branch (for example, main) or a prefix-matching · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. Build using multiple versions Add steps You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can use tasks for building, testing, publishing, or deploying your app. For Java, the Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. YAML Copy - task: PublishCodeCoverageResults@1 inputs: codeCoverageTool: "JaCoco" summaryFileLocation: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco/ reportDirectory: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco" failIfCoverageEmpty: true · Select Save and then confirm the changes. . You can view your test and code coverage results by selecting your build and going to the Test and Coverage tabs.

--- Page 166 ---
Build using multiple versions Rename/move pipeline Name ScheduledTriggerTest 1234 Select folder 1 .. . Cancel Save Pipeline settings X Processing of new run requests Enabled Paused Disabled YAML file path azure-pipelines.yml Automatically link work items included in this run Cancel Save YAML trigger: main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdkVersion: "1.10" jdk11_windows: imageName: "windows-latest" jdkversion: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdkVersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: **** /TEST -*. xml" goals: "package"

--- Page 151 ---
Azure DevOps Taking you to your Azure DevOps organization ... Congratulations, you're an organization owner! Sign in to your organization at any time, https://dev.azure.com/{yourorganization}. With your organization, the following aspects are included in the free tier: First five users free (Basic license): Azure Pipelines: o One Microsoft-hosted CI/CD (one concurrent job, upto 30 hours per month) o One self-hosted CI/CD concurrent job Azure Boards: Work item tracking and Kanban boards Azure Repos: Unlimited private Git repos Azure Artifacts: Two GB free per organization Build applications with Azure: Azure DevOps enables you to build, test, and deploy anyapplication to any cloud or on premises To configure build pipelines that continuously build, test, and verify your applications. 11

--- Page 152 ---
Part 1: Get started with Azure DevOps Part 2: Build applications with Azure DevOps Part 3: Deploy applications with Azure DevOps Create a build pipeline with Azure Pipelines: Prerequisites: Familiarity with forking and cloning a GitHub repo Account requirements: An Azure DevOps organization o To use Microsoft-hosted agents, your Azure DevOps organization must have access to Microsoft- hosted parallel jobs. Check your parallel jobs and request a free grant. O You can use GitHub Code spaces to complete the module, even if your Azure DevOps organization doesn't have any parallel jobs. A GitHub account Software requirements: If using GitHub Code spaces to complete the module, there are no software requirements as all software is included in the Codespace If using a local development environment withMicrosoft-hosted agents, you must have the following software installed: o Visual Studio Code o .NET 6.0 SDK o Git 12

--- Page 155 ---
10. When you're ready to make changes to your pipeline, select it in the Pipelines page, and then Edit the azure-pipelines.yml file. View and manage your pipelines You can view and manage your pipelines by choosing Pipelines from the left-hand menu to go to the pipelines landing page. Azure DevOps : FabrikamFiber 1 Pipelines 0 Search F FabrikamFiber + Pipelines New pipeline Overview Recent All Runs Y Filter pipelines Boards Repos Pipelines Pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... La 32m ago & Manually triggered 8º main & 42s Environments Releases FabrikamFiber #20191209.3 · Set up CI with Az ... La 1h ago & Manually triggered & main 1m 13s Library Task groups *** Deployment groups From the pipelines landing page you can view pipelines and pipeline runs, create and import pipelines, manage security, and drill down into pipeline and run details. Choose Recent to view recently run pipelines (the default view), or choose All to view all pipelines. Pipelines New pipeline : Recent All Runs Filter pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... & Manually triggered & main Là 36m ago ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Az ... Là 1h ago 1m 13s & Manually triggered & main 15

--- Page 156 ---
Select a pipeline to manage that pipeline and view the runs. Select the build number for the last run to view the results of that build, select the branch name toview the branch for that run, or select the context menu to run the pipeline and perform other management actions. Recently run pipelines Pipeline Last run #20191209.2 · Set up CI with Azure Pip ... > pipelines-dotnet-core 8 Manually triggered 12º main Là 1h ago * ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Azure Pip ... 試 2h 8 Manually triggered 8 main Edit in Run pipeline Manage security Rename/move Delete Select Runs to view all pipeline runs. You canoptionally filter the displayed runs. Pipelines Recent All Runs Filter by keywords State V Repository V New pipeline Requested forV Tags X All pipeline runs Description Stages Set up CI with Azure Pipelines & #20191209.2 on pipelines-dotnet-core & main d4964 .. Set up CI with Azure Pipelines #20191209.1 on pipelines-dotnet-core åº main d4964 ... Set up CI with Azure Pipelines & #20191209.3 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines & #20191209.2 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines #20191209.1 on FabrikamFiber & main 2b4b23c La 44m ago 42s La 53m ago ₾ 46s Là 1h ago 0 1m 13s Ld 1h ago 1m 555 Lo 1h ago 1m 6s 16

--- Page 153 ---
Create a pipeline in azure What is pipeline Azure Pipelines supports continuous integration (CI)and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline. The latest way to build pipelines is with the YAML pipeline editor. You can also use Classic pipelines with the Classic editor. code release build CI plan continuous testing CD deploy monitor operate Create your first pipeline This is a step-by-step guide to using Azure Pipelines to build a sample application from a Git repository. This guide uses YAML pipelines configured with the YAML pipeline editor. If you'd like to use Classic pipelines instead, see Define your Classic pipeline. For guidance on using TFVC, see Build TFVC repositories. Prerequisites - Azure DevOps A GitHub account where you can create are pository. An Azure DevOps organization. Create one for free. If your team already has one, then make sure you're an administrator of the Azure Devops. 12

--- Page 154 ---
Get the Java sample code To get started, fork the following repository into your GitHub account. https://github.com/MicrosoftDocs/pipelines-java Create your first Java pipeline 1. Sign-in to your Azure DevOps organization and go to your project. 2. Go to Pipelines, and then select New pipeline. 3. Do the steps of the wizard by first selecting GitHub as the location of your source code. 4. You might be redirected to GitHub to sign in. If so, enter your GitHub credentials. 5. When you see the list of repositories, select your repository. 6. You might be redirected to GitHub to install the Azure Pipelines app. If so,select Approve & install. 7. Azure Pipelines will analyze your repository and recommend the Maven pipeline template. 8. When your new pipeline appears, take a look at the YAML to see what it does. When you're ready, select Save and run. 9. You're prompted to commit a new azure pipelines.yml file to your repository. After you're happy with the message, select Save and run again. If you want to watch your pipeline in action, select the build job. You just created and ran a pipeline that we automatically created for you, because your code appeared to be a good match for the Maven template. You now have a working YAML pipeline (azure-pipelines.yml) in your repository that's ready for you to customize! 14

--- Page 143 ---
Unit: V Course Code/Title: CS3V15/Devops Email preferences Send me occasional product updates, announcements, and offers. Verify your account Pick one square that shows two identical objects. wiki How Step 4: Click the green Create account button. It's belowthe form, at the bottom of the page. This will take you to an email verification page. Verify your account Create account By creating an account, you agree to the Terms of Service. For more information about GitHub's privacy practices, see the GitHub Privacy Statement. We'll occasionally send you account-related wikiHow UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 3

--- Page 144 ---
Course Code/Title: CS3V15/Devops Unit: V Carefully review the Terms of Service at https://help.github.com/en/articles/github- terms-of-service and the Privacy Statement at https://help.github.com/en/articles/github- privacy-statement before you continue. You're almost done! We sent a launch code to wikihowneveconcepts@gmail.com + Enter code Didn't get your email? Resend the code or update your email address. wiki How Step 5: Verify your email by entering the code. After clicking Create account, you'll receive an email with a code.Enter this code on the verification page. Entering the code will automatically take you to the welcome page. Step 6: Select your preferences and click Continue. GitHub displays a quick survey that can help you tailor your experience to match what you're looking for. You'll be sent tothe plan selection page after completing the survey. How many team members will be working with you? This will help us guide you to the tools that are best suited for your projects. Just me 2 - 5 5 - 10 10 - 20 20 - 50 50+ Are you a student or teacher? Student Teacher Continue wikiHow CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 4 175° Rank ---

--- Page 149 ---
Course Code/Title: CS3V15/Devops Unit: V Step 6: Now you will see that all of our files uploaded in our github. Namanbhatia7 Add initial resume website files Latest commit f47be5b 1 minute ago Display_Picture.png Add initial resume website files 1 minute ago My_Resume.html Add initial resume website files 1 minute ago README.md Initial commit 15 minutes ago Styles.css Add initial resume website files 1 minute ago Branch: · A GitHub branch is used to work with different versions of a repository at the same time. · By default a repository has a master branch (aproduction branch). · Any other branch is a copy of the master branch (as itwas at a point in time). · New Branches are for bug fixes and feature work separate from the master branch. When changes are ready, they can be merged into the master branch. If youmake changes to the master branch while working on a new branch, these updates can be pulled in. Commits: At GitHub, changes are called commits. Each commit (change) has a description explaining why achange was made. Pull Requests : . Pull Requests are the heart of GitHub collaboration. · With a pull request you are proposing that your changesshould be merged (pulled in) with the master. · Pull requests show content differences, changes, additions, and subtractions in colors (green and red). · As soon as you have a commit, you can open a pull request and start a discussion, even before the code isfinished. Git: · Git was created by Linus Torvalds in 2005 to develop Linux Kernel · Git is an open-source distributed version control system. It is designed to handle minor to major projectswith high speed and efficiency. . It is developed to co-ordinate the work among the developers. The version control allows us to track andwork together with our team members at the same workspace. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 9 175" Rank

--- Page 150 ---
CREATE AZURE ORGANIZATION AZURE: Microsoft Azure, often referred to as Azure cloud computing platform run by Microsoft. It offers access, management, and the development of applications and services through global data centers It also provides a range of capabilities, including software as a service (SaaS), platform as a service, and infrastructure as a service (IaaS). It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014 Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Prerequisites: To plan your organizational structure. Microsoft accounts or authenticate users with Microsoft Entra ID. For more information, see Choosing your organization administrator account type. Create an organization: 1. Sign in to Azure DevOps. 2. Select New organization 3. Confirm information, and then select Continue 10

--- Page 163 ---
‹ FabrikamFiber Runs Branches Analytics Description Edit Run pipeline : Stages #20210609.3 Update azure-pipelines.yml for Azure Pipelines Manually triggered for main 9 61bba8c x2 La Wednesday Ø 215 #20210609.2 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for 1º main 9 bc6c105 x La Wednesday ₾ 305 #20210609.1 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for main 9 53eb92b x • #20210520.1 Updated README.md Individual CI for & new-branch 0 1ada387 ‹ azure-cli-example go main V azure-cli-example / azure-pipelines.yml 30 pool: default 31 32 steps: 33 # Specify python version and install if needed Settings 34 - task: UsePythonVersion@0 35 condition: false 36 inputs: 37 -versionSpec: '3.x' 38 architecture: 'x64' 39 40 # Update pip to latest 41 - bash: python -m pip install -- upgrade pip 42 condition: false 43 displayName: 'Upgrade pip' 44 45 container 46 &continueOnError 47 48 88988 name & parameters & resources & schedules 49 50 51 services 52 strategy 53 & variables 54 & workspace La Wednesday .455 Là May 20 ₾ 135 Variables Run : Tasks 7 Search tasks dotnet .NET Core Build, test, package, or publish a dotnet applicatio. Android signing Sign and align Android APK files Ant Build with Apache Ant 80 App Center distribute Distribute app builds to testers and users via Visu .. App Center test Test app packages with Visual Studio App Center Archive files Compress files into .7z, tar.gz on zip ARM template deployment Deploy an Azure Resource Manager (ARM) templ .. Azure App Service deploy Deploy to Azure App Service a web, mobile, or AP. · The YAML pipeline editor provides several keyboard shortcuts, which we show in the following examples. . Choose Ctrl+Space for Intellisense support while you're editing the YAML pipeline.

--- Page 164 ---
· The task assistant provides a method for adding tasksto your YAML pipeline. · To display the task assistant, edit your YAML pipeline and choose Show assistant. Understand the azure-pipelines.yml file · A pipeline is defined using a YAML file in your repo. Usually, this file is named azure-pipelines.yml and is located at the root of your repo. · Navigate to the Pipelines page in Azure Pipelines, select the pipeline you created, and choose Edit in the context menu of the pipeline to open the YAMLeditor for the pipeline. · This pipeline runs whenever your team pushes a change to the main branch of your repo or creates a pull request. It runs on a Microsoft-hosted Linux machine. · The pipeline process has a single step, which is to run the Maven task. YAML:Code trigger: - main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdk Version: "1.10" jdk11_windows: imageName: "windows-latest" jdk Version: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdk VersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: " ** /TEST -*. xml"goals: "package" Change the platform to build · Navigate to the editor for your pipeline by selectingEdit pipeline action on the build, or by selecting Edit from the pipeline's main page. · To choose a different platform like Windows or Mac, change thevmImage value: · pool: · vmImage: "windows-latest" Add steps . You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can usetasks for building, testing, publishing, or deploying your app. For Java, the

--- Page 53 ---
Course Code/Title:CS3V15/Devops Unit: II <execution> <phase>test</phase> <goals> <goal>run</goal> </goals> <configuration> <tasks> <echo>Using env.test.properties</echo> <copy file="src/main/resources/env.test.properties" tofile="${project.build.outputDirectory}/env.properties"/> </tasks> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> Now open the command console, go to the folder containing pom.xml and execute the following mvn command. Pass the profile name as argument using -P option. C:\MVN\project>mvn test -Ptest Maven will start processing and displaying the result of test build profile. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 12 N32 nirf 1750 Rank

--- Page 54 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Maven Settings Open Maven settings.xml file available in %USER_HOME%/.m2 directory where %USER_HOME% represents the user home directory. If settings.xml file is not there, then create a new one. Add test profile as an active profile using active Profiles node as shown below in example. <settings xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"> <mirrors> <mirror> <id>maven.dev.snaponglobal.com</id> <name>Internal Artifactory Maven repository</name> <url>http://repo1.maven.org/maven2/</url> <mirrorOf> *< /mirrorOf> /mirror </mirrors> <activeProfiles> <activeProfile>test</activeProfile> </activeProfiles> </settings> Now open command console, go to the folder containing pom.xml and execute the following mvn command. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 13 N33 nirf 1750 Rank

--- Page 31 ---
Course Code/Title:CS3V15/Devops Unit: I our work simpler. The pricing of Azure is also simpler and cost-effective. Popularly termed as "Pay As You Go", which means how much you use, pay only for that. Microsoft Azure Used for · Deployment Of applications: You can develop and deploy the application in the azure cloud by using the service called Azure App Service and Azure Functions after deploying the applications end users can access it. · Identity and Access Managment: The application and data which is deployed and stored in the Microsoft Azure can be secured with the help of Identity and Access Managment. It's commonly used for single sign-on, multi-factor authentication, and identity governance. · Data Storage and Databases: You can store the data in Microsoft azure in service like blob storage for unstructured data, table storage for NoSQL data, file storage, and Azure SQL Database for relational databases. The service can be scaled depending on the amount of data we are getting. · DevOps and Continuous Integration/Continuous Deployment (CI/CD): Azure DevOps will provide some tools like ncluding version control, build automation, release management, and application monitoring Following are some of the services Microsoft Azure offers: 1. Compute: Includes Virtual Machines, Virtual Machine Scale Sets, Functions for serverless computing, Batch for containerized batch workloads, Service Fabric for microservices and container orchestration, and Cloud Services for building cloud-based apps and APIs. 2. Networking: With Azure, you can use a variety of networking tools, like the Virtual Network, which can connect to on-premise data centers; Load Balancer; Application Gateway; VPN Gateway; Azure DNS for domain hosting, Content Delivery Network, Traffic Manager, ExpressRoute dedicated private network fiber connections; and Network Watcher monitoring and diagnostics 3. Storage: Includes Blob, Queue, File, and Disk Storage, as well as a Data Lake Store, Backup, and Site Recovery, among others. 4. Web + Mobile: Creating Web + Mobile applications is very easy as it includes several services for building and deploying applications. 5. Containers: Azure has a property that includes Container Service, which supports Kubernetes, DC/OS or Docker Swarm, and Container Registry, as well as tools for microservices. 6. Databases: Azure also included several SQL-based databases and related tools. 7. Data + Analytics: Azure has some big data tools like HDInsight for Hadoop Spark, R Server, HBase, and Storm clusters 8. AI + Cognitive Services: With Azure developing applications with artificial intelligence capabilities, like the Computer Vision API, Face API, Bing Web Search, Video Indexer, and Language Understanding Intelligent. 9. Internet of Things: Includes IoT Hub and IoT Edge services that can be combined with a variety of machine learning, analytics, and communications services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 31 N32 nirf 1750 Rank

--- Page 32 ---
Unit: I Course Code/Title:CS3V15/Devops 10. Security + Identity: Includes Security Center, Azure Active Directory, Key Vault, and Multi-Factor Authentication Services. 11. Developer Tools: Includes cloud development services like Visual Studio Team Services, Azure DevTest Labs, HockeyApp mobile app deployment and monitoring, Xamarin cross- platform mobile development, and more. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 32 N32 nirf 1750 Rank

--- Page 47 ---
Course Code/Title:CS3V15/Devops Unit: II <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> Any repository of maven is available in mvnrepository.com 5. Maven Plugins: The maven plugins are central part of maven framework, it is used to perform specific goal. According to Apache Maven, there are 2 types of maven plugins. 1. Build Plugins 2. Reporting Plugins Build Plugins These plugins are executed at the time of build. These plugins should be declared inside the <build> element. Reporting Plugins These plugins are executed at the time of site generation. These plugins should be declared inside the <reporting> element. Maven Core Plugins A list of maven core plugins are given below: Plugin Description clean clean up after build. compiler compiles java source code. deploy deploys the artifact to the remote repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY N32 nirf 6 175º Rank -

--- Page 48 ---
Course Code/Title:CS3V15/Devops Unit: II failsafe runs the JUnit integration tests in an isolated classloader. install installs the built artifact into the local repository. resources copies the resources to the output directory for including in the JAR. site generates a site for the current project. surefire runs the JUnit unit tests in an isolated classloader. verifier verifies the existence of certain conditions. It is useful for integration tests. Example for maven plugin: The compiler plugin is used to compile the source code of a Maven project. This plugin has two goals, which are already bound to specific phases of the default lifecycle: · compile - compile main source files · testCompile - compile test source files Here's the compiler plugin in the POM: <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.12.1</version> <configuration> <source>1.8</source> <target>1.8</target> </configuration> </plugin> 6. Maven Build Lifecycle: A Build Lifecycle is a well-defined sequence of phases, which define the order in which the goals are to be executed. Here phase represents a stage in life cycle. As an example, a typical Maven Build Lifecycle consists of the following sequence of phases. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 7 N33 nirf 175º Rank -

--- Page 37 ---
Course Code/Title:CS3V15/Devops Unit: I GIT INSTALLATION Git for Windows stand-alone installer · Download the latest Git for Windows installer. · When you've successfully started the installer, you should see the Git Setup wizard screen. Follow the Next and Finish prompts to complete the installation. The default options are pretty sensible for most users. · Open a Command Prompt (or Git Bash if during installation you elected not to use Git from the Windows Command Prompt). . Run the following commands to configure your Git username and email using the following commands, replacing Emma's name with your own. These details will be associated with any commits that you create: $ git config -- global user.name "CIT_CHENNAI" $ git config -- global user.email CITCHENNAI@atlassian.com • Optional: Install the Git credential helper on Windows Bitbucket supports pushing and pulling over HTTP to your remote Git repositories on Bitbucket. Every time you interact with the remote repository, you must supply a username/password combination. You can store these credentials, instead of supplying the combination every time, with the Git Credential Manager for Windows. BASIC COMMANDS OF GIT: Some basic Git commands along with their syntax and examples: · Initialize a Repository: Syntax: `git init' Example: `git init' · Clone a Repository: Syntax: `git clone <repository_url> Example: `git clone https://github.com/example/repository.git' · Check Repository Status: Syntax: `git status' Example: `git status' · Add Changes to Staging Area: Syntax: `git add <file(s)>' Example: `git add file.txt' · Commit Changes: Syntax:`git commit -m "Commit message" Example:`git commit -m "Add new feature" · Create a New Branch: Syntax: `git branch <branch_name>' Example: `git CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY (Autonomous] N33 37 nirf 1750 Rank

--- Page 38 ---
Course Code/Title:CS3V15/Devops branch feature-branch' Unit: I • Switch to a Branch: Syntax: `git checkout <branch_name>' Example:`git checkout feature-branch' OR Syntax: `git switch <branch_name>' (Git version 2.23 and later) Example: `git switch feature-branch' · Create and Switch to a New Branch: Syntax: `git checkout -b <new_branch name>' Example: `git checkout -b new-feature' OR Syntax: `git switch -c <new_branch_name>' (Git version 2.23 and later) Example: `git switch -c new-feature' . Merge Changes from One Branch to Another: Syntax: `git merge <branch_name>' Example: `git merge feature-branch' · View the Commit History: Syntax: `git log' Example: `git log' · Push Changes to a Remote Repository: Syntax: `git push <remote_name> <branch_name>' Example:`git push origin master' · Pull Changes from a Remote Repository: Syntax: `git pull <remote_name> <branch_name>' Example:`git pull origin master' . Show the Differences Between Working Directory and Staging Area: Syntax: `git diff Example: `git diff · Show the Differences Between Staging Area and Last Commit: Syntax: `git diff -- cached' Example: `git diff -- cached' . Show the Differences Between Working Directory and Last Commit: Syntax: `git diff HEAD' Example: `git diff HEAD' CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 38 N32 nirf 1750 Rank

--- Page 75 ---
Course Code/Title:CS3V15/Devops Customization This tool is highly customizable as it supports a variety of IDE's . Languages supported Project Configuration Based on Goal of time to get used to it. available. It supports software development in Java, C, C++, and Groovy. For declaring the project configuration, it does not use the XML files. On the phases of the fixed and linear model. Graph of task dependencies that do the work. To add functionality in the project is the main goal of the Gradle. To finish the project in the given timeline is the main goal of the Maven. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY Unit: II This tool serves a limited number of developers and is not that customizable. It supports software development in Java, Scala, C# , and Ruby and it does not natively support C and C+ but can support through plugins like "maven-native- plugin" or we can integrate other build systems like CMake or Makefile. For declaring the project configuration, it uses the XML files. 34 N33 nirf 1750 Rank

--- Page 76 ---
Course Code/Title:CS3V15/Devops Unit: III Unit - III CONTINUOUS INTEGRATION USING JENKINS Install & Configure Jenkins, Jenkins Architecture Overview, creating a Jenkins Job, configuring a Jenkins job, Introduction to Plugins, Adding Plugins to Jenkins, commonly used plugins (Git Plugin, Parameter Plugin, HTML Publisher, Copy Artifact and Extended choice parameters). Configuring Jenkins to work with java, Git and Maven, creating a Jenkins Build and Jenkins workspace. Installing and Configuring Jenkins Overview Jenkins is a widely-used open-source automation server that helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration and continuous delivery (CI/CD). Below are detailed steps and considerations for installing and configuring Jenkins. Installation 1. System Requirements: ○ Operating System: Jenkins can run on any major operating system. ○ Java: Jenkins requires Java Runtime Environment (JRE) 8 or later. 2. Download Jenkins: ○ Download the latest version of Jenkins from the official Jenkins website. 3. Installation Steps: ○ Windows: I Run the .msi installer and follow the installation wizard. ■ After installation, Jenkins will start automatically as a Windows service. ○ Linux: Add Jenkins repository and import the GPG key. I Use package manager to install Jenkins (e.g., apt-get install jenkins). I Start Jenkins using systemctl (systemctl start jenkins). 4. Initial Setup: ○ After installation, Jenkins can be accessed through a web browser at http : //< your _server _ip_or_domain>: 8080. o Unlock Jenkins by entering the initial admin password, which can be found in the jenkins_home directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 1 N32 nirf 1750 Rank

--- Page 89 ---
Course Code/Title:CS3V15/Devops Unit: III Key Features: · Source Code Management: Configures Jenkins to use Git repositories as the source code for jobs. Example Demo Step 1 : Create a simple Java program. I have created a folder named MyJavaPrograms and inside it created a simple Java program as follows - e.g., SSH test.java public class test { public static void main(String args[I) { for(int i=1;i <= 5;1++) System.out.printin("Welcome Anuradha"); } Plugin," Step 2 : Open the command prompt, switch to that folder and execute the above Java program. It is illustrated by following screenshot. 1 C:\Windows\System32\cmd.e X + Microsoft Windows [Version 10.0.22621.2134] (c) Microsoft Corporation. All rights reserved. 0 X before E:\MyJavaPrograms>javac test. java E: \MyJavaPrograms>java test Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha E: \MyJavaPrograms> Step 3 : Now we will create a Git repository and push this repository on GitHub. First of all we will initialise the Git repository my using git init command. CHENNAI INSTITUTE TECHNOLOGY 14 - N32 nirf 1750 Rank CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt)

--- Page 90 ---
Unit: III Course Code/Title:CS3V15/Devops C:\Windows\System32\cmd.e x X E:\MyJavaPrograms>git init Initialized empty Git repository in E:/MyJavaPrograms/.git/ E:\MyJavaPrograms>git status On branch master No commits yet Untracked files: (use "git add <file> ... " to include in what will be committed) test.class test. java nothing added to commit but untracked files present (use "git add" to track) E:\MyJavaPrograms> then add the java and class files to git repository - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git add . E:\MyJavaPrograms>git status On branch master No commits yet Changes to be committed: (use "git rm -- cached <file> ... " to unstage) new file: test.class new file: test. java E: \MyJavaPrograms> Now we will commit the changes - C:\Windows\System32\cmd.e X E:\MyJavaPrograms>git commit -m "First Commit for Java program" [master (root-commit) 4cb04db] First Commit for Java program 2 files changed, 6 insertions(+) create mode 100644 test.class create mode 100644 test. java E:\MyJavaPrograms> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) - INSTITUTE . TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 111 ---
Course Code/Title:CS3V15/Devops Unit: III Steps: 1. Install the Git Plugin: 0 Go to Manage Jenkins > Manage Plugins. ○ Under the Available tab, search for Git Plugin and install it. 2. Configure Git in Jenkins: ○ Navigate to Manage Jenkins> Global Tool Configuration. ○ Scroll down to the Git section and click Add Git. ○ Specify the path to the Git executable or use the default path. ○ Optionally, configure additional settings such as Git installation locations. 3. Example Configuration: ○ Name: Git ○ Path to Git executable: /usr/bin/ git (or use the default if Git is installed in a standard location). 4. Configure Job to Use Git: ○ Create or edit a Jenkins job and go to Source Code Management. ○ Select Git. ○ Enter the repository URL (e.g., https://github.com/user/repository.git). ○ Configure credentials if the repository is private. ○ Specify the branch to build (e.g., main). 5. Example Repository URL: ○ Repository URL: https://github.com/example/repo.git o Branch Specifier: main 3. Configuring Jenkins to Work with Maven Overview: The Maven Plugin integrates Jenkins with Apache Maven, allowing Jenkins to use Maven to build projects. Steps: 1. Install Maven: ○ Ensure that Apache Maven is installed on your system. You can download it from the Apache Maven website. 2. Configure Maven in Jenkins: ○ Go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to the Maven section and click Add Maven. ○ Enter a name for the Maven installation (e.g., Maven 3.8.6). ○ Check Install automatically to let Jenkins download Maven, or specify the path to an existing Maven installation. ○ CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 36 1750 Rank

--- Page 112 ---
Unit: III Course Code/Title:CS3V15/Devops 3. Example Configuration: ○ Name: Maven 3.8.6 ○ MAVEN_HOME: / usr / share / maven 4. Configure Maven in a Jenkins Job: ○ Create or edit a Jenkins job and go to Build section. 0 Add a build step and select Invoke top-level Maven targets. o Choose the Maven version configured earlier. ○ Specify the goals to run (e.g., clean install). 5. Example Maven Goals: ○ Goals: clean install 6. Configure Build Environment: ○ Ensure that the job's build environment is set up to use Maven. This might involve setting environment variables or configuring build scripts. 7. Example Environment Variables: ○ MAVEN_OPTS: - Xms512m -Xmx2048m (if you need to set specific JVM options for Maven). By following these steps, Jenkins will be properly configured to work with Java, Git, and Maven, allowing you to set up and manage continuous integration and delivery pipelines effectively. If you need further customization or run into issues, consulting the documentation for each tool or plugin can provide additional guidance. Creating a Jenkins Build and Understanding Jenkins Workspace 1. Creating a Jenkins Build Overview: A Jenkins build is a process where Jenkins executes a series of steps defined in a job configuration. This typically involves compiling code, running tests, and generating artifacts. Steps to Create a Jenkins Build: 1. Access Jenkins Dashboard: ○ Open your Jenkins dashboard, usually found at http://localhost: 8080. 2. Create a New Job: ○ Click on New Item on the left sidebar. Enter a name for your job. ○ Choose a job type. Common types include: I Freestyle project: For simple build jobs with basic configuration. · Pipeline: For more complex build processes using a Jenkinsfile. ○ Click OK to proceed. ○ TECHNOLOGY N33 37 nirf 1750 Rank CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous)

--- Page 117 ---
Course Code/Title:CS3V15/Devops Unit: IV • Explore Ansible modules. 4. Playbook - · Playbooks are a sequence of plays that define the order of tasks executed by Ansible. . A play consists of a list of tasks that target managed nodes in an inventory. · Tasks, in turn, are composed of one or more modules that specify operations. · Playbooks provide a structured way to define and automate complex workflows. 5. Roles - · Roles provide an organized environment for managing complex tasks. · They include templates, playbooks, inventories, error handlers, vars, and meta information. · Ideal for handling larger, multi-step automation processes. 6. Collections - · Collections are distribution formats for Ansible content. · They encompass playbooks, roles, modules, and plugins. · Find Ansible collections. 7. Galaxy - · Ansible Galaxy is a platform for sharing and downloading collections. . It allows the Ansible community to collaborate and exchange automation content. Setting Up the Ansible Control Node Prerequisite: Before you begin setting up Ansible, it's essential to ensure you meet the following prerequisites: · SSH: Make sure SSH is installed on your Linux system. SSH is crucial for secure communication between the control node and managed nodes. Installation Steps: 1. Open your terminal. 2. Run the following command to install Ansible: 1python3 -m pip install -- user ansible 3. SSH Key: o For secure and direct connections to managed nodes, you'll need to provide a public SSH key. You can generate one using the ssh-keygen command. o Follow the on-screen instructions to create your SSH key pair. This key pair is a crucial element of Ansible's secure communications. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 118 ---
Course Code/Title:CS3V15/Devops Unit: IV 4. For specific installation instructions tailored to your Linux distribution, click here. 5. Try to connect ssh using once your mange node is set up. 1ssh <user>@<ip> Note: If you're using a different operating system, find the appropriate installation instructions here. Setting Up the Ansible Mange Node Steps: 1. Install OpenSSH-Server: o To enable remote management of the managed node, you'll need to install the OpenSSH server. Use the following command (assuming you're using a Debian-based Linux distribution like Ubuntu): 1sudo apt install openssh-server o This command installs the OpenSSH server, allowing secure remote access. 2. Create a User: o For convenience and consistency, it's helpful to create a user on the managed node with the same name as the user on the control node. This makes it easier to manage SSH keys and ensures a smoother experience. 3. Configure SSH Key: o To establish secure and direct connections to the managed node, you need to paste the public SSH key from the control node into the authorized_keys file located in the .ssh directory of the user's home folder on the managed node. o The path is typically /home/<user>/.ssh/authorized_keys, where <user> is the username you created or are using on the managed node. You can use the ssh-copy-id command to automate the process of copying your public key to the managed node. For example: 1ssh-copy-id <user>@<managed_node_ip> o This command securely copies your public key to the authorized_keys file, allowing passwordless SSH authentication. Managing Ansible Inventories In the world of Ansible, inventories are like the backbone of your automation infrastructure. They serve as a vital component that lists and organizes the managed nodes, making automation tasks seamless and organized. Inventories use a parent-child concept that allows you to create groupings, which come in handy during specific tasks or scenarios. Why Are Inventories Important? · Inventories provide a comprehensive list of managed nodes, making it easy to interact with and manage them. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 4 N33 nirf 1750 Rank

--- Page 9 ---
Course Code/Title:CS3V15/Devops Unit: I 9. Slack Slack is a crucial tool for businesses and organizations worldwide. Its significance lies in facilitating seamless communication and collaboration among teams, whether working in the same office or remotely. Slack's real-time messaging, file sharing, and integration capabilities streamline workflow, enhance productivity and keep teams connected across different time zones and locations. As the work landscape evolves, with more companies embracing hybrid and remote work models, Slack is a vital hub for quick decision-making, project coordination, and knowledge sharing. With an ever- expanding ecosystem of integrations and features, Slack remains at the forefront of modern workplace communication, making it essential for businesses to stay agile, efficient, and competitive. 10. AWS Cloud Computing and Storage in DevOps AWS (Amazon Web Services) Cloud Computing and Storage are crucial in DevOps because they provide scalable, flexible, and cost-effective infrastructure for DevOps practices. AWS offers many services, including compute resources, databases, container orchestration, and serverless computing, which align perfectly with modern software development and deployment demands. Organizations adopt DevOps to accelerate software delivery. AWS provides the foundation for rapidly deploying and scaling applications, supporting continuous integration and continuous delivery (CI/CD) pipelines, and automating infrastructure provisioning through tools like AWS CloudFormation. Furthermore, AWS's storage solutions enable efficient data management, backup, and recovery, ensuring the resilience and reliability required for DevOps operations. As cloud technology evolves, AWS remains at the forefront, enabling DevOps teams to focus on innovation and efficiency. 11. Azure Cloud Computing and Storage in DevOps Azure Cloud Computing and Storage will be pivotal in DevOps practices in 2024 and beyond. Azure offers a comprehensive cloud ecosystem that enables organizations to scale their infrastructure, deploy applications, and store data efficiently. Azure provides essential services for continuous integration and continuous deployment (CI/CD), automation, monitoring, and security. Its cloud computing capabilities facilitate the provisioning of resources on demand, ensuring that development and testing environments are readily available. Azure's storage solutions, including Azure Blob Storage, Azure Files, and Azure SQL Database, enable secure data storage and retrieval, supporting the data-driven aspects of DevOps. Besides, Azure's integration with DevOps tools like Azure DevOps Services streamlines the software development lifecycle, enhancing collaboration and automation. 12. GCP Cloud Computing and Storage in DevOps Google Cloud Platform (GCP) offers robust cloud computing and storage solutions. GCP provides a scalable, reliable, and highly available infrastructure essential for modern DevOps practices. With its comprehensive set of services, including Google Compute Engine, Google Kubernetes Engine, Cloud Storage, and BigQuery, GCP empowers DevOps teams to build, deploy, and manage applications easily. Its emphasis on automation, infrastructure as code, and container orchestration aligns seamlessly with DevOps principles. Moreover, GCP's cutting-edge technologies, such as AI and machine learning capabilities, provide DevOps practitioners with advanced tools for monitoring, analytics, and automation, making it a powerful choice for organizations seeking to optimize their software development and delivery processes. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 10 ---
Course Code/Title:CS3V15/Devops Unit: I 13. Monitoring, Alerting, and Incident Response Tools: SignalFx Monitoring, alerting, and incident response tools like SignalFx are pivotal in DevOps and software development. As software systems become complex and distributed, the need for real-time visibility into performance and the ability to respond swiftly to incidents is significant. SignalFx excels in this regard by providing advanced monitoring and observability solutions that enable organizations to detect anomalies, trace issues across microservices proactively, and set up intelligent alerts. As applications scale, cloud-native architectures become the norm, and user expectations for reliability grow, SignalFx's capabilities are crucial. It empowers DevOps teams to ensure high availability, optimize resource utilization, and maintain a seamless user experience by identifying and addressing performance issues before they impact end-users. It is one of the most essential tools for modern software operations. 14. Appdynamics AppDynamics, a leading application performance management and monitoring platform, remains critically important as it ensures the optimal performance of modern digital businesses. As organizations rely on complex and distributed software systems, proactively monitoring, troubleshooting, and optimizing these applications becomes essential. AppDynamics provides real-time visibility into application performance, allowing businesses to swiftly identify bottlenecks, latency issues, and errors. With the ever-growing complexity of applications, the importance of AppDynamics lies in its ability to empower organizations to deliver exceptional user experiences, maintain application reliability, and swiftly respond to performance issues, thereby ensuring the continued success and competitiveness of digital businesses. 15. Raygun It is a crucial tool in software development and DevOps because it ensures application reliability and performance. Raygun is an application monitoring and error-tracking platform that empowers development teams to identify, diagnose, and resolve real-time issues. With software systems growing in complexity and the increased demand for seamless user experiences, Raygun's importance lies in providing actionable insights into application errors and performance bottlenecks. It enables organizations to proactively address issues, reduce downtime, and enhance user satisfaction, leading to higher software quality and improved customer experiences. Software is central to businesses across industries. Raygun's role in maintaining application health and facilitating rapid issue resolution makes it a fundamental tool for DevOps professionals and software developers. 16. Splunk Cloud Splunk Cloud helps organizations gain critical insights from the ever-expanding volume of data generated in today's digital landscape. As businesses increasingly rely on data- driven decision-making, Splunk Cloud stands out as a robust and scalable platform for monitoring, searching, analyzing, and visualizing machine-generated data. Its importance lies in providing real-time visibility into the health and performance of complex systems, applications, and infrastructures, enabling rapid incident detection and response. As cybersecurity threats evolve, Splunk Cloud's advanced security analytics and threat detection capabilities remain indispensable for safeguarding against cyberattacks and ensuring data integrity. In a world where data is a strategic asset, Splunk Cloud's role in harnessing the power of data for operational excellence and security cannot be overstated. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 10 N32 nirf 1750 Rank

--- Page 129 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible ad-hoc Commands The Ansible ad-hoc command uses the /usr/bin/ansible command-line tool to automate a single task on one or more managed nodes. The Ad-hoc commands are quick and easy, but they are not re-usable. The Ad-hoc commands demonstrate the simplicity and power of Ansible. Syntax ansible <hosts> [-m <module_name>] -a <"arguments"> -u <username> [ -- become] Explanation Hosts: It can be an entry in the inventory file. For specifying all hosts in the inventory, use all or "*" . module_name: It is an optional parameter. There are hundreds of modules available in the Ansible, such as shell, yum, apt, file, and copy. By default, it is the command. Arguments: We should pass values that are required by the module. It can change according to the module used. Username: It specifies the user account in which Ansible can execute commands. Become: It's an optional parameter specified when we want to run operations that need sudo privilege. By default, it becomes false. 1. Parallelism and shell commands You can reboot your company server in 12 parallel forks at the same time. For this, you need to set up the SSHagent for connection. $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa To run reboot for all your company servers in the group, 'abc', in 12 parallel forks: $ ansible abc -a "/sbin/reboot" -f 12 By default, Ansible will run the above ad-hoc commands from the current user account. If you want to change then pass the username in ad-hoc command as follows: $ ansible abc -a "/sbin/reboot" -f 12 -u username 2. File Transfer You can use ad-hoc commands for doing SCP (secure copy protocol) which means lots of files in parallel on multiple machines or servers. CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 15 N32 nirf 175º Rank

--- Page 130 ---
Course Code/Title:CS3V15/Devops Unit:IV Transferring file on many machines or servers $ ansible abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf" Creating new directory $ ansible abc -m file - a "dest = /path/user1/new mode = 888 owner = user group = user1 state = directory" Deleting all directory and files $ ansible abc -m file -a "dest = /path/user1/new state = absent" 3. Managing Packages Ad-hoc commands are available for apt and yum module. Here are the following ad-hoc commands using yum. Below command checks, if the yum package is installed or not, but not update it. $ ansible abc -m yum -a "name = demo-tomcat-1 state = present" Below command checks the package is not installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = absent" And below command checks the latest version of package is installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = latest" 4. Managing Users and Groups You can manage, create, and remove a user account on your managed nodes with ad-hoc commands. $ ansible all -m user -a "name=foo password =< crypted password here>" $ ansible all -m user -a "name=foo state=absent" 5. Managing Services Ensure a service is started on all the webservers. $ ansible webservers -m service -a "name=httpd state=started" Alternatively, restart a service on all webservers: $ ansible webservers -m service -a "name=httpd state=restarted" CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 nirf 1750 Rank

--- Page 69 ---
Unit: II Course Code/Title:CS3V15/Devops 1 build.gradle gradle wrapper 2 gradle-wrapper.jar - gradle-wrapper.properties 3 gradlew 4 gradlew.bat 5 settings.gradle 6 It is the default structure of a Gradle project. Gradle will generate the following things for us: 1. The gradle file is build script for configuring the current project. 2. An executable JAR file is used as a Gradle wrapper. 3. Configuration properties for Gradle Wrapper. 4. The gradlew is a Gradle wrapper script for UNIX based OS. 5. The bat is the Gradle Wrapper script for Windows. 6. The settings script for configuring the Gradle build. Step3: Create a task Gradle supports APIs for creating and managing tasks through a Groovy-based DSL or Kotlin-based DSL. Every project contains a collection of tasks for some basic operation. Gradle supports a library of tasks that configure the project. For example, there is a Copy task, which copies files from one location to another. The Copy task is one of the most used tasks In Gradle. To use the Copy task in build script, follow the below process. Step1: Create a directory called src C: \Users \HiMaNshU\demo>mkdir src C: \Users \HiMaNshU\demo>cd src Step2: Add a file called myfile.txt in the src directory. Add the single line "Hello, World!" to it, also, we can leave it empty. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 28 N32 nirf 1750 Rank

--- Page 70 ---
Unit: II Course Code/Title:CS3V15/Devops C:\Users \HiMaNshU\demo\src>echo myfile.txt myfile.txt Define a task called Copy in build.gradle file. It will copy the src directory to a new directory called dest. We don't have to create the dest directory; the Copy task will do it for us. 1. task copy(type: Copy, group: "Custom", description: "The sources are copied to dest directory") { 2. from "src" 3. into "dest" 4. } We can provide anything in a group and description. Also, we can omit them, but doing so will also be omitted from the report of the task used later. Now execute our new copy task: C: \Users \HiMaNshU\demo>gradle copy BUILD SUCCESSFUL in 2s 1 actionable task: 1 executed C: \Users \HiMaNshU\demo>_ After the successful execution of the task, we will get BUILD SUCCESSFUL message. The build.gradle file The build.gradle file is build script of a Gradle project. All the tasks and plugins are defined in this file. When we run a gradle command, it looks for a file called build.gradle in the current directory. Although we have called it a build script, strictly, it is a build configuration script. The build script defines a project and its tasks. The default build.gradle file looks like as follows: CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 29 N33 nirf 1750 Rank

--- Page 49 ---
Course Code/Title:CS3V15/Devops Unit: II Validate Deploy Compile Install Test Verify Package Integration Test · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. There are always pre and post phases to register goals, which must run prior to, or after a particular phase. When Maven starts building a project, it steps through a defined sequence of phases and executes goals, which are registered with each phase. Maven has the following three standard lifecycles - • default: This is the main lifecycle, as it's responsible for project deployment. CHENNAI CHENNAI INSTITUTE . TECHNOLOGY 8 clean: Handles project cleaning, ensuring that all artifacts generated by previous INSTITUTE OF TECHNOLOGY 175º Rank (Autonomous]

--- Page 50 ---
Course Code/Title:CS3V15/Devops Unit: II builds are removed. · site: Manages the creation of the project's site documentation. Default Lifecycle (default) Most Maven users will be familiar with the default lifecycle. It is a general model of a build process for a software application. The first phase is validate and the last phase is deploy. · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. Clean Lifecycle (clean) The first lifecycle in Maven. Running mvn clean invokes the clean lifecycle which consists of three lifecycle phases: · pre-clean : execute processes needed prior to the actual project cleaning · clean : remove all files generated by the previous build · post-clean : execute processes needed to finalize the project cleaning Site Lifecycle (site) Maven does more than build software artifacts from project, it can also generate project documentation and reports about the project, or a collection of projects. Project documentation and site generation have a dedicated lifecycle which contains four phases: 1. pre-site : execute processes needed prior to the actual project site generation 2. site : generate the project's site documentation 3. post-site : execute processes needed to finalize the site generation, and to prepare for site deployment 4. site-deploy: deploy the generated site documentation to the specified web server CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 71 ---
Unit: II Course Code/Title:CS3V15/Devops build.gradle &3 I 1 /* 2 * This file was generated by the Gradle 'init' task. 3 * 4 * This generated file contains a sample Java Library project to get you started. 5 * For more details take a look at the Java Libraries chapter in the Gradle 6 * User Manual available at https://docs.gradle.org/6.0.1/userguide/java_library_plugin.html 7 */ 8 9 plugins { 10 // Apply the java-library plugin to add support for Java Library 11 id 'java-library' 12 } 13 14 repositories { 15 // Use jcenter for resolving dependencies. 16 // You can declare any Maven/Ivy/file repository here. 17 jcenter() 18 } 19 20 dependencies { 21 // This dependency is exported to consumers, that is to say found on their compile classpath. 22 api 'org. apache. commons : commons-math3: 3.6.1' 23 24 // This dependency is used internally, and not exposed to consumers on their own compile classpath. 25 implementation 'com. google. guava: guava: 28.0-jre' 26 27 // Use JUnit test framework 30 28 testImplementation 'junit : junit: 4.12' 29 } 30 The build.gradle file contains three default sections. They are as follows: o plugins: In this section, we can apply the java-library plugin to add support for java library. o Repositories: In this section, we can declare internal and external repository for resolving dependencies. We can declare the different types of repository supported by Gradle like Maven, Ant, and Ivy. · Dependencies: In this section, we can declare dependencies that are necessary for a particular subject. Additionally, we can declare other project-related modules like a task in this file. Display the Information of the Gradle project To understand the structure, dependencies and debugging problems of a build, Gradle provides many built-in features that display information on a project. Following are some basic commands to display the information of the project: Listing projects In Gradle, all the sub-projects of a project in the workspace can be listed in a hierarchy. To do so, run the below command from the root directory of the project. 1. gradle -q projects CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 72 ---
Course Code/Title:CS3V15/Devops Unit: II To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> C:\Users \HiMaNshU\eclipse-workspace >gradle -q projects Root project Root project 'eclipse-workspace' No sub-projects To see a list of the tasks of a project, run gradle <project-path> : tasks For example, try running gradle :tasks C:\Users\HiMaNshU\eclipse-workspace> Listing Tasks Gradle allows us to list all the essential tasks of the project. To list the task, run the below command: 1. gradle -q tasks Output: C:\Users\HiMaNshUleclipse-workspace>gradle -q tasks Tasks runnable from root project Build Setup tasks CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 31 N33 nirf 1750 Rank

--- Page 83 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Click "OK" to proceed. 4. Configure the Job General Settings 1. Project Description: ○ Enter a description for your job. 2. Discard Old Builds: ○ Set up a policy to manage the number of builds Jenkins keeps. Source Code Management (SCM) 1. Select SCM: ○ Choose your source code management system (e.g., Git). 2. Configure Repository: ○ Enter the repository URL and provide credentials if needed. 3. Branch Specification: ○ Specify the branch to build (e.g., main, develop). Build Triggers 1. Choose Build Triggers: ○ Poll SCM: Check for changes at intervals. ○ Build Periodically: Schedule builds. ○ GitHub hook trigger: Trigger builds on GitHub push. ○ Build after other projects: Trigger builds based on other jobs. Build Environment 1. Configure Build Environment: ○ Clean workspace: Delete workspace before build starts. ○ Manage secrets: Use secret texts or files. ○ Node selection: Specify where to run the job. Build Steps 1. Add Build Steps: ○ Click "Add build step". ○ Choose the build step type (e.g., Execute shell, Invoke Gradle script). ○ Enter the commands or scripts needed for the build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 nirf 8 1750 Rank

--- Page 84 ---
Course Code/Title:CS3V15/Devops Unit: III Post-build Actions 1. Add Post-build Actions: Click "Add post-build action". ○ Common actions include: Archive artifacts: Store build artifacts. Email notification: Send build status emails. I Publish test results: Aggregate test results. Trigger other jobs: Trigger other jobs based on results. 5. Save and Build 1. Save the Configuration: ○ Click "Save". 2. Build the Job: ○ Click "Build Now" to manually trigger a build. ○ View progress and logs by clicking on the build number. 6. Monitor and Manage Builds 1. Check Build History: ○ View past builds and their statuses. 2. Console Output: ○ Click on a build number to see detailed logs. 3. Build Artifacts: ○ Access any artifacts produced by the build. 7. Advanced Configuration (Optional) 1. Pipeline as Code: 0 For complex workflows, use Jenkins Pipelines. Create a Jenkinsfile in your repository. 2. Plugins: ○ Extend Jenkins functionality with plugins from the Jenkins Plugin Manager. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) 9 N32 nirf 1750 Rank

--- Page 91 ---
Course Code/Title:CS3V15/Devops Unit: III Now we will create a repository on GitHub and then push the above committed Git repository on the GitHub. Open a web browser, and login to www.GitHub.com. Create a repository by the name MyJavaPrograms. Start a new repository A repository contains all of your project's files, revision history, and collaborator discussion. AnurdhaP / MyJavaPrograms Public Anyone on the internet can see this repository Private You choose who can see and commit to this repository Create a new repository 1 then copy the URL of this repository. HTTPS SSH https://github.com/AnurdhaP/MyJavaPrograms.git Now open the command prompt window and go to the MyJavaPrograms directory and issue the following command. git remote add origin https://github.com/AnurdhaP/MyJavaPrograms.git then push the Git to GitHub repository using the command push -u origin master CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 16

--- Page 92 ---
Course Code/Title:CS3V15/Devops Unit: III It is illustrated as follows - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git push -u origin master Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 719 bytes | 719.00 KiB/s, done. Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 To https://github.com/AnurdhaP/MyJavaPrograms.git * [new branch] master -> master branch 'master' set up to track 'origin/master'. E: \MyJavaPrograms> · Now it we refresh the repository on GitHub then we can see our test.java and test.class programs are present in that repository. MyJavaPrograms Public !" master - 1º 1 branch 0 tags Go to f AnurdhaP First Commit for Java program .4cb test.class First Commit for Java program test.java First Commit for Java program Help people interested in this repository understand your project by adding a README. Step 4 : Now create a job in Jenkins that makes use of GitHub plugin in order to access the GitHub repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 17 N3 nirf 1750 Rank

--- Page 7 ---
Course Code/Title:CS3V15/Devops Unit: I DevOps Tools 1. Git (GitLab, GitHub, Bitbucket) Git remains indispensable in software development and DevOps due to its pivotal role in version control, collaborative coding, and efficient project management. As technology has accelerated, the need for streamlined and organized code management has never been greater. Git empowers developers to collaborate on codebases, effortlessly creating and merging branches for new features and bug fixes. Its distributed nature ensures developers can work seamlessly offline, an increasingly valuable feature in today's remote and distributed work environments. Additionally, Git facilitates the tracking of code modifications, making it easier to identify when and why specific changes were made, a critical aspect of maintaining code quality and security. Software development is essential in driving innovation and advancing progress, and Git maintains its prominent position as the bedrock of efficient, cooperative, and secure coding methodologies. 2. Maven Due to its enduring significance in managing project dependencies, building, and project lifecycle management, Maven remains a pivotal tool in SD and DevOps. As a robust build automation and project management tool, Maven simplifies the complexities of Java-based project development by streamlining the compilation, testing, packaging, and distribution processes. It ensures consistent and reproducible builds, making it easier for development teams to collaborate efficiently and deliver high-quality software. Maven's role in managing dependencies and facilitating continuous integration and deployment remains crucial. Its ability to handle complex build scenarios and integrate seamlessly with modern DevOps practices makes it indispensable for ensuring software projects' reliability, maintainability, and scalability in 2024 and beyond. 3. Jenkins Its importance lies in its role as a powerful automation server that enables continuous integration and continuous delivery (CI/CD) pipelines. Jenkins streamlines software development by automating tasks such as building, testing, and deploying code changes, ensuring that software is delivered quickly and highly. With the growing complexity of modern applications, the need for efficient CI/CD processes has become even more paramount. Jenkins provides flexibility, extensibility, and a vast library of plugins that cater to a wide range of technologies and tools, making it adaptable to diverse development environments. As organizations prioritize speed, reliability, and collaboration in their software development practices, Jenkins stands as a cornerstone tool, enabling teams to achieve seamless automation and efficient delivery of software solutions. 4. Chef Chef, a powerful automation platform, is crucial in managing infrastructure as code. Chef empowers organizations to achieve scalability, reliability, and speed seamlessly. By allowing the automation of server provisioning, configuration, and maintenance, Chef enhances efficiency and consistency across the entire infrastructure, reducing manual errors and ensuring that infrastructure remains desired. Moreover, Chef integrates smoothly with various cloud providers, containerization technologies, and other DevOps tools, making it adaptable to the ever-evolving tech TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 7 N32 nirf 1750 Rank

--- Page 8 ---
Course Code/Title:CS3V15/Devops Unit: I landscape. As organizations prioritize agility and scalability, Chef remains a vital tool in automating complex infrastructure tasks and enabling DevOps teams to focus on innovation and delivery. 5. Puppet Puppet is essential because it simplifies the management and orchestration of complex IT infrastructures by allowing administrators to define infrastructure as code. It ensures consistency and repeatability in configuration across servers, cloud instances, and containers. Businesses increasingly rely on diverse, dynamic, and hybrid infrastructures. Puppet's importance lies in its ability to streamline provisioning, configuration, and continuous compliance, thus reducing operational complexity, minimizing errors, and accelerating software delivery. Puppet continues to empower organizations to efficiently manage and scale their infrastructure while maintaining high levels of security and compliance, making it a crucial tool for DevOps teams. 6. Ansible Ansible is a powerful and widely adopted automation and configuration management tool important in 2024 for several reasons. This tool stands out for its simplicity and versatility. It empowers organizations to automate repetitive tasks, provisioning of infrastructure, and configuration management across diverse environments, making it an invaluable asset for DevOps and IT teams. Furthermore, Ansible's agentless architecture, declarative language, and a vast library of pre- built modules make it accessible to both beginners and seasoned professionals. As organizations prioritize efficiency, scalability, and the rapid deployment of applications and services, Ansible remains an indispensable DevOps toolkit, helping teams streamline operations, enhance security, and maintain infrastructure at scale, all while reducing manual errors and increasing agility in a fast-paced technological landscape. 7. Docker Docker is crucial in modern software development and DevOps practices. It can simplify and streamline the management of applications across various environments. Docker containers encapsulate an app and its dependencies, ensuring consistent and reproducible deployments from development to production. This technology enhances portability and scalability, accelerates development cycles, and reduces the "it works on my machine" problem. In a rapidly evolving software landscape, Docker's containerization approach remains crucial for achieving efficient, isolated, and highly flexible application deployment, making it an essential component of DevOps and continuous delivery pipelines. 8. Kubernetes Kubernetes, often abbreviated as K8s, play a central role in modern software development and operations. Its importance lies in its ability to orchestrate, manage, and automate containerized applications at scale. As organizations increasingly embrace microservices architectures and containerization for their applications, Kubernetes provides the essential infrastructure for deploying, scaling, and maintaining these containers efficiently. The tool's resilience, self-healing capabilities, and support for hybrid and multi-cloud environments make it vital for achieving agility, reliability, and cost-effectiveness in application deployment. It serves as the backbone of cloud-native ecosystems, enabling organizations to accelerate software delivery, improve resource utilization, and respond effectively to the evolving demands of the digital landscape. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N32 nirf 1750 Rank

--- Page 55 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Environment Variables Now remove active profile from maven settings.xml and update the test profile mentioned in pom.xml. Add activation element to profile element as shown below. The test profile will trigger when the system property "env" is specified with the value "test". Create an environment variable "env" and set its value as "test". <profile> <id>test</id> <activation> <property> <name>env</name> <value>test</value> </property> </activation> </profile> Let's open command console, go to the folder containing pom.xml and execute the following mvn command. C:\MVN\project>mvn test Profile Activation via Operating System Activation element to include os detail as shown below. This test profile will trigger when the system is windows XP. <profile> <id>test</id> <activation> <os> <name>Windows XP</name> <family>Windows</family> <arch>x86</arch> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 14 N32 narf 1750 Rank -

--- Page 56 ---
Course Code/Title:CS3V15/Devops Unit: II <version>5.1.2600</version> </os> </activation> </profile> Now open command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test Profile Activation via Present/Missing File Now activation element to include OS details as shown below. The test profile will triggerwhen target/generated-sources/axistools/wsdl2java/com/companyname/group is missing. <profile> <id>test</id> <activation> <file> <missing>target/generated-sources/axistools/wsdl2java/ com/companyname/group</missing> </file> </activation> </profile> Now open the command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test 8. Maven create and build artifacts: The first step is to open Eclipse, which comes with the integrated Maven environment. The Eclipse window opens on the screen. Complete the following steps: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 15 N32 nirf 175º Rank -

--- Page 113 ---
Course Code/Title:CS3V15/Devops Unit: III 3. Configure Job Details: ○ General: I Enter a description for your job. Configure options such as discarding old builds if needed. ○ Source Code Management: I Choose Git, Subversion, or another source control system. Enter the repository URL and credentials if the repository is private. Specify the branch or tag to build from. ○ Build Triggers: Set up triggers to start the build. Common options include: Poll SCM: Jenkins will periodically check for changes in the source code repository. . Build periodically: Schedule builds at specific intervals. I GitHub hook trigger for GITScm polling: Trigger builds based on GitHub webhooks. 0 Build Environment: ■ Configure the build environment, such as setting up environment variables or cleaning up before the build starts. ○ Build Steps: Add build steps by clicking Add build step. · Common build steps include: Execute shell: Run shell commands or scripts. . Invoke Gradle script: Use Gradle to build the project. I Invoke Ant: Use Apache Ant for building. ○ Post-build Actions: · Configure actions to perform after the build completes, such as: I Archive the artifacts: Save build artifacts for later use. I Publish JUnit test result report: Display test results in Jenkins. I Send build notifications: Notify stakeholders of build status. 4. Save and Build: ○ Click Save to store the job configuration. o To start a build, click Build Now in the job dashboard. Example Build Configuration: . Job Name: MyApp-Build · Source Code Management: Git o Repository URL: https://github.com/user/myapp.git ○ Branch: main · Build Step: Execute shell ○ Command : myn clean install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 38 nirf 1750 Rank N33

--- Page 114 ---
Course Code/Title:CS3V15/Devops Unit: III · Post-build Action: Archive artifacts ○ Files to archive: target/ *. jar 2. Understanding Jenkins Workspace Overview: The Jenkins workspace is a directory where Jenkins stores files and artifacts related to a particular build. Each job has its own workspace, which is used to perform build operations. Key Aspects of Jenkins Workspace: 1. Workspace Location: o By default, the workspace is located in the Jenkins home directory, typically at /var/lib/jenkins/workspace/ on Linux systems or C:\Program Files (x86) \Jenkins\workspace\ on Windows. 2. Workspace Structure: ○ Each job gets its own subdirectory within the workspace. For example, a job named MyApp-Build will have its workspace at /var/lib/jenkins/workspace/MyApp - Build/. ○ The workspace contains: Source Code: The code pulled from the repository. . Build Artifacts: Files generated during the build process. I Logs: Logs related to the build process. 3. Workspace Usage: ○ Building: During the build process, Jenkins checks out the code into the workspace, executes build steps, and generates artifacts. ○ Archiving Artifacts: After the build, files specified in the post-build actions are archived from the workspace. ○ Cleaning Up: Jenkins may clean up workspaces based on job configurations or policies to save disk space. 4. Customizing Workspace: ○ Configure Custom Workspace Location: In the job configuration, under the Advanced Project Options, you can specify a custom workspace directory. ○ Workspace Cleanup: I Use plugins like the Workspace Cleanup Plugin to manage and clean up workspaces automatically. Example of Workspace Structure: · Workspace Directory: /var/lib/jenkins/workspace/MyApp-Build/ o Source Code: /var/lib/jenkins/workspace/MyApp-Build/src/ o Build Artifacts: /var/lib/jenkins/workspace/MyApp-Build/target/ o Logs: /var/lib/jenkins/workspace/MyApp-Build/logs/ This guide should provide a comprehensive understanding of creating a Jenkins build and managing Jenkins workspaces. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY N32 39 nirf 175º Rank

--- Page 11 ---
Course Code/Title:CS3V15/Devops Unit: I 17. Selenium It remains a vital tool in software testing and automation due to its enduring relevance in ensuring the quality of web applications. As technology evolves, web applications become increasingly complex, requiring thorough testing across various browsers and platforms. With its robust automation capabilities and extensive browser compatibility, Selenium allows developers and QA teams to automate repetitive testing tasks efficiently, conduct cross- browser testing, and ensure that web applications function flawlessly across diverse environments. Its open-source nature, active community support, and integration with other DevOps tools make Selenium a go-to choice for organizations striving for continuous delivery and the rapid deployment of high-quality software, a cornerstone of modern software development practices. 18. Gremlin Gremlin is an essential tool in chaos engineering, which has become increasingly critical for ensuring the resilience and reliability of modern software systems. As technology advances and complex distributed systems become the norm, the potential for unexpected failures and outages also rises. Gremlin allows organizations to proactively identify weaknesses and vulnerabilities in their infrastructure and applications by simulating controlled failures, such as network disruptions, service outages, and resource constraints. By intentionally inducing chaos and monitoring the system's response, teams can uncover weaknesses before they lead to costly downtime or security breaches. Gremlin facilitates organizations to build more robust, fault-tolerant systems that can withstand real-world challenges and deliver uninterrupted services to users. 19. ServiceNow ServiceNow is a vital platform for organizations seeking to streamline their IT service management and beyond. Its significance lies in its ability to provide a unified, cloud-based solution for automating and optimizing various business processes, including ITSM, ITOM, HR, customer service, and more. Due to the rapid digitization of services, remote work, and the growing complexity of technology infrastructures, ServiceNow offers a comprehensive approach to managing workflows, resolving issues, and delivering services efficiently. Its intelligent automation capabilities, analytics, and AI-driven insights empower organizations to enhance productivity, agility, and customer satisfaction while reducing operational costs. ServiceNow's role in orchestrating and integrating diverse systems and processes makes it an indispensable tool for driving digital transformation and ensuring smooth operations in the ever-evolving business landscape of 2024. 20. Status Service Updates: The Status Page "Status Service Updates: The Status Page" is a critical tool for organizations and businesses of all sizes. In today's world, where online services and applications are integral to operations, ensuring their availability and reliability is essential. It provides real-time information to users and stakeholders about the operational status of services, applications, and infrastructure. The Status Page plays a crucial role in transparency, trust-building, and customer satisfaction by promptly communicating service disruptions, planned maintenance, and incident resolutions. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 1750 Rank

--- Page 12 ---
Course Code/Title:CS3V15/Devops Unit: I Downtime can often lead to significant financial losses and damage to a company's reputation, so having a practical Status Page becomes not just a convenience but a necessity. It allows organizations to showcase their commitment to transparency and responsiveness in addressing service-related issues, ultimately fostering stronger customer relationships and trust. 21. ELK (Elasticsearch, Logstash and Kibana) ELK, which stands for Elasticsearch, Logstash, and Kibana, continues to shine in DevOps and IT operations. This powerful trio of tools remains essential for organizations seeking effective log management, monitoring, and data visualization. Elasticsearch is a highly scalable and fast search engine that enables real-time data indexing and search. Logstash facilitates the collection, processing, and transformation of log data from various sources, making it compatible with Elasticsearch. Kibana, on the other hand, provides a user- friendly interface for visualizing and analyzing data, offering customizable dashboards and powerful data exploration capabilities. ELK's significance in 2024 lies in its ability to empower organizations with comprehensive insights into their systems, applications, and infrastructure. It ultimately facilitates quick problem resolution, proactive monitoring, and data-driven decision-making in an increasingly complex and fast-paced technological landscape. 22. GitLab CI/CD GitLab CI/CD's significance lies in its ability to automate the complete software delivery pipeline, from code changes to deployment, in a single integrated environment. GitLab CI/CD ensures rapid and reliable delivery of software updates. It enables continuous integration (CI) by automatically building and testing code changes, allowing teams to catch issues early in the development cycle. Furthermore, the continuous deployment (CD) aspect automates the release and deployment process, reducing the risk of human errors and enabling organizations to deliver features and updates to users swiftly and confidently. GitLab CI/CD's importance is further accentuated as businesses seek to accelerate digital transformation efforts, respond rapidly to changing market demands, and maintain a competitive edge through efficient and automated software delivery practices. 23. Scripting Scripting remains vital due to its pivotal role in automating and streamlining various aspects of software development, system administration, and DevOps practices. Scripting languages like Python, Bash, and PowerShell empower tech professionals to write code that can execute repetitive tasks, manipulate data, and orchestrate complex processes efficiently. Scripting facilitates rapid prototyping, configuration management, and the creation of automated deployment pipelines. It enhances productivity, ensures consistency and reduces human error in tasks ranging from software testing and deployment to infrastructure provisioning and monitoring. As organizations increasingly embrace DevOps and cloud- native technologies, scripting stays competitive and adaptive in the tech landscape. 24. Terraform Terraform plays a crucial role in modern infrastructure provisioning and management. It allows organizations to define and deploy infrastructure as code, enabling the automated creation and configuration of cloud resources, containers, and other infrastructure components. Cloud computing, microservices, and containerization have become the norm in CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 12 N32 nirf 1750 Rank

--- Page 13 ---
Course Code/Title:CS3V15/Devops Unit: I 2024. Terraform provides the agility and scalability required to keep up with the dynamic demands of modern applications. Terraform's importance lies in its ability to bring consistency, version control, and automation to infrastructure operations, thereby reducing manual errors, streamlining DevOps workflows, and facilitating applications' rapid and reliable deployment in an increasingly complex and cloud-centric environment. As organizations adopt cloud-native technologies, Terraform remains essential to ensure efficient and consistent infrastructure management. 25. Phantom Phantom enhances security automation and incident response capabilities. In today's rapidly evolving threat landscape, organizations face a constant barrage of cybersecurity incidents, and the ability to respond swiftly and effectively is necessary. It provides a platform for automating security workflows, from detecting and investigating potential threats to orchestrating responses and mitigating risks. Phantom's importance lies in its capacity to reduce response times, increase consistency in incident handling, and free up manual resources from repetitive tasks. With the growing complexity of cyber threats, Phantom empowers security teams to defend against attacks and safeguard critical assets proactively. 26. Nagios Nagios, an open-source monitoring and alerting system, remains vital due to its enduring significance in maintaining the reliability and performance of IT infrastructure and applications. Organizations increasingly rely on complex systems and services. Nagios plays a crucial role by providing real-time monitoring and alerting capabilities, allowing IT teams to detect and address issues before they impact users or cause system outages. Its versatility, extensibility, and support for both on-premises and cloud environments make Nagios a valuable tool for ensuring critical systems' availability, stability, and security, aligning perfectly with the demands of modern IT operations and DevOps practices. 27. Vagrant Vagrant continues to play a crucial role in software development and DevOps. It is a tool that simplifies creating and managing reproducible development environments. Its importance lies in its ability to provide developers and DevOps teams with a consistent and isolated environment for software development, testing, and deployment. With the ever-evolving complexity of software stacks, dependencies, and infrastructure configurations, Vagrant remains essential in ensuring these environments are easily shareable, scalable, and maintainable. It allows developers to work seamlessly across various operating systems and provides a standardized setup that minimizes compatibility issues. 28. Sentry Sentry plays a critical role in modern software development and DevOps practices. With software applications' increasing complexity and scale, identifying and addressing errors and issues has become crucial. Sentry is vital because it provides real-time error tracking and monitoring, allowing development teams to proactively detect and diagnose issues, whether they occur in production or during development. Its importance is minimizing downtime, improving user experience, and maintaining software systems' overall health and reliability. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nirf 1750 Rank

--- Page 14 ---
Course Code/Title:CS3V15/Devops Unit: I 29. Gradle Gradle continues to be a vital tool in software development and DevOps. Gradle is an advanced build automation system that plays a crucial role in managing dependencies, building projects, and orchestrating complex workflows efficiently. Its importance lies in its versatility and scalability, as it caters to various project sizes and types. Gradle's ability to easily handle multi-language, multi-project builds and its support for plugin-based customization make it indispensable in modern software development. As organizations increasingly adopt microservices architectures and cloud-native technologies, Gradle's capabilities are instrumental in managing the complexity of building, testing, and deploying applications across diverse environments. DevOps Architecture DevOps Architecture Plan Release Code Dep Deploy Development and operations both play essential roles in order to deliver applications. The deployment comprises analyzing the requirements, designing, developing, and testing of the software components or frameworks. The operation consists of the administrative processes, services, and support for the software. When both the development and operations cture is the solution to fix the gap between Test Monitor Build Operate ined with collaborating, then the DevOps al www deployment and operation terms; therefore, delivery can be faster. DevOps architecture is used for the applications hosted on the cloud platform and large distributed applications. Agile Development is used in the DevOps architecture so that integration and delivery can be contiguous. When the development and operations team works separately from each other, then it is time- consuming to design, test, and deploy. And if the terms are not in sync with each other, then it may cause a delay in the delivery. So DevOps enables the teams to change their shortcomings and increases productivity. Below are the various components that are used in the DevOps architecture: 1) Build: Without DevOps, the cost of the consumption of the resources was evaluated based on the pre-defined individual usage with fixed hardware allocation. And with DevOps, the usage of cloud, sharing of resources comes into the picture, and the build is dependent upon the user's need, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Build Code Test Plan DevOps Components Moniter Dev Ops Deploy Operate Release 14 N33 nirf 1750 Rank

--- Page 33 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between AWS (Amazon Web Services), Google Cloud, and Azure Technology EC2 (Elastic Compute Cloud) AWS Databases Supported Pricing Models Difficulties Many enterprises find it difficult to understand the company cost AWS fully relational and supports NoSQL databases and Big Data. Per hour - rounded up. On demand, reserved spot. structure. Google Cloud Google Engine(GCE) Compute Technologies pioneered by Google, like Big Query, Big Table, and Hadoop, are databases, and Big Data,naturally fully supported. Per minute - rounded up On demand sustained use. — Fewer services. features Azure VHD Disk) (Virtual Hard Azure supports relational and both NoSQL Windows and through AzureTable HDInsight. Per minute - rounded up. Per minute- rounded up commitments(Pre-paid or monthly) and Less "Enterprise-ready. Storage Services Simple Storage Service(S3) Elastic Block Storage. Elastic File storage. Machine Learing Sage maker. Lex. polly.And many more Blob Storage Queue Storage. File Storage Disk Storage. Data Lake Store Machine learning Azure Bot service Cognitive service GIT Cloud storage. Persistent Disk Transfer appliance. Cloud speech AI Cloud Video Intelligence. Cloud Machine learning engine Git is a distributed version control system (DVCS) that helps manage and track changes in source code during software development. It was created by Linus Torvalds in 2005 and has become one of the most widely used version control systems in the software development industry. Some key concepts and features of Git: · Version Control: Git allows developers to keep track of changes made to their code over time. This includes modifications, additions, and deletions of files. · Distributed System: Git is a distributed version control system, meaning that each developer has a complete copy of the entire repository, including its full history. This allows developers to work independently and merge their changes when necessary. · Branching: Git enables developers to create branches, which are essentially separate lines of development. This allows for the parallel development of features or bug fixes without affecting the main codebase. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) LEN 33 nirf 1750 Rank

--- Page 34 ---
Course Code/Title:CS3V15/Devops Unit: I · Merging: Git provides tools for merging changes from one branch into another. This is essential when multiple developers are working on different branches and need to bring their changes together. · History Tracking: Git maintains a complete history of changes made to the codebase. Developers can view, revert, or analyze changes made over time. · Remote Repositories: Git supports remote repositories, allowing developers to collaborate with others by pushing and pulling changes to and from a shared repository. Platforms like GitHub, GitLab, and Bitbucket provide hosting services for Git repositories. · Staging Area: Git uses a staging area (also known as the index) to prepare and review changes before committing them to the repository. This allows developers to selectively include or exclude specific changes. · Open Source: Git is an open-source tool, and its source code is freely available for modification and distribution. Various GIT Components: Git is composed of several key components that work together to enable version control and collaborative development. Here are the main components of Git: · Repository (Repo): A repository is a directory or storage space where your project and its version history are stored. It contains all the files and directories associated with your project, along with the metadata and configuration information. · Working Directory: The working directory is the directory on your local machine where you manipulate files and make changes to your project. It is essentially your local copy of the repository. · Index (Staging Area): The index, also known as the staging area, is a middle ground where changes are prepared before being committed to the repository. It allows you to selectively stage changes, which means you can choose which modifications to include in the next commit. · Commit: A commit is a snapshot of the changes made to the files in the repository. It represents a specific point in the project's history and is accompanied by a commit message that describes the changes. · Branch: A branch is a parallel line of development within a repository. It allows developers to work on different features or bug fixes simultaneously without affecting the main codebase. Branches can be merged to incorporate changes into other branches. . Head: HEAD is a reference to the latest commit in the currently checked-out branch. It essentially points to the tip of the branch you are currently on. · Remote: A remote is a version of the repository stored on a different server. Git supports collaboration by allowing developers to push and pull changes between their local repository and remote repositories. Platforms like GitHub, GitLab, and Bitbucket are examples of remote repositories. · Clone: Cloning is the process of creating a copy of a remote repository on your local machine. This allows you to start working on your own copy of a project. · Fetch: The fetch operation retrieves changes from a remote repository but does not automatically merge them into your working directory. It is useful for reviewing changes before deciding to merge. · Pull: Pull is a combination of fetch and merge. It retrieves changes from a remote repository and automatically merges them into your working directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 85 ---
Course Code/Title:CS3V15/Devops Unit: III By following these steps, you can set up a Jenkins job to automate builds, integrate with your source code management system, and perform various actions based on build results. Introduction to Jenkins Plugins What Are Jenkins Plugins? Jenkins plugins are extensions that add extra functionality to Jenkins. They allow you to customize and extend Jenkins to better fit your specific needs. Plugins can provide integrations with other tools, add new features, and improve existing functionalities. Why Use Plugins? . Extend Functionality: Add features not available in the core Jenkins. . Integrate Tools: Seamlessly integrate with other tools and platforms (e.g., GitHub, Docker). . Improve Productivity: Automate more tasks and streamline your workflow. · Customize UI: Tailor the Jenkins interface to better suit your preferences and needs. How to Manage Plugins 1. Accessing the Plugin Manager 1. Go to Jenkins Dashboard: ○ Open your Jenkins dashboard. 2. Navigate to Plugin Manager: ○ Click on "Manage Jenkins". ○ Select "Manage Plugins". 2. Installing Plugins 1. Available Plugins: ○ Go to the "Available" tab to see a list of plugins that can be installed. 2. Search for Plugins: ○ Use the search bar to find specific plugins. 3. Select Plugins: ○ Check the box next to the plugins you want to install. 4. Install Plugins: ○ Click "Install without restart" or "Download now and install after restart". 3. Updating Plugins CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 narf 10 1750 Rank

--- Page 86 ---
Course Code/Title:CS3V15/Devops Unit: III 1. Go to Updates: ○ Navigate to the "Updates" tab to see plugins with available updates. 2. Select Plugins: ○ Check the box next to the plugins you want to update. 3. Update Plugins: ○ Click "Download now and install after restart". 4. Managing Installed Plugins 1. Installed Plugins: ○ Go to the "Installed" tab to see all installed plugins. 2. Uninstall Plugins: ○ Click the "Uninstall" button next to the plugin you want to remove. 3. Check Plugin Versions: ○ View the current version of each installed plugin. Adding Plugins to Jenkins Plugins are a crucial part of Jenkins, allowing you to extend its functionality to suit your project's specific needs. Whether you're integrating Jenkins with other tools, customizing your job configuration, or adding new build steps, plugins provide the flexibility you need. Here's a detailed guide on how to add plugins to Jenkins. 1. Why Plugins Are Important - Extending Jenkins Functionality: Plugins allow Jenkins to integrate with various tools and technologies, such as Git, Maven, Docker, and more. - Customizing Jobs: With plugins, you can add specific steps, triggers, and post-build actions to your jobs, making Jenkins adaptable to any workflow. - Automation: Plugins help automate processes like code quality checks, notifications, and deployments. 2. Accessing the Plugin Manager 1. Navigate to Jenkins Dashboard: Open your Jenkins dashboard in a web browser (e.g., http://localhost:8080). CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 175º Rank

--- Page 51 ---
Course Code/Title:CS3V15/Devops Unit: II 7. Maven Profiles: A Build profile is a set of configuration values, which can be used to set or override default values of Maven build. Using a build profile, you can customize build for different environments such as Production v/s Development environments. Types of Build Profile Build profiles are majorly of three types. Type Where it is defined Per Project Defined in the project POM file, pom.xml Per User Defined in Maven settings xml file (%USER_HOME%/.m2/settings.xml) Global Defined in Maven global settings xml file (%M2_HOME%/conf/settings.xml) Profile Activation A Maven Build Profile can be activated in various ways. · Explicitly using command console input. · Through maven settings. · Based on environment variables (User/System variables). · OS Settings (for example, Windows family). · Present/missing files. Now, under src/main/resources, there are three environment specific files - Sl.No. File Name & Description env.properties 1 default configuration used if no profile is mentioned. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 10 N33 nirf 1750 Rank

--- Page 52 ---
Course Code/Title:CS3V15/Devops Unit: II env.test.properties 2 test configuration when test profile is used. 3 env.prod.properties production configuration when prod profile is used. Explicit Profile Activation In the following example, we will attach maven-antrun-plugin:run goal to test the phase. This will allow us to echo text messages for different profiles. We will be using pom.xml to define different profiles and will activate profile at command console using maven command. Assume, we've created the following pom.xml in C:\MVN\project folder. <project xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation = "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.companyname.projectgroup</groupId> <artifactId>project</artifactId> <version>1.0</version> <profiles> <profile> <id>test</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <version>1.1</version> <executions> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 11 N33 nirf 1750 Rank

--- Page 73 ---
Course Code/Title:CS3V15/Devops Unit: II init - Initializes a new Gradle build. wrapper - Generates Gradle wrapper files. Help tasks Build Environment - Displays all build script dependencies declared in root project 'eclipse-workspace'. components - Displays the components produced by root project 'eclipse-workspace '. [incubating] dependencies - Displays all dependencies declared in root project 'eclipse-works pace'. Dependency Insight - Displays the insight into a specific dependency in root project 'eclipse-workspace.' Dependent Components : It displays the dependent components of components in the root project 'eclipse-workspace.' [incubating] help - Displays a help message. model - Displays the configuration model of root project 'eclipse-workspace.' [incubating] projects - Displays the sub-projects of root project 'eclipse-workspace.' properties - Displays the properties of root project 'eclipse-workspace.' tasks - Displays the tasks runnable from root project 'eclipse-workspace.' To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> To list all the tasks of the project, run the below command: 1. gradle tasks -all CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 32 N33 nirf 1750 Rank

--- Page 74 ---
Course Code/Title:CS3V15/Devops Unit: II To display more details about a task, run the below command: 1. gradle help -- task Listing Dependencies In Gradle, we can list the dependencies which are broken down by the configuration. To list the dependencies, run the below command: 1. gradle -q dependencies Difference between Gradle and Maven Basis Based on Gradle is based on developing Maven is based on developing pure Java language-based software. Configuration It uses a Groovy-based Domain-specific language (DSL) for creating project structure. Focuses on Performance It performs better than maven as it optimized for tracking only current running task. It is necessary to compile. Java Compilation Usability It is a new tool , which requires users to spend a lot This tool is a known tool for many users and is easily Gradle domain-specific language projects. It uses Extensible Markup Language (XML) for creating project structure. Developing applications by adding new features to them . It avoids compilation. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] Maven Developing applications in a given time limit. It does not create local temporary files during software creation and is hence - slower . 33 N33 nirf 1750 Rank -

--- Page 137 ---
Course Code/Title:CS3V15/Devops Unit:IV I Example 2: Get the Hostname and Version of remote servers with UNAME we have used the command module and executing an uname -a command as AD-HOC command $ ansible testservers -m command -a "uname -a" -i ansible hosts As Playbook - name: Check the remote host Hostname, Version, Distribution with UNAME hosts: testservers tasks: - name: Execute the UNAME command register: unameout command: "uname -a" - debug: var: unameout.stdout lines I Example 3: Check the Disk Usage of Remote server To get the disk usage, we are using df -h , here -h is human readable As Ad Hoc command $ ansible testservers -m command -a "df -h" -i ansible hosts As Playbook - name: Check the disk usage of all the file system in the remote servers hosts: testservers tasks: - name: Execute the df command register: dfout command: "df -h" - debug: var: dfout.stdout lines CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 138 ---
Course Code/Title:CS3V15/Devops Unit:IV Example 4: Restart Apache Server using Ansible Command Module So far, In all the sample we have seen we are using one application server and one web server. Totally two servers. But in this example, we need to limit our execution only to the web server as we are going to check the restart the apache web server. this is done using -- limit parameter As Ad hoc command $ ansible testservers -m command -a "httpd -k restart" -i ansible_hosts -b - limit As Playbook - name: restart apache web server hosts: testservers tasks: - name: restartapache register: httpdresout become: yes command: "httpd -k restart" when: ansible_hostname == "mwiweb02" - debug: var: httpdresout.stdout_lines I Example 5: Execute a command when a file exists or not exists There are two most useful parameters in ansible command module such as removes and creates removes - used to tell ansible to Execute the command only if the file exist creates - used to tell ansible to Execute the specified command only if the file does not exist Here we are going to do a very simple file creation and removal based on the file availability or existence. In General using the ansible command module, This method of creating and removing the file is not recommended as Ansible has a dedicated module named file to do the same effortlessly. As Ad hoc command Remove the file, if it does exist (or) present $ ansible testservers -a "rm -rf /tmp/testfile removes=/tmp/testfile" -i ansible_hosts CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 24 N33 nirf 1750 Rank

--- Page 149 ---
Course Code/Title: CS3V15/Devops Unit: V Step 6: Now you will see that all of our files uploaded in our github. Namanbhatia7 Add initial resume website files Latest commit f47be5b 1 minute ago Display_Picture.png Add initial resume website files 1 minute ago My_Resume.html Add initial resume website files 1 minute ago README.md Initial commit 15 minutes ago Styles.css Add initial resume website files 1 minute ago Branch: · A GitHub branch is used to work with different versions of a repository at the same time. · By default a repository has a master branch (aproduction branch). · Any other branch is a copy of the master branch (as itwas at a point in time). · New Branches are for bug fixes and feature work separate from the master branch. When changes are ready, they can be merged into the master branch. If youmake changes to the master branch while working on a new branch, these updates can be pulled in. Commits: At GitHub, changes are called commits. Each commit (change) has a description explaining why achange was made. Pull Requests : . Pull Requests are the heart of GitHub collaboration. · With a pull request you are proposing that your changesshould be merged (pulled in) with the master. · Pull requests show content differences, changes, additions, and subtractions in colors (green and red). · As soon as you have a commit, you can open a pull request and start a discussion, even before the code isfinished. Git: · Git was created by Linus Torvalds in 2005 to develop Linux Kernel · Git is an open-source distributed version control system. It is designed to handle minor to major projectswith high speed and efficiency. . It is developed to co-ordinate the work among the developers. The version control allows us to track andwork together with our team members at the same workspace. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 9 175" Rank

--- Page 150 ---
CREATE AZURE ORGANIZATION AZURE: Microsoft Azure, often referred to as Azure cloud computing platform run by Microsoft. It offers access, management, and the development of applications and services through global data centers It also provides a range of capabilities, including software as a service (SaaS), platform as a service, and infrastructure as a service (IaaS). It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014 Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Prerequisites: To plan your organizational structure. Microsoft accounts or authenticate users with Microsoft Entra ID. For more information, see Choosing your organization administrator account type. Create an organization: 1. Sign in to Azure DevOps. 2. Select New organization 3. Confirm information, and then select Continue 10

--- Page 39 ---
Course Code/Title:CS3V15/Devops Unit: I GITHUB GitHub is an increasingly popular programming resource used for code sharing. It's a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in October 2020, it is the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 56 million total developers. GitHub is a Git repository hosting service that provides a web-based graphical interface. It is the world's largest coding community. Putting a code or a project into GitHub brings it increased, widespread exposure. Programmers can find source codes in many different languages and use the command-line interface, Git, to make and keep track of any changes. GitHub helps every team member work together on a project from any location while facilitating collaboration. You can also review previous versions created at an earlier point in time. GitHub's Features? 1. Easy Project Management: GitHub is a place where project managers and developers come together to coordinate, track, and update their work so that projects are transparent and stay on schedule. 2. Increased Safety With Packages Packages can be published privately, within the team, or publicly to the open-source community. The packages can be used or reused by downloading them from GitHub. 3. Effective Team Management GitHub helps all the team members stay on the same page and organized. Moderation tools like Issue and Pull Request Locking help the team to focus on the code. 4. Improved Code Writing Pull requests help the organizations to review, develop, and propose new code. Team members can discuss any implementations and proposals through these before changing the source code. 5. Increased Code Safety GitHub uses dedicated tools to identify and analyze vulnerabilities to the code that other tools tend to miss. Development teams everywhere work together to secure the software supply chain, from start to finish. 6. Easy Code Hosting All the code and documentation are in one place. There are millions of repositories on GitHub, and each repository has its own tools to help you host and release code. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 39 N32 nirf 1750 Rank

--- Page 40 ---
Course Code/Title:CS3V15/Devops Unit: I HOSTING SERVICE FOR GIT REPOSITORY When it comes to hosting Git repositories, various platforms provide a robust infrastructure for collaborative development, version control, and project management. Choosing the right hosting service depends on factors like ease of use, collaboration features, and integration capabilities. Here are some popular Git hosting services widely utilized in the software development community: 1. GitHub: GitHub stands out as one of the most prevalent Git hosting platforms, offering a user- friendly interface, powerful collaboration features, and seamless integration with various tools. It serves as an ideal choice for open-source projects, private repositories, and team collaboration. 2. GitLab: GitLab is a comprehensive web-based Git repository manager that not only provides source code management but also includes features like continuous integration. It caters to both cloud-based and self-hosted solutions, giving users flexibility in hosting their repositories. 3. Bitbucket: Owned by Atlassian, Bitbucket is another popular Git repository hosting service. Supporting both Git and Mercurial repositories, it offers features like code collaboration, issue tracking, and continuous integration. Bitbucket is often preferred by teams using other Atlassian tools such as Jira and Confluence. 4. GitKraken Glo Boards: GitKraken Glo Boards is an integrated task and issue tracking service linked with GitKraken, a Git client. This platform allows teams to manage tasks directly associated with their Git repositories and provides a visual approach to monitoring project progress. 5. SourceForge: SourceForge, with a long history, hosts open-source software projects and offers version control, bug tracking, and project management tools. While not as prominent as some other options, it remains a viable choice for numerous projects. 6. AWS CodeCommit: As part of Amazon Web Services (AWS), AWS CodeCommit is a fully managed source control service. It seamlessly integrates with other AWS services and provides a secure and scalable environment for hosting Git repositories. Selecting the most suitable Git hosting service depends on your team's requirements, project size, and preferences for cloud-based or self-hosted solutions. Each platform has its strengths, catering to specific use cases within the software development landscape. Difference between GIT and GITHUB Git and GitHub are related concepts but serve different purposes in the context of version control and collaborative software development. Git: Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows. GitHub: GitHub is a web-based Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 40 N32 nirf 1750 Rank

--- Page 15 ---
Course Code/Title:CS3V15/Devops Unit: I which is a mechanism to control the usage of resources or capacity. 2) Code: Many good practices such as Git enables the code to be used, which ensures writing the code for business, helps to track changes, getting notified about the reason behind the difference in the actual and the expected output, and if necessary reverting to the original code developed. The code can be appropriately arranged in files, folders, etc. And they can be reused. 3) Test: The application will be ready for production after testing. In the case of manual testing, it consumes more time in testing and moving the code to the output. The testing can be automated, which decreases the time for testing so that the time to deploy the code to production can be reduced as automating the running of the scripts will remove many manual steps. 4) Plan: DevOps use Agile methodology to plan the development. With the operations and development team in sync, it helps in organizing the work to plan accordingly to increase productivity. 5) Monitor: Continuous monitoring is used to identify any risk of failure. Also, it helps in tracking the system accurately so that the health of the application can be checked. The monitoring becomes more comfortable with services where the log data may get monitored through many third-party tools such as Splunk. 6) Deploy: Many systems can support the scheduler for automated deployment. The cloud management platform enables users to capture accurate insights and view the optimization scenario, analytics on trends by the deployment of dashboards. 7) Operate: DevOps changes the way traditional approach of developing and testing separately. The teams operate in a collaborative way where both the teams actively participate throughout the service lifecycle. The operation team interacts with developers, and they come up with a monitoring plan which serves the IT and business requirements. 8) Release: Deployment to an environment can be done by automation. But when the deployment is made to the production environment, it is done by manual triggering. Many processes involved in release management commonly used to do the deployment in the production environment manually to lessen the impact on the customers. DevOps Lifecycle DevOps defines an agile relationship between operations and Development. It is a process that is practiced by the development team and operational engineers Learning DevOps is not complete without understanding the DevOps lifecycle phases. The DevOps lifecycle includes seven phases as given below: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 16 ---
Unit: I Course Code/Title:CS3V15/Devops together from beginning to the final stage of the product. Development Operations Integration Deployment DevOps Lidecycle Feedback 2) Continuous Integration Testing Monitering 1) Continuous Development This phase involves the planning and coding of the software. The vision of the project is decided during the planning phase. And the developers begin developing the code for the application. There are no DevOps tools that are required for planning, but there are several tools for maintaining the code. This stage is the heart of the entire DevOps lifecycle. It is a software development practice in which the developers require to commit changes to the source code more frequently. This may be on a daily or weekly basis. Then every commit is built, and this allows early detection of problems if they are present. Building code is not only involved compilation, but it also includes unit testing, integration testing, code review, and packaging. The code supporting new functionality is continuously integrated with the existing code. Therefore, there is continuous development of software. The updated code needs to be integrated continuously and smoothly with the systems to reflect changes to the end-users. Commit Build O O Test O Stage Deploy Dev/QA O O > Continuous Integration/Delivery Development </> git Code Commit = production server. 3) Continuous Testing Production Jenkins is a popular tool used in this phase. Whenever there is a change in the Git repository, then Jenkins fetches the updated code and prepares a build of that code, which is an executable file in the form of war or jar. Then this build is forwarded to the test server or the This phase, where the developed software is continuously testing for bugs. For constant testing, automation testing tools such as TestNG, JUnit, Selenium, etc are used. These tools allow QAs to test multiple code- bases thoroughly in parallel to ensure that there is no flaw in the functionality. In this phase, Docker Containers can be used for simulating the test environment. It may occur in the form of documentation files or maybe produce large-scale data about the application parameters when it is in a continuous use position. The system errors such as server not reachable, low memory, etc are resolved in this phase. It maintains the security and availability of the service. Automation testing saves a lot of time and effort for executing the tests instead of doing this manually. Apart from that, report generation is a big plus. The task of evaluating the test cases INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 narf 1750 Rank

--- Page 25 ---
Course Code/Title:CS3V15/Devops Unit: I · backups · automated multi-region replication in-memory caching · data export tools. Global Infrastructure of AWS · AWS is a cloud computing platform which is globally available. . Global infrastructure is a region around the world in which AWS is based. Global infrastructure is a bunch of high-level IT services which is shown below: AWS is available in 19 regions, and 57 availability zones in December 2018 and 5 more regions 15 more availability zones for 2019. The following are the components that make up the AWS infrastructure: o Availability Zones o Region o Edge locations o Regional Edge Caches Compnents of Global Infrastructure Availability zones Region Edge Locations Regional Edge Caches Availability zone as a Data Center An availability zone is a facility that can be somewhere in a country or in a city. Inside this facility, i.e., Data Centre, we can have multiple servers, switches, load balancing, firewalls. The things which interact with the cloud sits inside the data centers. o An availability zone can be a several data centers, but if they are close together, they are counted as 1 availability zone. Region o A region is a geographical area. Each region consists of 2 more availability zones. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 26 ---
Course Code/Title:CS3V15/Devops Unit: I o A region is a collection of data centers which are completely isolated from other regions. · A region consists of more than two availability zones connected to each other through links. AZ-a AZ-b AZ-c Availability zones are connected through redundant and isolated metro fibers. Edge Locations o Edge locations are the endpoints for AWS used for caching content. o Edge locations consist of CloudFront, Amazon's Content Delivery Network (CDN). o Edge locations are more than regions. Currently, there are over 150 edge locations. Edge location is not a region but a small location that AWS have. It is used for caching the content. o Edge locations are mainly located in most of the major cities to distribute the content to end users with reduced latency. o For example, some user accesses your website from Singapore; then this request would be redirected to the edge location closest to Singapore where cached data can be read. Regional Edge Cache o AWS announced a new type of edge location in November 2016, known as a Regional Edge Cache. o Regional Edge cache lies between CloudFront Origin servers and the edge locations. o A regional edge cache has a large cache than an individual edge location. o Data is removed from the cache at the edge location while the data is retained at the Regional Edge Caches. o When the user requests the data, then data is no longer available at the edge location. Therefore, the edge location retrieves the cached data from the Regional edge cache instead of the Origin servers that have high latency. What Is Cloud Computing? Cloud computing is the use of hardware and software components in an off-premises location to deliver a service to a network. Users can access files and applications from any device that can access the internet. Some features and capabilities include: · Cloud providers can pull the computing resources to provide services to multiple customers with the help of a multi-tenant model · Cloud computing proves an on-demand self-service, which helps administrators monitor performance CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 26 N32 nirf 1750 Rank -

--- Page 141 ---
Course Code/Title: CS3V15/Devops Unit: V UNIT V - BUILDING DEVOPS PIPELINES USING AZURE Create Github Account, Create Repository, Create Azure Organization, Create a new pipeline, Build a sample code, Modify azure-pipelines.yaml file Github Account Creation What is Github? GitHub is a code hosting platform for collaboration and version control. GitHub lets you and others work together on projects fromanywhere. Github is owned by Microsoft, provides access to public(free) and private(paid) repositories. Stepsto create Github Account: Join GitHub · GitHub × + < C https://github.com/join f Facebook - Log In o ... Twitter G Google f Facebook YouTub Product v Solu wiki How Step 1:Go to https://github.com/join in a web browser. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 1 175" Rank

--- Page 142 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: Enter your personal details. In addition to creating a username and entering an email address, you'll also have to create a password. Your password must be at least 15 characters in length or at least 8 characters with at least one number and lowercase letter. Join GitHub First, let's create your user account Username * wikihowneveconcepts Email address * Password * .......... Make sure it's at least 15 characters OR at least 8 characters including a number and a lowercase letter. Learn more. Email preferences Send me occasional product updates, announcements, and offers. Verify your account wiki How Step 3: Click Verify to start the verification puzzle. The instructions vary by puzzle, so just follow the on-screen instructions to confirm that you are a human. A green checkmark will appear after completing the puzzle. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 2

--- Page 63 ---
Course Code/Title:CS3V15/Devops Unit: II 10. Introduction of Gradle: The list of features that Gradle provides, · Gradle is available with separate Domain Specific Language (DSL) based on Groovy language. · It provides the declarative language elements. Those elements also provide build-by- convention support for Java, Groovy, OSGI, Web and Scala. Language for dependency based programming The declarative language lies on a top of a general purpose task graph, which can be fully supported in the build. Structure your build Gradle allows you to apply common design principles to your build. It will give you a perfect structure for build, so that, you can design well-structured and easily maintained, comprehensible build. Deep API By using this API, you can monitor and customize its configuration and execution behavior to the core. Gradle scales Gradle can easily increase the productivity, from simple and single project builds to huge enterprise multi-project builds. Multi-project builds Gradle supports the multi-project builds and partial builds. If you build a subproject, Gradle takes care of building all the subprojects, that the subproject depends on. Different ways to manage your builds Gradle supports different strategies to manage your dependencies. Gradle is the first build integration tool Gradle is fully supported for your ANT tasks, Maven and lvy repository infrastructure for publishing and retrieving dependencies. It also provides a converter for turning a Maven pom.xml to Gradle script. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 22 N32 nirf 175º Rank -

--- Page 64 ---
Course Code/Title:CS3V15/Devops Unit: II Ease of migration Gradle can easily adapt to any structure. Therefore, you can always develop your Gradle build in the same branch, where you can build live script. Gradle Wrapper Gradle Wrapper allows you to execute the Gradle builds on machines, where Gradle is not installed. This is useful for continuous integration of servers. Free open source Gradle is an open source project, and licensed under the Apache Software License (ASL). Groovy Gradle's build script are written in Groovy programming language. The whole design of Gradle is oriented towards being used as a language and not as a rigid framework. Groovy allows you to write your own script with some abstractions. The whole Gradle API is fully designed in Groovy language. Installation of Gradle: Prerequisites to install Gradle JDK and Groovy are the prerequisites for Gradle installation. Gradle requires JDK version 6 or later to be installed in the system. It uses the JDK libraries which are installed, and sets to the JAVA_HOME environmental variable. Gradle carries its own Groovy library, therefore, we need not install Groovy explicitly. If it is installed, that is ignored by Gradle. The steps to install Gradle in your system are explained below. Step 1 - Verify JAVA Installation First of all, you need to have Java Software Development Kit (SDK) installed on your system. To verify this, execute Java -version command in any of the platform you are working on. In Windows Execute the following command to verify Java installation. I have installed JDK 1.8 in my system. C:\> java - version CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE . TECHNOLOGY 23 N32 narf 1750 Rank

--- Page 29 ---
Unit: I Course Code/Title:CS3V15/Devops Enterprise services. AWS virtual private cloud. Outsourced cloud services. Salesforce User Controls Operating System, Runtime, Middleware, and Application data Others It is highly scalable and flexible. Microsoft Azure. Force.com, Gigaspaces. Data of the application It is highly scalable to suit the different businesses according to resources. Google cloud services Google offers a seven wide range of Services: · Compute · Networking · Storage and Databases · Big Data · Machine Learning · Identity & Security · Management and Developer Tools IBM cloud analysis. AWS, Terremark Nothing It is highly scalable to suit the small, mid and enterprise level business 1. Compute: GCP provides a scalable range of computing options you can tailor to match your needs. It provides highly customizable virtual machines. and the option to deploy your code directly or via containers. · Google Compute Engine · Google App Engine · Google Kubernetes Engine · Google Cloud Container Registry · Cloud Functions 2. Networking: The Storage domain includes services related to networking, it includes the following services • Google Virtual Private Cloud (VPC) . Google Cloud Load Balancing · Content Delivery Network · What is Google Cloud Connect · Google Cloud DNS · What is Google Cloud Web Hosting 3. Storage and Databases: The Storage domain includes services related to data storage, it includes the following services · Google Cloud Storage . Cloud SQL · Cloud Bigtable · Google Cloud Datastore • Persistent Disk TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 29 N32 nirf 1750 Rank

--- Page 30 ---
Unit: I Course Code/Title:CS3V15/Devops 4. Big Data: The Storage domain includes services related to big data, it includes the following services · Google BigQuery · Google Cloud Dataproc · Google Cloud Datalab · Google Cloud Pub/Sub 5. Cloud AI: The Storage domain includes services related to machine learning, it includes the following services · Cloud Machine Learning · Vision API · Speech API · Natural Language API · Translation API • Jobs API 6. Identity & Security: The Storage domain includes services related to security, it includes the following services · Cloud Resource Manager · Cloud IAM · Cloud Security Scanner · Cloud Platform Security 7. Management Tools: The Storage domain includes services related to monitoring and management, it includes the following services • Stackdriver · Monitoring · Logging · Error Reporting · Trace · Cloud Console 8. Developer Tools: The Storage domain includes services related to development, it includes the following services . Cloud SDK · Deployment Manager · Cloud Source Repositories · Cloud Test Lab AZURE Azure is Microsoft's cloud platform, just like Google has its Google Cloud and Amazon has its Amazon Web Service or AWS.000. Generally, it is a platform through which we can use Microsoft's resources. For example, to set up a huge server, we will require huge investment, effort, physical space, and so on. In such situations, Microsoft Azure comes to our rescue. It will provide us with virtual machines, fast processing of data, analytical and monitoring tools, and so on to make TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 30 N33 narf 1750 Rank

--- Page 131 ---
Course Code/Title:CS3V15/Devops Unit: IV Ensure a service is stopped: $ ansible webservers -m service -a "name=httpd state=stopped" 6. Gathering Facts Fact represents the discovered variables about a system. You can use the facts to implement conditional execution of tasks, and also used to get ad-hoc information about your systems. To see all the facts: $ ansible all -m setup Ansible Playbooks Playbooks are the files where the Ansible code is written. Playbooks are written in YAML format. YAML means "Yet Another Markup Language," so there is not much syntax needed. Playbooks are one of the core features of Ansible and tell Ansible what to execute, and it is used in complex scenarios. They offer increased flexibility. Playbooks contain the steps which the user wants to execute on a particular machine. And playbooks are run sequentially. Playbooks are the building blocks for all the use cases of Ansible. Ansible playbooks tend to be more configuration language than a programming language. Through a playbook, you can designate specific roles to some of the hosts and other roles to other hosts. By doing this, you can orchestrate multiple servers in very different scenarios, all in one playbook. Playbook Structure Each playbook is a collection of one or more plays. Playbooks are structured by using Plays. There can be more than one play inside a playbook. Playbook Play Task Module Target Environment O Task Module Play Task Target Environment Play Module O Task CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY 17 N33 nirf 1750 Rank

--- Page 132 ---
Course Code/Title:CS3V15/Devops Unit:IV name: install and configure DB hosts: testServer become: yes vars: oracle_db_port_value : 1521 tasks: -name: Install the Oracle DB yum: < code to install the DB> -name: Ensure the installed service is enabled and running service: name: < your service name> The Different YAML Tags name This tag specifies the name of the Ansible playbook. As in what this playbook will be doing. Any logical name can be given to the playbook. hosts This tag specifies the lists of hosts or host group against which we want to run the task. The hosts field/tag is mandatory. It tells Ansible on which hosts to run the listed tasks. The tasks can be run on the same machine or on a remote machine. One can run the tasks on multiple machines and hence hosts tag can have a group of hosts' entry as well. vars Vars tag lets you define the variables which you can use in your playbook. Usage is similar to variables in any programming language. tasks All playbooks should contain tasks or a list of tasks to be executed. Tasks are a list of actions one needs to perform. A tasks field contains the name of the task. This works as the help text for the user. It is not mandatory but proves useful in debugging the playbook. Each task internally links to a piece of code called a module. A module that should be executed, and arguments that are required for the module you want to execute. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 18 N33 nirf 1750 Rank

--- Page 151 ---
Azure DevOps Taking you to your Azure DevOps organization ... Congratulations, you're an organization owner! Sign in to your organization at any time, https://dev.azure.com/{yourorganization}. With your organization, the following aspects are included in the free tier: First five users free (Basic license): Azure Pipelines: o One Microsoft-hosted CI/CD (one concurrent job, upto 30 hours per month) o One self-hosted CI/CD concurrent job Azure Boards: Work item tracking and Kanban boards Azure Repos: Unlimited private Git repos Azure Artifacts: Two GB free per organization Build applications with Azure: Azure DevOps enables you to build, test, and deploy anyapplication to any cloud or on premises To configure build pipelines that continuously build, test, and verify your applications. 11

--- Page 152 ---
Part 1: Get started with Azure DevOps Part 2: Build applications with Azure DevOps Part 3: Deploy applications with Azure DevOps Create a build pipeline with Azure Pipelines: Prerequisites: Familiarity with forking and cloning a GitHub repo Account requirements: An Azure DevOps organization o To use Microsoft-hosted agents, your Azure DevOps organization must have access to Microsoft- hosted parallel jobs. Check your parallel jobs and request a free grant. O You can use GitHub Code spaces to complete the module, even if your Azure DevOps organization doesn't have any parallel jobs. A GitHub account Software requirements: If using GitHub Code spaces to complete the module, there are no software requirements as all software is included in the Codespace If using a local development environment withMicrosoft-hosted agents, you must have the following software installed: o Visual Studio Code o .NET 6.0 SDK o Git 12

--- Page 107 ---
Course Code/Title:CS3V15/Devops Unit: III Step 8 : Then configure the Build Steps by selecting the option Copy artifacts from another project as follows - . . First of all give the name of the project from which you want to copy the files. In my case it is App1. . Then select the option "copy from WORKSPACE of latest completed build" · Give the regular expression ** / *.*. Here's a breakdown of what ** / *.* means : o ** : The double asterisk ( ** ) is a wildcard that matches zero or more directories and subdirectories. It's often used to recursively search through all directories and subdirectories. o / : The forward slash (/) is used as a directory separator, typically on Unix-like systems. o *.* : This part of the regular expression is used to match files with an extension. In regular expressions, the * symbol represents "zero or more" of the preceding character or pattern, and the . character is used to match any character. So, *.* essentially matches any file with any extension. So, when you use the regular expression ** / *.* in a Jenkins build configuration, you are instructing Jenkins to include all files with any extension ( *.* ) located in any directory and subdirectory ( ** ) within the workspace or specified location. Build Steps E Copy artifacts from another project Project name ? App: Which build ? Copy from WORKSPACE of latest completed buld Limitation Note ? Artifacts to copy ? Artifacts not to copy ? : Toget directory CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 32

--- Page 108 ---
Course Code/Title:CS3V15/Devops Unit: III Add one more Build Step and select Execute Windows batch Command. Give the command dir so that the name of files and directories that are copied from App1 to App2 will be displayed on the Console. = Execute Windows batch command ? Command See the list of available environment variables ---- dir ----- Advanced v Add build step * Click Apply and Save. Step 8 : Now Build this project (App2). The output will be as follows - Started by user Anuradha P Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\.jenkins\workspace\App2 Copied 16 artifacts from "App1" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621109248227895.bat C:\ProgramData\Jenkins\,jenkins\workspace\App2>dir Volume in drive C Is OS Volume Serial Number is CGC2-BA9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023* 15:07 <DIR> 01-09-2023 15:07 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 474 test.class 144 test. java 2 File(s) 616 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 33 N32 narf 1750 Rank

--- Page 57 ---
Course Code/Title:CS3V15/Devops · Go to the File option · In the drop-down menu, select New · Select the Project option Unit: II If you want to create a Java project, you can select the "Java Project" option. Since we are not creating a Java project specifically, we have chosen the "Project" option. eclipse-workspace - Eclipse File Edit Source Refactor Navigate Search Project Run Window Help New Open File ... G) Open Projects from File System ... Shift+Alt+N Close Close All Shift + Cb1+W Save Save As. Save All Shift+Ctrl+S Revert Move .:. Rename 2 Refresh ES Convert Line Delimiters To Print Import ... Export ... Ctrl+P de java Project ui Project ... El Package G Class Interface G Enum @ Annotation 69 Source Folder 18 Java Working Set Folder File Untitled Text File Task JUnit Test Case Example ... Other ... Ctrl+N The dialog box that appears on the screen will display different types of projects. · Select the Maven Project option . Click on Next New Project + Select a wizard Create a Maven Project Wizards: type filter text irroject Gradle Java 2º Java Project #Java Project from Existing Ant Buildfile - @Maven Lu Check out Maven Projects from SCM M& Maven Module Maven Project Examples ? < Back Next > X Cancel Finish A dialog box will appear. Select the default workspace. . Click on "Next" Several Group IDs, Artifact IDs, and Versions will then appear. · Select a plugin there and click on "Next" CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 16 N33 narf 1750 Rank

--- Page 58 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Select an Archetype M Catalog: All Catalogs Configure .. Biter: × Group Id Artifact id Version org.apache.maven.archetypes maven-archetype-archetype 1.0 org.apache.maven.archetypes maven-archetype-|2ee-simple 1.0 org.apache.maven archetypes maven-archetype-plugin 1.2 org.apache.maven.archetypes maven-archetype-plugin-site 1.1 org.apache.maven.archetypes maven-archetype-portlet 1.0.1 org.apache.maven.archetypes maven-archetype-profiles 1.0-alpha-4 org.apache.maven.archetypes maven-archetype-quickstart 1.1 org.apache.maven.archetypes maven-archetype-site 11 org anacha mauan archabenes masan.archchina.cito.cimnia 11 An archetype which contains a sample Maven project. Show the last version of Archetype only Include snapshot archetypes Add Archetype ... > Advanced ? < Back Next > Cancel Finish In the next dialog box that appears, you'll complete the following steps: · Enter the Group ID “com.xyz" · Enter the Artifact ID "mavenproject" · The version will appear on the screen These items can all be modified at a later time if needed. · Click on "Finish" CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 17 N33 nirf 1750 Rank

--- Page 81 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Credentials: Add credentials if required. ○ Branch Specifier: Specify the branch (e.g., */main). 7. Build Triggers: ○ Build Periodically: Use CRON syntax to schedule builds. ○ Poll SCM: Jenkins will check the repository for changes at specified intervals. ○ Other Triggers: Configure other triggers such as GitHub hooks, upstream projects, etc. 8. Build Environment: o Configure the build environment settings like setting environment variables, running scripts before the build, etc. 9. Build Steps: ○ Click on "Add Build Step" and choose the appropriate build step (e.g., "Execute Shell", "Invoke Ant", "Invoke Gradle script"). ○ Example: For executing a shell script, add the necessary shell commands. 10. Post-build Actions: ○ Click on "Add post-build action" and choose the appropriate action (e.g., "Archive the artifacts", "Publish JUnit test result report"). ○ Configure the settings for each post-build action. 11. Save the Configuration: ○ Once you have configured all the necessary settings, click "Save" at the bottom of the page. 12. Run the Job: ○ On the job's main page, click "Build Now" to run the job immediately. 13. Monitor the Job: ○ Click on the build number in the "Build History" to view the detailed output and logs of the build process. ○ Check the console output for any errors or warnings. Practical Example Setting up a Freestyle Project for a Maven Build: 1. Create New Item: ○ Name: MyMavenProject o Type: Freestyle project 2. Source Code Management: ○ Git: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 82 ---
Unit: III Course Code/Title:CS3V15/Devops I Repository URL: https://github.com/example/my-maven-project.git · Branch Specifier: * /main 3. Build Triggers: ○ Poll SCM: H/15 * (poll every 15 minutes) 4. Build Steps: ○ Invoke Top-Level Maven Targets: Goals: clean install 5. Post-build Actions: ○ Publish JUnit test result report: I Test report XMLs: * /target/surefire-reports/ *. xml 6. Save and Build: ○ Click "Save". ○ Click "Build Now" on the job's main page. By following these steps, you can set up a Jenkins job to automate your build process for various types of projects. Jenkins provides extensive customization options, allowing you to tailor the job to meet your specific requirements. Configuring a Jenkins Job 1. Install Jenkins First, make sure Jenkins is installed. You can download it from the official Jenkins website. 2. Access the Jenkins Dashboard Open your web browser and go to http://your-server-ip: 8080 to access the Jenkins dashboard. 3. Create a New Job 1. Start a New Job: ○ Click on "New Item" on the left-hand menu. 2. Name the Job: ○ Enter a name for your job in the "Enter an item name" field. 3. Select Job Type: ○ Choose the type of job (e.g., Freestyle project, Pipeline). CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) - TECHNOLOGY N32 nirf 7 1750 Rank

--- Page 133 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible Roles Roles provide a framework for fully independent or interdependent collections of files, tasks, templates, variables, and modules. The role is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks and makes them easier to reuse. The breaking of the playbook allows you to break the playbook into reusable components. Each role is limited to a particular functionality or desired output, with all the necessary steps to provide that result either within the same role itself or in other roles listed as dependencies. Roles are not playbooks. Roles are small functionality that can be used within the playbooks independently. Roles have no specific setting for which hosts the role will apply. Top-level playbooks are the bridge holding the hosts from your inventory file to roles that should be applied to those hosts. Creating a Role The directory structure for roles is essential to creating a new role, such as: Role Structure The roles have a structured layout on the file system. You can change the default structured of the roles as well. For example, let us stick to the default structure of the roles. Each role is a directory tree in itself. So the role name is the directory name within the /roles directory. $ ansible-galaxy -h Usage ansible-galaxy[delete|import|info|init|install|list|login|remove|search|setup][ -- help] [options] ... Options o -h: (help) it shows this help message and exit. o -v: (verbose) Verbose mode (-vvv for more, -vvvv to enable connection debugging). o -- version: it shows program version number and exit. Roles are stored in separate directories and have a particular directory structure CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 19 N33 nirf 1750 Rank

--- Page 134 ---
Course Code/Title:CS3V15/Devops Unit: IV Typical structure of ansible role myweb defaults L main.yml files · handlers L main.yml meta. L main.yml README.md tasks L_ main.yml templates tests inventory test.yml vars L main.yml · defaults : It stores the default variable for the role. For example the default port number is http is 8080, then it can be stored in defaults. · files : This folder contains the files required to transfer or deploy to the target machines. · handlers : It contains handlers, which may be used by some role. · meta : It defines some data or information about the role. · tasks : It contains the main list of tasks to be executed by the role. These tasks might be defined in separate files as per the functionalities. · templates : It contains the templates which can be deployed via a role. Ansible Module Ansible Command module is used to execute commands on a remote node. The Command module, is used mostly to run simple Linux commands on a remote node/server which is part of a host group or Stand alone server mentioned in the host group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 79 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Plugins: ○ Role: Extend Jenkins functionality without modifying the core software. ○ Types: I Source Control Plugins: Git, SVN, Mercurial. Build Tools Plugins: Maven, Gradle, Ant. Notification Plugins: Email, Slack, HipChat. · Reporting Plugins: JUnit, Cobertura, Checkstyle. ○ Management: Plugins can be installed, updated, and configured via the Jenkins web interface. 5. Master-Slave Architecture: ○ Concept: The Jenkins master distributes build tasks to multiple agents, enabling distributed builds. ○ Benefits: Scalability: Handle more builds simultaneously. I Resource Management: Assign builds to agents with the necessary resources. Isolation: Run builds in isolated environments to avoid conflicts. Workflow and Data Flow 1. Job Trigger: ○ Jobs can be triggered by various events: Manual Trigger: Initiated by a user through the Jenkins interface. Scheduled Trigger: Using CRON-like syntax to schedule jobs. · SCM Trigger: Automatically triggered by changes in the source code repository. Upstream/Downstream Trigger: Triggered by the completion of other jobs. 2. Build Execution: ○ Job Assignment: The master assigns the job to an available agent based on labels, availability, and resource requirements. Build Environment Setup: The agent sets up the environment, including checking out the code, installing dependencies, and configuring the workspace. ○ Execution: The agent runs the build steps as defined in the job configuration. 3. Build Results: ○ Logs: Captures console output and logs from the build process. ○ Artifacts: Stores build artifacts like binaries, packages, and reports. ○ Test Results: Collects and displays test results and code coverage reports. ○ Build History: Maintains a history of all builds, including status, duration, and changes. 4. Notification and Reporting: ○ Notifications: Jenkins can send notifications through various channels (email, chat, etc.) upon build completion or failure. ○ Dashboards: Provides dashboards for visualizing build status, trends, and metrics. ○ Reports: Generates and displays reports on test results, code quality, and other metrics. INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 4 1750 Rank

--- Page 80 ---
Unit: III Course Code/Title:CS3V15/Devops Summary Jenkins' architecture is designed to be flexible and scalable. The master-agent model allows for efficient distribution of build tasks, while plugins provide extensive customization options. By leveraging Jenkins' comprehensive job configuration and robust notification and reporting capabilities, teams can streamline their CI/CD pipelines and improve software quality and delivery speed. Creating a Jenkins Job Creating a Jenkins job involves setting up a new project within Jenkins to automate various tasks such as building code, running tests, and deploying applications. Here is a step-by-step guide to creating a Jenkins job. Prerequisites · Jenkins installed and running. · Basic understanding of the Jenkins interface. . Access to the Jenkins dashboard. Step-by-Step Guide 1. Access Jenkins Dashboard: . Open your web browser and navigate to your Jenkins instance URL (e.g., http://localhost:8080). 2. Create a New Job: ○ On the Jenkins dashboard, click on the "New Item" link on the left-hand side menu. 3. Enter Job Name: ○ Enter a name for your new job in the "Enter an item name" field. o Choose the type of job you want to create. For most uses, "Freestyle project" is a good starting point. ○ Click "OK". 4. Configure the Job: ○ After clicking "OK", you will be directed to the job configuration page. 5. General Configuration: ○ Description: Provide a brief description of the job. ○ Discard Old Builds: Optionally set this to limit the number of old builds Jenkins keeps. ○ Select Version Control System: Choose the version control system (e.g., Git, Subversion). 6. Source Code Management: ○ Repository URL: Enter the repository URL. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 5 nirf 175 Rank N33

--- Page 67 ---
Course Code/Title:CS3V15/Devops Unit: II Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013 JVM: 1.7.0_60 (Oracle Corporation 24.60-b09) OS: Windows 8.1 6.3 amd64 Understanding build using Gradle: The Gradle build is a process of creating a Gradle project. When we run a gradle command, it will look for a file called build.gradle in the current directory. This file is also called the Gradle build script. The build configuration, tasks, and plugins are described in this file. The build script describes a project and its tasks. Let's create a small Gradle project, run some of the basic Gradle commands, and understand how Gradle manages the project. Follow the steps below to create and test a Gradle project. Step1: Open the command line and create a directory First, Open the command line and create a directory for the project and change directory to it. Let's create a demo directory. C: \Users \HiMaNshU>mkdir demo C: \Users \HiMaNshU>cd demo Step2: Initialize a Gradle project To generate a Gradle project, run the gradle init command. It will generate a simple project. With this project, we will explore and understand everything that is generated. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 26 N33 nirf 1750 Rank

--- Page 68 ---
Course Code/Title:CS3V15/Devops Unit: II When we run the gradle init command, it will ask for some basic requirements. First, it will ask the type of project that we want to create. It will give four options: 1. 1: basic 2. 2: application 3. 3: library 4. 4: Gradle plugin Select our requirements. Hence, we are just making a demo project so that we will select the basic option. To select basic option, press 1 and Enter key. Consider the below output: C: \Users \HiMaNshU\demo>gradle init Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could ed, use -- status for details Select type of project to generate: 1: basic 2: application 3: library 4: Gradle plugin Enter selection (default: basic) [1 .. 4] 1 Next, it will ask for DSL. There are two options that are available for DSL: 1. 1: Groovy 2. 2: Kotlin Groovy is the default DSL provided by Gradle. Select build script DSL. Select build script DSL: 1: Groovy 2: Kotlin Enter selection (default: Groovy) [1 .. 2] 1 Next, it will ask for the project name. Type the project name and press Enter key. It will take a while to build a project. After the successful execution of the project, we will get a message BUILD SUCCESSFUL. Project name (default: demo): First_Gradle > Task : init Get more help with your project: https: //guides. gradle.org/creating- uilds BUILD SUCCESSFUL in 2m 56s 2 actionable tasks: 2 executed C: \Users\HiMaNshU\demo>mkdir src Now we have successfully created a Gradle project. Now, what will happen to our specified directory? Consider the below structure of the Gradle project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 27 N33 narf 1750 Rank

--- Page 103 ---
Course Code/Title:CS3V15/Devops Unit: III =Publish HTML reports ? Reports HTML directory to archive ? E:\Eclipse_workplace\SeleniumDemo\test-output Index page[s] ? index.html Index page title[s] (Optional) Report title ? HTML Report Publishing options Step 11 : Click on Apply and then Save. Now build this project. Step 12 : On successful build, it creates the HTML Report as follows - x + € > C @ localhost:0000/juby/HTML/\portDemo/HTML_20Report/ Back to HTMLBepsatDemo Test results All suites Default suite 11 D myPackage.myTestPage Info . Repetir output Results . Thus using HTML Publish Report Plugin we can successfully generate HTML report of some testing framework application. · Troubleshooting : Sometimes, the raw file is displayed as HTML report. To get the HTML formatted report we must click on Manage Jenkins section, scroll down and locate Script Console section. Click on it. Tools and Actions C Broad Configuration Sium Disk Discard all the loaded alla lo memory and reloud Jerking CLI Script Console Ewecubes arbitrary suyt lor and run below given command : System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", ") . Then restart the Jenkins Page and Build the project once again. The HTML report can then be displayed as given in above step. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 28 N32 nirf 1750 Rank

--- Page 104 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Copy Artifact Overview: The Copy Artifact Plugin allows Jenkins jobs to copy build artifacts from other jobs, facilitating artifact reuse across different jobs. Key Features: · Artifact Copying: Copies files from one build to another, which can be from a specific build or the latest successful build. · Triggering Builds: Can be used in downstream jobs to fetch artifacts from upstream jobs. Configuration: 1. Install the Plugin: Go to Manage Jenkins > Manage Plugins > Available tab, search for "Copy Artifact Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Build section. ○ Add a Copy artifacts from another project build step. o Specify the project name, build number or criteria, and the target directory for copied artifacts. Usage: · Commonly used in multi-job pipelines where artifacts need to be shared between different jobs or stages. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 29 N3 nirf 1750 Rank

--- Page 3 ---
Course Code/Title:CS3V15/Devops Unit: I Key Goals and Benefits of DevOps Goals of DevOps The fast-paced growth of the IT industry and continuous advancements in technology make it critical to set DevOps goals that are experimental and challenging for companies to compete and thrive in the market. Here are the key goals and principles that every successful DevOps program has in common. 1. Ensures effective collaboration between teams: Effective collaboration in any process relies on shared ownership. During the development process, all those involved should embrace the fact that everyone is equally responsible for the entire development process. Whether it is development, testing, or deployment, each team member should be involved. They should understand that they have an equal stake in the final outcome. In the DevOps paradigm, passing of work from one team to another is completely defined and broken down. This accelerates the entire process of development since collaboration between all the teams involved is streamlined. 2. Creates scalable infrastructure platforms: The primary focus of DevOps is to create a sustainable infrastructure for applications that make them highly scalable. According to the demands of the modern-day business world, scalable apps have become an absolute necessity. In an ideal situation, the process of scaling should be reliable and fully automated. As a result, the app will have the ability to adapt to any situation when a marketing effort goes viral. With the app being scalable, it can adjust itself to large traffic volumes and provide an immaculate user experience. 3. Builds on-demand release capabilities: Companies must focus on keeping their software in a 'releasable' state. Continuous delivery will allow the software to add new features and go live at any stage. DevOps aims to automate the process of release management because it has a plethora of advantages. Automated release management is predictable, fast, and very consistent. Moreover, through automation, companies can release new versions as per their requirements. Automated release management also has complete and thorough audit trials, as these are essential for compliance purposes. 4. Provides faster feedback: Automating monotonous tasks such as testing and reporting will accelerate the process of rapid feedback. Since the development team will know what has to change, it can roll out the updated version faster. In addition, the team can better understand the impact of the changes that it has done in the software lifecycle. A concrete understanding of changes will assist team members in working efficiently in tandem. With rapid feedback, the operations team and developers can make better decisions collectively and enhance the app's performance. Benefits of DevOps DevOps helps organizations deliver added value to their customers. Here are some compelling benefits of DevOps. 1. Smarter work and faster release: With DevOps, your development team can release the required deliverables quickly. Faster release of deliverables will keep you miles ahead of your competitors, which is very important in today's cut-throat business realm. Businesses should understand that if their review cycle is not automated, it will slow down the release process. Moreover, the inclusion of disparate tools will lead to context switching and higher costs. Thus, DevOps can help rectify this worrisome business situation. 2. Quick resolution of issues: In a business world where speed and accuracy are paramount, a fast feedback loop will help you thrive. With DevOps, the communication process becomes seamless, and, as such, it minimizes the time required to solve issues. Without open communication, key issues can slip out of mind, which will have serious repercussions in the long run. DevOps fosters open communication that helps resolve issues, thus unblocking the release pipeline faster. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 3 N32 nirf 1750 Rank

--- Page 4 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Better collaboration between teams: DevOps paves the way for more dynamic and round-the- clock communication between teams. It renders an environment for mutual collaboration and integration among teams that are distributed globally. Eliminating the traditional departmental barriers between teams forms a new sense of ownership, wherein each team member feels equally responsible for meeting delivery timelines. This collaboration contributes to happier and more engaged employees. 4. Fostering innovative mindsets: With DevOps, deployment phases of the application are more relaxed as compared to traditional methods. This is because it streamlines the entire process, ensures that there are no lapses in quality, and allows on-time and efficient release. Thus, as everything is in order, the development team is more at peace. This allows it to think out of the box and provide additional value to the user. Having a development team with an innovative mindset is a boon for any business organization. An innovative approach, in itself, has immense scope and leads to better quality and resolution of issues at hand. Thus, through DevOps, the process of expanding the horizon of an app becomes much easier. 5. Faster threat detection: Automated and continuous testing of the code will make the process of threat detection faster. As developers can locate problem areas at an early stage, they can then resolve them faster. Thus, DevOps is a vital cog in maintaining and enhancing the quality and performance of an app. As the overall build of the app is in capable hands, teams working together are empowered to share feedback as and when necessary. 6. Increased customer satisfaction: Customer satisfaction is paramount in any day and age, irrespective of the business one is involved in. DevOps is known for enhancing customer experience, which ultimately increases the level of customer satisfaction. Dissatisfied customers are never a good sign for any business. Feedback loops are an important component of DevOps. These loops empower end users to track the progress of app development at various stages. 7. In addition, they can suggest changes (if any) or give their inputs to make the app more customer- centric. Due to their dynamic nature, feedback loops help developers and customers remain on the same page. Moreover, DevOps accelerates the process of app development, which eventually lessens the delivery timer. This has a positive impact on the customer satisfaction ratio. 8. Providing the much-needed edge: Along with staying true to their development process, companies need to ensure that they sustain themselves in the cut-throat competition. Implementing DevOps can be your trump card to provide your organization with that much- needed edge. Competitive advantage is necessary, as it can become the deciding factor in the popularity of an application in many cases. Some factors set expert businesses apart from mediocre ones: · Top-quality features · Quicker and timely software releases · Maximizing return on investments · Listening to constructive feedback Difference between Agile and DevOps Agile: Agile program advancement comprises different approaches to computer program improvement beneath which prerequisites and arrangements advance through the collaborative exertion of self- organizing and cross-functional groups and their customer/end client. DevOps: DevOps could be a set of hones that combines program improvement and information- technology operations which points to abbreviating the framework's advancement life cycle and giving nonstop conveyance with tall program quality. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 4 N32 nirf 1750 Rank

--- Page 145 ---
Course Code/Title: CS3V15/Devops Unit: V Step 7:Note the types of plans offered by GitHub. There are a few different plans to choose from, varying in the amount of features provided. Learn to ship software like a pro. GitHub gives students free access to the best developer tools so they can learn by doing. Free Get additional student benefits GitHub Pro Unlimited public/private repositories 2,000 CI/CD minutes/month Free for public repositories 500MB of Packages storage Free for public repositories Protect your branches Ensure that collaborators on your repository cannot make irrevocable chang branches. O Draft pull requests O 120 core-hours of Codespaces compute O Pages and Wikis 15GB of Codespaces storage O Community support ® 3,000 CI/CD minutes/month Free for public repositories 2GB of Packages storage Free for public repositories O 180 core-hours of Codespaces compute 20GB of Codespaces storage Web-based support wikiHow Step 8: Select the free plan. On the plan selection page, scrolldown to click the button for choosing a free plan. This will immediately take you to your GitHub dashboard. Continue for free wiki How UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 5 N33 In 175º Rank ---

--- Page 146 ---
Course Code/Title: CS3V15/Devops Unit: V · If you choose a paid plan, you'll have to enter yourpayment information as requested before you can continue. . If you want to upgrade your Github account in the future, click the menu at the top- right corner, select Settings, andchoose Billing and plans to view your options. GitHub essentials are: · Repositories · Branches · Commits · Pull Requests · Git (the version control software GitHub is built on) Repository: A GitHub repository can be used to store a developmentproject. It can contain folders and any type of files (HTML, CSS,Javascript, Documents, Data, Images). A GitHub repository should also include a licence file and a README file about the project. A GitHub repository can also be used to store ideas, or any resources that you want to share. Github Repository Creation Step 1: Click on the new repository option Marketplace Explore New repository and GitHub without any code! Catch Universe Join a community Universe in San Fr ticket pricing-he Import repository × New gist bird New organization New project llo World guide, you'll create a repository, start a , write comments, and open a pull request. GitHub Sponsors Matching Fund × Ready to support open source? GitHub will match your contribution to developers during their first year in GitHub Sponsors. d the guide Start a project × Welcome to the new dashboard. Get closer to the stuff you care about most. Explore repositories atlas-engineer/next Next browser - Be productive. UnENVIVAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 175° Rank 6

--- Page 165 ---
Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. Customize CI triggers Pipeline triggers cause a pipeline to run. You can use trigger: to cause a pipeline to run whenever you push an update to a branch. YAML pipelines are configured by default with a CI trigger on your default branch (which is usually main). You can set up triggers for specific branches or for pull request validation. For a pull request validation trigger, just replace the trigger: step with pr: as shown in the two examples below. By default, the pipeline runs for each pull request change. . If you'd like to set up triggers, add either of the following snippets at the beginning of your azure-pipelines.yml file. YAML I Copy trigger: - main - releases/* YAML Copy pr: - main - releases/* You can specify the full name of the branch (for example, main) or a prefix-matching · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. Build using multiple versions Add steps You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can use tasks for building, testing, publishing, or deploying your app. For Java, the Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. YAML Copy - task: PublishCodeCoverageResults@1 inputs: codeCoverageTool: "JaCoco" summaryFileLocation: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco/ reportDirectory: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco" failIfCoverageEmpty: true · Select Save and then confirm the changes. . You can view your test and code coverage results by selecting your build and going to the Test and Coverage tabs.

--- Page 166 ---
Build using multiple versions Rename/move pipeline Name ScheduledTriggerTest 1234 Select folder 1 .. . Cancel Save Pipeline settings X Processing of new run requests Enabled Paused Disabled YAML file path azure-pipelines.yml Automatically link work items included in this run Cancel Save YAML trigger: main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdkVersion: "1.10" jdk11_windows: imageName: "windows-latest" jdkversion: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdkVersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: **** /TEST -*. xml" goals: "package"

--- Page 19 ---
Course Code/Title:CS3V15/Devops Unit: I 5. Compliance: By compliance, we mean that certain businesses are required to follow specific regulations. Financial services companies in the United States, for example, must comply with the Sarbanes-Oxley Act, while healthcare, education, and energy companies must comply with HIPAA and other regulations. AWS provides a number of compliance-related features and services, such as data encryption and identity and access management, to help businesses meet these requirements. 6. High-Performance: Interested in delivering your applications quickly and efficiently? Taking advantage of AWS features such as auto-scaling and load balancing will help ensure your applications are always available and running optimally. AWS can help businesses improve their performance by offering a variety of cloud-based services, including Amazon Elastic Compute Cloud (EC2), which provides high-performance computing resources, and Amazon CloudFront, which delivers content quickly and securely to users around the world. Others include machine learning (ML) and analytics services, such as Amazon SageMaker and Amazon Athena. These services provide the tools businesses need to quickly and easily analyze their data for insights. Fast networking in the cloud is also possible with AWS, thanks to its Elastic Load Balancing (ELB) and Amazon Virtual Private Cloud (VPC). With ELB, businesses can balance their workloads across multiple instances for increased performance, while VPC allows businesses to create isolated private networks in the cloud. 7. Developer Tools: Developer tools are designed to make it easier for developers to create, deploy, and manage applications - and AWS provides developers with what they need to build applications quickly and easily. By leveraging developer tools, developers can save time and money by automating tedious tasks. They also benefit from access to AWS's extensive library of pre-built applications that can help them get their projects off the ground quickly. Services such as Amazon Elastic Beanstalk and Amazon CloudFormation can help them automate the process of creating and deploying applications. Other ways developers can improve productivity with AWS include using AWS Code Commit to store and manage source code. 8. Integration: Thanks to its many integrations with other Amazon services, as well as third-party services, AWS makes it easy for businesses to get started with cloud computing. AWS provides a wide range of services that can be easily integrated into existing business infrastructure. This allows businesses to add new features and capabilities without having to make major changes or invest in new hardware or software. For instance, if a business wants to add mobile capabilities to its website, it can take advantage of Amazon's Mobile SDK and Web Services. These tools allow businesses to quickly develop and deploy mobile apps that connect directly with their existing infrastructure. 9. Management Console: The AWS management console is a web-based interface that provides users with a simple way to interact with and manage their AWS resources - essentially a place where you can access and manage everything on the cloud. It provides a graphical view of all the resources associated with an account, as well as tools for creating and configuring new resources. Compared to traditional command- line interfaces, the AWS management console saves time and makes it easier for users to get the most out of their AWS services. Not only that, but your business gets access to 350+ free digital training courses through the AWS Academy, covering topics such as cloud fundamentals, DevOps, security, and big data. This means you can train your employees on how to use AWS, and in turn, help them become more efficient at their jobs. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 19 N32 nirf 1750 Rank

--- Page 20 ---
Course Code/Title:CS3V15/Devops Unit: I 10. Scalability: With an on-demand service, businesses can quickly spin up new servers as needed with just a few clicks. This makes it much easier to scale resources up or down as demand changes, allowing businesses to save costs and maintain performance even during peak periods. For example, if a business is expecting a sudden surge in traffic due to an advertising campaign or seasonal event, they can easily add more capacity to their server infrastructure to handle the increased load. Bru Textiles, a specialty textile company in Belgium, was able to quickly scale its infrastructure by leveraging AWS. Bru Textiles went digital to grow and offer new services. Embracing technology, they brought in digital twin technology to give their customers an idea of the texture and essence of their physical fabrics. AWS Applications · Storage and Backup: Storage and backup are important for any Cloud Computing service. AWS provides you with reliable storage services like Amazon Simple Storage Service to store large- scale data and backup services like AWS Backup to take backups of this data, which is stored in other AWS services. AWS stores the data in three different availability zones so that if one fails, you can still access your data. This makes AWS storage reliable and easily accessible. Therefore, companies with huge application data to store and backup securely can use AWS. · Big Data: One of the biggest challenges faced by companies these days is Big Data. The companies are struggling to store their large amounts of data using traditional methods. With AWS Big Data storage services, they can manage to store their data even if the data limit increases unexpectedly as AWS provides virtually unlimited data storage with scale-in and scale- out options. AWS offers easy access and faster data retrieval as well. For data processing, it offers services like EMR, with which the companies can easily set up, operate, and scale their big data. Therefore, efficiently storing and managing Big Data is among the top AWS applications. · Enterprise IT: AWS is a one-stop solution for any IT business. Many features of it such as secure storage, scalability, flexibility, and elasticity support companies to innovate faster than ever before. Using AWS for IT enterprises makes them profitable in terms of both money and time. As AWS maintains its cloud architecture, it need not waste time and money on professionals to do the same. · Social Networking: Social networking is essential for businesses in the present-day scenario where Digital Marketing is key, and it is easier with AWS. Companies can connect with customers and stakeholders and communicate through social networking sites and develop their business. Services like AWS social networking engine, which is powered by Turn Key GNU/Linux (HVM) AMI stack, are used for performance and scalability to help companies build a suitable social networking site and gain profits. · Mobile Apps: Mobile applications are embedded with day-to-day life. With AWS, you have the facility to create an app in your desired programming language. You can also keep up the applications that are consistently accessible and solid with high compute, storage, database, and application services. You can take advantage of AWS auto-scaling and managed relational database service for the better performance of your apps. · Websites: AWS offers a wide range of website hosting options to create the best website for customers. Its services like Amazon Light sail have everything, such as a virtual machine, SSD- based storage, data transfer, DNS management, and a static IP, to launch a website in such a way that the user can manage the website easily. Amazon EC2, AWS Lambda, Elastic Load Balancing, AWS Amplify, Amazon S3, etc. also help users build reliable and scalable websites. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 41 ---
Unit: I Course Code/Title:CS3V15/Devops S.No. Git 1 Git is a software. 2 Git is a command-line tool GitHub is a graphical user interface 3 Git is installed locally on the system GitHub is hosted on the web 4 Git is maintained by linux. GitHub is maintained by Microsoft. 5 Git is focused on version control and code sharing. GitHub is a hosting service for Git repositories. 6 Git is a version control system to manage source code history. 7 Git was first released in 2005. GHub was launched in 2008. 8 Git has no user management feature. 9 Git is open-source licensed. GitHub includes a free-tier and pay- for- use tier. 10 Git has minimal external tool configuration. 11 Git provides a Desktop interface named Git Gui. GitHub provides a Desktop interface named GitHub Desktop. 12 Git competes with CVS, Azure DevOps Server, Subversion, Mercurial, etc. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY GitHub GitHub is a service. GitHub is focused on centralized source code hosting. GitHub has a built-in user management feature. GitHub has an active marketplace for tool integration. GitHub competes with GitLab, Bit Bucket, AWS Code Commit, etc. 41 N32 nirf 1750 Rank

--- Page 42 ---
Course Code/Title:CS2V15/Devops Unit: II UNIT - II COMPILE AND BUILD USING MAVEN AND GRADLE Introduction, Installation of Maven, POM files, Maven Build lifecycle, Build phases(compile build, test, package) Maven Profiles, Maven repositories(local, central, global),Maven plugins, Maven create and build Artifacts, Dependency management, Installation of Gradle, Understand build using Gradle. 1. Introduction of Maven: Apache Maven is an automation tool. The tool is written in Java. It was initially released on 13 July 2004. It is developed by the Apache software foundation. It is part of the Jakarta Project. It is working on two aspects: how software is built, and its dependencies. It was created by Jason van Zyl. It is built by using a plugin-based architecture that allows it to make the use of any application controllable by standard input. It dynamically downloads Java libraries. Understanding the problem without Maven There are many problems that we face during the project development. They are discussed below: 1) Adding set of Jars in each project: In case of struts, spring, hibernate frameworks, we need to add set of jar files in each project. It must include all the dependencies of jars also. 2) Creating the right project structure: We must create the right project structure in servlet, struts etc, otherwise it will not be executed. 3) Building and Deploying the project: We must have to build and deploy the project so that it may work. Maven simplifies the above mentioned problems. It does mainly following tasks. 1. It makes a project easy to build 2. It provides uniform build process (maven project can be shared by all the maven projects) 3. It provides project information (log document, cross referenced sources, mailing list, dependency list, unit test reports etc.) 4. It is easy to migrate for new features of Maven Apache Maven helps to manage o Builds Documentation o Reporing o SCMs o Releases o Distribution What is Build Tool? A build tool is used for building a process. It does following: CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE TECHNOLOGY N32 nirf 1 1750 Rank

--- Page 61 ---
Course Code/Title:CS3V15/Devops Unit: II Return to the previous screen and undo the deletion. The errors that occurred will disappear. *mavenproject/pom.xml & IApp.java AppTest.java 6 <artifactId>mavenproject</artifactId> 7 <version>0.0.1-SNAPSHOT</version> <packaging>jar</packaging> 8 9 16 <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 138 <properties> 14 15 16 17% <dependencies> 18 19 <groupId>junit</groupId> I 28 21 22 23 24 <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> cdependency> <artifactId>junit</artifactid> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> 25 </project> 26 9. Dependency Management: The dependencyManagement and dependencies are especially useful for multi-module projects. Dependency Management : This tag consists of a dependencies tag which itself might contain multiple dependency tags. Each dependency is supposed to have at least three maintags: groupId, artifactId, and version. For example: <dependencyManagement> <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 20 N33 nirf 1750 Rank -

--- Page 62 ---
Course Code/Title:CS3V15/Devops Unit: II The above code just declares the new artifact commons-lang3, but it doesn't really add it to the project dependency resource list. Dependencies: This tag contains a list of dependency tags. Each dependency is supposed to have at least two main tags, which are groupId and artifactId. For example: <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> The version and scope tags can be inherited implicitly if we have used the dependencyManagement tag before in the POM file. DependencyManagement is just a declaration, and it does not really add a dependency. Dependencies tag adds the actual dependency to the project. An example for adding the JUnit library dependency: <dependencyManagement> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.2</version> <scope>test</scope> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 21 N33 nirf 175º Rank

--- Page 161 ---
Add a status badge to your repository Many developers like to show that they're keeping their code quality high by displaying a status badge in their repo. Azure Pipelines succeeded To copy the status badge to your clipboard: 1. In Azure Pipelines, go to the Pipelines page to view the list of pipelines.Select the pipeline you created in the previous section. 2. Select , and then select Status badge. 3. Select Status badge. 4. Copy the sample Markdown from the Sample markdown section. Now with the badge Markdown in your clipboard, take the following steps in GitHub: 1. Go to the list of files and select Readme.md. Select the pencil icon to edit. 2. Paste the status badge Markdown at the beginning of the file. 3. Commit the change to the main branch. 4. Notice that the status badge appears in the description of your repository. To configure anonymous access to badges for private projects: 1. Navigate to Project Settings in the bottom left corner of the page 2. Open the Settings tab under Pipelines 3. Toggle the Disable anonymous access to badges slider under General 21

--- Page 162 ---
Azure-Pipeline • Azure Pipelineș provides a YAML pipeline editor that you can use to author and edit your pipelines. • The YAML editor is based on the Monaco Editor. • The editor provides tools like Intellisense support anda task assistant to provide guidance while you edit a pipeline. • You can also edit pipelines by modifying the azure pipelines.yml file directly in your pipeline's repository using a text editor of your choice Edit a YAML pipeline To access the YAML pipeline editor, do the following steps. • Sign in to your organization • (https://dev.azure.com/{yourorganization}). • Select your project, choose Pipelines, and then select the pipeline you want to edit. You can browse pipelines by Recent, All, and Runs. • Choose Edit. • Make edits to your pipeline using Intellisense and the task assistant forguidance. Azure DevOps fabrikam-tailspin FabrikamFiber F FabrikamFiber + Pipelines Overview Recent All Runs Boards Pipelines Recently run pipelines Repos 1 Pipeline Pipelines Pipelines FabrikamFiber 2 3 17

--- Page 87 ---
Course Code/Title:CS3V15/Devops Unit: III 2. Go to Plugin Manager: From the dashboard, click on 'Manage Jenkins' on the left-hand side menu. On the Manage Jenkins page, click on 'Manage Plugins'. This will take you to the Plugin Manager, where you can view, install, and update plugins. 3. Exploring Available Plugins 1. Available Tab: The 'Available' tab lists all plugins that are available for installation. This includes thousands of plugins categorized by functionality (e.g., SCM, build tools, user interface enhancements). 2. Search for Plugins: Use the search box to quickly find the plugin you need. For example, if you're looking to integrate Jenkins with Git, type 'Git' in the search box. 3. Popular Plugins: Jenkins highlights popular plugins at the top of the Available tab. These are commonly used plugins that most users find essential. 4. Installing Plugins 1. Selecting Plugins to Install: Check the box next to each plugin you want to install. You can select multiple plugins at once. 2. Install Without Restart: After selecting your plugins, scroll down and click 'Install without restart'. Jenkins will install the plugins immediately, and you can continue using Jenkins during the installation. 3. Install After Restart: If you prefer, you can choose to 'Download now and install after restart'. This will install the plugins after Jenkins is restarted, ensuring that all changes are applied without interference. 4. Installation Progress: The installation process will show a progress bar for each plugin. Once a plugin is installed, it will move to the 'Installed' tab. 5. Configuring Installed Plugins 1. Plugin Configuration: After installation, some plugins may require additional configuration. Navigate to 'Manage Jenkins' > 'Configure System' or 'Configure Global Security' to adjust settings for your new plugins. 2. Global Tool Configuration: For build tools like Maven or JDKs installed via plugins, you can configure them under 'Global Tool Configuration'. Here, you define the paths to the tools or specify installation options. 6. Updating and Managing Plugins 1. Updating Plugins: Regularly update your plugins to ensure compatibility with the latest version of Jenkins and to benefit from new features or security patches. Go to the 'Updates' tab in the Plugin CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N32 nirf 12 1750 Rank

--- Page 88 ---
Unit: III Course Code/Title:CS3V15/Devops Manager. Jenkins will show you all plugins with available updates. Click 'Update' to install the latest versions. 2. Uninstalling Plugins: If a plugin is no longer needed, you can uninstall it from the 'Installed' tab. Select the plugin and click 'Uninstall'. Uninstallation typically requires a Jenkins restart to complete. 7. Commonly Used Plugins Here are a few essential plugins that you might consider adding to your Jenkins setup: 1. Git Plugin: Integrates Jenkins with Git, allowing you to pull code from GitHub, Bitbucket, or other Git repositories. 2. Pipeline Plugin: Enables the creation and management of Jenkins pipelines, a powerful way to define complex build, test, and deployment processes. 3. Maven Integration Plugin: Adds support for Maven projects, allowing you to build and manage Maven-based projects directly within Jenkins. 4. Email Extension Plugin: Provides advanced email notification options for build statuses, including customizable email templates. 5. Blue Ocean: An alternative user interface for Jenkins that simplifies pipeline creation and offers a modern, user-friendly design. 8. Troubleshooting Plugin Issues 1. Compatibility Issues: Sometimes, plugins might conflict with each other or with the Jenkins version. In such cases, refer to the plugin's documentation or Jenkins logs to diagnose the issue. 2. Rollback: If a plugin update causes problems, you can rollback to a previous version from the 'Installed' tab by selecting the desired version. 3. Restart Jenkins: Some plugin changes require a Jenkins restart. Ensure all jobs are completed before restarting to avoid any interruptions. Commonly Used Jenkins Plugins 1. Git Plugin Overview: The Git Plugin integrates Jenkins with Git repositories, allowing Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nårf 1750 Rank

--- Page 105 ---
Course Code/Title:CS3V15/Devops Unit: III How to install copy artifact plugin ? Step 1 : Start the Jenkins tool by entering the login and password. Step 2 : Click on Manage Jenkins. Click on Plugins. Step 3 : In the search window, type the name copy Artifact. Step 4 : The plugin name will be displayed if it is not already installed. Select it. Name Adde a build atop to copy artifacts from another project. Thủa plugin le up for adoption! We are looking for naw maintainent. Velt our Adopt a Plugin intuitive for mort information. then click on Install. The plugin gets installed. Example Demo · Let us discuss how to use Copy Artifact plugin. Step 1 : Create a new Job by clicking on item New Item. Give the some name to your job. I have given the name App1 and selected the Freestyle Project. Step 2: Give suitable description in the description box. Step 3 : Under the source code management, click on Git option and specify the repository URL of GitHub. Source Code Management None Git ? Repositories 1 Repository URL + ·hong · CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 30 N3 nirf 1750 Rank

--- Page 106 ---
Unit: III Course Code/Title:CS3V15/Devops Step 4 : Under the Build Steps section, choose the Execute Windows batch command option. And type the javac and java commands. Build Steps = Execute Windows batch command Command See the list of available environment variables javac test.java java test Advanced Add bulld stop * Step 5 : Click in Apply and Save. Step 6 : Select the Build Now option. On successful built, the test, class file gets generated in the Jenkins workspace of the App1 job. C:\ProgramData\Jenkins\.jenkins\workspace\Appl>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\Appi>java test Good Morning Pathill Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! C:\ProgramData\Jenkins\.jenkins\workspace\Appl>exit @ Finished: SUCCESS Step 7 : Now we have to copy test.class file from the workspace of Appl to another job. For that' purpose we will create another Job named App2 by clicking New Item. Select the Freestyle Project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 31 N3 nirf 1750 Rank

--- Page 139 ---
Course Code/Title:CS3V15/Devops Unit: IV Create the file, if it does NOT exist (or) absent $ ansible testservers -a "touch /tmp/testfile creates=/tmp/testfile" -i ansible_hosts As Playbook - name: "Validate if a file is present or not present using Ansible Command module" hosts: testservers tasks: - name: "Create a file if it does not exist" command: "touch /tmp/latestfile" args: creates: "/tmp/latestfile" register: createif - name: "Display the file to make sure its created" command: "ls -lrt /tmp/latestfile" register: displayif when: createif is changed - debug: var=displayif.stdout - name: "Remove the file if it exist" command: "rm -rf /tmp/latestfile" args: removes: "/tmp/latestfile" register: removeif In the playbook we execute three tasks, • First one is a create a file if it does not exist • Second to display if the creation is successful • Third one is to delete the file if it exists Example 5: Execute or Run the Script when a file exists or not exists Now for this example let us take something relatable to real world scenario. Like Start the Server instance (or) program if the PID file or LOCK file does not exist Here is the sample playbook for you to get started. I am giving a general playbook here as I do not want to pick some server/technology of my own which you cannot relate to. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 140 ---
Course Code/Title:CS3V15/Devops Unit:IV you can feel free to modify the script to your needs as this is just a template. - name: Start of Stop Server instance based on PID/LOCK file availability hosts: appservers tasks: - name: Start the instance when the PID file is not present become: yes become_user: appuser command: "startserver.sh" args: creates: "/path/to/pid/instance.pid" register: startinst - name: Stop the instance when the lock file is present become: yes become_user: appuser command: "stopserver.sh" args: removes: "/path/to/lockfile/instance.lck" register: stopinst CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 26 N33 nirf 1750 Rank

--- Page 147 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: After clicking new repository option, we will have to initialize some things like, naming our project, choosing the visibility etc. After performing these steps click Create Repository button. Create a new repository A repository contains all project files, including the revision history. Already have a project repository elsewhere? Import a repository. Owner Repository name * / Namanbhatia7 - Resume This is going to be name of our project Great repository names are short and memorable. Need inspiration? How about animated-memory? Description (optional) Public Anyone Keep this as public selected ository. You choose who can commit. see this rer Private You choose who can see and commit to this repository. We can add a project description if we want. Skip this step if you're importing an existing repository. Initialize this repository with a README This will let you immediately clone the repository to your comp Tick the README option Add .gitignore: None - Add a license: None ~ Create repository After performing above steps, Click this button Step 3: After clicking the button, we will be directed to below page. Right now the only file we have is a readme file. Namanbhatia7 / Resume o Unwatch - 1 * Star 0 Y Fork 0 <> Code Issues 0 (*) Pull requests 0 ||| Projects 0 BB Wiki Security the Insights # Settings No description, website, or topics provided. Edit Manage topics T 1 commit įº 1 branch > 0 releases 2 1 contributor Branch: master New pull request Create new file Upload files Find File Clone or download TT Namanbhatia7 Initial commit Latest commit 676ac98 now README.md Initial commit now DO README.md Resume UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 7 175" Rank

--- Page 148 ---
Course Code/Title: CS3V15/Devops Unit: V Step 4: Now click on the "Upload files" button. 4 > C https://github.com/avicnotes/html-css-projects ... Search or jump to ... Click to End Session Total: $80.83 Pull requests Issues Marketplace Explore Google" Custom Search @ avicnotes / html-css-projects Unwatch - 1 Star 0 Fork 0 <> Code Issues 1'] Pull requests Actions Projects Wiki O Security ~ Insights ... go main - ¿º 1 branch 0 tags Go to file Add file - { Code About avicnotes screenshots Create new file Upload files 2 24 commits No description, website, or topics provided. README.md Update README.md 2 days ago Readme Screenshot (10).png screenshots 2 days ago Screenshot (11).png screenshots 2 days ago Releases Screenshot (12).png screenshots 2 days ago No releases published Create a new release 19 Screenshot (6).png screenshots 2 days ago accordion.JPG Add files via upload 4 days ago Packages background-dots.png Add files via upload 2 days ago clone-project-1.html Updated navbar items to my original ones 2 days ago index.html Create index.html 's ago No packages published Publish your first package Activate Windows Go to Settings to activate Windows, Environments 1 O Type here to search O W. 11:32 AM 2/1/2021 Step 5:Follow the steps mentioned and click "commitchanges". Drag additional files here to add them to your repository Or choose your files Display_Picture.png Simply drag or add files to be uploaded × My_Resume.html X Styles.css × Commit changes Add initial resume website files Add an optional ended des Give this particular version a name -o- Commit directly to the master branch. Ensure this option is checked !] Create a new branch for this commit and start a pull request. Learn more about pull requests. Commit changes Cancel UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 8

--- Page 1 ---
Course Code/Title:CS3V15/Devops CHENNAI INSTITUTE OF TECHNOLOGY Transforming Lives CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) NAME OF THE PROGRAMME B. E (CSE) YEAR III SEMESTER V REGULATIONS 2022R COURSE CODE CS3V15 COURSE NAME DEVOPS FACULTY NAME (Prepared by) NAME OF SUBJECT EXPERT (Verified by) M.SUNDHARI CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Unit: I N33 INATIONAL BOARD . ACCREDITATION 100% Accreditation All eligible UG & PG Programs NIRF 151 - 200 Band Engineering 2023 A+ DE NAAC Contact 8681927167 Contact 1 N32 nirf 1750 Rank

--- Page 2 ---
Course Code/Title:CS3V15/Devops Unit: I UNIT - I Introduction to Devops Devops Essentials - Introduction To AWS, GCP, Azure - Version control systems: Git and Github. The word "DevOps" was coined in 2009 by Patrick Debois, who became one of its gurus. The term was formed by combining "development" and "operations," which provides a starting point for understanding exactly what people typically mean when they say "DevOps." Notably, DevOps isn't a process or a technology or a standard. Many devotees refer to DevOps as a "culture"-a viewpoint that New Relic favors. We also use the term "DevOps movement" when talking about topics such as adoption rates and trends for the future, and "DevOps environment" to refer to an IT organization that has adopted a DevOps culture. "DevOps represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile, lean practices in the context of a system-oriented approach. DevOps emphasizes people (and culture), and seeks to improve collaboration between operations and development teams. DevOps implementations utilize technology- especially automation tools that can leverage an increasingly programmable and dynamic infrastructure from a life cycle perspective." DevOps is defined as a combination of processes and tools created to facilitate organizations in delivering services and applications much faster than they can through conventional software development processes. It helps increase customers' confidence in the applications that an organization offers, thereby allowing the company to flourish and achieve its business goals faster. Development 3 3 DevOps 13 51 Operations Quality Assurance (QA) How DevOps Works? A DevOps process can be summarized as an infinite loop that comprises the following stages - build, test, and release through the delivery pipeline and plan and monitor through feedback, which resets the loop again. With such an amazing combination, teams use tech stack and tooling that assists them in reliably developing apps. Moreover, going away from the Under the DevOps model, development and operations teams work in constant cohesion throughout the entire project lifecycle, starting right from development to deployment. When security is the main focus, the quality assurance team is tightly knitted with the DevOps team throughout the app lifecycle. In this situation, some DevOps teams are also referred to as DevSecOps. Close coordination with the QA team ensures that no loopholes are left unchecked in the provided service/app. COMPANY HOW DEVOPS WORKS DELIVERY PIPELINE BUILD TEST RELEASE PLAN MONITOR FEEDBACK LOOP TOOLBOX" CUSTOMER norm, teams use automated processes here. DevOps tools also allow engineers to complete different tasks independently. Be it provisioning infrastructure or deploying code, they can accomplish these tasks without being dependent on one another. As such, the DevOps model accelerates the overall application development process. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 45 ---
Course Code/Title:CS3V15/Devops Unit: II Example for pom.xml file <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> 4. Maven Repository A maven repository is a directory of packaged JAR file with pom.xml file. Maven searches for dependencies in the repositories. There are 3 types of maven repository: 1. Local Repository 2. Central Repository 3. Remote Repository Maven searches for the dependencies in the following order: Local repository then Central repository then Remote repository. Local Central Remote Repository Repository Repository CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 4 N32 nirf 1750 Rank -

--- Page 46 ---
Course Code/Title:CS3V15/Devops Unit: II If dependency is not found in these repositories, maven stops processing and throws an error. 1) Maven Local Repository Maven local repository is located in your local system. It is created by the maven when you run any maven command. By default, maven local repository is %USER_HOME%/.m2 directory. For example: C:\Users\SSS IT\.m2. Update location of Local Repository We can change the location of maven local repository by changing the settings.xml file. It is located in MAVEN_HOME/conf/settings.xml, for example: E:\apache-maven- 3.1.1\conf\settings.xml. 2) Maven Central Repository Maven central repository is located on the web. It has been created by the apache maven community itself. The path of central repository is: http://repo1.maven.org/maven2/. The central repository contains a lot of common libraries that can be viewed by this url http://search.maven.org/#browse. 3) Maven Remote Repository Maven remote repository is located on the web. Most of libraries can be missing from the central repository such as JBoss library etc, so we need to define remote repository in pom.xml file. Let's see the code to add the jUnit library in pom.xml file. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 5 N32 narf 1750 Rank -

--- Page 97 ---
Course Code/Title:CS3V15/Devops Unit: III Build Steps Execute Windows batch command ? Command See the list of available environment variables javac welcome. java java welcome Advanced = Execute Windows batch command 7 Command See the list of avalable chuironment variables acho User name is Nuernane% Advanced Step 5 : Click on Apply and then Save. Step 6 : Select Build with parameters option. Dashboard > welcome > Status Changes Workspace Build with Parameters Configure Delete Project 0 Rename CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 22 N3 nirf 1750 Rank

--- Page 98 ---
Course Code/Title:CS3V15/Devops Unit: III Project welcome This build requires parameters: Username Parth D Build Cancel We can change the username as well. Suppose I changed the Username as Anand then click the Build, we get the console output as follows - C: \ProgramData\Jenkins\.jenkins\workspace\welcome>javac welcome.java C:\ProgramData\Jenkins\.jenkins\workspace\welcomeyjava welcome Welcome to first Job of Jenkins [welcome] $ cmd /c call C:\WINDOWS\TEMP\jenkins16582464065687933173.bat C:\ProgramData\Jenkins\.jenkins\workspace\welcome>echo User name is Anand User name is Anand C:\ProgramData\Jenkins\.jenkins\workspace\welcome>exit 0 Finished: SUCCESS Thus we set variety of parameters to the Jenkins job. We can set the Boolean parameter, choice parameter, credentials, file parameter, password parameter and so on. 3. HTML Publisher Filter Plain Boolean Parameter Choice Parameter Credentials Parameter File Parameter Multi-line String Parameter Password Parameter Run Parameter String Parameter Add Parameter Overview: The HTML Publisher Plugin allows Jenkins to publish HTML reports and artifacts generated during the build process. Key Features: · Report Publishing: Publishes HTML reports, dashboards, or other HTML artifacts. · Report Directory: Allows specifying the directory containing HTML reports. · Index Page: Sets a default HTML file to be displayed when accessing the report. Configuration: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 23 nirf 1750 Rank N33

--- Page 23 ---
Course Code/Title:CS3V15/Devops Unit: I application. Amazon EC2 enables you to increase or decrease capacity within minutes. You can use one or hundreds or even thousands of server instances simultaneously. Because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs. Amazon EC2 is integrated with most AWS services, such as Amazon Simple Storage Service (Amazon S3), Amazon Relational Database Service (Amazon RDS), and Amazon Virtual Private Cloud (Amazon VPC) to provide a complete, secure solution for computing applications. Amazon EC2 is an example of Infrastructure as a Service(IaaS). EC2 delivers secure, reliable, cost-effective compute and high-performance compute infrastructure so as to meet the needs of demanding businesses. Amazon EC2 is one of the easiest ways of providing servers on AWS Cloud and also the access to Operating system. · AWS Lambda: AWS Lambda is a serverless, event-driven compute service that allows you to run code without managing servers. You pay only for the compute time you consume and there is no charge when your code is not running. With AWS Lambda, you can run code for any type of application with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services, or you can call it directly from any web or mobile app. But triggering Lambda is possible with over 200 AWS services. You can only pay for what you have used. The compute time that you consume, you are needed to pay for it. You just only need to upload your code and everything required to run will take care of by Lambda and it automatically scales your code with high availability. · AWS Elastic Beanstalk: AWS Elastic Beanstalk is a Platform as a Service that facilitates quick deployment of your applications by providing all the application services that you need for your application. Beanstalk is a plug- and-play platform that allows working with multiple programming languages and environments. Elastic Beanstalk supports a large range of platforms like Node js, Java, PHP, Python, and Ruby. So, you can develop your application to meet your requirements and simply deploy it on Elastic Beanstalk. The main aim to use AWS Elastic Beanstalk is to allow you to focus on the deployment and management of your applications. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. 2. Networking · Amazon VPC: Amazon VPC is your network environment in the cloud. It allows you to create a private network within the AWS cloud that uses many of the same concepts and constructs as an on-premises network. Amazon VPC also gives you complete control of the network configuration. Customers can define normal networking configuration items such as IP address ranges, subnet creation, route table creation, network gateways, and security settings. Amazon VPC is an AWS foundational service and integrates with numerous AWS services. For instance, Amazon EC2 instances are deployed into your CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 24 ---
Course Code/Title:CS3V15/Devops Unit: I Amazon VPC. Similarly, Amazon Relational Database Service (Amazon RDS) database instances deploy into your Amazon VPC, where the database is protected by the structure of the network just like your on-premises network. You can easily launch AWS resources into a virtual network by Amazon Virtual Private Cloud. An isolated virtual network environment in the AWS cloud is created by Amazon VPC. . Amazon Route 53: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating human-readable names, such as www.geeksforgeeks.com, into the numeric IP addresses that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. 2. Storage · Amazon S3 (Simple Storage Service): Amazon Simple Storage Service (Amazon S3) is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the web. It is designed to provide an infinite amount of storage and it is delivered with 99.999999999% durability. You can use Amazon S3 as primary storage for cloud-native applications as a target for backup and recovery and disaster recovery. It offers industry-leading scalability, data availability, security, and performance. It's simple to move large volumes of data into or out of Amazon S3 with Amazon's cloud data migration options. Once data is stored in Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like Amazon S3 Standard - Infrequent Access and Amazon Glacier for archiving. · Amazon Glacier: Amazon Glacier is a secure, durable, and extremely low- cost storage service for data archiving and long-term backup. Data stored in Amazon Glacier takes several hours to retrieve, which is why it's ideal for archiving. The fastest access to your archive data is via Amazon Glacier. 3. Databases · Amazon RDS (Relational Database Service): Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost- efficient and resizable capacity while managing time-consuming database administration tasks, freeing you up to focus on your applications and business. You can find Amazon RDS is also available on several database instance types - optimized for memory, performance, or I/O. Amazon RDS provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. · Amazon DynamoDB (Non-Relational Database): Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. When you create a database table that can store and retrieve any amount of data you can simply use Amazon DynamoDB that will serve any level of requested traffic. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, Internet of Things (IoT), and many other applications. DynamoDB provides many features like · built-in security CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 24 N32 nirf 1750 Rank

--- Page 93 ---
Course Code/Title:CS3V15/Devops Unit: III And configure the Jenkins job as follows - General Description In this job we have used GitHub plugin Plain text Preview Discard old builds ? GitHub project This project is parameterized ? Throttle builds Execute concurrent builds if necessary Advanced V Source Code Management None Git ? Repositories ? Repository URL https://github.com/AnurdhaP/MyJavaPrograms.gil Please enter Git repository. Credentials ? - none - Add Advanced CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 18 N3 nirf 1750 Rank

--- Page 94 ---
Course Code/Title:CS3V15/Devops Unit: III Build Triggers Trigger builds remotely (e.g. from scripts) ? Build after other projects are built ? Build periodically ? GitHub hook trigger for GITSem polling ? Poll SCM ? Schedule ? A Do you really mean "every minute" when you say "" "" " ""? Perhaps you meant "H" "" "" to poll once per hour Would last have run at Wednesday, 30 August, 2023 at 4:11:02 pm india Standard Time; would'nest run at Wednesday, 30.A Ignore post-commit hooks ? Build Steps Execute Windows batch command ? Command See the list of available environment variables javas test.java java test. java ----- Advanced V Add build sten Save this configuration. Step 5 : Now if we build this job the output that we see is as follows on Console Output. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 19 N3 nirf 1750 Rank

--- Page 59 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Specify Archetype parameters M Group Id: com.simplileam Artifact Id: mavenproject Version: 0.0.1-SNAPSHOT Package: com,simplilearn.mavenproject . Properties available from archetype: Name Value Add ... Remove Advanced ? < Back Next > Cancel Finish The project is now created. · Open the pom.xml file You can see all the basic information that you have entered on the screen, such as the Artifact ID, Group ID, etc. You can see the junit dependencies have been added. This process takes place by default in Eclipse. There will also be some by default test cases. File Edit Source Navigate Search Project Run Window Help # Package Explorer E Jo JUnit - mavenproject/pom.xml 28 mavenproject > ( src/main/java ( src/test/java > = JRE System Library (|2SE-1:5] i Maven Dependencies target 7 <artifactIdaavenproject</artifactId> <version>0. 0. 1-SNAPSHOT</version>: <packaging>jar</packaging> <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 136 sproperties> 14 <project. build.sourceEncoding>UTF-8</project. build.sourceEncoding> </properties> 15 16 «dependenciess 18- <dependency> 19 <groupId>junit</grouptasI <artifactId=junit</artifactle> sversion>3.8.1</version> <scope>tests/scope> 21 22 223 </dependency> 24 </dependencies> 25 </project> Overview Dependencies Dependency Hierarchy |Effective POM pom.xml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 18 N32 nirf 175º Rank -

--- Page 60 ---
Course Code/Title:CS3V15/Devops Unit: II There you can find AppTest.java to be a default test case. When you click on that, you can see the test cases written in JUnit on your Eclipse screen. # Package Explorer 13 Ju JUnit mavenproject/pom.xml Z App.java AppTest.java 12 1 package com.simplilearn.mavenproject; - fl mavenproject + ( src/main/java - il com.simplileam.mavenproject U App.java = ( src/test/java il com.simplileam.mavenproject , E AppTest.java > > JRE System Library ||2SE-1.5] al Maven Dependencies - target pom.xml 3w import jumit. framework. Test:[] 6 76 / ** . Unit test for simple App. 10 public class AppTest extends TestCase 12 1 14 . Create the test case 15 16 * @param testName name of the test cole 17 18% public AppTest( String testNane ) 19 1 super( testName ); 28 21 1 22 :23% 24 * @return the suite of tests being tested 75 When it comes to adding more test cases, it will depend on the user, but these test cases and commands can easily be added in the workspace. If we try to remove certain dependencies from our file, we will receive error messages. To troubleshoot this, complete the following steps: · Go to another tab: mavenproject/pom.xml · Delete any dependencies · Save the file Immediately, there will be several error messages in the AppTest.java. R mavenproject/pom.xml App.java AppTest.java 83 1 package com.simplilearn.mavenproject; 2 3@ import junit. framework. Test;[] 6 mavenproject/src/test/java/com/simplileam/mavenproject/AppTest java 8 * Unit test for simple App. 9 18 public class AppTest extends TestCase 011 12 { 136 14 . Create the test case 15 16 * @param testName name of the test case 17 188 19 € public AppTest( String testName ) 28 super( testName ); 21 22 - 230 24 * @return the suite of tests being tested Problems E @ Javadoc ) Declaration History 7 errors, 1 warning, 0 others Description + @ Errors (7 items) a Test cannot be resolved to a type Resource Path Location AppTest.java /mavenproject/src/te line 26 Java Problem Type CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 19 N32 nirf 1750 Rank -

--- Page 77 ---
Unit: III Course Code/Title:CS3V15/Devops 5. Customize Jenkins: ○ Install suggested plugins or select the required plugins manually. ○ Create the first admin user. Configuration 1. Global Configuration: ○ Navigate to Manage Jenkins > Configure System. ○ Set up global environment variables, email notifications, and other system-wide settings. 2. Security Configuration: ○ Configure security settings under Manage Jenkins > Configure Global Security. o Use the built-in user database or integrate with external authentication mechanisms like LDAP. 3. Node Configuration: ○ Jenkins can distribute build loads to multiple nodes. ○ Configure nodes by navigating to Manage Jenkins > Manage Nodes and Clouds. 4. Job Configuration: ○ Create new jobs by clicking New Item on the Jenkins dashboard. o Configure job-specific settings such as source code management, build triggers, and post-build actions. 5. Pipeline Configuration: ○ Jenkins supports pipeline as code using Jenkinsfile. ○ Define your pipeline stages in a Jenkinsfile placed in the root of your project repository. 6. Plugins: ○ Enhance Jenkins functionality by installing plugins from the Manage Jenkins > Manage Plugins section. 7. Backup and Restore: ○ Regularly back up the jenkins_home directory, which contains Jenkins configurations, job configurations, and build history. ○ Use plugins like ThinBackup for automated backups. Ref: https://youtube.com/playlist?list=PL6flErFppaj35spJjPy41-IruDjw2kRV-&si=Vzd9AAMv53xwWB S Ref: https://bit.ly/youtube_jenkins (click here) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 78 ---
Course Code/Title:CS3V15/Devops Unit: III Jenkins Architecture Overview Jenkins is a robust automation server widely used for continuous integration and continuous delivery (CI/CD) processes. Here's a detailed breakdown of the Jenkins architecture and its core components. Core Components of Jenkins Architecture 1. Jenkins Master: ○ Role: The Jenkins master is the central control unit. 0 Functions: Scheduling Jobs: Assigns build jobs to the appropriate agents. Monitoring Agents: Keeps track of the state and health of agents. I Job Execution: Executes build jobs directly if no agents are available. I User Interface: Provides a web-based interface for users to configure jobs, view build results, and manage the Jenkins environment. 2. Build Agents (Nodes): ○ Role: Agents are responsible for executing the build jobs assigned by the master. ○ Types: Agents can run on different operating systems and hardware configurations. ○ Configuration: Static Agents: Permanently assigned to the Jenkins master. . Dynamic Agents: Created and destroyed as needed, often used in cloud environments. ○ Communication: Agents communicate with the master using the Jenkins Remoting protocol. 3. Job Configuration: ○ Types of Jobs: I Freestyle Projects: Basic job type with simple configurations. Pipeline Jobs: Scripted or declarative pipelines that define the entire build process as code. I Multi-Configuration (Matrix) Projects: Allows testing across different environments and configurations. ○ Components: I Source Code Management: Integration with version control systems like Git, SVN, etc. . Build Triggers: Conditions that start the job, such as code commits, scheduled times, or manual triggers. Build Steps: Actions performed during the build, such as compiling code, running tests, and packaging artifacts. . Post-Build Actions: Steps executed after the build, like deploying artifacts, sending notifications, or archiving results. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 135 ---
Course Code/Title:CS3V15/Devops Unit: IV Syntax for writing the module on command line While writing the module we use the options such as -m and -a. The -m is for module and -a is for argument, after the -a option we specify the executable command in double quotes. here 'all' stands for all the remote hosts, otherwise we can mention the group name of the hosts 1 $ ansible all -m command -a "echo 'Hello hosts'" The -m option followed by module name 'command' Command to be executed on remote hosts, given in double quotes Commonly used modules 1. Command : Following is a command module that executes a command on remote hosts $ ansible all -m command -a "echo 'Hello hosts'" 2. User : The user module is used to create, modify or delete users. For example - To create a user named 'new_user' on all the hosts in the 'dev' group using ansible, you can use the user module. Here's the ansible command to do that : $ ansible dev -m user -a "name=new_user state=present" Sometimes the above command fails as there is no privilege to create user on remote host. Then we can issue the command as follows $ ansible all -m user -a "name=new_user state=present" -- become -- ask-become-pass The above command asks for the password for the remote host to login. On supplying the password the user will be created. We can verify. if new user is getting created or not on remote host by issuing following command on each remote host $ less /etc/passwd If we want to create new user without asking for password on all the remote hosts then we can issue the command as - $ ansible all -m user -a "name=new_user1 state=present" -- become -b The -b option is used to become superuser. By this the sudo or su privileges can be granted. .3. Ping : The ping module is used to connectivity to remote hosts $ ansible all -m ping 4. Copy : The copy module is used to copy files from control machine to remote hosts $ ansible dev -m copy -a "src=/home/ansible_user/myfile.txt dest=/home/ansible_user/myfile.txt" The above command will copy the myfile.txt file from control node to all the managed nodes of dev group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 21 N32 nirf 1750 Rank

--- Page 136 ---
Course Code/Title:CS3V15/Devops Unit: IV 5. Package : The package module is used to install, remove or upgrade the packages, For example, the following command installs the nginx web server package : $ ansible package -a "name=nginx state=present" Here name indicates the name of the package to be installed, the state=present means install the package. Normally "present" is for installation, "remove" is for removing the package and "latest" is used to upgrade the package. 6. Shell : The shell module is used to run shell commands with more flexibility on remote hosts. For example - Following shell command executes on remote hosts. $ ansible dev -m shell -a "cmd=is" 7. Service : The service module is used to start, stop or restart a specific service on remote hosts. For example - Following service will start the apache web service on all the remote hosts. $ ansible dev -m service -a "name=apache2 state=started" Note that the state can started for starting the service, stopped for stopping the service. o started/stopped are idempotent actions that will not run commands unless necessary. o restarted will always bounce the service. o reloaded will always reload. I Example 1: Get the Uptime of remote servers We have used command module to run the uptime command and we have given both the ad hoc and the playbook form of execution. as AD-HOC Command $ ansible testservers -m command -a uptime -i ansible_hosts as Playbook - name: Check the remote host uptime hosts: testservers tasks: - name: Execute the Uptime command over Command module register: uptimeoutput command: "uptime" - debug: var: uptimeoutput.stdout_lines CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 22 N32 nirf 1750 Rank

--- Page 101 ---
Course Code/Title:CS3V15/Devops Unit: III v => SeleniumDemo src/main/java > src/main/resources > src/test/java > 15 src/test/resources > al JRE System Library [J2SE-1.5] › al Maven Dependencies » drivers v src > @ main v fs test v java v 2> myPackage 10 myTestPage.java 23 resources › › target > > test-output 2 pom.xml > Servers myTestPage.java package myPackage; import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver; import org.testng.annotations.Test; public class myTestPage { @Test public void myFun() { System.setProperty("webdriver.chrome.driver","./drivers/chromedriver.exe"); WebDriver driver = new ChromeDriver(); driver.get("https://google.com"); String title = driver.getTitle(); System.out.println("Hey Anuradha, the tile of this page is: "+title); driver.quit(); } Program explanation : In above program, we have written one test case by the function myFun. In this function, using the Chromedriver which we have installed, we open the web page google.com Then we try to get the tile of this page and display it on the console using System.out statement. Finally we quit the driver. Step 7 : Right click on the Project name in Project Explorer window, Run As-> Maven Test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 26 N3 nirf 1750 Rank

--- Page 102 ---
Unit: III Course Code/Title:CS3V15/Devops Step 8 : Now, locate the Java file myTestPage.java and right click Run As->TestNG Test. It is as follows - 0 Run As # Debug As Profile As Web Services > > 1 TestNG Test > Run Configurations ... > Alt+Shift+X, N Step 9 : We will get the output as follows - Hey Anuradha, the tile of this page is: Google PASSED: myFun Default test Tests run: 1, Failures: 0, Skips: 0 Default suite Total tests run: 1, Failures: 0, Skips: 0 Step 10 : Now, we will open the Jenkin's page and create the HTML Publisher report for the above SeleniumDemo project. The prerequisite to this activity is that the HTML Publisher report plugin must be installed in Jenkins. · Click on New Item, Enter the Item Name and select as Freestyle project, I have given the name HTMLReportDemo. · Then click on configure. Write the suitable description in General section. · The in Build Steps and Add Build steps as Invoke top-level Maven targets provide the Maven version, and under goals write clean test. . Under Post Build Actions, click on Add Post Build Action and select Publish HTML Report. Now give the path of index.html file which is created by Selenium testNG framework in Maven. It is the path of test-output folder. Also give some suitable HTML report title. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 27 N3 nirf 1750 Rank

--- Page 65 ---
Course Code/Title:CS3V15/Devops Unit: II Output The output is as follows - java version "1.8.0 66" Java(TM) SE Runtime Environment (build 1.8.0_66-b18) Java HotSpot(TM) 64-Bit Server VM (build 25.66-b18, mixed mode) Step 2 - Download Gradle Build File Download the latest https://gradle.org/install/. version of Gradle from the link available at Step 3 - Set Up Environment for Gradle Setting up environment means, we have to extract the distribution file and copy the library files into proper location. Set up GRADLE_HOME and PATH environmental variables. This step is platform dependent. In Windows Extract the downloaded zip file named gradle-2.11-all.zip and copy the distribution files from Downloads\gradle-2.11\ to C:\gradlel location. After that, add the C: \gradle and C: \gradlelbin directories to the GRADLE_HOME and PATH system variables. Follow the given instructions - Right Click On My Computers -> Click On Properties -> Advanced System Settings -> Click On Environmental Variables. There you will find a dialog box for creating and editing system variables. Click on new button for creating GRADLE_HOME variable (follow the left side screenshot). Click on Edit for editing the existing Path system variable (follow the right side screenshot). Follow the below given screenshots. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 24 N33 nirf 1750 Rank

--- Page 66 ---
Course Code/Title:CS3V15/Devops System Properties X Computer Name Hardware Advanced System Protection Remote Environment Variables x Unit: II System Properties × Computer Name |Hardware Advanced System Protection Remote Environment Variables × New System Variable × Variable name: GRADLE_HOME| Variable value: C:\grade Cancel System variables Variable Value ComSpec C:\Windows\system32\and.exe FP_NO_HOST_C ... NO GRADLE_HOME E:\work\22- Orientdb \gradle-2.11 Variable name: Path Variable value: #C:\gradle bin - System variables Variable Value Edit System Variable × OK Cancel < ORIENTDB_HOME E:\work\22- Orientdblorientdb-communi ... " JAVA_HOME C:\Program Files\Java\jdk1.7.0_60 New ... Edt ... Delete OK Cancel Step 4 - Verify the Gradle installation In windows os Windows_NT Path C:\oraclexe \app \prade\product\11.2.0\, .. PATHEXT .COM ;. EXE ;. BAT ;. CMD ;. VBS ;. VBE ;. JS ;.... v New ... Edit ... Delete OK Cancel You can execute the following command in command prompt. C:\> gradle -v Output Here you will find the Gradle version. Gradle 2.11 Build time: 2016-02-08 07:59:16 UTC Build number: none Revision: 584db1c7c90bdd1de1d1c4c51271c665bfcba978 Groovy: 2.4.4 CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 25 N33 nirf 1750 Rank

--- Page 95 ---
Course Code/Title:CS3V15/Devops Unit: III C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha C: \ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>exit e Finished: SUCCESS Step 6 : Now go to GitHub repository and make some changes to your test.java file and commit these changes. BE AnurdhaP Changes in the test java Code Blame 6 lines (6 loc) · 158 Bytes 89 Code 55% faster with GitHub Copilot > public class test { 2 public static void main(String args[]) { for(int 1=1;i <= 5;1++) 3 4 System.out.println("Good Morning Path !!! "); 5 3 6 } Step 7 : Now, if you check on the Jenkins page, it automatically executes the above updated test.java file and creates a build. It is as follows .- C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test. java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Good Morning Path !!! Good Morning Pathi !! Good Morning Path !!! Good Morning Path !!! Good Morning Pathill 2. Parameter Plugin Overview: The Parameter Plugin allows Jenkins jobs to accept parameters at build time, enabling dynamic and flexible builds. Key Features: · Parameter Types: Supports various parameter types such as string, choice, boolean, and more. · Default Values: Provides default values for parameters. · Prompt for Parameters: Users are prompted to enter values for parameters when starting a build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 20

--- Page 96 ---
Course Code/Title:CS3V15/Devops Unit: III Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Parameterized Builds," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add parameters of different types (e.g., string, choice) and configure their options and default values. Usage: · Useful for creating jobs that require user input or need to be customized for different build scenarios. A Example Demo Step 1 : Click on New item and create a job. I have created a job by the name Welcome (Refer section 3.5) Step 2 : Click on the existing job and configure it with parameters. Step 3 : Check the option "This project is parameterized". Select the String Parameter. Create some string parameter. I have created a parameter named Username and default value to it. This project is parameterized ? =String Parameter ? Name ? Username Default Value ? Parth Description ? Plain text Preview Trim the string ? Step 4 : Under the Build Steps section, using echo command I tried to display the parameter value CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 21

--- Page 155 ---
10. When you're ready to make changes to your pipeline, select it in the Pipelines page, and then Edit the azure-pipelines.yml file. View and manage your pipelines You can view and manage your pipelines by choosing Pipelines from the left-hand menu to go to the pipelines landing page. Azure DevOps : FabrikamFiber 1 Pipelines 0 Search F FabrikamFiber + Pipelines New pipeline Overview Recent All Runs Y Filter pipelines Boards Repos Pipelines Pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... La 32m ago & Manually triggered 8º main & 42s Environments Releases FabrikamFiber #20191209.3 · Set up CI with Az ... La 1h ago & Manually triggered & main 1m 13s Library Task groups *** Deployment groups From the pipelines landing page you can view pipelines and pipeline runs, create and import pipelines, manage security, and drill down into pipeline and run details. Choose Recent to view recently run pipelines (the default view), or choose All to view all pipelines. Pipelines New pipeline : Recent All Runs Filter pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... & Manually triggered & main Là 36m ago ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Az ... Là 1h ago 1m 13s & Manually triggered & main 15

--- Page 156 ---
Select a pipeline to manage that pipeline and view the runs. Select the build number for the last run to view the results of that build, select the branch name toview the branch for that run, or select the context menu to run the pipeline and perform other management actions. Recently run pipelines Pipeline Last run #20191209.2 · Set up CI with Azure Pip ... > pipelines-dotnet-core 8 Manually triggered 12º main Là 1h ago * ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Azure Pip ... 試 2h 8 Manually triggered 8 main Edit in Run pipeline Manage security Rename/move Delete Select Runs to view all pipeline runs. You canoptionally filter the displayed runs. Pipelines Recent All Runs Filter by keywords State V Repository V New pipeline Requested forV Tags X All pipeline runs Description Stages Set up CI with Azure Pipelines & #20191209.2 on pipelines-dotnet-core & main d4964 .. Set up CI with Azure Pipelines #20191209.1 on pipelines-dotnet-core åº main d4964 ... Set up CI with Azure Pipelines & #20191209.3 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines & #20191209.2 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines #20191209.1 on FabrikamFiber & main 2b4b23c La 44m ago 42s La 53m ago ₾ 46s Là 1h ago 0 1m 13s Ld 1h ago 1m 555 Lo 1h ago 1m 6s 16

--- Page 123 ---
Course Code/Title:CS3V15/Devops Unit: IV Ad-Hoc Limitations: While ad-hoc commands are ideal for single, immediate tasks, they are not designed for complex, multi- step automation. Playbooks come to the rescue when tasks have interdependencies and need to be executed in a coordinated manner. Readability and Reusability: Playbooks are authored in YAML, a human-readable and straightforward format. This not only makes them easy to write but also facilitates sharing and collaboration. You can reuse playbooks across various scenarios, saving time and effort. Conditional and Looping Logic: Playbooks offer advanced features, including conditional statements and looping, that allow you to adapt automation to different situations. This flexibility makes playbooks versatile and capable of handling a wide range of automation needs. In essence, playbooks are your tool of choice when automation tasks become multi-faceted and require a structured and logical approach. They provide the power to streamline and automate complex workflows with precision and efficiency. Example: Ping All Inventory Devices To illustrate the use of a playbook, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: playbook.yaml 1 --- 2 -name : My First Play #Name of play 3 hosts: virtualmachines #defining host can be all 4 tasks: 5 -name:Ping My Hosts #Name of task 6 ansible.builtin.ping: # can also write ping To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 124 ---
Course Code/Title:CS3V15/Devops Unit: IV Output and Result: Activities Terminal - Nov 2 12:29 . O auriga@auriga-Latitude-E7470: - /Desktop/ansible X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook.eg1.yaml PLAY [My First Play] TASK [Gathering Facts] ok: [ vri01] TASK [Ping Devices] ok: [vm01] PLAY RECAP vn01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$[] rescued=0 ignored=0 X As you can see there is gathering facts task which we didn't create is running it is the default task which ping the connection. Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: playbook.yaml 1 --- 2- name: Install Nginx 3 hosts: all 4 become: yes 5 tasks: 6 - name: Install Nginx using apt 7 apt: 8 name: nginx 9 state: present To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml -- ask-become-pass CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 10 N33 nirf 1750 Rank

--- Page 153 ---
Create a pipeline in azure What is pipeline Azure Pipelines supports continuous integration (CI)and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline. The latest way to build pipelines is with the YAML pipeline editor. You can also use Classic pipelines with the Classic editor. code release build CI plan continuous testing CD deploy monitor operate Create your first pipeline This is a step-by-step guide to using Azure Pipelines to build a sample application from a Git repository. This guide uses YAML pipelines configured with the YAML pipeline editor. If you'd like to use Classic pipelines instead, see Define your Classic pipeline. For guidance on using TFVC, see Build TFVC repositories. Prerequisites - Azure DevOps A GitHub account where you can create are pository. An Azure DevOps organization. Create one for free. If your team already has one, then make sure you're an administrator of the Azure Devops. 12

--- Page 154 ---
Get the Java sample code To get started, fork the following repository into your GitHub account. https://github.com/MicrosoftDocs/pipelines-java Create your first Java pipeline 1. Sign-in to your Azure DevOps organization and go to your project. 2. Go to Pipelines, and then select New pipeline. 3. Do the steps of the wizard by first selecting GitHub as the location of your source code. 4. You might be redirected to GitHub to sign in. If so, enter your GitHub credentials. 5. When you see the list of repositories, select your repository. 6. You might be redirected to GitHub to install the Azure Pipelines app. If so,select Approve & install. 7. Azure Pipelines will analyze your repository and recommend the Maven pipeline template. 8. When your new pipeline appears, take a look at the YAML to see what it does. When you're ready, select Save and run. 9. You're prompted to commit a new azure pipelines.yml file to your repository. After you're happy with the message, select Save and run again. If you want to watch your pipeline in action, select the build job. You just created and ran a pipeline that we automatically created for you, because your code appeared to be a good match for the Maven template. You now have a working YAML pipeline (azure-pipelines.yml) in your repository that's ready for you to customize! 14

--- Page 5 ---
Unit: I Course Code/Title:CS3V15/Devops Example: Facebook's mobile app which is updated every two weeks effectively tells users you can have what you want and you can have it. Now ever wondered how Facebook was able to do social smoothing? It's the DevOps philosophy that helps facebook and sure that apps aren't outdated and that users get the best experience on Facebook. Facebook accomplishes this true code ownership model that makes its developers responsible that includes testing and supporting through production and delivery for each kernel of code. They write and update their true policies like this but Facebook has developed a DevOps culture and has successfully accelerated its development lifecycle. Difference between Agile and DevOps: S. No. Agile 1 It started in the year 2001. It started in the year 2007. 2 Invented by John Kern, and Martin Fowler. Invented by John Allspaw and Paul Hammond at Flickr, and the Phoenix Project by Gene Kim. 3 Agile is a method for creating software. It is not related to software development. Instead, the software that is used by DevOps is pre-built, dependable, and simple to deploy. 4 An advancement and administration approach. Typically a conclusion of administration related to designing. 5 The agile handle centers on consistent changes. DevOps centers on steady testing and conveyance. 6 A few of the finest steps embraced in Agile are recorded underneath - 1. Backlog Building 2.Sprint advancement DevOps to have a few best hones that ease the method - 1. Focus on specialized greatness. 2. Collaborate straightforwardly with clients and join their feedback 7 Agile relates generally to the way advancement is carried of, any division of the company can be spry in its hones. This may be accomplished through preparation. 8 All the group individuals working in a spry hone have a wide assortment of comparable ability sets. This is often one of the points of interest of having such a group since within the time of requirement any of the group individuals can loan help instead of holding up for the group leads or any pro impedances. 9 Spry accepts "smaller and concise". Littler the group superior it would be to convey with fewer complexities. 10 Since Agile groups are brief, a foreordained sum of time is there which are sprints. Tough, it happens that a sprint has endured longer than a month but regularly a week long. 11 A big team for your project is not required. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY DevOps DevOps centers more on program arrangement choosing the foremost dependable and most secure course. DevOps features a diverse approach and is very viable, most of the time it takes after "Divide and Conquer". Work partitioned among the improvement and operation groups. DevOps, on the other hand, accepts that "bigger is better". DevOps, on the other hand, prioritizes reliabilities. It is since of this behavior that they can center on a long-term plan that minimizes commerce's unsettling influences. It demands collaboration among different teams for the completion of work. 5 N33 nirf 1750 Rank

--- Page 6 ---
Unit: I Course Code/Title:CS3V15/Devops 12 Some of the Tools- · Bugzilla · JIRA · Kanboard and more. 13 It is suitable for managing complex projects in any department. It centers on the complete engineering process. 14 It does not focus on the automation. It focusses on automation. 15 Working system gets more significance in Agile than documentation. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Some of the Tools- · Puppet • Ansible • AWS · Chef · team City OpenStack and more. The process documentation is significant in DevOps. 6 N32 nirf 1750 Rank

--- Page 109 ---
Course Code/Title:CS3V15/Devops Unit: III Note that the git repository and the files from that repository are copied from the workspace of App1 to App2. Also note that test.class and test.java files are copied to the workspace of App2. Started by user Anuradha R Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\. jenkins\workspace\App2 Copied 16 artifacts from "Appl" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621189248227895.bat C:\ProgramData\Jenkins\.jenkins\workspace\App2>dir Volume in drive C is OS Volume Serial Number is C6C2-8A9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 .git 474 test. class 144 test.java 2 File(s) 618 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ Finished: SUCCESS 5. Extended Choice Parameter Overview: The Extended Choice Parameter Plugin provides advanced parameter types for Jenkins jobs, such as multi-select lists, checkboxes, and more complex formats. Key Features: · Parameter Types: Includes multi-select, checkboxes, and other advanced input options. . Dynamic Choices: Allows dynamic generation of choices from scripts or external sources. Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Extended Choice Parameter Plugin," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add an Extended Choice Parameter. ○ Configure the parameter type, choices, and other settings. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 110 ---
Unit: III Course Code/Title:CS3V15/Devops Usage: · Useful for scenarios where complex user input is needed or where multiple selection options are required. Configuring Jenkins to Work with Java, Git, and Maven 1. Configuring Jenkins to Work with Java Overview: Jenkins requires Java to run. Configuring Java in Jenkins involves specifying the Java Development Kit (JDK) installations Jenkins should use. Steps: 1. Install Java Development Kit (JDK): ○ Ensure that JDK is installed on your system. You can download it from the Oracle website or use OpenJDK. 2. Configure JDK in Jenkins: ○ Open Jenkins and go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to JDK section and click Add JDK. ○ Enter a name for the JDK installation (e.g., JDK 11). Check Install automatically to let Jenkins download and install the JDK, or specify the path to an existing JDK installation. ○ If specifying the path manually, provide the JAVA_HOME directory. 3. Example Configuration: ○ Name: JDK 11 ○ JAVA_HOME: /usr/lib/jvm/java-11-openjdk 4. Verify JDK Configuration: You can verify the JDK configuration by creating a simple Jenkins job and adding a build step that prints the Java version using: bash Copy code java -version 2. Configuring Jenkins to Work with Git Overview: The Git Plugin integrates Jenkins with Git repositories, enabling Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 35 1750 Rank

--- Page 27 ---
Course Code/Title:CS3V15/Devops Unit: I · Servers are maintained easily and there is nearly zero downtime · Users can access cloud data and upload it on the cloud from any device with a solid internet connection · Cloud environments can be modified according to the user's requirements and is easily accessible · Clouds are highly secure, making data breaches more unlikely · Migrating to the cloud eliminates the need to buy on-premises infrastructure . It offers pay-as-you-go pricing, meaning you only pay for the resources you use 1. Infrastructure as a Service: IaaS delivers virtualized computing resources over the Internet. Users can rent virtual machines, storage, and networking infrastructure, allowing for easy scalability without investing in physical hardware. Examples include AWS EC2 and Azure Virtual Machines. 2. Platform as a Service: PaaS offers a robust platform for developers to build, deploy, and manage apps without worrying about the underlying infrastructure. It simplifies application development and deployment, with services like Google App Engine and Heroku leading the way. 3. Software as a Service: SaaS offers software applications on a subscription basis, accessible via a web browser. Users don't need to install or maintain software locally, making it ideal for collaboration tools (e.g., Microsoft 365, Google Workspace) and CRM systems (e.g., Salesforce). 4. Function as a Service: FaaS allows developers to execute code responding to events without managing servers. It's highly scalable and cost-efficient, exemplified by AWS Lambda and Azure Functions. FaaS is also known as serverless computing. 5. Container as a Service: CaaS enables the deployment and management of containerized applications using orchestration tools like Kubernetes. It provides portability and scalability for applications across different cloud environments. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 27 N33 nif 1750 Rank

--- Page 28 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between main cloud computing services Terms Stands for Uses Access Model Technical understanding. Popularity Percentage rise Usage Cloud services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY IAAS Infrastructure as a service. IAAS is used by network architects. IAAS gives access to the resources like virtual machines and virtual storage. It is a service model that provides virtualized computing resources over the internet. It requires technical knowledge. It is popular among developers and researchers. It has around a 12% increment. Used by the skilled developer to develop unique applications. Amazon Web Services, sun, vCloud Express. PAAS Platform as a service. PAAS is used by developers. PAAS gives access to run time environment to deployment and development tools for application. It is a cloud computing model that delivers tools that are used for the development of applications. Some knowledge is required for the basic setup. It is popular among developers who focus on the development of apps and scripts. It has around 32% increment. Used by mid-level developers to build applications. Facebook, and Google search engine. SAAS Software as a service. SAAS is used by the end user. SAAS gives access to the end user. It is a service model in cloud computing that hosts software to make it available to clients. There is no requirement about technicalities company handles everything. It is popular among consumers and companies, such as file sharing, email, and networking. It has about a 27 % rise in the cloud computing model. Used among the users of entertainment. MS Office web, Facebook and Google Apps. 28 N32 nirf 1750 Rank

--- Page 119 ---
Course Code/Title:CS3V15/Devops Unit: IV · Groupings in inventories enable you to organize your infrastructure, allowing you to target specific subsets of nodes based on your needs. For instance, when you need to perform tasks in a particular location, groupings become essential. Creating Inventories There are two primary methods to create an inventory on the control node: using a YAML file or an INI file. 1. INI Inventory Example (inventory.ini): 1[virtualmachines] 2host1 ansible_host:192.168.0.1 3192.168.0.2 2. In this INI file, it captures the IP addresses of managed nodes. The default username used for SSH connections is typically the working user of your control node. 3. ansible_host: This parameter is used to specify the IP address, although you can write the IP directly. 4. Inventory.yaml 1 --- 2virtualmachines: # Define a class name to represent a particular group of devices. 1 hosts: # Define the hosts. 2 vm01: # Define a parent name to indicate a class of parent type. 3 ansible_host: 192.168.0.1 # Specify the IP address. 4 http_port: 80 # (optional) Define the HTTP port. 5 ansible_user: auriga # (optional) Provide the username if it's different from the control node's user. o ansible_user: Use this to specify a different user if the managed node's user is not the same as the control node's user. o http_port: Specify a port if necessary. In real-world scenarios, inventory files can be more complex, reflecting the diverse and extensive infrastructure of organizations. You can explore more configurations here. Verifying Your Inventory After creating your inventory, it's a good practice to verify its correctness. This step ensures that the inventory is correctly structured and accessible to Ansible. To do this, use the following command: 1ansible-inventory -i inventory.yaml -list · - i: Indicates the inventory file you want to use. . - list: Requests the listing of the inventory content. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 5 N33 nirf 1750 Rank

--- Page 120 ---
Course Code/Title:CS3V15/Devops Unit:IV output: auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-inventory ·i inventory.yaml -- list "_meta": { "hostvars": { "vm01": "ansible_host": "192.168.0.208", "ansible_user"; "au" "children": [ "ungrouped" "ungrouped": { "hosts": "vn01 1 By following these steps, you have effectively set up and verified your inventory, ensuring a solid foundation for your Ansible automation. Using Ansible Ad-Hoc Commands Introduction: In Ansible, ad-hoc commands are your go-to solution for executing quick, one-off tasks on remote nodes. They provide a straightforward and efficient way to interact with managed nodes without the need for creating full-fledged playbooks. Ad-hoc commands are particularly useful when you need immediate results without the overhead of playbook development. When to Use Ad-Hoc Commands: Ad-hoc commands are best suited for scenarios where the task at hand is simple and doesn't require the complexity of a playbook. They are perfect for tasks like system health checks, package installation, service management, or any other single-operation job. Ad-Hoc Command Syntax: Ad-hoc commands follow a specific syntax that comprises various components, each serving a unique role: . Target Group: This is the group of hosts you intend to target with the ad-hoc command. · Module (-m): Specifies the module to execute. Modules are Ansible's building blocks for performing tasks, and they can range from basic operations like "ping" to more advanced tasks such as package management. · Inventory File (-i): Indicates the location of your inventory file, which defines the list of target devices. Example: Ping All Inventory Devices To illustrate the use of ad-hoc commands, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: 1ansible virtualmachines -m ping -i inventory.yaml virtualmachines is the target group. In this case, it could be any group, or you can use all to target all devices in your inventory. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 159 ---
Choose a job to see the steps for that job From the < Jobs in run #20191 ... FabrikamFiber Build O Build Build 40s Initialize job 1s 2 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 Duration: 40s 6 Pool: Azure Pipelines Image: Ubuntu-16.04 7 > Job preparation parameters Checkout 3s CmdLine 25 Component Detect 32s Post-job: Checkout <1s Finalize Job <1s Deploy DeployWeb 10s Finalize build Report build status <1s steps view, you can review the status and details of eachstep. From the Moreactions you can toggle timestamps or view a raw log of all steps in the pipeline. Build 9 Pool: Azure Pipelines 2 Image: Ubuntu-22.04 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 View job raw log Toggle timestamps 6 Duration: 40s Job preparation parameters 10

--- Page 160 ---
Cancel and re-run a pipeline If the pipeline is running, you can cancel it by choosing Cancel. If the run has completed, you can re-run the pipeline by choosing Run new. Pipeline run more actions menu: #20191210.3 Update azure-pipelines.yml for Azure Pip ... on FabrikamFiber Cancel Stages Jobs € Build C Deploy 0/1 compléted 48s Not started Build 48s Cancel From the More actions menu you can download logs, add tags, edit the pipeline, delete the run, and configure retention for the run. cure Pipe ... Run new Download logs Add tags Edit pipeline View retention leases Retain Delete 20

--- Page 99 ---
Unit: III Course Code/Title:CS3V15/Devops 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "HTML Publisher Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Post-build Actions section. ○ Select Publish HTML reports. ○ Specify the directory containing HTML reports and the index page. Usage: . Ideal for displaying build reports such as test results or code coverage metrics in a user-friendly HTML format. DEMO: Step 1 : We will create a Maven project using Selenium and TestNG framework. Open Eclipse : IDE and Create a Maven Project. Select the Create Simple Project (Skip archetype selection) and enter the Group Id and Artifact Id as per your choice. My project name is SeleniumDemo. Step 2 : Add the Selenium dependency in your pom.xml file as follows - <dependency> <groupId>org.seleniumhq.selenium</groupId> <artifactId> selenium-java</artifactId> <version>3.141.59</version> </dependency > Save the pom.xml file. Step 3 : Now install testNG plugin. For that, from Menu select Help-> Eclipse Marketplace Help Welcome 7 Help Contents Search Show Context Help Show Active Keybindings ... Ctrl+Shift+L Tip of the Day Tips and Tricks ... Cheat Sheets ... @ Eclipse User Storage Open the Eclipse Marketpla ** Perform Setup Tasks ... % Check for Updates Install New Software ... Eclipse Marketplace ... = About Eclipse IDE · Donate Contribute Type TestNG in the search window. The testNG installation option will be available. click Install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 24 N32 nirf 1750 Rank ---

--- Page 100 ---
Course Code/Title:CS3V15/Devops Unit: III C Eclipse Meketplace × Eclipse Marketplace Select solutions to initiit, Prese leuralt Now to proceed with lutallation. Press the "more info" link to learn more about a solution Search Recent Popular Favorites itwled - Research at the Eclipse × All Categories Go TestNG for Eclipse . This plug-in late you run your TextNG wits from Erfiore. You can run sulter, groupe er individuel methods. Errors are reported in a pepwane tab that lets you. more info by Chat Bent Apache 20 tantog zielt testingunit integration functional selection * 737 ** * stal: 1,83M (25,021 last month) Install MoreUnit 3.3.0 Morelinit is an Solipoe plugin that should mult you in wilting mory unk with. It supports ali programming languages (witching between teds and clanset under. mone.info Install Infinitest 5.4.0.3 Infinitest is a continuont tout runner for tova,'and is valuable to developers valing a Marketplaces (7) Cancel Then confirm the selected features by clicking on Confirm button. x Confirá Selected Features These Confirm to gorthus with the induction. Cs go back te chocie bis sules it CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 25 N3 nirf 1750 Rank

--- Page 115 ---
Unit: IV Course Code/Title:CS3V15/Devops UNIT IV CONFIGURATION MANAGEMENT USING ANSIBLE Ansible Introduction, Installation, Ansible master/slave configuration, YAML basics, Ansible modules, Ansible Inventory files, Ansible playbooks, Ansible Roles, adhoc commands in ansible. Ansible A SSH ANSIBLE TARGET NODE HOSTS PLAYBOOK Ansible is a powerful DevOps tool designed for automating tasks on remote servers or nodes. In simple terms, it allows you to automate commands and functions on multiple remote machines from a central 'master' node. To illustrate its usefulness, consider a scenario where you need to reboot dozens or even hundreds of remote hosts. You could manually SSH into each one and initiate the reboot, or you can use Ansible to streamline the process, making it efficient and offering a wide range of additional functionalities. In essence, Ansible operates much like the second method mentioned, using SSH (Secure Shell), a secure communication protocol, to control remote nodes in a secure and optimized manner. Ansible Components Ansible Core Components Control node Ansible Inventory Managed node 1 Managed node 2 Managed node 3 1. Control Node - · The central or main node where Ansible is installed. · Used to trigger commands like ansible and ansible-inventory on other nodes. · Acts as the orchestrator for Ansible operations. • TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 1 N33 nirf 1750 Rank

--- Page 116 ---
Course Code/Title:CS3V15/Devops Unit: IV 2. Manage Node- · A remote or slave node where tasks are executed or controlled by Ansible. . These are the servers or devices you want to manage or automate. 3. Inventory - · A list of managed node IPs and configurations. · Logically organized, typically using file formats like YAML or INI. · Created on the control node to describe the deployment of hosts to Ansible. Ansible Additional Components Ansible Architecture CMDB Users Public / Private Cloud Hosts ... ... Inventory .. Modules 000 Ansible Playbook API Plugins Networking www.educba.com 1. Ad-Hoc Commands - . These are one-off commands that you can execute using the ansible command. · Useful for quick tasks or tests on remote nodes. 2. Plugins - · Plugins are pieces of code that extend Ansible's core functionality. · Ansible uses a plugin architecture for flexibility and expandability. · Examples include connectivity plugins for establishing connections and cache plugins. · Learn more about Ansible plugins. 3. Module- · Modules are built-in functions that can be used to perform various tasks. . They eliminate the need to write custom code for common operations. · Examples include modules for package management (apt, yum), service management, and more. CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) CHENNAI INSTITUTE . TECHNOLOGY 2 N33 nirf 1750 Rank

--- Page 43 ---
Course Code/Title:CS3V15/Devops Unit: II o Generates source code (if auto-generated code is used) o Generates documentation from source code o Compiles source code o Packages compiled code into JAR of ZIP file o Installs the packaged code in local repository, server repository, or central repository 2. Installation of Maven: To install maven on windows, you need to perform following steps: 1. Download maven and extract it 2. Add JAVA_HOME and MAVEN_HOME in environment variable 3. Add maven path in environment variable 4. Verify Maven 1) Download Maven To install maven on windows, you need to download apache maven first. Download Maven latest Maven software from Download latest version of Maven For example: apache-maven-3.1.1-bin.zip 2) Add MAVEN_HOME in environment variable Right click on MyComputer -> properties -> Advanced > Environment variables -> click new button System Settings - Now add MAVEN_HOME in variable name and path of maven in variable value. It must be the home directory of maven i.e. outer directory of bin. For example: E:\apache-maven-3.1.1 3) Add Maven Path in environment variable Click on new tab if path is not set, then set the path of maven. If it is set, edit the path and append the path of maven.Here, we have installed JDK and its path is set by default, so we are going to append the path of maven. The path of maven should be %maven home%/bin. For example, E:\apache-maven- 3.1.1\bin . 4) Verify maven To verify whether maven is installed or not, open the command prompt and write: mvn -version Now it will display the version of maven and jdk including the maven home and java home. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 2 N32 nirf 175º Rank

--- Page 44 ---
Unit: II Course Code/Title:CS3V15/Devops 3. POM FILES: POM is an acronym for Project Object Model. The pom.xml file contains information of project and configuration information for the maven to build the project such as dependencies, build directory, source directory, test source directory, plugin, goals etc. Maven reads the pom.xml file, then executes the goal. Before maven 2, it was named as project.xml file. But, since maven 2 (also in maven 3), it is renamed as pom.xml. Elements of maven pom.xml file For creating the simple pom.xml file, you need to have following elements: Element Description project It is the root element of pom.xml file. modelVersion It is the sub element of project. It specifies the modelVersion. It should be set to 4.0.0. groupId It is the sub element of project. It specifies the id for the project group. artifactId It is the sub element of project. It specifies the id for the artifact (project). An artifact is something that is either produced or used by a project. Examples of artifacts produced by Maven for a project include: JARs, source and binary distributions, and WARs. version It is the sub element of project. It specifies the version of the artifact under given group. Maven pom.xml file with additional elements Here, we are going to add other elements in pom.xml file such as: Element Description packaging defines packaging type such as jar, war etc. name defines name of the maven project. url defines url of the project. dependencies defines dependencies for this project. dependency defines a dependency. It is used inside dependencies. scope defines scope for this maven project. It can be compile, provided, runtime, test and system. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 3 N32 narf 1750 Rank -

--- Page 157 ---
Select a pipeline run to view information about that run. You can choose to Retain or Delete a run from the context menu. For more information on run retention, see Build and release retention policies. Stages La 51m ago > ... 6 42s Retain Delete Ld 2h ago 0 1m 13s - View pipeline details The details page for a pipeline allows you to view and manage that pipeline. Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. Azure DevOps · Search F 4 FabrikamFiber Edit Run pipeline : + Runs Branches Analytics Y Description Stages Ac #20191209.3 Set up CI with Azure Pipelines Manually triggered { main 2b4b23c #20191209.2 Set up CI with Azure Pipelines & Manually triggered & main 2b4b23c Là 54m ago Č 1m 13s Là 55m ago Č 1m 55s #20191209.1 Set up CI with Azure Pipelines Individual CI 1º main 2b4b23c Lô 56m ago 1m 6s 17

--- Page 158 ---
Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. View pipeline run details From the pipeline run summary you can view the status of your run, both while itis running and when it is complete. #20191210.2 Update azure-pipelines.yml for Azure Pipe ... on FabrikamFiber Summary Environments Triggered by Steve Danielson FabrikamFiber & main b2f795e Today at 12:56 PM Duration: 1m 9s Tests: Get started Changes: ¢ 2 commits Work items: ₾ 1 linked Artifacts: 1 published Stages Jobs Build 1 job completed 415 Run new : Deploy 1 job completed 13s 1 artifact From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, test results, and work Items From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, testresults, and work items Jobs and stages The jobs pane displays an overview of the status of your stages and jobs. This pane may have multiple tabs depending on whether your pipeline has stages and jobs, or just jobs. In this example, the pipeline has two stages named Build and Deploy. You can drill down into the pipeline steps by choosing the job from either the Stages or Jobs pane. 10

--- Page 143 ---
Unit: V Course Code/Title: CS3V15/Devops Email preferences Send me occasional product updates, announcements, and offers. Verify your account Pick one square that shows two identical objects. wiki How Step 4: Click the green Create account button. It's belowthe form, at the bottom of the page. This will take you to an email verification page. Verify your account Create account By creating an account, you agree to the Terms of Service. For more information about GitHub's privacy practices, see the GitHub Privacy Statement. We'll occasionally send you account-related wikiHow UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 3

--- Page 144 ---
Course Code/Title: CS3V15/Devops Unit: V Carefully review the Terms of Service at https://help.github.com/en/articles/github- terms-of-service and the Privacy Statement at https://help.github.com/en/articles/github- privacy-statement before you continue. You're almost done! We sent a launch code to wikihowneveconcepts@gmail.com + Enter code Didn't get your email? Resend the code or update your email address. wiki How Step 5: Verify your email by entering the code. After clicking Create account, you'll receive an email with a code.Enter this code on the verification page. Entering the code will automatically take you to the welcome page. Step 6: Select your preferences and click Continue. GitHub displays a quick survey that can help you tailor your experience to match what you're looking for. You'll be sent tothe plan selection page after completing the survey. How many team members will be working with you? This will help us guide you to the tools that are best suited for your projects. Just me 2 - 5 5 - 10 10 - 20 20 - 50 50+ Are you a student or teacher? Student Teacher Continue wikiHow CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 4 175° Rank ---

--- Page 17 ---
Course Code/Title:CS3V15/Devops Unit: I that failed in a test suite gets simpler. Also, we can schedule the execution of the test cases at predefined times. After testing, the code is continuously integrated with the existing code. 4) Continuous Monitoring Monitoring is a phase that involves all the operational factors of the entire DevOps process, where important information about the use of the software is recorded and carefully processed to find out trends and identify problem areas. Usually, the monitoring is integrated within the operational capabilities of the software application. 5) Continuous Feedback The application development is consistently improved by analyzing the results from the operations of the software. This is carried out by placing the critical phase of constant feedback between the operations and the development of the next version of the current software application. The continuity is the essential factor in the DevOps as it removes the unnecessary steps which are required to take a software application from development, using it to find out its issues and then producing a better version. It kills the efficiency that may be possible with the app and reduce the number of interested customers. 6) Continuous Deployment In this phase, the code is deployed to the production servers. Also, it is essential to ensure that the code is correctly used on all the servers. The new code is deployed continuously, and configuration management tools play an essential role in executing tasks frequently and quickly. Here are some popular tools which are used in this phase, such as Chef, Puppet, Ansible, and SaltStack. Containerization tools are also playing an essential role in the deployment phase. Vagrant and Docker are popular tools that are used for this purpose. These tools help to produce consistency across development, staging, testing, and production environment. They also help in scaling up and scaling down instances softly. Containerization tools help to maintain consistency across the environments where the application is tested, developed, and deployed. There is no chance of errors or failure in the production environment as they package and replicate the same dependencies and packages used in the testing, development, and staging environment. It makes the application easy to run on different computers. 7) Continuous Operations All DevOps operations are based on the continuity with complete automation of the release process and allow the organization to accelerate the overall time to market continuingly. It is clear from the discussion that continuity is the critical factor in the DevOps in removing steps that often distract the development, take it longer to detect issues and produce a better version of the product after several months. With DevOps, we can make any software product more efficient and increase the overall count of interested customers in your product. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 17 N32 nirf 1750 Rank

--- Page 18 ---
Course Code/Title:CS3V15/Devops Unit: I AWS AWS stands for Amazon Web Services, It is an expanded cloud computing platform provided by Amazon Company. AWS provides a wide range of services with a pay-as-per-use pricing model over the Internet such as Storage, Computing power, Databases, Machine Learning services, and much more. AWS facilitates for both businesses and individual users with effectively hosting the applications, storing the data securely, and making use of a wide variety of tools and services improving management flexibility for IT resources. Advantages & Features of AWS: 1. Cost savings: One of the biggest benefits of AWS is that it can help businesses save money. As mentioned previously, businesses can avoid the high upfront costs of traditional infrastructure with AWS and pay only for the resources they use. Traditionally, businesses had to invest in hardware and software upfront, which often led to overspending. Let's look at this for example - if a business needs to run a website that gets 1000 visitors per day, they would need to purchase and maintain enough servers to support this traffic. With AWS, the business only pays for the compute resources they use when someone visits their website. This can result in significant cost savings. 2. Flexibility: Another key benefit of AWS is its flexibility. Businesses are able to customize their virtual environment - whether the operating system, database, programming language, or something else - to meet their specific needs. Especially in today's climate, the migration process to the cloud should be as frictionless as possible - and AWS makes that possible. Regardless of your use case or industry, AWS can be tailored to fit your needs, whether you're looking for a single cloud- hosted application or an entire suite of integrated solutions. 3. Reliability: AWS is known for being reliable, with an uptime of 99.9%. This makes it a great platform for mission-critical applications that need to be available 24/7. AWS also offers the ability to deploy resources across multiple availability zones for even greater reliability. The cloud platform also has a number of features that make it easier to ensure reliability, such as autoscaling and auto-healing. Autoscaling allows businesses to automatically scale their resources up or down based on demand, while auto-healing enables them to quickly identify and replace any faulty components. 4. Security: Businesses can take advantage of advanced security features, such as identity and access management, to help protect their data. Their tough infrastructure with an end-to-end approach is designed to withstand attacks and AWS provides customers with tools to help them monitor and respond to threats. When it comes to storage, Amazon S3 provides customers with a secure and reliable way to store and access data. The service is designed to be highly scalable and resilient, with built-in redundancy. Fine-grain identity and access controls can be applied to S3 buckets and objects, giving customers control over who has access to their data. Security tasks can be automated with AWS CloudFormation, making it easier for businesses to manage their security policies. And, you can rest easy knowing that AWS takes privacy seriously, with comprehensive customer data protection and compliance measures. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 18 N32 nirf 1750 Rank

--- Page 35 ---
Unit: I Course Code/Title:CS3V15/Devops Push: Push is the operation that sends your committed changes to a remote repository, making them accessible to others. Git workflow v0.1 v0.2 v1.0 Master Hotfix Release Develop Feature Feature Git Flow is a structured branching model designed for projects with well- defined release cycles and a need for strict quality control. Branches: The branching model described is commonly known as the Gitflow Workflow. It's a branching strategy that defines a strict branching model designed to facilitate collaboration and streamline the release process. Let's go into detail about each branch: clone operation Working Copy edit, add, move files update operation Modify working copy status and diff operation Review changes commit and push operation push operation Commit changes push operation amend and push operation Review changes 1. Master Branch: The `master'branch represents the main codebase and contains production- ready code. This branch is typically stable and should only include thoroughly tested and approved changes. Each commit on the 'master' branch represents a new version or release of the software. 2. Develop Branch: The 'develop' branch is an integration branch where various feature branches are merged. It serves as a staging area for testing new features and ensuring they work well together before merging into the 'master' branch. This branch may have ongoing development work and is not necessarily always in a production- ready state. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 35 nirf 1750 Rank N33

--- Page 36 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Feature Branches: Feature branches are created for developing new features or implementing changes. These branches are typically based on the 'develop'branch. Once a feature is complete, the branch is merged back into the 'develop' branch. Feature branches allow developers to work on specific tasks without affecting the main codebase. 4. Release Branch: The 'release' branch is created when the 'develop' branch reaches a point where it is ready for a production release. This branch is used for final testing, bug fixes, and preparing the code for deployment. No new features should be added to the release branch. Once the release is deemed stable, it is merged into both the 'master' branch and the 'develop' branch. 5. Hotfix Branch: The 'hotfix' branch is used to quickly address critical issues or bugs in the production code. It is created directly from the 'master' branch. Hotfixes are intended to be small and focused on resolving the specific issue at hand. Once the hotfix is complete, it is merged into both the 'master'branch and the `develop'branch to ensure that the fix is applied to future releases. Here is the typical flow: · Developers work on feature branches based on the 'develop' branch. · Completed features are merged into the 'develop' branch. · When ready for a release, a 'release' branch is created from 'develop". · The release branch undergoes testing and bug fixes. · The release branch is merged into both 'master' and 'develop' once it's stable. · If a critical issue arises in production, a 'hotfix' branch is created from 'master'. · The hotfix is merged into both 'master' and 'develop to keep both branches in sync. · This Gitflow Workflow helps maintain a structured development process, ensuring that features are developed, tested, and released in a controlled manner. Example Scenario: · Imagine you're working on a large software project with a team of 10 developers. You have a major release planned for every six months. · You create a "feature/Ticket-Id" branch to develop a new login system. · Once the feature is complete, it's merged into the "develop" branch for integration and testing. · As the release date approaches, you create a "release/v1.0" branch to freeze code for the upcoming release. · Any critical issues discovered in the production environment are fixed in "hotfix" branch and merged into "master" and "develop." CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 36 N33 nirf 1750 Rank

--- Page 21 ---
Unit: I Course Code/Title:CS3V15/Devops · Gaming: AWS has been serving many gaming studios. Combining Amazon EC2 and S3 services with CloudFront enables gaming websites to deliver high-quality gaming experiences to their customers regardless of location. Use Cases of AWS · Netflix Netflix is an entertainment platform that started in the United States, but eventually, it expanded to many countries and soon became popular. However, once Netflix confronted the scalability problem because of the sudden increase in viewers. That made Netflix choose AWS services. Netflix reports that when it started using AWS services like DynamoDB and Cassandra for its distributed databases, it could handle the data easily. So, scalability is a great advantage of AWS. Netflix has adapted around 100,000 server instances from AWS for computing and storage databases, analytics, recommendation engines, and video transcoding as well. . McDonald's McDonald's is the world's largest fast-food company that serves around 64 million people per day. The growth of this company has gone to another level when it started home deliveries. By utilizing AWS services, McDonald's created a platform that integrates local restaurants with delivery partners such as Uber Eats. Scalability is also a reason for the company to choose AWS services. Moreover, with AWS Microservices Architecture, McDonald's platform can scale 20,000 orders per second and integrate with the global partners easily. · Airbnb Airbnb is an international online marketplace for rental homes. This platform connects people who are looking for rental accommodation with those who want to rent out their houses. Quite soon, Airbnb became unable to handle the constant streaming of data on the website from its customers. That is when it started using Amazon EC2 service and Elastic Load Balancing, which distributes incoming traffic to multiple Amazon EC2 instances. In this way, Airbnb could avoid traffic, and customers could use the online platform without any disruption. • Novartis Novartis is the best example for AWS use cases in healthcare. Novartis is one of the world's largest healthcare companies that provides solutions for patients' well-being. It adapted Amazon EC2 services and built a platform using other services such as Amazon Simple Storage Service, Amazon Elastic Block Store, and four availability zones. Data Analysts of Novartis are taking advantage of the AWS services and still implementing new solutions for the patients. · Expedia Expedia is a worldwide online travel agency that has always focused on the constant development and innovation of its platform to offer an extraordinary user experience for its clients. Since 2010, Expedia has been using AWS services to build a standard deployment model for better infrastructure as AWS offers the best data security through different availability zones. • Samsung If you are using Samsung mobile phones, then you may know about the Samsung app store. For setting up the apps stacked in its store, the company started using AWS services. Using AWS app development services, Samsung wanted to provide its customers with the facility to download the apps anywhere without any network traffic. • NASA NASA (National Aeronautics and Space Administration) has always wondered about creating a library to present people with all its achievements through pictures and videos of space. Later on, it created such platforms, but because it had 10 different NASA centers, it couldn't provide the best experience for viewers. So, all it wanted was to create an easy-access platform for TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 21 N32 nirf 1750 Rank

--- Page 22 ---
Unit: I Course Code/Title:CS3V15/Devops people to search for and view images and videos. Then, NASA started adopting many services from AWS to solve this problem, which included Amazon Elastic Compute Cloud, Elastic Load Balancing, Amazon Simple Storage Service, Amazon Simple Queue Service, etc. Among these, Amazon S3 helped the company store all the incoming data such as photos, videos, and audio files without any hassle. • Facebook Facebook, without a doubt, is a widespread social media platform. To build a scalable application, Facebook used services such as Amazon Elastic Compute Cloud, Amazon Simple Storage Service, Amazon Relational Database Service, Amazon SimpleDB, Amazon CloudFront, Amazon Simple Queue Service, etc. Amazon RDS helps the platform to make it easy to set up, operate, and scale the database in the cloud. Various Services offered by AWS · Amazon EC2 (Elastic Cloud computing) · Amazon RDS (Relational Database Services) · Bonus Service: Amazon Connect · Amazon S3 (Simple Storage Service) · Amazon Lambda · Amazon Cognito · Amazon Glacier · Amazon SNS (Simple Notification Service) · Bonus Service: Amazon Lex · Amazon Lightsail · Amazon VPC (Virtual Private Cloud) · Amazon Kinesis · Amazon Inspector · Amazon Auto-scaling · Amazon IAM (Identity and Access Management) · Dynamo DB · Amazon SQS (Simple Queue Service) · Amazon ElastiCache · Amazon Chime · AWS Athena · Code Catalyst · Web Application Firewall · AWS Amplify · AWS Rekognition · AWS QuickSight · AWS CloudFormation · AWS Management Console The Important Cloud Services according to various categories that are provided by AWS are given below : 1. Compute • Amazon EC2: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It allows organizations to obtain and configure virtual compute capacity in the cloud. You can select from a variety of operating systems and resource configurations like memory, CPU, and storage that are required for your TECHNOLOGY (Autonomous) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY 22 N33 narf 1750 Rank

--- Page 7 ---
Course Code/Title:CS3V15/Devops Unit: I DevOps Tools 1. Git (GitLab, GitHub, Bitbucket) Git remains indispensable in software development and DevOps due to its pivotal role in version control, collaborative coding, and efficient project management. As technology has accelerated, the need for streamlined and organized code management has never been greater. Git empowers developers to collaborate on codebases, effortlessly creating and merging branches for new features and bug fixes. Its distributed nature ensures developers can work seamlessly offline, an increasingly valuable feature in today's remote and distributed work environments. Additionally, Git facilitates the tracking of code modifications, making it easier to identify when and why specific changes were made, a critical aspect of maintaining code quality and security. Software development is essential in driving innovation and advancing progress, and Git maintains its prominent position as the bedrock of efficient, cooperative, and secure coding methodologies. 2. Maven Due to its enduring significance in managing project dependencies, building, and project lifecycle management, Maven remains a pivotal tool in SD and DevOps. As a robust build automation and project management tool, Maven simplifies the complexities of Java-based project development by streamlining the compilation, testing, packaging, and distribution processes. It ensures consistent and reproducible builds, making it easier for development teams to collaborate efficiently and deliver high-quality software. Maven's role in managing dependencies and facilitating continuous integration and deployment remains crucial. Its ability to handle complex build scenarios and integrate seamlessly with modern DevOps practices makes it indispensable for ensuring software projects' reliability, maintainability, and scalability in 2024 and beyond. 3. Jenkins Its importance lies in its role as a powerful automation server that enables continuous integration and continuous delivery (CI/CD) pipelines. Jenkins streamlines software development by automating tasks such as building, testing, and deploying code changes, ensuring that software is delivered quickly and highly. With the growing complexity of modern applications, the need for efficient CI/CD processes has become even more paramount. Jenkins provides flexibility, extensibility, and a vast library of plugins that cater to a wide range of technologies and tools, making it adaptable to diverse development environments. As organizations prioritize speed, reliability, and collaboration in their software development practices, Jenkins stands as a cornerstone tool, enabling teams to achieve seamless automation and efficient delivery of software solutions. 4. Chef Chef, a powerful automation platform, is crucial in managing infrastructure as code. Chef empowers organizations to achieve scalability, reliability, and speed seamlessly. By allowing the automation of server provisioning, configuration, and maintenance, Chef enhances efficiency and consistency across the entire infrastructure, reducing manual errors and ensuring that infrastructure remains desired. Moreover, Chef integrates smoothly with various cloud providers, containerization technologies, and other DevOps tools, making it adaptable to the ever-evolving tech TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 7 N32 nirf 1750 Rank

--- Page 8 ---
Course Code/Title:CS3V15/Devops Unit: I landscape. As organizations prioritize agility and scalability, Chef remains a vital tool in automating complex infrastructure tasks and enabling DevOps teams to focus on innovation and delivery. 5. Puppet Puppet is essential because it simplifies the management and orchestration of complex IT infrastructures by allowing administrators to define infrastructure as code. It ensures consistency and repeatability in configuration across servers, cloud instances, and containers. Businesses increasingly rely on diverse, dynamic, and hybrid infrastructures. Puppet's importance lies in its ability to streamline provisioning, configuration, and continuous compliance, thus reducing operational complexity, minimizing errors, and accelerating software delivery. Puppet continues to empower organizations to efficiently manage and scale their infrastructure while maintaining high levels of security and compliance, making it a crucial tool for DevOps teams. 6. Ansible Ansible is a powerful and widely adopted automation and configuration management tool important in 2024 for several reasons. This tool stands out for its simplicity and versatility. It empowers organizations to automate repetitive tasks, provisioning of infrastructure, and configuration management across diverse environments, making it an invaluable asset for DevOps and IT teams. Furthermore, Ansible's agentless architecture, declarative language, and a vast library of pre- built modules make it accessible to both beginners and seasoned professionals. As organizations prioritize efficiency, scalability, and the rapid deployment of applications and services, Ansible remains an indispensable DevOps toolkit, helping teams streamline operations, enhance security, and maintain infrastructure at scale, all while reducing manual errors and increasing agility in a fast-paced technological landscape. 7. Docker Docker is crucial in modern software development and DevOps practices. It can simplify and streamline the management of applications across various environments. Docker containers encapsulate an app and its dependencies, ensuring consistent and reproducible deployments from development to production. This technology enhances portability and scalability, accelerates development cycles, and reduces the "it works on my machine" problem. In a rapidly evolving software landscape, Docker's containerization approach remains crucial for achieving efficient, isolated, and highly flexible application deployment, making it an essential component of DevOps and continuous delivery pipelines. 8. Kubernetes Kubernetes, often abbreviated as K8s, play a central role in modern software development and operations. Its importance lies in its ability to orchestrate, manage, and automate containerized applications at scale. As organizations increasingly embrace microservices architectures and containerization for their applications, Kubernetes provides the essential infrastructure for deploying, scaling, and maintaining these containers efficiently. The tool's resilience, self-healing capabilities, and support for hybrid and multi-cloud environments make it vital for achieving agility, reliability, and cost-effectiveness in application deployment. It serves as the backbone of cloud-native ecosystems, enabling organizations to accelerate software delivery, improve resource utilization, and respond effectively to the evolving demands of the digital landscape. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N32 nirf 1750 Rank

--- Page 21 ---
Unit: I Course Code/Title:CS3V15/Devops · Gaming: AWS has been serving many gaming studios. Combining Amazon EC2 and S3 services with CloudFront enables gaming websites to deliver high-quality gaming experiences to their customers regardless of location. Use Cases of AWS · Netflix Netflix is an entertainment platform that started in the United States, but eventually, it expanded to many countries and soon became popular. However, once Netflix confronted the scalability problem because of the sudden increase in viewers. That made Netflix choose AWS services. Netflix reports that when it started using AWS services like DynamoDB and Cassandra for its distributed databases, it could handle the data easily. So, scalability is a great advantage of AWS. Netflix has adapted around 100,000 server instances from AWS for computing and storage databases, analytics, recommendation engines, and video transcoding as well. . McDonald's McDonald's is the world's largest fast-food company that serves around 64 million people per day. The growth of this company has gone to another level when it started home deliveries. By utilizing AWS services, McDonald's created a platform that integrates local restaurants with delivery partners such as Uber Eats. Scalability is also a reason for the company to choose AWS services. Moreover, with AWS Microservices Architecture, McDonald's platform can scale 20,000 orders per second and integrate with the global partners easily. · Airbnb Airbnb is an international online marketplace for rental homes. This platform connects people who are looking for rental accommodation with those who want to rent out their houses. Quite soon, Airbnb became unable to handle the constant streaming of data on the website from its customers. That is when it started using Amazon EC2 service and Elastic Load Balancing, which distributes incoming traffic to multiple Amazon EC2 instances. In this way, Airbnb could avoid traffic, and customers could use the online platform without any disruption. • Novartis Novartis is the best example for AWS use cases in healthcare. Novartis is one of the world's largest healthcare companies that provides solutions for patients' well-being. It adapted Amazon EC2 services and built a platform using other services such as Amazon Simple Storage Service, Amazon Elastic Block Store, and four availability zones. Data Analysts of Novartis are taking advantage of the AWS services and still implementing new solutions for the patients. · Expedia Expedia is a worldwide online travel agency that has always focused on the constant development and innovation of its platform to offer an extraordinary user experience for its clients. Since 2010, Expedia has been using AWS services to build a standard deployment model for better infrastructure as AWS offers the best data security through different availability zones. • Samsung If you are using Samsung mobile phones, then you may know about the Samsung app store. For setting up the apps stacked in its store, the company started using AWS services. Using AWS app development services, Samsung wanted to provide its customers with the facility to download the apps anywhere without any network traffic. • NASA NASA (National Aeronautics and Space Administration) has always wondered about creating a library to present people with all its achievements through pictures and videos of space. Later on, it created such platforms, but because it had 10 different NASA centers, it couldn't provide the best experience for viewers. So, all it wanted was to create an easy-access platform for TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 21 N32 nirf 1750 Rank

--- Page 22 ---
Unit: I Course Code/Title:CS3V15/Devops people to search for and view images and videos. Then, NASA started adopting many services from AWS to solve this problem, which included Amazon Elastic Compute Cloud, Elastic Load Balancing, Amazon Simple Storage Service, Amazon Simple Queue Service, etc. Among these, Amazon S3 helped the company store all the incoming data such as photos, videos, and audio files without any hassle. • Facebook Facebook, without a doubt, is a widespread social media platform. To build a scalable application, Facebook used services such as Amazon Elastic Compute Cloud, Amazon Simple Storage Service, Amazon Relational Database Service, Amazon SimpleDB, Amazon CloudFront, Amazon Simple Queue Service, etc. Amazon RDS helps the platform to make it easy to set up, operate, and scale the database in the cloud. Various Services offered by AWS · Amazon EC2 (Elastic Cloud computing) · Amazon RDS (Relational Database Services) · Bonus Service: Amazon Connect · Amazon S3 (Simple Storage Service) · Amazon Lambda · Amazon Cognito · Amazon Glacier · Amazon SNS (Simple Notification Service) · Bonus Service: Amazon Lex · Amazon Lightsail · Amazon VPC (Virtual Private Cloud) · Amazon Kinesis · Amazon Inspector · Amazon Auto-scaling · Amazon IAM (Identity and Access Management) · Dynamo DB · Amazon SQS (Simple Queue Service) · Amazon ElastiCache · Amazon Chime · AWS Athena · Code Catalyst · Web Application Firewall · AWS Amplify · AWS Rekognition · AWS QuickSight · AWS CloudFormation · AWS Management Console The Important Cloud Services according to various categories that are provided by AWS are given below : 1. Compute • Amazon EC2: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It allows organizations to obtain and configure virtual compute capacity in the cloud. You can select from a variety of operating systems and resource configurations like memory, CPU, and storage that are required for your TECHNOLOGY (Autonomous) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY 22 N33 narf 1750 Rank

--- Page 9 ---
Course Code/Title:CS3V15/Devops Unit: I 9. Slack Slack is a crucial tool for businesses and organizations worldwide. Its significance lies in facilitating seamless communication and collaboration among teams, whether working in the same office or remotely. Slack's real-time messaging, file sharing, and integration capabilities streamline workflow, enhance productivity and keep teams connected across different time zones and locations. As the work landscape evolves, with more companies embracing hybrid and remote work models, Slack is a vital hub for quick decision-making, project coordination, and knowledge sharing. With an ever- expanding ecosystem of integrations and features, Slack remains at the forefront of modern workplace communication, making it essential for businesses to stay agile, efficient, and competitive. 10. AWS Cloud Computing and Storage in DevOps AWS (Amazon Web Services) Cloud Computing and Storage are crucial in DevOps because they provide scalable, flexible, and cost-effective infrastructure for DevOps practices. AWS offers many services, including compute resources, databases, container orchestration, and serverless computing, which align perfectly with modern software development and deployment demands. Organizations adopt DevOps to accelerate software delivery. AWS provides the foundation for rapidly deploying and scaling applications, supporting continuous integration and continuous delivery (CI/CD) pipelines, and automating infrastructure provisioning through tools like AWS CloudFormation. Furthermore, AWS's storage solutions enable efficient data management, backup, and recovery, ensuring the resilience and reliability required for DevOps operations. As cloud technology evolves, AWS remains at the forefront, enabling DevOps teams to focus on innovation and efficiency. 11. Azure Cloud Computing and Storage in DevOps Azure Cloud Computing and Storage will be pivotal in DevOps practices in 2024 and beyond. Azure offers a comprehensive cloud ecosystem that enables organizations to scale their infrastructure, deploy applications, and store data efficiently. Azure provides essential services for continuous integration and continuous deployment (CI/CD), automation, monitoring, and security. Its cloud computing capabilities facilitate the provisioning of resources on demand, ensuring that development and testing environments are readily available. Azure's storage solutions, including Azure Blob Storage, Azure Files, and Azure SQL Database, enable secure data storage and retrieval, supporting the data-driven aspects of DevOps. Besides, Azure's integration with DevOps tools like Azure DevOps Services streamlines the software development lifecycle, enhancing collaboration and automation. 12. GCP Cloud Computing and Storage in DevOps Google Cloud Platform (GCP) offers robust cloud computing and storage solutions. GCP provides a scalable, reliable, and highly available infrastructure essential for modern DevOps practices. With its comprehensive set of services, including Google Compute Engine, Google Kubernetes Engine, Cloud Storage, and BigQuery, GCP empowers DevOps teams to build, deploy, and manage applications easily. Its emphasis on automation, infrastructure as code, and container orchestration aligns seamlessly with DevOps principles. Moreover, GCP's cutting-edge technologies, such as AI and machine learning capabilities, provide DevOps practitioners with advanced tools for monitoring, analytics, and automation, making it a powerful choice for organizations seeking to optimize their software development and delivery processes. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 10 ---
Course Code/Title:CS3V15/Devops Unit: I 13. Monitoring, Alerting, and Incident Response Tools: SignalFx Monitoring, alerting, and incident response tools like SignalFx are pivotal in DevOps and software development. As software systems become complex and distributed, the need for real-time visibility into performance and the ability to respond swiftly to incidents is significant. SignalFx excels in this regard by providing advanced monitoring and observability solutions that enable organizations to detect anomalies, trace issues across microservices proactively, and set up intelligent alerts. As applications scale, cloud-native architectures become the norm, and user expectations for reliability grow, SignalFx's capabilities are crucial. It empowers DevOps teams to ensure high availability, optimize resource utilization, and maintain a seamless user experience by identifying and addressing performance issues before they impact end-users. It is one of the most essential tools for modern software operations. 14. Appdynamics AppDynamics, a leading application performance management and monitoring platform, remains critically important as it ensures the optimal performance of modern digital businesses. As organizations rely on complex and distributed software systems, proactively monitoring, troubleshooting, and optimizing these applications becomes essential. AppDynamics provides real-time visibility into application performance, allowing businesses to swiftly identify bottlenecks, latency issues, and errors. With the ever-growing complexity of applications, the importance of AppDynamics lies in its ability to empower organizations to deliver exceptional user experiences, maintain application reliability, and swiftly respond to performance issues, thereby ensuring the continued success and competitiveness of digital businesses. 15. Raygun It is a crucial tool in software development and DevOps because it ensures application reliability and performance. Raygun is an application monitoring and error-tracking platform that empowers development teams to identify, diagnose, and resolve real-time issues. With software systems growing in complexity and the increased demand for seamless user experiences, Raygun's importance lies in providing actionable insights into application errors and performance bottlenecks. It enables organizations to proactively address issues, reduce downtime, and enhance user satisfaction, leading to higher software quality and improved customer experiences. Software is central to businesses across industries. Raygun's role in maintaining application health and facilitating rapid issue resolution makes it a fundamental tool for DevOps professionals and software developers. 16. Splunk Cloud Splunk Cloud helps organizations gain critical insights from the ever-expanding volume of data generated in today's digital landscape. As businesses increasingly rely on data- driven decision-making, Splunk Cloud stands out as a robust and scalable platform for monitoring, searching, analyzing, and visualizing machine-generated data. Its importance lies in providing real-time visibility into the health and performance of complex systems, applications, and infrastructures, enabling rapid incident detection and response. As cybersecurity threats evolve, Splunk Cloud's advanced security analytics and threat detection capabilities remain indispensable for safeguarding against cyberattacks and ensuring data integrity. In a world where data is a strategic asset, Splunk Cloud's role in harnessing the power of data for operational excellence and security cannot be overstated. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 10 N32 nirf 1750 Rank

--- Page 93 ---
Course Code/Title:CS3V15/Devops Unit: III And configure the Jenkins job as follows - General Description In this job we have used GitHub plugin Plain text Preview Discard old builds ? GitHub project This project is parameterized ? Throttle builds Execute concurrent builds if necessary Advanced V Source Code Management None Git ? Repositories ? Repository URL https://github.com/AnurdhaP/MyJavaPrograms.gil Please enter Git repository. Credentials ? - none - Add Advanced CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 18 N3 nirf 1750 Rank

--- Page 94 ---
Course Code/Title:CS3V15/Devops Unit: III Build Triggers Trigger builds remotely (e.g. from scripts) ? Build after other projects are built ? Build periodically ? GitHub hook trigger for GITSem polling ? Poll SCM ? Schedule ? A Do you really mean "every minute" when you say "" "" " ""? Perhaps you meant "H" "" "" to poll once per hour Would last have run at Wednesday, 30 August, 2023 at 4:11:02 pm india Standard Time; would'nest run at Wednesday, 30.A Ignore post-commit hooks ? Build Steps Execute Windows batch command ? Command See the list of available environment variables javas test.java java test. java ----- Advanced V Add build sten Save this configuration. Step 5 : Now if we build this job the output that we see is as follows on Console Output. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 19 N3 nirf 1750 Rank

--- Page 65 ---
Course Code/Title:CS3V15/Devops Unit: II Output The output is as follows - java version "1.8.0 66" Java(TM) SE Runtime Environment (build 1.8.0_66-b18) Java HotSpot(TM) 64-Bit Server VM (build 25.66-b18, mixed mode) Step 2 - Download Gradle Build File Download the latest https://gradle.org/install/. version of Gradle from the link available at Step 3 - Set Up Environment for Gradle Setting up environment means, we have to extract the distribution file and copy the library files into proper location. Set up GRADLE_HOME and PATH environmental variables. This step is platform dependent. In Windows Extract the downloaded zip file named gradle-2.11-all.zip and copy the distribution files from Downloads\gradle-2.11\ to C:\gradlel location. After that, add the C: \gradle and C: \gradlelbin directories to the GRADLE_HOME and PATH system variables. Follow the given instructions - Right Click On My Computers -> Click On Properties -> Advanced System Settings -> Click On Environmental Variables. There you will find a dialog box for creating and editing system variables. Click on new button for creating GRADLE_HOME variable (follow the left side screenshot). Click on Edit for editing the existing Path system variable (follow the right side screenshot). Follow the below given screenshots. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 24 N33 nirf 1750 Rank

--- Page 66 ---
Course Code/Title:CS3V15/Devops System Properties X Computer Name Hardware Advanced System Protection Remote Environment Variables x Unit: II System Properties × Computer Name |Hardware Advanced System Protection Remote Environment Variables × New System Variable × Variable name: GRADLE_HOME| Variable value: C:\grade Cancel System variables Variable Value ComSpec C:\Windows\system32\and.exe FP_NO_HOST_C ... NO GRADLE_HOME E:\work\22- Orientdb \gradle-2.11 Variable name: Path Variable value: #C:\gradle bin - System variables Variable Value Edit System Variable × OK Cancel < ORIENTDB_HOME E:\work\22- Orientdblorientdb-communi ... " JAVA_HOME C:\Program Files\Java\jdk1.7.0_60 New ... Edt ... Delete OK Cancel Step 4 - Verify the Gradle installation In windows os Windows_NT Path C:\oraclexe \app \prade\product\11.2.0\, .. PATHEXT .COM ;. EXE ;. BAT ;. CMD ;. VBS ;. VBE ;. JS ;.... v New ... Edit ... Delete OK Cancel You can execute the following command in command prompt. C:\> gradle -v Output Here you will find the Gradle version. Gradle 2.11 Build time: 2016-02-08 07:59:16 UTC Build number: none Revision: 584db1c7c90bdd1de1d1c4c51271c665bfcba978 Groovy: 2.4.4 CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 25 N33 nirf 1750 Rank

--- Page 105 ---
Course Code/Title:CS3V15/Devops Unit: III How to install copy artifact plugin ? Step 1 : Start the Jenkins tool by entering the login and password. Step 2 : Click on Manage Jenkins. Click on Plugins. Step 3 : In the search window, type the name copy Artifact. Step 4 : The plugin name will be displayed if it is not already installed. Select it. Name Adde a build atop to copy artifacts from another project. Thủa plugin le up for adoption! We are looking for naw maintainent. Velt our Adopt a Plugin intuitive for mort information. then click on Install. The plugin gets installed. Example Demo · Let us discuss how to use Copy Artifact plugin. Step 1 : Create a new Job by clicking on item New Item. Give the some name to your job. I have given the name App1 and selected the Freestyle Project. Step 2: Give suitable description in the description box. Step 3 : Under the source code management, click on Git option and specify the repository URL of GitHub. Source Code Management None Git ? Repositories 1 Repository URL + ·hong · CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 30 N3 nirf 1750 Rank

--- Page 106 ---
Unit: III Course Code/Title:CS3V15/Devops Step 4 : Under the Build Steps section, choose the Execute Windows batch command option. And type the javac and java commands. Build Steps = Execute Windows batch command Command See the list of available environment variables javac test.java java test Advanced Add bulld stop * Step 5 : Click in Apply and Save. Step 6 : Select the Build Now option. On successful built, the test, class file gets generated in the Jenkins workspace of the App1 job. C:\ProgramData\Jenkins\.jenkins\workspace\Appl>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\Appi>java test Good Morning Pathill Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! C:\ProgramData\Jenkins\.jenkins\workspace\Appl>exit @ Finished: SUCCESS Step 7 : Now we have to copy test.class file from the workspace of Appl to another job. For that' purpose we will create another Job named App2 by clicking New Item. Select the Freestyle Project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 31 N3 nirf 1750 Rank

--- Page 85 ---
Course Code/Title:CS3V15/Devops Unit: III By following these steps, you can set up a Jenkins job to automate builds, integrate with your source code management system, and perform various actions based on build results. Introduction to Jenkins Plugins What Are Jenkins Plugins? Jenkins plugins are extensions that add extra functionality to Jenkins. They allow you to customize and extend Jenkins to better fit your specific needs. Plugins can provide integrations with other tools, add new features, and improve existing functionalities. Why Use Plugins? . Extend Functionality: Add features not available in the core Jenkins. . Integrate Tools: Seamlessly integrate with other tools and platforms (e.g., GitHub, Docker). . Improve Productivity: Automate more tasks and streamline your workflow. · Customize UI: Tailor the Jenkins interface to better suit your preferences and needs. How to Manage Plugins 1. Accessing the Plugin Manager 1. Go to Jenkins Dashboard: ○ Open your Jenkins dashboard. 2. Navigate to Plugin Manager: ○ Click on "Manage Jenkins". ○ Select "Manage Plugins". 2. Installing Plugins 1. Available Plugins: ○ Go to the "Available" tab to see a list of plugins that can be installed. 2. Search for Plugins: ○ Use the search bar to find specific plugins. 3. Select Plugins: ○ Check the box next to the plugins you want to install. 4. Install Plugins: ○ Click "Install without restart" or "Download now and install after restart". 3. Updating Plugins CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 narf 10 1750 Rank

--- Page 86 ---
Course Code/Title:CS3V15/Devops Unit: III 1. Go to Updates: ○ Navigate to the "Updates" tab to see plugins with available updates. 2. Select Plugins: ○ Check the box next to the plugins you want to update. 3. Update Plugins: ○ Click "Download now and install after restart". 4. Managing Installed Plugins 1. Installed Plugins: ○ Go to the "Installed" tab to see all installed plugins. 2. Uninstall Plugins: ○ Click the "Uninstall" button next to the plugin you want to remove. 3. Check Plugin Versions: ○ View the current version of each installed plugin. Adding Plugins to Jenkins Plugins are a crucial part of Jenkins, allowing you to extend its functionality to suit your project's specific needs. Whether you're integrating Jenkins with other tools, customizing your job configuration, or adding new build steps, plugins provide the flexibility you need. Here's a detailed guide on how to add plugins to Jenkins. 1. Why Plugins Are Important - Extending Jenkins Functionality: Plugins allow Jenkins to integrate with various tools and technologies, such as Git, Maven, Docker, and more. - Customizing Jobs: With plugins, you can add specific steps, triggers, and post-build actions to your jobs, making Jenkins adaptable to any workflow. - Automation: Plugins help automate processes like code quality checks, notifications, and deployments. 2. Accessing the Plugin Manager 1. Navigate to Jenkins Dashboard: Open your Jenkins dashboard in a web browser (e.g., http://localhost:8080). CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 175º Rank

--- Page 71 ---
Unit: II Course Code/Title:CS3V15/Devops build.gradle &3 I 1 /* 2 * This file was generated by the Gradle 'init' task. 3 * 4 * This generated file contains a sample Java Library project to get you started. 5 * For more details take a look at the Java Libraries chapter in the Gradle 6 * User Manual available at https://docs.gradle.org/6.0.1/userguide/java_library_plugin.html 7 */ 8 9 plugins { 10 // Apply the java-library plugin to add support for Java Library 11 id 'java-library' 12 } 13 14 repositories { 15 // Use jcenter for resolving dependencies. 16 // You can declare any Maven/Ivy/file repository here. 17 jcenter() 18 } 19 20 dependencies { 21 // This dependency is exported to consumers, that is to say found on their compile classpath. 22 api 'org. apache. commons : commons-math3: 3.6.1' 23 24 // This dependency is used internally, and not exposed to consumers on their own compile classpath. 25 implementation 'com. google. guava: guava: 28.0-jre' 26 27 // Use JUnit test framework 30 28 testImplementation 'junit : junit: 4.12' 29 } 30 The build.gradle file contains three default sections. They are as follows: o plugins: In this section, we can apply the java-library plugin to add support for java library. o Repositories: In this section, we can declare internal and external repository for resolving dependencies. We can declare the different types of repository supported by Gradle like Maven, Ant, and Ivy. · Dependencies: In this section, we can declare dependencies that are necessary for a particular subject. Additionally, we can declare other project-related modules like a task in this file. Display the Information of the Gradle project To understand the structure, dependencies and debugging problems of a build, Gradle provides many built-in features that display information on a project. Following are some basic commands to display the information of the project: Listing projects In Gradle, all the sub-projects of a project in the workspace can be listed in a hierarchy. To do so, run the below command from the root directory of the project. 1. gradle -q projects CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 72 ---
Course Code/Title:CS3V15/Devops Unit: II To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> C:\Users \HiMaNshU\eclipse-workspace >gradle -q projects Root project Root project 'eclipse-workspace' No sub-projects To see a list of the tasks of a project, run gradle <project-path> : tasks For example, try running gradle :tasks C:\Users\HiMaNshU\eclipse-workspace> Listing Tasks Gradle allows us to list all the essential tasks of the project. To list the task, run the below command: 1. gradle -q tasks Output: C:\Users\HiMaNshUleclipse-workspace>gradle -q tasks Tasks runnable from root project Build Setup tasks CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 31 N33 nirf 1750 Rank

--- Page 69 ---
Unit: II Course Code/Title:CS3V15/Devops 1 build.gradle gradle wrapper 2 gradle-wrapper.jar - gradle-wrapper.properties 3 gradlew 4 gradlew.bat 5 settings.gradle 6 It is the default structure of a Gradle project. Gradle will generate the following things for us: 1. The gradle file is build script for configuring the current project. 2. An executable JAR file is used as a Gradle wrapper. 3. Configuration properties for Gradle Wrapper. 4. The gradlew is a Gradle wrapper script for UNIX based OS. 5. The bat is the Gradle Wrapper script for Windows. 6. The settings script for configuring the Gradle build. Step3: Create a task Gradle supports APIs for creating and managing tasks through a Groovy-based DSL or Kotlin-based DSL. Every project contains a collection of tasks for some basic operation. Gradle supports a library of tasks that configure the project. For example, there is a Copy task, which copies files from one location to another. The Copy task is one of the most used tasks In Gradle. To use the Copy task in build script, follow the below process. Step1: Create a directory called src C: \Users \HiMaNshU\demo>mkdir src C: \Users \HiMaNshU\demo>cd src Step2: Add a file called myfile.txt in the src directory. Add the single line "Hello, World!" to it, also, we can leave it empty. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 28 N32 nirf 1750 Rank

--- Page 70 ---
Unit: II Course Code/Title:CS3V15/Devops C:\Users \HiMaNshU\demo\src>echo myfile.txt myfile.txt Define a task called Copy in build.gradle file. It will copy the src directory to a new directory called dest. We don't have to create the dest directory; the Copy task will do it for us. 1. task copy(type: Copy, group: "Custom", description: "The sources are copied to dest directory") { 2. from "src" 3. into "dest" 4. } We can provide anything in a group and description. Also, we can omit them, but doing so will also be omitted from the report of the task used later. Now execute our new copy task: C: \Users \HiMaNshU\demo>gradle copy BUILD SUCCESSFUL in 2s 1 actionable task: 1 executed C: \Users \HiMaNshU\demo>_ After the successful execution of the task, we will get BUILD SUCCESSFUL message. The build.gradle file The build.gradle file is build script of a Gradle project. All the tasks and plugins are defined in this file. When we run a gradle command, it looks for a file called build.gradle in the current directory. Although we have called it a build script, strictly, it is a build configuration script. The build script defines a project and its tasks. The default build.gradle file looks like as follows: CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 29 N33 nirf 1750 Rank

--- Page 73 ---
Course Code/Title:CS3V15/Devops Unit: II init - Initializes a new Gradle build. wrapper - Generates Gradle wrapper files. Help tasks Build Environment - Displays all build script dependencies declared in root project 'eclipse-workspace'. components - Displays the components produced by root project 'eclipse-workspace '. [incubating] dependencies - Displays all dependencies declared in root project 'eclipse-works pace'. Dependency Insight - Displays the insight into a specific dependency in root project 'eclipse-workspace.' Dependent Components : It displays the dependent components of components in the root project 'eclipse-workspace.' [incubating] help - Displays a help message. model - Displays the configuration model of root project 'eclipse-workspace.' [incubating] projects - Displays the sub-projects of root project 'eclipse-workspace.' properties - Displays the properties of root project 'eclipse-workspace.' tasks - Displays the tasks runnable from root project 'eclipse-workspace.' To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> To list all the tasks of the project, run the below command: 1. gradle tasks -all CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 32 N33 nirf 1750 Rank

--- Page 74 ---
Course Code/Title:CS3V15/Devops Unit: II To display more details about a task, run the below command: 1. gradle help -- task Listing Dependencies In Gradle, we can list the dependencies which are broken down by the configuration. To list the dependencies, run the below command: 1. gradle -q dependencies Difference between Gradle and Maven Basis Based on Gradle is based on developing Maven is based on developing pure Java language-based software. Configuration It uses a Groovy-based Domain-specific language (DSL) for creating project structure. Focuses on Performance It performs better than maven as it optimized for tracking only current running task. It is necessary to compile. Java Compilation Usability It is a new tool , which requires users to spend a lot This tool is a known tool for many users and is easily Gradle domain-specific language projects. It uses Extensible Markup Language (XML) for creating project structure. Developing applications by adding new features to them . It avoids compilation. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] Maven Developing applications in a given time limit. It does not create local temporary files during software creation and is hence - slower . 33 N33 nirf 1750 Rank -

--- Page 57 ---
Course Code/Title:CS3V15/Devops · Go to the File option · In the drop-down menu, select New · Select the Project option Unit: II If you want to create a Java project, you can select the "Java Project" option. Since we are not creating a Java project specifically, we have chosen the "Project" option. eclipse-workspace - Eclipse File Edit Source Refactor Navigate Search Project Run Window Help New Open File ... G) Open Projects from File System ... Shift+Alt+N Close Close All Shift + Cb1+W Save Save As. Save All Shift+Ctrl+S Revert Move .:. Rename 2 Refresh ES Convert Line Delimiters To Print Import ... Export ... Ctrl+P de java Project ui Project ... El Package G Class Interface G Enum @ Annotation 69 Source Folder 18 Java Working Set Folder File Untitled Text File Task JUnit Test Case Example ... Other ... Ctrl+N The dialog box that appears on the screen will display different types of projects. · Select the Maven Project option . Click on Next New Project + Select a wizard Create a Maven Project Wizards: type filter text irroject Gradle Java 2º Java Project #Java Project from Existing Ant Buildfile - @Maven Lu Check out Maven Projects from SCM M& Maven Module Maven Project Examples ? < Back Next > X Cancel Finish A dialog box will appear. Select the default workspace. . Click on "Next" Several Group IDs, Artifact IDs, and Versions will then appear. · Select a plugin there and click on "Next" CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 16 N33 narf 1750 Rank

--- Page 58 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Select an Archetype M Catalog: All Catalogs Configure .. Biter: × Group Id Artifact id Version org.apache.maven.archetypes maven-archetype-archetype 1.0 org.apache.maven.archetypes maven-archetype-|2ee-simple 1.0 org.apache.maven archetypes maven-archetype-plugin 1.2 org.apache.maven.archetypes maven-archetype-plugin-site 1.1 org.apache.maven.archetypes maven-archetype-portlet 1.0.1 org.apache.maven.archetypes maven-archetype-profiles 1.0-alpha-4 org.apache.maven.archetypes maven-archetype-quickstart 1.1 org.apache.maven.archetypes maven-archetype-site 11 org anacha mauan archabenes masan.archchina.cito.cimnia 11 An archetype which contains a sample Maven project. Show the last version of Archetype only Include snapshot archetypes Add Archetype ... > Advanced ? < Back Next > Cancel Finish In the next dialog box that appears, you'll complete the following steps: · Enter the Group ID “com.xyz" · Enter the Artifact ID "mavenproject" · The version will appear on the screen These items can all be modified at a later time if needed. · Click on "Finish" CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 17 N33 nirf 1750 Rank

--- Page 17 ---
Course Code/Title:CS3V15/Devops Unit: I that failed in a test suite gets simpler. Also, we can schedule the execution of the test cases at predefined times. After testing, the code is continuously integrated with the existing code. 4) Continuous Monitoring Monitoring is a phase that involves all the operational factors of the entire DevOps process, where important information about the use of the software is recorded and carefully processed to find out trends and identify problem areas. Usually, the monitoring is integrated within the operational capabilities of the software application. 5) Continuous Feedback The application development is consistently improved by analyzing the results from the operations of the software. This is carried out by placing the critical phase of constant feedback between the operations and the development of the next version of the current software application. The continuity is the essential factor in the DevOps as it removes the unnecessary steps which are required to take a software application from development, using it to find out its issues and then producing a better version. It kills the efficiency that may be possible with the app and reduce the number of interested customers. 6) Continuous Deployment In this phase, the code is deployed to the production servers. Also, it is essential to ensure that the code is correctly used on all the servers. The new code is deployed continuously, and configuration management tools play an essential role in executing tasks frequently and quickly. Here are some popular tools which are used in this phase, such as Chef, Puppet, Ansible, and SaltStack. Containerization tools are also playing an essential role in the deployment phase. Vagrant and Docker are popular tools that are used for this purpose. These tools help to produce consistency across development, staging, testing, and production environment. They also help in scaling up and scaling down instances softly. Containerization tools help to maintain consistency across the environments where the application is tested, developed, and deployed. There is no chance of errors or failure in the production environment as they package and replicate the same dependencies and packages used in the testing, development, and staging environment. It makes the application easy to run on different computers. 7) Continuous Operations All DevOps operations are based on the continuity with complete automation of the release process and allow the organization to accelerate the overall time to market continuingly. It is clear from the discussion that continuity is the critical factor in the DevOps in removing steps that often distract the development, take it longer to detect issues and produce a better version of the product after several months. With DevOps, we can make any software product more efficient and increase the overall count of interested customers in your product. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 17 N32 nirf 1750 Rank

--- Page 18 ---
Course Code/Title:CS3V15/Devops Unit: I AWS AWS stands for Amazon Web Services, It is an expanded cloud computing platform provided by Amazon Company. AWS provides a wide range of services with a pay-as-per-use pricing model over the Internet such as Storage, Computing power, Databases, Machine Learning services, and much more. AWS facilitates for both businesses and individual users with effectively hosting the applications, storing the data securely, and making use of a wide variety of tools and services improving management flexibility for IT resources. Advantages & Features of AWS: 1. Cost savings: One of the biggest benefits of AWS is that it can help businesses save money. As mentioned previously, businesses can avoid the high upfront costs of traditional infrastructure with AWS and pay only for the resources they use. Traditionally, businesses had to invest in hardware and software upfront, which often led to overspending. Let's look at this for example - if a business needs to run a website that gets 1000 visitors per day, they would need to purchase and maintain enough servers to support this traffic. With AWS, the business only pays for the compute resources they use when someone visits their website. This can result in significant cost savings. 2. Flexibility: Another key benefit of AWS is its flexibility. Businesses are able to customize their virtual environment - whether the operating system, database, programming language, or something else - to meet their specific needs. Especially in today's climate, the migration process to the cloud should be as frictionless as possible - and AWS makes that possible. Regardless of your use case or industry, AWS can be tailored to fit your needs, whether you're looking for a single cloud- hosted application or an entire suite of integrated solutions. 3. Reliability: AWS is known for being reliable, with an uptime of 99.9%. This makes it a great platform for mission-critical applications that need to be available 24/7. AWS also offers the ability to deploy resources across multiple availability zones for even greater reliability. The cloud platform also has a number of features that make it easier to ensure reliability, such as autoscaling and auto-healing. Autoscaling allows businesses to automatically scale their resources up or down based on demand, while auto-healing enables them to quickly identify and replace any faulty components. 4. Security: Businesses can take advantage of advanced security features, such as identity and access management, to help protect their data. Their tough infrastructure with an end-to-end approach is designed to withstand attacks and AWS provides customers with tools to help them monitor and respond to threats. When it comes to storage, Amazon S3 provides customers with a secure and reliable way to store and access data. The service is designed to be highly scalable and resilient, with built-in redundancy. Fine-grain identity and access controls can be applied to S3 buckets and objects, giving customers control over who has access to their data. Security tasks can be automated with AWS CloudFormation, making it easier for businesses to manage their security policies. And, you can rest easy knowing that AWS takes privacy seriously, with comprehensive customer data protection and compliance measures. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 18 N32 nirf 1750 Rank

--- Page 67 ---
Course Code/Title:CS3V15/Devops Unit: II Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013 JVM: 1.7.0_60 (Oracle Corporation 24.60-b09) OS: Windows 8.1 6.3 amd64 Understanding build using Gradle: The Gradle build is a process of creating a Gradle project. When we run a gradle command, it will look for a file called build.gradle in the current directory. This file is also called the Gradle build script. The build configuration, tasks, and plugins are described in this file. The build script describes a project and its tasks. Let's create a small Gradle project, run some of the basic Gradle commands, and understand how Gradle manages the project. Follow the steps below to create and test a Gradle project. Step1: Open the command line and create a directory First, Open the command line and create a directory for the project and change directory to it. Let's create a demo directory. C: \Users \HiMaNshU>mkdir demo C: \Users \HiMaNshU>cd demo Step2: Initialize a Gradle project To generate a Gradle project, run the gradle init command. It will generate a simple project. With this project, we will explore and understand everything that is generated. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 26 N33 nirf 1750 Rank

--- Page 68 ---
Course Code/Title:CS3V15/Devops Unit: II When we run the gradle init command, it will ask for some basic requirements. First, it will ask the type of project that we want to create. It will give four options: 1. 1: basic 2. 2: application 3. 3: library 4. 4: Gradle plugin Select our requirements. Hence, we are just making a demo project so that we will select the basic option. To select basic option, press 1 and Enter key. Consider the below output: C: \Users \HiMaNshU\demo>gradle init Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could ed, use -- status for details Select type of project to generate: 1: basic 2: application 3: library 4: Gradle plugin Enter selection (default: basic) [1 .. 4] 1 Next, it will ask for DSL. There are two options that are available for DSL: 1. 1: Groovy 2. 2: Kotlin Groovy is the default DSL provided by Gradle. Select build script DSL. Select build script DSL: 1: Groovy 2: Kotlin Enter selection (default: Groovy) [1 .. 2] 1 Next, it will ask for the project name. Type the project name and press Enter key. It will take a while to build a project. After the successful execution of the project, we will get a message BUILD SUCCESSFUL. Project name (default: demo): First_Gradle > Task : init Get more help with your project: https: //guides. gradle.org/creating- uilds BUILD SUCCESSFUL in 2m 56s 2 actionable tasks: 2 executed C: \Users\HiMaNshU\demo>mkdir src Now we have successfully created a Gradle project. Now, what will happen to our specified directory? Consider the below structure of the Gradle project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 27 N33 narf 1750 Rank

--- Page 77 ---
Unit: III Course Code/Title:CS3V15/Devops 5. Customize Jenkins: ○ Install suggested plugins or select the required plugins manually. ○ Create the first admin user. Configuration 1. Global Configuration: ○ Navigate to Manage Jenkins > Configure System. ○ Set up global environment variables, email notifications, and other system-wide settings. 2. Security Configuration: ○ Configure security settings under Manage Jenkins > Configure Global Security. o Use the built-in user database or integrate with external authentication mechanisms like LDAP. 3. Node Configuration: ○ Jenkins can distribute build loads to multiple nodes. ○ Configure nodes by navigating to Manage Jenkins > Manage Nodes and Clouds. 4. Job Configuration: ○ Create new jobs by clicking New Item on the Jenkins dashboard. o Configure job-specific settings such as source code management, build triggers, and post-build actions. 5. Pipeline Configuration: ○ Jenkins supports pipeline as code using Jenkinsfile. ○ Define your pipeline stages in a Jenkinsfile placed in the root of your project repository. 6. Plugins: ○ Enhance Jenkins functionality by installing plugins from the Manage Jenkins > Manage Plugins section. 7. Backup and Restore: ○ Regularly back up the jenkins_home directory, which contains Jenkins configurations, job configurations, and build history. ○ Use plugins like ThinBackup for automated backups. Ref: https://youtube.com/playlist?list=PL6flErFppaj35spJjPy41-IruDjw2kRV-&si=Vzd9AAMv53xwWB S Ref: https://bit.ly/youtube_jenkins (click here) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 78 ---
Course Code/Title:CS3V15/Devops Unit: III Jenkins Architecture Overview Jenkins is a robust automation server widely used for continuous integration and continuous delivery (CI/CD) processes. Here's a detailed breakdown of the Jenkins architecture and its core components. Core Components of Jenkins Architecture 1. Jenkins Master: ○ Role: The Jenkins master is the central control unit. 0 Functions: Scheduling Jobs: Assigns build jobs to the appropriate agents. Monitoring Agents: Keeps track of the state and health of agents. I Job Execution: Executes build jobs directly if no agents are available. I User Interface: Provides a web-based interface for users to configure jobs, view build results, and manage the Jenkins environment. 2. Build Agents (Nodes): ○ Role: Agents are responsible for executing the build jobs assigned by the master. ○ Types: Agents can run on different operating systems and hardware configurations. ○ Configuration: Static Agents: Permanently assigned to the Jenkins master. . Dynamic Agents: Created and destroyed as needed, often used in cloud environments. ○ Communication: Agents communicate with the master using the Jenkins Remoting protocol. 3. Job Configuration: ○ Types of Jobs: I Freestyle Projects: Basic job type with simple configurations. Pipeline Jobs: Scripted or declarative pipelines that define the entire build process as code. I Multi-Configuration (Matrix) Projects: Allows testing across different environments and configurations. ○ Components: I Source Code Management: Integration with version control systems like Git, SVN, etc. . Build Triggers: Conditions that start the job, such as code commits, scheduled times, or manual triggers. Build Steps: Actions performed during the build, such as compiling code, running tests, and packaging artifacts. . Post-Build Actions: Steps executed after the build, like deploying artifacts, sending notifications, or archiving results. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 53 ---
Course Code/Title:CS3V15/Devops Unit: II <execution> <phase>test</phase> <goals> <goal>run</goal> </goals> <configuration> <tasks> <echo>Using env.test.properties</echo> <copy file="src/main/resources/env.test.properties" tofile="${project.build.outputDirectory}/env.properties"/> </tasks> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> Now open the command console, go to the folder containing pom.xml and execute the following mvn command. Pass the profile name as argument using -P option. C:\MVN\project>mvn test -Ptest Maven will start processing and displaying the result of test build profile. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 12 N32 nirf 1750 Rank

--- Page 54 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Maven Settings Open Maven settings.xml file available in %USER_HOME%/.m2 directory where %USER_HOME% represents the user home directory. If settings.xml file is not there, then create a new one. Add test profile as an active profile using active Profiles node as shown below in example. <settings xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"> <mirrors> <mirror> <id>maven.dev.snaponglobal.com</id> <name>Internal Artifactory Maven repository</name> <url>http://repo1.maven.org/maven2/</url> <mirrorOf> *< /mirrorOf> /mirror </mirrors> <activeProfiles> <activeProfile>test</activeProfile> </activeProfiles> </settings> Now open command console, go to the folder containing pom.xml and execute the following mvn command. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 13 N33 nirf 1750 Rank

--- Page 49 ---
Course Code/Title:CS3V15/Devops Unit: II Validate Deploy Compile Install Test Verify Package Integration Test · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. There are always pre and post phases to register goals, which must run prior to, or after a particular phase. When Maven starts building a project, it steps through a defined sequence of phases and executes goals, which are registered with each phase. Maven has the following three standard lifecycles - • default: This is the main lifecycle, as it's responsible for project deployment. CHENNAI CHENNAI INSTITUTE . TECHNOLOGY 8 clean: Handles project cleaning, ensuring that all artifacts generated by previous INSTITUTE OF TECHNOLOGY 175º Rank (Autonomous]

--- Page 50 ---
Course Code/Title:CS3V15/Devops Unit: II builds are removed. · site: Manages the creation of the project's site documentation. Default Lifecycle (default) Most Maven users will be familiar with the default lifecycle. It is a general model of a build process for a software application. The first phase is validate and the last phase is deploy. · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. Clean Lifecycle (clean) The first lifecycle in Maven. Running mvn clean invokes the clean lifecycle which consists of three lifecycle phases: · pre-clean : execute processes needed prior to the actual project cleaning · clean : remove all files generated by the previous build · post-clean : execute processes needed to finalize the project cleaning Site Lifecycle (site) Maven does more than build software artifacts from project, it can also generate project documentation and reports about the project, or a collection of projects. Project documentation and site generation have a dedicated lifecycle which contains four phases: 1. pre-site : execute processes needed prior to the actual project site generation 2. site : generate the project's site documentation 3. post-site : execute processes needed to finalize the site generation, and to prepare for site deployment 4. site-deploy: deploy the generated site documentation to the specified web server CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 75 ---
Course Code/Title:CS3V15/Devops Customization This tool is highly customizable as it supports a variety of IDE's . Languages supported Project Configuration Based on Goal of time to get used to it. available. It supports software development in Java, C, C++, and Groovy. For declaring the project configuration, it does not use the XML files. On the phases of the fixed and linear model. Graph of task dependencies that do the work. To add functionality in the project is the main goal of the Gradle. To finish the project in the given timeline is the main goal of the Maven. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY Unit: II This tool serves a limited number of developers and is not that customizable. It supports software development in Java, Scala, C# , and Ruby and it does not natively support C and C+ but can support through plugins like "maven-native- plugin" or we can integrate other build systems like CMake or Makefile. For declaring the project configuration, it uses the XML files. 34 N33 nirf 1750 Rank

--- Page 76 ---
Course Code/Title:CS3V15/Devops Unit: III Unit - III CONTINUOUS INTEGRATION USING JENKINS Install & Configure Jenkins, Jenkins Architecture Overview, creating a Jenkins Job, configuring a Jenkins job, Introduction to Plugins, Adding Plugins to Jenkins, commonly used plugins (Git Plugin, Parameter Plugin, HTML Publisher, Copy Artifact and Extended choice parameters). Configuring Jenkins to work with java, Git and Maven, creating a Jenkins Build and Jenkins workspace. Installing and Configuring Jenkins Overview Jenkins is a widely-used open-source automation server that helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration and continuous delivery (CI/CD). Below are detailed steps and considerations for installing and configuring Jenkins. Installation 1. System Requirements: ○ Operating System: Jenkins can run on any major operating system. ○ Java: Jenkins requires Java Runtime Environment (JRE) 8 or later. 2. Download Jenkins: ○ Download the latest version of Jenkins from the official Jenkins website. 3. Installation Steps: ○ Windows: I Run the .msi installer and follow the installation wizard. ■ After installation, Jenkins will start automatically as a Windows service. ○ Linux: Add Jenkins repository and import the GPG key. I Use package manager to install Jenkins (e.g., apt-get install jenkins). I Start Jenkins using systemctl (systemctl start jenkins). 4. Initial Setup: ○ After installation, Jenkins can be accessed through a web browser at http : //< your _server _ip_or_domain>: 8080. o Unlock Jenkins by entering the initial admin password, which can be found in the jenkins_home directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 1 N32 nirf 1750 Rank

--- Page 29 ---
Unit: I Course Code/Title:CS3V15/Devops Enterprise services. AWS virtual private cloud. Outsourced cloud services. Salesforce User Controls Operating System, Runtime, Middleware, and Application data Others It is highly scalable and flexible. Microsoft Azure. Force.com, Gigaspaces. Data of the application It is highly scalable to suit the different businesses according to resources. Google cloud services Google offers a seven wide range of Services: · Compute · Networking · Storage and Databases · Big Data · Machine Learning · Identity & Security · Management and Developer Tools IBM cloud analysis. AWS, Terremark Nothing It is highly scalable to suit the small, mid and enterprise level business 1. Compute: GCP provides a scalable range of computing options you can tailor to match your needs. It provides highly customizable virtual machines. and the option to deploy your code directly or via containers. · Google Compute Engine · Google App Engine · Google Kubernetes Engine · Google Cloud Container Registry · Cloud Functions 2. Networking: The Storage domain includes services related to networking, it includes the following services • Google Virtual Private Cloud (VPC) . Google Cloud Load Balancing · Content Delivery Network · What is Google Cloud Connect · Google Cloud DNS · What is Google Cloud Web Hosting 3. Storage and Databases: The Storage domain includes services related to data storage, it includes the following services · Google Cloud Storage . Cloud SQL · Cloud Bigtable · Google Cloud Datastore • Persistent Disk TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 29 N32 nirf 1750 Rank

--- Page 30 ---
Unit: I Course Code/Title:CS3V15/Devops 4. Big Data: The Storage domain includes services related to big data, it includes the following services · Google BigQuery · Google Cloud Dataproc · Google Cloud Datalab · Google Cloud Pub/Sub 5. Cloud AI: The Storage domain includes services related to machine learning, it includes the following services · Cloud Machine Learning · Vision API · Speech API · Natural Language API · Translation API • Jobs API 6. Identity & Security: The Storage domain includes services related to security, it includes the following services · Cloud Resource Manager · Cloud IAM · Cloud Security Scanner · Cloud Platform Security 7. Management Tools: The Storage domain includes services related to monitoring and management, it includes the following services • Stackdriver · Monitoring · Logging · Error Reporting · Trace · Cloud Console 8. Developer Tools: The Storage domain includes services related to development, it includes the following services . Cloud SDK · Deployment Manager · Cloud Source Repositories · Cloud Test Lab AZURE Azure is Microsoft's cloud platform, just like Google has its Google Cloud and Amazon has its Amazon Web Service or AWS.000. Generally, it is a platform through which we can use Microsoft's resources. For example, to set up a huge server, we will require huge investment, effort, physical space, and so on. In such situations, Microsoft Azure comes to our rescue. It will provide us with virtual machines, fast processing of data, analytical and monitoring tools, and so on to make TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 30 N33 narf 1750 Rank

--- Page 41 ---
Unit: I Course Code/Title:CS3V15/Devops S.No. Git 1 Git is a software. 2 Git is a command-line tool GitHub is a graphical user interface 3 Git is installed locally on the system GitHub is hosted on the web 4 Git is maintained by linux. GitHub is maintained by Microsoft. 5 Git is focused on version control and code sharing. GitHub is a hosting service for Git repositories. 6 Git is a version control system to manage source code history. 7 Git was first released in 2005. GHub was launched in 2008. 8 Git has no user management feature. 9 Git is open-source licensed. GitHub includes a free-tier and pay- for- use tier. 10 Git has minimal external tool configuration. 11 Git provides a Desktop interface named Git Gui. GitHub provides a Desktop interface named GitHub Desktop. 12 Git competes with CVS, Azure DevOps Server, Subversion, Mercurial, etc. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY GitHub GitHub is a service. GitHub is focused on centralized source code hosting. GitHub has a built-in user management feature. GitHub has an active marketplace for tool integration. GitHub competes with GitLab, Bit Bucket, AWS Code Commit, etc. 41 N32 nirf 1750 Rank

--- Page 42 ---
Course Code/Title:CS2V15/Devops Unit: II UNIT - II COMPILE AND BUILD USING MAVEN AND GRADLE Introduction, Installation of Maven, POM files, Maven Build lifecycle, Build phases(compile build, test, package) Maven Profiles, Maven repositories(local, central, global),Maven plugins, Maven create and build Artifacts, Dependency management, Installation of Gradle, Understand build using Gradle. 1. Introduction of Maven: Apache Maven is an automation tool. The tool is written in Java. It was initially released on 13 July 2004. It is developed by the Apache software foundation. It is part of the Jakarta Project. It is working on two aspects: how software is built, and its dependencies. It was created by Jason van Zyl. It is built by using a plugin-based architecture that allows it to make the use of any application controllable by standard input. It dynamically downloads Java libraries. Understanding the problem without Maven There are many problems that we face during the project development. They are discussed below: 1) Adding set of Jars in each project: In case of struts, spring, hibernate frameworks, we need to add set of jar files in each project. It must include all the dependencies of jars also. 2) Creating the right project structure: We must create the right project structure in servlet, struts etc, otherwise it will not be executed. 3) Building and Deploying the project: We must have to build and deploy the project so that it may work. Maven simplifies the above mentioned problems. It does mainly following tasks. 1. It makes a project easy to build 2. It provides uniform build process (maven project can be shared by all the maven projects) 3. It provides project information (log document, cross referenced sources, mailing list, dependency list, unit test reports etc.) 4. It is easy to migrate for new features of Maven Apache Maven helps to manage o Builds Documentation o Reporing o SCMs o Releases o Distribution What is Build Tool? A build tool is used for building a process. It does following: CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE TECHNOLOGY N32 nirf 1 1750 Rank

--- Page 25 ---
Course Code/Title:CS3V15/Devops Unit: I · backups · automated multi-region replication in-memory caching · data export tools. Global Infrastructure of AWS · AWS is a cloud computing platform which is globally available. . Global infrastructure is a region around the world in which AWS is based. Global infrastructure is a bunch of high-level IT services which is shown below: AWS is available in 19 regions, and 57 availability zones in December 2018 and 5 more regions 15 more availability zones for 2019. The following are the components that make up the AWS infrastructure: o Availability Zones o Region o Edge locations o Regional Edge Caches Compnents of Global Infrastructure Availability zones Region Edge Locations Regional Edge Caches Availability zone as a Data Center An availability zone is a facility that can be somewhere in a country or in a city. Inside this facility, i.e., Data Centre, we can have multiple servers, switches, load balancing, firewalls. The things which interact with the cloud sits inside the data centers. o An availability zone can be a several data centers, but if they are close together, they are counted as 1 availability zone. Region o A region is a geographical area. Each region consists of 2 more availability zones. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 26 ---
Course Code/Title:CS3V15/Devops Unit: I o A region is a collection of data centers which are completely isolated from other regions. · A region consists of more than two availability zones connected to each other through links. AZ-a AZ-b AZ-c Availability zones are connected through redundant and isolated metro fibers. Edge Locations o Edge locations are the endpoints for AWS used for caching content. o Edge locations consist of CloudFront, Amazon's Content Delivery Network (CDN). o Edge locations are more than regions. Currently, there are over 150 edge locations. Edge location is not a region but a small location that AWS have. It is used for caching the content. o Edge locations are mainly located in most of the major cities to distribute the content to end users with reduced latency. o For example, some user accesses your website from Singapore; then this request would be redirected to the edge location closest to Singapore where cached data can be read. Regional Edge Cache o AWS announced a new type of edge location in November 2016, known as a Regional Edge Cache. o Regional Edge cache lies between CloudFront Origin servers and the edge locations. o A regional edge cache has a large cache than an individual edge location. o Data is removed from the cache at the edge location while the data is retained at the Regional Edge Caches. o When the user requests the data, then data is no longer available at the edge location. Therefore, the edge location retrieves the cached data from the Regional edge cache instead of the Origin servers that have high latency. What Is Cloud Computing? Cloud computing is the use of hardware and software components in an off-premises location to deliver a service to a network. Users can access files and applications from any device that can access the internet. Some features and capabilities include: · Cloud providers can pull the computing resources to provide services to multiple customers with the help of a multi-tenant model · Cloud computing proves an on-demand self-service, which helps administrators monitor performance CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 26 N32 nirf 1750 Rank -

--- Page 125 ---
Course Code/Title:CS3V15/Devops Unit: IV This command executes the specified playbook while prompting for the necessary privilege escalation password. Output and Result: Activities Terminal - Nov 2 13:00 . auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook-eg2.yaml PLAY [Install Nginx] TASK [Gathering Facts] Total: [vnet]: FAILEDT .. ['msg": "Missing sudo password') PLAY RECAP : ok=0 changed=0 unreachable=0 Fatled =! skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory. yaml playbook-eg2.yaml - - ask-becone-pass BECOME password: PLAY [Install Nginx] TASK [Gathering Facts] ok: [vri01] TASK [Install Nginx using apt] ok: [vri01] PLAY RECAP * VMD1 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ [] As shown in the example image above, the first command encountered an error due to the absence of the -ask-become-pass flag. This flag is essential when your task requires elevated privileges, as it prompts Ansible to request the sudo password for authentication. The corrected command includes the -ask-become-pass flag, ensuring that the necessary privileges are obtained before executing the task. This is particularly important when working with tasks that require administrative access, such as package installations or system configurations. Note: In this blog, we primarily focus on the core and fundamental components of Ansible, providing an introductory overview and understanding of its key concepts. While Ansible offers advanced features such as "Collections," "Ansible Galaxy," "Roles," "Dynamic Inventory," and "Custom Modules and Plugins," we recognize that these topics are extensive and may require separate dedicated discussions. For the purpose of this blog, we aim to establish a strong foundation of Ansible's core components. If you're interested in diving deeper into these advanced topics, we recommend exploring Ansible's official documentation and additional resources dedicated to each subject. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 11 N32 nirf 1750 Rank

--- Page 126 ---
Course Code/Title:CS3V15/Devops Unit:IV Understanding YAML Ansible uses YAML syntax for expressing Ansible playbooks. This chapter provides an overview of YAML. Ansible uses YAML because it is very easy for humans to understand, read and write when compared to other data formats like XML and JSON. Every YAML file optionally starts with " --- " and ends with " ... ". In this section, we will learn the different ways in which the YAML data is represented. key-value pair YAML uses simple key-value pair to represent the data. The dictionary is represented in key: value pair. Note - There should be space between : and value. Example: A student record --- #Optional YAML start syntax james: name: james john rollNo: 34 div: B sex: male ... #Optional YAML end syntax Abbreviation You can also use abbreviation to represent dictionaries. Example James: {name: james john, rollNo: 34, div: B, sex: male} Representing List We can also represent List in YAML. Every element(member) of list should be written in a new line with same indentation starting with "- " (- and space). Example countries: - America - China - Canada - Iceland CHENNAI INSTITUTE OF TECHNOLOGY CHENNAI INSTITUTE . TECHNOLOGY (Autonomous) 12 N33 nirf 1750 Rank

--- Page 39 ---
Course Code/Title:CS3V15/Devops Unit: I GITHUB GitHub is an increasingly popular programming resource used for code sharing. It's a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in October 2020, it is the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 56 million total developers. GitHub is a Git repository hosting service that provides a web-based graphical interface. It is the world's largest coding community. Putting a code or a project into GitHub brings it increased, widespread exposure. Programmers can find source codes in many different languages and use the command-line interface, Git, to make and keep track of any changes. GitHub helps every team member work together on a project from any location while facilitating collaboration. You can also review previous versions created at an earlier point in time. GitHub's Features? 1. Easy Project Management: GitHub is a place where project managers and developers come together to coordinate, track, and update their work so that projects are transparent and stay on schedule. 2. Increased Safety With Packages Packages can be published privately, within the team, or publicly to the open-source community. The packages can be used or reused by downloading them from GitHub. 3. Effective Team Management GitHub helps all the team members stay on the same page and organized. Moderation tools like Issue and Pull Request Locking help the team to focus on the code. 4. Improved Code Writing Pull requests help the organizations to review, develop, and propose new code. Team members can discuss any implementations and proposals through these before changing the source code. 5. Increased Code Safety GitHub uses dedicated tools to identify and analyze vulnerabilities to the code that other tools tend to miss. Development teams everywhere work together to secure the software supply chain, from start to finish. 6. Easy Code Hosting All the code and documentation are in one place. There are millions of repositories on GitHub, and each repository has its own tools to help you host and release code. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 39 N32 nirf 1750 Rank

--- Page 40 ---
Course Code/Title:CS3V15/Devops Unit: I HOSTING SERVICE FOR GIT REPOSITORY When it comes to hosting Git repositories, various platforms provide a robust infrastructure for collaborative development, version control, and project management. Choosing the right hosting service depends on factors like ease of use, collaboration features, and integration capabilities. Here are some popular Git hosting services widely utilized in the software development community: 1. GitHub: GitHub stands out as one of the most prevalent Git hosting platforms, offering a user- friendly interface, powerful collaboration features, and seamless integration with various tools. It serves as an ideal choice for open-source projects, private repositories, and team collaboration. 2. GitLab: GitLab is a comprehensive web-based Git repository manager that not only provides source code management but also includes features like continuous integration. It caters to both cloud-based and self-hosted solutions, giving users flexibility in hosting their repositories. 3. Bitbucket: Owned by Atlassian, Bitbucket is another popular Git repository hosting service. Supporting both Git and Mercurial repositories, it offers features like code collaboration, issue tracking, and continuous integration. Bitbucket is often preferred by teams using other Atlassian tools such as Jira and Confluence. 4. GitKraken Glo Boards: GitKraken Glo Boards is an integrated task and issue tracking service linked with GitKraken, a Git client. This platform allows teams to manage tasks directly associated with their Git repositories and provides a visual approach to monitoring project progress. 5. SourceForge: SourceForge, with a long history, hosts open-source software projects and offers version control, bug tracking, and project management tools. While not as prominent as some other options, it remains a viable choice for numerous projects. 6. AWS CodeCommit: As part of Amazon Web Services (AWS), AWS CodeCommit is a fully managed source control service. It seamlessly integrates with other AWS services and provides a secure and scalable environment for hosting Git repositories. Selecting the most suitable Git hosting service depends on your team's requirements, project size, and preferences for cloud-based or self-hosted solutions. Each platform has its strengths, catering to specific use cases within the software development landscape. Difference between GIT and GITHUB Git and GitHub are related concepts but serve different purposes in the context of version control and collaborative software development. Git: Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows. GitHub: GitHub is a web-based Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 40 N32 nirf 1750 Rank

--- Page 15 ---
Course Code/Title:CS3V15/Devops Unit: I which is a mechanism to control the usage of resources or capacity. 2) Code: Many good practices such as Git enables the code to be used, which ensures writing the code for business, helps to track changes, getting notified about the reason behind the difference in the actual and the expected output, and if necessary reverting to the original code developed. The code can be appropriately arranged in files, folders, etc. And they can be reused. 3) Test: The application will be ready for production after testing. In the case of manual testing, it consumes more time in testing and moving the code to the output. The testing can be automated, which decreases the time for testing so that the time to deploy the code to production can be reduced as automating the running of the scripts will remove many manual steps. 4) Plan: DevOps use Agile methodology to plan the development. With the operations and development team in sync, it helps in organizing the work to plan accordingly to increase productivity. 5) Monitor: Continuous monitoring is used to identify any risk of failure. Also, it helps in tracking the system accurately so that the health of the application can be checked. The monitoring becomes more comfortable with services where the log data may get monitored through many third-party tools such as Splunk. 6) Deploy: Many systems can support the scheduler for automated deployment. The cloud management platform enables users to capture accurate insights and view the optimization scenario, analytics on trends by the deployment of dashboards. 7) Operate: DevOps changes the way traditional approach of developing and testing separately. The teams operate in a collaborative way where both the teams actively participate throughout the service lifecycle. The operation team interacts with developers, and they come up with a monitoring plan which serves the IT and business requirements. 8) Release: Deployment to an environment can be done by automation. But when the deployment is made to the production environment, it is done by manual triggering. Many processes involved in release management commonly used to do the deployment in the production environment manually to lessen the impact on the customers. DevOps Lifecycle DevOps defines an agile relationship between operations and Development. It is a process that is practiced by the development team and operational engineers Learning DevOps is not complete without understanding the DevOps lifecycle phases. The DevOps lifecycle includes seven phases as given below: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 16 ---
Unit: I Course Code/Title:CS3V15/Devops together from beginning to the final stage of the product. Development Operations Integration Deployment DevOps Lidecycle Feedback 2) Continuous Integration Testing Monitering 1) Continuous Development This phase involves the planning and coding of the software. The vision of the project is decided during the planning phase. And the developers begin developing the code for the application. There are no DevOps tools that are required for planning, but there are several tools for maintaining the code. This stage is the heart of the entire DevOps lifecycle. It is a software development practice in which the developers require to commit changes to the source code more frequently. This may be on a daily or weekly basis. Then every commit is built, and this allows early detection of problems if they are present. Building code is not only involved compilation, but it also includes unit testing, integration testing, code review, and packaging. The code supporting new functionality is continuously integrated with the existing code. Therefore, there is continuous development of software. The updated code needs to be integrated continuously and smoothly with the systems to reflect changes to the end-users. Commit Build O O Test O Stage Deploy Dev/QA O O > Continuous Integration/Delivery Development </> git Code Commit = production server. 3) Continuous Testing Production Jenkins is a popular tool used in this phase. Whenever there is a change in the Git repository, then Jenkins fetches the updated code and prepares a build of that code, which is an executable file in the form of war or jar. Then this build is forwarded to the test server or the This phase, where the developed software is continuously testing for bugs. For constant testing, automation testing tools such as TestNG, JUnit, Selenium, etc are used. These tools allow QAs to test multiple code- bases thoroughly in parallel to ensure that there is no flaw in the functionality. In this phase, Docker Containers can be used for simulating the test environment. It may occur in the form of documentation files or maybe produce large-scale data about the application parameters when it is in a continuous use position. The system errors such as server not reachable, low memory, etc are resolved in this phase. It maintains the security and availability of the service. Automation testing saves a lot of time and effort for executing the tests instead of doing this manually. Apart from that, report generation is a big plus. The task of evaluating the test cases INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 narf 1750 Rank

--- Page 19 ---
Course Code/Title:CS3V15/Devops Unit: I 5. Compliance: By compliance, we mean that certain businesses are required to follow specific regulations. Financial services companies in the United States, for example, must comply with the Sarbanes-Oxley Act, while healthcare, education, and energy companies must comply with HIPAA and other regulations. AWS provides a number of compliance-related features and services, such as data encryption and identity and access management, to help businesses meet these requirements. 6. High-Performance: Interested in delivering your applications quickly and efficiently? Taking advantage of AWS features such as auto-scaling and load balancing will help ensure your applications are always available and running optimally. AWS can help businesses improve their performance by offering a variety of cloud-based services, including Amazon Elastic Compute Cloud (EC2), which provides high-performance computing resources, and Amazon CloudFront, which delivers content quickly and securely to users around the world. Others include machine learning (ML) and analytics services, such as Amazon SageMaker and Amazon Athena. These services provide the tools businesses need to quickly and easily analyze their data for insights. Fast networking in the cloud is also possible with AWS, thanks to its Elastic Load Balancing (ELB) and Amazon Virtual Private Cloud (VPC). With ELB, businesses can balance their workloads across multiple instances for increased performance, while VPC allows businesses to create isolated private networks in the cloud. 7. Developer Tools: Developer tools are designed to make it easier for developers to create, deploy, and manage applications - and AWS provides developers with what they need to build applications quickly and easily. By leveraging developer tools, developers can save time and money by automating tedious tasks. They also benefit from access to AWS's extensive library of pre-built applications that can help them get their projects off the ground quickly. Services such as Amazon Elastic Beanstalk and Amazon CloudFormation can help them automate the process of creating and deploying applications. Other ways developers can improve productivity with AWS include using AWS Code Commit to store and manage source code. 8. Integration: Thanks to its many integrations with other Amazon services, as well as third-party services, AWS makes it easy for businesses to get started with cloud computing. AWS provides a wide range of services that can be easily integrated into existing business infrastructure. This allows businesses to add new features and capabilities without having to make major changes or invest in new hardware or software. For instance, if a business wants to add mobile capabilities to its website, it can take advantage of Amazon's Mobile SDK and Web Services. These tools allow businesses to quickly develop and deploy mobile apps that connect directly with their existing infrastructure. 9. Management Console: The AWS management console is a web-based interface that provides users with a simple way to interact with and manage their AWS resources - essentially a place where you can access and manage everything on the cloud. It provides a graphical view of all the resources associated with an account, as well as tools for creating and configuring new resources. Compared to traditional command- line interfaces, the AWS management console saves time and makes it easier for users to get the most out of their AWS services. Not only that, but your business gets access to 350+ free digital training courses through the AWS Academy, covering topics such as cloud fundamentals, DevOps, security, and big data. This means you can train your employees on how to use AWS, and in turn, help them become more efficient at their jobs. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 19 N32 nirf 1750 Rank

--- Page 20 ---
Course Code/Title:CS3V15/Devops Unit: I 10. Scalability: With an on-demand service, businesses can quickly spin up new servers as needed with just a few clicks. This makes it much easier to scale resources up or down as demand changes, allowing businesses to save costs and maintain performance even during peak periods. For example, if a business is expecting a sudden surge in traffic due to an advertising campaign or seasonal event, they can easily add more capacity to their server infrastructure to handle the increased load. Bru Textiles, a specialty textile company in Belgium, was able to quickly scale its infrastructure by leveraging AWS. Bru Textiles went digital to grow and offer new services. Embracing technology, they brought in digital twin technology to give their customers an idea of the texture and essence of their physical fabrics. AWS Applications · Storage and Backup: Storage and backup are important for any Cloud Computing service. AWS provides you with reliable storage services like Amazon Simple Storage Service to store large- scale data and backup services like AWS Backup to take backups of this data, which is stored in other AWS services. AWS stores the data in three different availability zones so that if one fails, you can still access your data. This makes AWS storage reliable and easily accessible. Therefore, companies with huge application data to store and backup securely can use AWS. · Big Data: One of the biggest challenges faced by companies these days is Big Data. The companies are struggling to store their large amounts of data using traditional methods. With AWS Big Data storage services, they can manage to store their data even if the data limit increases unexpectedly as AWS provides virtually unlimited data storage with scale-in and scale- out options. AWS offers easy access and faster data retrieval as well. For data processing, it offers services like EMR, with which the companies can easily set up, operate, and scale their big data. Therefore, efficiently storing and managing Big Data is among the top AWS applications. · Enterprise IT: AWS is a one-stop solution for any IT business. Many features of it such as secure storage, scalability, flexibility, and elasticity support companies to innovate faster than ever before. Using AWS for IT enterprises makes them profitable in terms of both money and time. As AWS maintains its cloud architecture, it need not waste time and money on professionals to do the same. · Social Networking: Social networking is essential for businesses in the present-day scenario where Digital Marketing is key, and it is easier with AWS. Companies can connect with customers and stakeholders and communicate through social networking sites and develop their business. Services like AWS social networking engine, which is powered by Turn Key GNU/Linux (HVM) AMI stack, are used for performance and scalability to help companies build a suitable social networking site and gain profits. · Mobile Apps: Mobile applications are embedded with day-to-day life. With AWS, you have the facility to create an app in your desired programming language. You can also keep up the applications that are consistently accessible and solid with high compute, storage, database, and application services. You can take advantage of AWS auto-scaling and managed relational database service for the better performance of your apps. · Websites: AWS offers a wide range of website hosting options to create the best website for customers. Its services like Amazon Light sail have everything, such as a virtual machine, SSD- based storage, data transfer, DNS management, and a static IP, to launch a website in such a way that the user can manage the website easily. Amazon EC2, AWS Lambda, Elastic Load Balancing, AWS Amplify, Amazon S3, etc. also help users build reliable and scalable websites. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 63 ---
Course Code/Title:CS3V15/Devops Unit: II 10. Introduction of Gradle: The list of features that Gradle provides, · Gradle is available with separate Domain Specific Language (DSL) based on Groovy language. · It provides the declarative language elements. Those elements also provide build-by- convention support for Java, Groovy, OSGI, Web and Scala. Language for dependency based programming The declarative language lies on a top of a general purpose task graph, which can be fully supported in the build. Structure your build Gradle allows you to apply common design principles to your build. It will give you a perfect structure for build, so that, you can design well-structured and easily maintained, comprehensible build. Deep API By using this API, you can monitor and customize its configuration and execution behavior to the core. Gradle scales Gradle can easily increase the productivity, from simple and single project builds to huge enterprise multi-project builds. Multi-project builds Gradle supports the multi-project builds and partial builds. If you build a subproject, Gradle takes care of building all the subprojects, that the subproject depends on. Different ways to manage your builds Gradle supports different strategies to manage your dependencies. Gradle is the first build integration tool Gradle is fully supported for your ANT tasks, Maven and lvy repository infrastructure for publishing and retrieving dependencies. It also provides a converter for turning a Maven pom.xml to Gradle script. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 22 N32 nirf 175º Rank -

--- Page 64 ---
Course Code/Title:CS3V15/Devops Unit: II Ease of migration Gradle can easily adapt to any structure. Therefore, you can always develop your Gradle build in the same branch, where you can build live script. Gradle Wrapper Gradle Wrapper allows you to execute the Gradle builds on machines, where Gradle is not installed. This is useful for continuous integration of servers. Free open source Gradle is an open source project, and licensed under the Apache Software License (ASL). Groovy Gradle's build script are written in Groovy programming language. The whole design of Gradle is oriented towards being used as a language and not as a rigid framework. Groovy allows you to write your own script with some abstractions. The whole Gradle API is fully designed in Groovy language. Installation of Gradle: Prerequisites to install Gradle JDK and Groovy are the prerequisites for Gradle installation. Gradle requires JDK version 6 or later to be installed in the system. It uses the JDK libraries which are installed, and sets to the JAVA_HOME environmental variable. Gradle carries its own Groovy library, therefore, we need not install Groovy explicitly. If it is installed, that is ignored by Gradle. The steps to install Gradle in your system are explained below. Step 1 - Verify JAVA Installation First of all, you need to have Java Software Development Kit (SDK) installed on your system. To verify this, execute Java -version command in any of the platform you are working on. In Windows Execute the following command to verify Java installation. I have installed JDK 1.8 in my system. C:\> java - version CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE . TECHNOLOGY 23 N32 narf 1750 Rank

--- Page 3 ---
Course Code/Title:CS3V15/Devops Unit: I Key Goals and Benefits of DevOps Goals of DevOps The fast-paced growth of the IT industry and continuous advancements in technology make it critical to set DevOps goals that are experimental and challenging for companies to compete and thrive in the market. Here are the key goals and principles that every successful DevOps program has in common. 1. Ensures effective collaboration between teams: Effective collaboration in any process relies on shared ownership. During the development process, all those involved should embrace the fact that everyone is equally responsible for the entire development process. Whether it is development, testing, or deployment, each team member should be involved. They should understand that they have an equal stake in the final outcome. In the DevOps paradigm, passing of work from one team to another is completely defined and broken down. This accelerates the entire process of development since collaboration between all the teams involved is streamlined. 2. Creates scalable infrastructure platforms: The primary focus of DevOps is to create a sustainable infrastructure for applications that make them highly scalable. According to the demands of the modern-day business world, scalable apps have become an absolute necessity. In an ideal situation, the process of scaling should be reliable and fully automated. As a result, the app will have the ability to adapt to any situation when a marketing effort goes viral. With the app being scalable, it can adjust itself to large traffic volumes and provide an immaculate user experience. 3. Builds on-demand release capabilities: Companies must focus on keeping their software in a 'releasable' state. Continuous delivery will allow the software to add new features and go live at any stage. DevOps aims to automate the process of release management because it has a plethora of advantages. Automated release management is predictable, fast, and very consistent. Moreover, through automation, companies can release new versions as per their requirements. Automated release management also has complete and thorough audit trials, as these are essential for compliance purposes. 4. Provides faster feedback: Automating monotonous tasks such as testing and reporting will accelerate the process of rapid feedback. Since the development team will know what has to change, it can roll out the updated version faster. In addition, the team can better understand the impact of the changes that it has done in the software lifecycle. A concrete understanding of changes will assist team members in working efficiently in tandem. With rapid feedback, the operations team and developers can make better decisions collectively and enhance the app's performance. Benefits of DevOps DevOps helps organizations deliver added value to their customers. Here are some compelling benefits of DevOps. 1. Smarter work and faster release: With DevOps, your development team can release the required deliverables quickly. Faster release of deliverables will keep you miles ahead of your competitors, which is very important in today's cut-throat business realm. Businesses should understand that if their review cycle is not automated, it will slow down the release process. Moreover, the inclusion of disparate tools will lead to context switching and higher costs. Thus, DevOps can help rectify this worrisome business situation. 2. Quick resolution of issues: In a business world where speed and accuracy are paramount, a fast feedback loop will help you thrive. With DevOps, the communication process becomes seamless, and, as such, it minimizes the time required to solve issues. Without open communication, key issues can slip out of mind, which will have serious repercussions in the long run. DevOps fosters open communication that helps resolve issues, thus unblocking the release pipeline faster. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 3 N32 nirf 1750 Rank

--- Page 4 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Better collaboration between teams: DevOps paves the way for more dynamic and round-the- clock communication between teams. It renders an environment for mutual collaboration and integration among teams that are distributed globally. Eliminating the traditional departmental barriers between teams forms a new sense of ownership, wherein each team member feels equally responsible for meeting delivery timelines. This collaboration contributes to happier and more engaged employees. 4. Fostering innovative mindsets: With DevOps, deployment phases of the application are more relaxed as compared to traditional methods. This is because it streamlines the entire process, ensures that there are no lapses in quality, and allows on-time and efficient release. Thus, as everything is in order, the development team is more at peace. This allows it to think out of the box and provide additional value to the user. Having a development team with an innovative mindset is a boon for any business organization. An innovative approach, in itself, has immense scope and leads to better quality and resolution of issues at hand. Thus, through DevOps, the process of expanding the horizon of an app becomes much easier. 5. Faster threat detection: Automated and continuous testing of the code will make the process of threat detection faster. As developers can locate problem areas at an early stage, they can then resolve them faster. Thus, DevOps is a vital cog in maintaining and enhancing the quality and performance of an app. As the overall build of the app is in capable hands, teams working together are empowered to share feedback as and when necessary. 6. Increased customer satisfaction: Customer satisfaction is paramount in any day and age, irrespective of the business one is involved in. DevOps is known for enhancing customer experience, which ultimately increases the level of customer satisfaction. Dissatisfied customers are never a good sign for any business. Feedback loops are an important component of DevOps. These loops empower end users to track the progress of app development at various stages. 7. In addition, they can suggest changes (if any) or give their inputs to make the app more customer- centric. Due to their dynamic nature, feedback loops help developers and customers remain on the same page. Moreover, DevOps accelerates the process of app development, which eventually lessens the delivery timer. This has a positive impact on the customer satisfaction ratio. 8. Providing the much-needed edge: Along with staying true to their development process, companies need to ensure that they sustain themselves in the cut-throat competition. Implementing DevOps can be your trump card to provide your organization with that much- needed edge. Competitive advantage is necessary, as it can become the deciding factor in the popularity of an application in many cases. Some factors set expert businesses apart from mediocre ones: · Top-quality features · Quicker and timely software releases · Maximizing return on investments · Listening to constructive feedback Difference between Agile and DevOps Agile: Agile program advancement comprises different approaches to computer program improvement beneath which prerequisites and arrangements advance through the collaborative exertion of self- organizing and cross-functional groups and their customer/end client. DevOps: DevOps could be a set of hones that combines program improvement and information- technology operations which points to abbreviating the framework's advancement life cycle and giving nonstop conveyance with tall program quality. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 4 N32 nirf 1750 Rank

--- Page 79 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Plugins: ○ Role: Extend Jenkins functionality without modifying the core software. ○ Types: I Source Control Plugins: Git, SVN, Mercurial. Build Tools Plugins: Maven, Gradle, Ant. Notification Plugins: Email, Slack, HipChat. · Reporting Plugins: JUnit, Cobertura, Checkstyle. ○ Management: Plugins can be installed, updated, and configured via the Jenkins web interface. 5. Master-Slave Architecture: ○ Concept: The Jenkins master distributes build tasks to multiple agents, enabling distributed builds. ○ Benefits: Scalability: Handle more builds simultaneously. I Resource Management: Assign builds to agents with the necessary resources. Isolation: Run builds in isolated environments to avoid conflicts. Workflow and Data Flow 1. Job Trigger: ○ Jobs can be triggered by various events: Manual Trigger: Initiated by a user through the Jenkins interface. Scheduled Trigger: Using CRON-like syntax to schedule jobs. · SCM Trigger: Automatically triggered by changes in the source code repository. Upstream/Downstream Trigger: Triggered by the completion of other jobs. 2. Build Execution: ○ Job Assignment: The master assigns the job to an available agent based on labels, availability, and resource requirements. Build Environment Setup: The agent sets up the environment, including checking out the code, installing dependencies, and configuring the workspace. ○ Execution: The agent runs the build steps as defined in the job configuration. 3. Build Results: ○ Logs: Captures console output and logs from the build process. ○ Artifacts: Stores build artifacts like binaries, packages, and reports. ○ Test Results: Collects and displays test results and code coverage reports. ○ Build History: Maintains a history of all builds, including status, duration, and changes. 4. Notification and Reporting: ○ Notifications: Jenkins can send notifications through various channels (email, chat, etc.) upon build completion or failure. ○ Dashboards: Provides dashboards for visualizing build status, trends, and metrics. ○ Reports: Generates and displays reports on test results, code quality, and other metrics. INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 4 1750 Rank

--- Page 80 ---
Unit: III Course Code/Title:CS3V15/Devops Summary Jenkins' architecture is designed to be flexible and scalable. The master-agent model allows for efficient distribution of build tasks, while plugins provide extensive customization options. By leveraging Jenkins' comprehensive job configuration and robust notification and reporting capabilities, teams can streamline their CI/CD pipelines and improve software quality and delivery speed. Creating a Jenkins Job Creating a Jenkins job involves setting up a new project within Jenkins to automate various tasks such as building code, running tests, and deploying applications. Here is a step-by-step guide to creating a Jenkins job. Prerequisites · Jenkins installed and running. · Basic understanding of the Jenkins interface. . Access to the Jenkins dashboard. Step-by-Step Guide 1. Access Jenkins Dashboard: . Open your web browser and navigate to your Jenkins instance URL (e.g., http://localhost:8080). 2. Create a New Job: ○ On the Jenkins dashboard, click on the "New Item" link on the left-hand side menu. 3. Enter Job Name: ○ Enter a name for your new job in the "Enter an item name" field. o Choose the type of job you want to create. For most uses, "Freestyle project" is a good starting point. ○ Click "OK". 4. Configure the Job: ○ After clicking "OK", you will be directed to the job configuration page. 5. General Configuration: ○ Description: Provide a brief description of the job. ○ Discard Old Builds: Optionally set this to limit the number of old builds Jenkins keeps. ○ Select Version Control System: Choose the version control system (e.g., Git, Subversion). 6. Source Code Management: ○ Repository URL: Enter the repository URL. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 5 nirf 175 Rank N33

--- Page 89 ---
Course Code/Title:CS3V15/Devops Unit: III Key Features: · Source Code Management: Configures Jenkins to use Git repositories as the source code for jobs. Example Demo Step 1 : Create a simple Java program. I have created a folder named MyJavaPrograms and inside it created a simple Java program as follows - e.g., SSH test.java public class test { public static void main(String args[I) { for(int i=1;i <= 5;1++) System.out.printin("Welcome Anuradha"); } Plugin," Step 2 : Open the command prompt, switch to that folder and execute the above Java program. It is illustrated by following screenshot. 1 C:\Windows\System32\cmd.e X + Microsoft Windows [Version 10.0.22621.2134] (c) Microsoft Corporation. All rights reserved. 0 X before E:\MyJavaPrograms>javac test. java E: \MyJavaPrograms>java test Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha E: \MyJavaPrograms> Step 3 : Now we will create a Git repository and push this repository on GitHub. First of all we will initialise the Git repository my using git init command. CHENNAI INSTITUTE TECHNOLOGY 14 - N32 nirf 1750 Rank CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt)

--- Page 90 ---
Unit: III Course Code/Title:CS3V15/Devops C:\Windows\System32\cmd.e x X E:\MyJavaPrograms>git init Initialized empty Git repository in E:/MyJavaPrograms/.git/ E:\MyJavaPrograms>git status On branch master No commits yet Untracked files: (use "git add <file> ... " to include in what will be committed) test.class test. java nothing added to commit but untracked files present (use "git add" to track) E:\MyJavaPrograms> then add the java and class files to git repository - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git add . E:\MyJavaPrograms>git status On branch master No commits yet Changes to be committed: (use "git rm -- cached <file> ... " to unstage) new file: test.class new file: test. java E: \MyJavaPrograms> Now we will commit the changes - C:\Windows\System32\cmd.e X E:\MyJavaPrograms>git commit -m "First Commit for Java program" [master (root-commit) 4cb04db] First Commit for Java program 2 files changed, 6 insertions(+) create mode 100644 test.class create mode 100644 test. java E:\MyJavaPrograms> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) - INSTITUTE . TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 55 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Environment Variables Now remove active profile from maven settings.xml and update the test profile mentioned in pom.xml. Add activation element to profile element as shown below. The test profile will trigger when the system property "env" is specified with the value "test". Create an environment variable "env" and set its value as "test". <profile> <id>test</id> <activation> <property> <name>env</name> <value>test</value> </property> </activation> </profile> Let's open command console, go to the folder containing pom.xml and execute the following mvn command. C:\MVN\project>mvn test Profile Activation via Operating System Activation element to include os detail as shown below. This test profile will trigger when the system is windows XP. <profile> <id>test</id> <activation> <os> <name>Windows XP</name> <family>Windows</family> <arch>x86</arch> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 14 N32 narf 1750 Rank -

--- Page 56 ---
Course Code/Title:CS3V15/Devops Unit: II <version>5.1.2600</version> </os> </activation> </profile> Now open command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test Profile Activation via Present/Missing File Now activation element to include OS details as shown below. The test profile will triggerwhen target/generated-sources/axistools/wsdl2java/com/companyname/group is missing. <profile> <id>test</id> <activation> <file> <missing>target/generated-sources/axistools/wsdl2java/ com/companyname/group</missing> </file> </activation> </profile> Now open the command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test 8. Maven create and build artifacts: The first step is to open Eclipse, which comes with the integrated Maven environment. The Eclipse window opens on the screen. Complete the following steps: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 15 N32 nirf 175º Rank -

--- Page 5 ---
Unit: I Course Code/Title:CS3V15/Devops Example: Facebook's mobile app which is updated every two weeks effectively tells users you can have what you want and you can have it. Now ever wondered how Facebook was able to do social smoothing? It's the DevOps philosophy that helps facebook and sure that apps aren't outdated and that users get the best experience on Facebook. Facebook accomplishes this true code ownership model that makes its developers responsible that includes testing and supporting through production and delivery for each kernel of code. They write and update their true policies like this but Facebook has developed a DevOps culture and has successfully accelerated its development lifecycle. Difference between Agile and DevOps: S. No. Agile 1 It started in the year 2001. It started in the year 2007. 2 Invented by John Kern, and Martin Fowler. Invented by John Allspaw and Paul Hammond at Flickr, and the Phoenix Project by Gene Kim. 3 Agile is a method for creating software. It is not related to software development. Instead, the software that is used by DevOps is pre-built, dependable, and simple to deploy. 4 An advancement and administration approach. Typically a conclusion of administration related to designing. 5 The agile handle centers on consistent changes. DevOps centers on steady testing and conveyance. 6 A few of the finest steps embraced in Agile are recorded underneath - 1. Backlog Building 2.Sprint advancement DevOps to have a few best hones that ease the method - 1. Focus on specialized greatness. 2. Collaborate straightforwardly with clients and join their feedback 7 Agile relates generally to the way advancement is carried of, any division of the company can be spry in its hones. This may be accomplished through preparation. 8 All the group individuals working in a spry hone have a wide assortment of comparable ability sets. This is often one of the points of interest of having such a group since within the time of requirement any of the group individuals can loan help instead of holding up for the group leads or any pro impedances. 9 Spry accepts "smaller and concise". Littler the group superior it would be to convey with fewer complexities. 10 Since Agile groups are brief, a foreordained sum of time is there which are sprints. Tough, it happens that a sprint has endured longer than a month but regularly a week long. 11 A big team for your project is not required. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY DevOps DevOps centers more on program arrangement choosing the foremost dependable and most secure course. DevOps features a diverse approach and is very viable, most of the time it takes after "Divide and Conquer". Work partitioned among the improvement and operation groups. DevOps, on the other hand, accepts that "bigger is better". DevOps, on the other hand, prioritizes reliabilities. It is since of this behavior that they can center on a long-term plan that minimizes commerce's unsettling influences. It demands collaboration among different teams for the completion of work. 5 N33 nirf 1750 Rank

--- Page 6 ---
Unit: I Course Code/Title:CS3V15/Devops 12 Some of the Tools- · Bugzilla · JIRA · Kanboard and more. 13 It is suitable for managing complex projects in any department. It centers on the complete engineering process. 14 It does not focus on the automation. It focusses on automation. 15 Working system gets more significance in Agile than documentation. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Some of the Tools- · Puppet • Ansible • AWS · Chef · team City OpenStack and more. The process documentation is significant in DevOps. 6 N32 nirf 1750 Rank

--- Page 91 ---
Course Code/Title:CS3V15/Devops Unit: III Now we will create a repository on GitHub and then push the above committed Git repository on the GitHub. Open a web browser, and login to www.GitHub.com. Create a repository by the name MyJavaPrograms. Start a new repository A repository contains all of your project's files, revision history, and collaborator discussion. AnurdhaP / MyJavaPrograms Public Anyone on the internet can see this repository Private You choose who can see and commit to this repository Create a new repository 1 then copy the URL of this repository. HTTPS SSH https://github.com/AnurdhaP/MyJavaPrograms.git Now open the command prompt window and go to the MyJavaPrograms directory and issue the following command. git remote add origin https://github.com/AnurdhaP/MyJavaPrograms.git then push the Git to GitHub repository using the command push -u origin master CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 16

--- Page 92 ---
Course Code/Title:CS3V15/Devops Unit: III It is illustrated as follows - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git push -u origin master Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 719 bytes | 719.00 KiB/s, done. Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 To https://github.com/AnurdhaP/MyJavaPrograms.git * [new branch] master -> master branch 'master' set up to track 'origin/master'. E: \MyJavaPrograms> · Now it we refresh the repository on GitHub then we can see our test.java and test.class programs are present in that repository. MyJavaPrograms Public !" master - 1º 1 branch 0 tags Go to f AnurdhaP First Commit for Java program .4cb test.class First Commit for Java program test.java First Commit for Java program Help people interested in this repository understand your project by adding a README. Step 4 : Now create a job in Jenkins that makes use of GitHub plugin in order to access the GitHub repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 17 N3 nirf 1750 Rank

--- Page 37 ---
Course Code/Title:CS3V15/Devops Unit: I GIT INSTALLATION Git for Windows stand-alone installer · Download the latest Git for Windows installer. · When you've successfully started the installer, you should see the Git Setup wizard screen. Follow the Next and Finish prompts to complete the installation. The default options are pretty sensible for most users. · Open a Command Prompt (or Git Bash if during installation you elected not to use Git from the Windows Command Prompt). . Run the following commands to configure your Git username and email using the following commands, replacing Emma's name with your own. These details will be associated with any commits that you create: $ git config -- global user.name "CIT_CHENNAI" $ git config -- global user.email CITCHENNAI@atlassian.com • Optional: Install the Git credential helper on Windows Bitbucket supports pushing and pulling over HTTP to your remote Git repositories on Bitbucket. Every time you interact with the remote repository, you must supply a username/password combination. You can store these credentials, instead of supplying the combination every time, with the Git Credential Manager for Windows. BASIC COMMANDS OF GIT: Some basic Git commands along with their syntax and examples: · Initialize a Repository: Syntax: `git init' Example: `git init' · Clone a Repository: Syntax: `git clone <repository_url> Example: `git clone https://github.com/example/repository.git' · Check Repository Status: Syntax: `git status' Example: `git status' · Add Changes to Staging Area: Syntax: `git add <file(s)>' Example: `git add file.txt' · Commit Changes: Syntax:`git commit -m "Commit message" Example:`git commit -m "Add new feature" · Create a New Branch: Syntax: `git branch <branch_name>' Example: `git CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY (Autonomous] N33 37 nirf 1750 Rank

--- Page 38 ---
Course Code/Title:CS3V15/Devops branch feature-branch' Unit: I • Switch to a Branch: Syntax: `git checkout <branch_name>' Example:`git checkout feature-branch' OR Syntax: `git switch <branch_name>' (Git version 2.23 and later) Example: `git switch feature-branch' · Create and Switch to a New Branch: Syntax: `git checkout -b <new_branch name>' Example: `git checkout -b new-feature' OR Syntax: `git switch -c <new_branch_name>' (Git version 2.23 and later) Example: `git switch -c new-feature' . Merge Changes from One Branch to Another: Syntax: `git merge <branch_name>' Example: `git merge feature-branch' · View the Commit History: Syntax: `git log' Example: `git log' · Push Changes to a Remote Repository: Syntax: `git push <remote_name> <branch_name>' Example:`git push origin master' · Pull Changes from a Remote Repository: Syntax: `git pull <remote_name> <branch_name>' Example:`git pull origin master' . Show the Differences Between Working Directory and Staging Area: Syntax: `git diff Example: `git diff · Show the Differences Between Staging Area and Last Commit: Syntax: `git diff -- cached' Example: `git diff -- cached' . Show the Differences Between Working Directory and Last Commit: Syntax: `git diff HEAD' Example: `git diff HEAD' CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 38 N32 nirf 1750 Rank

--- Page 99 ---
Unit: III Course Code/Title:CS3V15/Devops 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "HTML Publisher Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Post-build Actions section. ○ Select Publish HTML reports. ○ Specify the directory containing HTML reports and the index page. Usage: . Ideal for displaying build reports such as test results or code coverage metrics in a user-friendly HTML format. DEMO: Step 1 : We will create a Maven project using Selenium and TestNG framework. Open Eclipse : IDE and Create a Maven Project. Select the Create Simple Project (Skip archetype selection) and enter the Group Id and Artifact Id as per your choice. My project name is SeleniumDemo. Step 2 : Add the Selenium dependency in your pom.xml file as follows - <dependency> <groupId>org.seleniumhq.selenium</groupId> <artifactId> selenium-java</artifactId> <version>3.141.59</version> </dependency > Save the pom.xml file. Step 3 : Now install testNG plugin. For that, from Menu select Help-> Eclipse Marketplace Help Welcome 7 Help Contents Search Show Context Help Show Active Keybindings ... Ctrl+Shift+L Tip of the Day Tips and Tricks ... Cheat Sheets ... @ Eclipse User Storage Open the Eclipse Marketpla ** Perform Setup Tasks ... % Check for Updates Install New Software ... Eclipse Marketplace ... = About Eclipse IDE · Donate Contribute Type TestNG in the search window. The testNG installation option will be available. click Install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 24 N32 nirf 1750 Rank ---

--- Page 100 ---
Course Code/Title:CS3V15/Devops Unit: III C Eclipse Meketplace × Eclipse Marketplace Select solutions to initiit, Prese leuralt Now to proceed with lutallation. Press the "more info" link to learn more about a solution Search Recent Popular Favorites itwled - Research at the Eclipse × All Categories Go TestNG for Eclipse . This plug-in late you run your TextNG wits from Erfiore. You can run sulter, groupe er individuel methods. Errors are reported in a pepwane tab that lets you. more info by Chat Bent Apache 20 tantog zielt testingunit integration functional selection * 737 ** * stal: 1,83M (25,021 last month) Install MoreUnit 3.3.0 Morelinit is an Solipoe plugin that should mult you in wilting mory unk with. It supports ali programming languages (witching between teds and clanset under. mone.info Install Infinitest 5.4.0.3 Infinitest is a continuont tout runner for tova,'and is valuable to developers valing a Marketplaces (7) Cancel Then confirm the selected features by clicking on Confirm button. x Confirá Selected Features These Confirm to gorthus with the induction. Cs go back te chocie bis sules it CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 25 N3 nirf 1750 Rank

--- Page 23 ---
Course Code/Title:CS3V15/Devops Unit: I application. Amazon EC2 enables you to increase or decrease capacity within minutes. You can use one or hundreds or even thousands of server instances simultaneously. Because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs. Amazon EC2 is integrated with most AWS services, such as Amazon Simple Storage Service (Amazon S3), Amazon Relational Database Service (Amazon RDS), and Amazon Virtual Private Cloud (Amazon VPC) to provide a complete, secure solution for computing applications. Amazon EC2 is an example of Infrastructure as a Service(IaaS). EC2 delivers secure, reliable, cost-effective compute and high-performance compute infrastructure so as to meet the needs of demanding businesses. Amazon EC2 is one of the easiest ways of providing servers on AWS Cloud and also the access to Operating system. · AWS Lambda: AWS Lambda is a serverless, event-driven compute service that allows you to run code without managing servers. You pay only for the compute time you consume and there is no charge when your code is not running. With AWS Lambda, you can run code for any type of application with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services, or you can call it directly from any web or mobile app. But triggering Lambda is possible with over 200 AWS services. You can only pay for what you have used. The compute time that you consume, you are needed to pay for it. You just only need to upload your code and everything required to run will take care of by Lambda and it automatically scales your code with high availability. · AWS Elastic Beanstalk: AWS Elastic Beanstalk is a Platform as a Service that facilitates quick deployment of your applications by providing all the application services that you need for your application. Beanstalk is a plug- and-play platform that allows working with multiple programming languages and environments. Elastic Beanstalk supports a large range of platforms like Node js, Java, PHP, Python, and Ruby. So, you can develop your application to meet your requirements and simply deploy it on Elastic Beanstalk. The main aim to use AWS Elastic Beanstalk is to allow you to focus on the deployment and management of your applications. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. 2. Networking · Amazon VPC: Amazon VPC is your network environment in the cloud. It allows you to create a private network within the AWS cloud that uses many of the same concepts and constructs as an on-premises network. Amazon VPC also gives you complete control of the network configuration. Customers can define normal networking configuration items such as IP address ranges, subnet creation, route table creation, network gateways, and security settings. Amazon VPC is an AWS foundational service and integrates with numerous AWS services. For instance, Amazon EC2 instances are deployed into your CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 24 ---
Course Code/Title:CS3V15/Devops Unit: I Amazon VPC. Similarly, Amazon Relational Database Service (Amazon RDS) database instances deploy into your Amazon VPC, where the database is protected by the structure of the network just like your on-premises network. You can easily launch AWS resources into a virtual network by Amazon Virtual Private Cloud. An isolated virtual network environment in the AWS cloud is created by Amazon VPC. . Amazon Route 53: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating human-readable names, such as www.geeksforgeeks.com, into the numeric IP addresses that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. 2. Storage · Amazon S3 (Simple Storage Service): Amazon Simple Storage Service (Amazon S3) is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the web. It is designed to provide an infinite amount of storage and it is delivered with 99.999999999% durability. You can use Amazon S3 as primary storage for cloud-native applications as a target for backup and recovery and disaster recovery. It offers industry-leading scalability, data availability, security, and performance. It's simple to move large volumes of data into or out of Amazon S3 with Amazon's cloud data migration options. Once data is stored in Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like Amazon S3 Standard - Infrequent Access and Amazon Glacier for archiving. · Amazon Glacier: Amazon Glacier is a secure, durable, and extremely low- cost storage service for data archiving and long-term backup. Data stored in Amazon Glacier takes several hours to retrieve, which is why it's ideal for archiving. The fastest access to your archive data is via Amazon Glacier. 3. Databases · Amazon RDS (Relational Database Service): Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost- efficient and resizable capacity while managing time-consuming database administration tasks, freeing you up to focus on your applications and business. You can find Amazon RDS is also available on several database instance types - optimized for memory, performance, or I/O. Amazon RDS provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. · Amazon DynamoDB (Non-Relational Database): Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. When you create a database table that can store and retrieve any amount of data you can simply use Amazon DynamoDB that will serve any level of requested traffic. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, Internet of Things (IoT), and many other applications. DynamoDB provides many features like · built-in security CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 24 N32 nirf 1750 Rank

--- Page 97 ---
Course Code/Title:CS3V15/Devops Unit: III Build Steps Execute Windows batch command ? Command See the list of available environment variables javac welcome. java java welcome Advanced = Execute Windows batch command 7 Command See the list of avalable chuironment variables acho User name is Nuernane% Advanced Step 5 : Click on Apply and then Save. Step 6 : Select Build with parameters option. Dashboard > welcome > Status Changes Workspace Build with Parameters Configure Delete Project 0 Rename CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 22 N3 nirf 1750 Rank

--- Page 98 ---
Course Code/Title:CS3V15/Devops Unit: III Project welcome This build requires parameters: Username Parth D Build Cancel We can change the username as well. Suppose I changed the Username as Anand then click the Build, we get the console output as follows - C: \ProgramData\Jenkins\.jenkins\workspace\welcome>javac welcome.java C:\ProgramData\Jenkins\.jenkins\workspace\welcomeyjava welcome Welcome to first Job of Jenkins [welcome] $ cmd /c call C:\WINDOWS\TEMP\jenkins16582464065687933173.bat C:\ProgramData\Jenkins\.jenkins\workspace\welcome>echo User name is Anand User name is Anand C:\ProgramData\Jenkins\.jenkins\workspace\welcome>exit 0 Finished: SUCCESS Thus we set variety of parameters to the Jenkins job. We can set the Boolean parameter, choice parameter, credentials, file parameter, password parameter and so on. 3. HTML Publisher Filter Plain Boolean Parameter Choice Parameter Credentials Parameter File Parameter Multi-line String Parameter Password Parameter Run Parameter String Parameter Add Parameter Overview: The HTML Publisher Plugin allows Jenkins to publish HTML reports and artifacts generated during the build process. Key Features: · Report Publishing: Publishes HTML reports, dashboards, or other HTML artifacts. · Report Directory: Allows specifying the directory containing HTML reports. · Index Page: Sets a default HTML file to be displayed when accessing the report. Configuration: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 23 nirf 1750 Rank N33

--- Page 61 ---
Course Code/Title:CS3V15/Devops Unit: II Return to the previous screen and undo the deletion. The errors that occurred will disappear. *mavenproject/pom.xml & IApp.java AppTest.java 6 <artifactId>mavenproject</artifactId> 7 <version>0.0.1-SNAPSHOT</version> <packaging>jar</packaging> 8 9 16 <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 138 <properties> 14 15 16 17% <dependencies> 18 19 <groupId>junit</groupId> I 28 21 22 23 24 <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> cdependency> <artifactId>junit</artifactid> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> 25 </project> 26 9. Dependency Management: The dependencyManagement and dependencies are especially useful for multi-module projects. Dependency Management : This tag consists of a dependencies tag which itself might contain multiple dependency tags. Each dependency is supposed to have at least three maintags: groupId, artifactId, and version. For example: <dependencyManagement> <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 20 N33 nirf 1750 Rank -

--- Page 62 ---
Course Code/Title:CS3V15/Devops Unit: II The above code just declares the new artifact commons-lang3, but it doesn't really add it to the project dependency resource list. Dependencies: This tag contains a list of dependency tags. Each dependency is supposed to have at least two main tags, which are groupId and artifactId. For example: <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> The version and scope tags can be inherited implicitly if we have used the dependencyManagement tag before in the POM file. DependencyManagement is just a declaration, and it does not really add a dependency. Dependencies tag adds the actual dependency to the project. An example for adding the JUnit library dependency: <dependencyManagement> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.2</version> <scope>test</scope> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 21 N33 nirf 175º Rank

--- Page 103 ---
Course Code/Title:CS3V15/Devops Unit: III =Publish HTML reports ? Reports HTML directory to archive ? E:\Eclipse_workplace\SeleniumDemo\test-output Index page[s] ? index.html Index page title[s] (Optional) Report title ? HTML Report Publishing options Step 11 : Click on Apply and then Save. Now build this project. Step 12 : On successful build, it creates the HTML Report as follows - x + € > C @ localhost:0000/juby/HTML/\portDemo/HTML_20Report/ Back to HTMLBepsatDemo Test results All suites Default suite 11 D myPackage.myTestPage Info . Repetir output Results . Thus using HTML Publish Report Plugin we can successfully generate HTML report of some testing framework application. · Troubleshooting : Sometimes, the raw file is displayed as HTML report. To get the HTML formatted report we must click on Manage Jenkins section, scroll down and locate Script Console section. Click on it. Tools and Actions C Broad Configuration Sium Disk Discard all the loaded alla lo memory and reloud Jerking CLI Script Console Ewecubes arbitrary suyt lor and run below given command : System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", ") . Then restart the Jenkins Page and Build the project once again. The HTML report can then be displayed as given in above step. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 28 N32 nirf 1750 Rank

--- Page 104 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Copy Artifact Overview: The Copy Artifact Plugin allows Jenkins jobs to copy build artifacts from other jobs, facilitating artifact reuse across different jobs. Key Features: · Artifact Copying: Copies files from one build to another, which can be from a specific build or the latest successful build. · Triggering Builds: Can be used in downstream jobs to fetch artifacts from upstream jobs. Configuration: 1. Install the Plugin: Go to Manage Jenkins > Manage Plugins > Available tab, search for "Copy Artifact Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Build section. ○ Add a Copy artifacts from another project build step. o Specify the project name, build number or criteria, and the target directory for copied artifacts. Usage: · Commonly used in multi-job pipelines where artifacts need to be shared between different jobs or stages. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 29 N3 nirf 1750 Rank

--- Page 33 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between AWS (Amazon Web Services), Google Cloud, and Azure Technology EC2 (Elastic Compute Cloud) AWS Databases Supported Pricing Models Difficulties Many enterprises find it difficult to understand the company cost AWS fully relational and supports NoSQL databases and Big Data. Per hour - rounded up. On demand, reserved spot. structure. Google Cloud Google Engine(GCE) Compute Technologies pioneered by Google, like Big Query, Big Table, and Hadoop, are databases, and Big Data,naturally fully supported. Per minute - rounded up On demand sustained use. — Fewer services. features Azure VHD Disk) (Virtual Hard Azure supports relational and both NoSQL Windows and through AzureTable HDInsight. Per minute - rounded up. Per minute- rounded up commitments(Pre-paid or monthly) and Less "Enterprise-ready. Storage Services Simple Storage Service(S3) Elastic Block Storage. Elastic File storage. Machine Learing Sage maker. Lex. polly.And many more Blob Storage Queue Storage. File Storage Disk Storage. Data Lake Store Machine learning Azure Bot service Cognitive service GIT Cloud storage. Persistent Disk Transfer appliance. Cloud speech AI Cloud Video Intelligence. Cloud Machine learning engine Git is a distributed version control system (DVCS) that helps manage and track changes in source code during software development. It was created by Linus Torvalds in 2005 and has become one of the most widely used version control systems in the software development industry. Some key concepts and features of Git: · Version Control: Git allows developers to keep track of changes made to their code over time. This includes modifications, additions, and deletions of files. · Distributed System: Git is a distributed version control system, meaning that each developer has a complete copy of the entire repository, including its full history. This allows developers to work independently and merge their changes when necessary. · Branching: Git enables developers to create branches, which are essentially separate lines of development. This allows for the parallel development of features or bug fixes without affecting the main codebase. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) LEN 33 nirf 1750 Rank

--- Page 34 ---
Course Code/Title:CS3V15/Devops Unit: I · Merging: Git provides tools for merging changes from one branch into another. This is essential when multiple developers are working on different branches and need to bring their changes together. · History Tracking: Git maintains a complete history of changes made to the codebase. Developers can view, revert, or analyze changes made over time. · Remote Repositories: Git supports remote repositories, allowing developers to collaborate with others by pushing and pulling changes to and from a shared repository. Platforms like GitHub, GitLab, and Bitbucket provide hosting services for Git repositories. · Staging Area: Git uses a staging area (also known as the index) to prepare and review changes before committing them to the repository. This allows developers to selectively include or exclude specific changes. · Open Source: Git is an open-source tool, and its source code is freely available for modification and distribution. Various GIT Components: Git is composed of several key components that work together to enable version control and collaborative development. Here are the main components of Git: · Repository (Repo): A repository is a directory or storage space where your project and its version history are stored. It contains all the files and directories associated with your project, along with the metadata and configuration information. · Working Directory: The working directory is the directory on your local machine where you manipulate files and make changes to your project. It is essentially your local copy of the repository. · Index (Staging Area): The index, also known as the staging area, is a middle ground where changes are prepared before being committed to the repository. It allows you to selectively stage changes, which means you can choose which modifications to include in the next commit. · Commit: A commit is a snapshot of the changes made to the files in the repository. It represents a specific point in the project's history and is accompanied by a commit message that describes the changes. · Branch: A branch is a parallel line of development within a repository. It allows developers to work on different features or bug fixes simultaneously without affecting the main codebase. Branches can be merged to incorporate changes into other branches. . Head: HEAD is a reference to the latest commit in the currently checked-out branch. It essentially points to the tip of the branch you are currently on. · Remote: A remote is a version of the repository stored on a different server. Git supports collaboration by allowing developers to push and pull changes between their local repository and remote repositories. Platforms like GitHub, GitLab, and Bitbucket are examples of remote repositories. · Clone: Cloning is the process of creating a copy of a remote repository on your local machine. This allows you to start working on your own copy of a project. · Fetch: The fetch operation retrieves changes from a remote repository but does not automatically merge them into your working directory. It is useful for reviewing changes before deciding to merge. · Pull: Pull is a combination of fetch and merge. It retrieves changes from a remote repository and automatically merges them into your working directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 1 ---
Course Code/Title:CS3V15/Devops CHENNAI INSTITUTE OF TECHNOLOGY Transforming Lives CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) NAME OF THE PROGRAMME B. E (CSE) YEAR III SEMESTER V REGULATIONS 2022R COURSE CODE CS3V15 COURSE NAME DEVOPS FACULTY NAME (Prepared by) NAME OF SUBJECT EXPERT (Verified by) M.SUNDHARI CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Unit: I N33 INATIONAL BOARD . ACCREDITATION 100% Accreditation All eligible UG & PG Programs NIRF 151 - 200 Band Engineering 2023 A+ DE NAAC Contact 8681927167 Contact 1 N32 nirf 1750 Rank

--- Page 2 ---
Course Code/Title:CS3V15/Devops Unit: I UNIT - I Introduction to Devops Devops Essentials - Introduction To AWS, GCP, Azure - Version control systems: Git and Github. The word "DevOps" was coined in 2009 by Patrick Debois, who became one of its gurus. The term was formed by combining "development" and "operations," which provides a starting point for understanding exactly what people typically mean when they say "DevOps." Notably, DevOps isn't a process or a technology or a standard. Many devotees refer to DevOps as a "culture"-a viewpoint that New Relic favors. We also use the term "DevOps movement" when talking about topics such as adoption rates and trends for the future, and "DevOps environment" to refer to an IT organization that has adopted a DevOps culture. "DevOps represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile, lean practices in the context of a system-oriented approach. DevOps emphasizes people (and culture), and seeks to improve collaboration between operations and development teams. DevOps implementations utilize technology- especially automation tools that can leverage an increasingly programmable and dynamic infrastructure from a life cycle perspective." DevOps is defined as a combination of processes and tools created to facilitate organizations in delivering services and applications much faster than they can through conventional software development processes. It helps increase customers' confidence in the applications that an organization offers, thereby allowing the company to flourish and achieve its business goals faster. Development 3 3 DevOps 13 51 Operations Quality Assurance (QA) How DevOps Works? A DevOps process can be summarized as an infinite loop that comprises the following stages - build, test, and release through the delivery pipeline and plan and monitor through feedback, which resets the loop again. With such an amazing combination, teams use tech stack and tooling that assists them in reliably developing apps. Moreover, going away from the Under the DevOps model, development and operations teams work in constant cohesion throughout the entire project lifecycle, starting right from development to deployment. When security is the main focus, the quality assurance team is tightly knitted with the DevOps team throughout the app lifecycle. In this situation, some DevOps teams are also referred to as DevSecOps. Close coordination with the QA team ensures that no loopholes are left unchecked in the provided service/app. COMPANY HOW DEVOPS WORKS DELIVERY PIPELINE BUILD TEST RELEASE PLAN MONITOR FEEDBACK LOOP TOOLBOX" CUSTOMER norm, teams use automated processes here. DevOps tools also allow engineers to complete different tasks independently. Be it provisioning infrastructure or deploying code, they can accomplish these tasks without being dependent on one another. As such, the DevOps model accelerates the overall application development process. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 81 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Credentials: Add credentials if required. ○ Branch Specifier: Specify the branch (e.g., */main). 7. Build Triggers: ○ Build Periodically: Use CRON syntax to schedule builds. ○ Poll SCM: Jenkins will check the repository for changes at specified intervals. ○ Other Triggers: Configure other triggers such as GitHub hooks, upstream projects, etc. 8. Build Environment: o Configure the build environment settings like setting environment variables, running scripts before the build, etc. 9. Build Steps: ○ Click on "Add Build Step" and choose the appropriate build step (e.g., "Execute Shell", "Invoke Ant", "Invoke Gradle script"). ○ Example: For executing a shell script, add the necessary shell commands. 10. Post-build Actions: ○ Click on "Add post-build action" and choose the appropriate action (e.g., "Archive the artifacts", "Publish JUnit test result report"). ○ Configure the settings for each post-build action. 11. Save the Configuration: ○ Once you have configured all the necessary settings, click "Save" at the bottom of the page. 12. Run the Job: ○ On the job's main page, click "Build Now" to run the job immediately. 13. Monitor the Job: ○ Click on the build number in the "Build History" to view the detailed output and logs of the build process. ○ Check the console output for any errors or warnings. Practical Example Setting up a Freestyle Project for a Maven Build: 1. Create New Item: ○ Name: MyMavenProject o Type: Freestyle project 2. Source Code Management: ○ Git: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 82 ---
Unit: III Course Code/Title:CS3V15/Devops I Repository URL: https://github.com/example/my-maven-project.git · Branch Specifier: * /main 3. Build Triggers: ○ Poll SCM: H/15 * (poll every 15 minutes) 4. Build Steps: ○ Invoke Top-Level Maven Targets: Goals: clean install 5. Post-build Actions: ○ Publish JUnit test result report: I Test report XMLs: * /target/surefire-reports/ *. xml 6. Save and Build: ○ Click "Save". ○ Click "Build Now" on the job's main page. By following these steps, you can set up a Jenkins job to automate your build process for various types of projects. Jenkins provides extensive customization options, allowing you to tailor the job to meet your specific requirements. Configuring a Jenkins Job 1. Install Jenkins First, make sure Jenkins is installed. You can download it from the official Jenkins website. 2. Access the Jenkins Dashboard Open your web browser and go to http://your-server-ip: 8080 to access the Jenkins dashboard. 3. Create a New Job 1. Start a New Job: ○ Click on "New Item" on the left-hand menu. 2. Name the Job: ○ Enter a name for your job in the "Enter an item name" field. 3. Select Job Type: ○ Choose the type of job (e.g., Freestyle project, Pipeline). CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) - TECHNOLOGY N32 nirf 7 1750 Rank

--- Page 35 ---
Unit: I Course Code/Title:CS3V15/Devops Push: Push is the operation that sends your committed changes to a remote repository, making them accessible to others. Git workflow v0.1 v0.2 v1.0 Master Hotfix Release Develop Feature Feature Git Flow is a structured branching model designed for projects with well- defined release cycles and a need for strict quality control. Branches: The branching model described is commonly known as the Gitflow Workflow. It's a branching strategy that defines a strict branching model designed to facilitate collaboration and streamline the release process. Let's go into detail about each branch: clone operation Working Copy edit, add, move files update operation Modify working copy status and diff operation Review changes commit and push operation push operation Commit changes push operation amend and push operation Review changes 1. Master Branch: The `master'branch represents the main codebase and contains production- ready code. This branch is typically stable and should only include thoroughly tested and approved changes. Each commit on the 'master' branch represents a new version or release of the software. 2. Develop Branch: The 'develop' branch is an integration branch where various feature branches are merged. It serves as a staging area for testing new features and ensuring they work well together before merging into the 'master' branch. This branch may have ongoing development work and is not necessarily always in a production- ready state. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 35 nirf 1750 Rank N33

--- Page 36 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Feature Branches: Feature branches are created for developing new features or implementing changes. These branches are typically based on the 'develop'branch. Once a feature is complete, the branch is merged back into the 'develop' branch. Feature branches allow developers to work on specific tasks without affecting the main codebase. 4. Release Branch: The 'release' branch is created when the 'develop' branch reaches a point where it is ready for a production release. This branch is used for final testing, bug fixes, and preparing the code for deployment. No new features should be added to the release branch. Once the release is deemed stable, it is merged into both the 'master' branch and the 'develop' branch. 5. Hotfix Branch: The 'hotfix' branch is used to quickly address critical issues or bugs in the production code. It is created directly from the 'master' branch. Hotfixes are intended to be small and focused on resolving the specific issue at hand. Once the hotfix is complete, it is merged into both the 'master'branch and the `develop'branch to ensure that the fix is applied to future releases. Here is the typical flow: · Developers work on feature branches based on the 'develop' branch. · Completed features are merged into the 'develop' branch. · When ready for a release, a 'release' branch is created from 'develop". · The release branch undergoes testing and bug fixes. · The release branch is merged into both 'master' and 'develop' once it's stable. · If a critical issue arises in production, a 'hotfix' branch is created from 'master'. · The hotfix is merged into both 'master' and 'develop to keep both branches in sync. · This Gitflow Workflow helps maintain a structured development process, ensuring that features are developed, tested, and released in a controlled manner. Example Scenario: · Imagine you're working on a large software project with a team of 10 developers. You have a major release planned for every six months. · You create a "feature/Ticket-Id" branch to develop a new login system. · Once the feature is complete, it's merged into the "develop" branch for integration and testing. · As the release date approaches, you create a "release/v1.0" branch to freeze code for the upcoming release. · Any critical issues discovered in the production environment are fixed in "hotfix" branch and merged into "master" and "develop." CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 36 N33 nirf 1750 Rank

--- Page 107 ---
Course Code/Title:CS3V15/Devops Unit: III Step 8 : Then configure the Build Steps by selecting the option Copy artifacts from another project as follows - . . First of all give the name of the project from which you want to copy the files. In my case it is App1. . Then select the option "copy from WORKSPACE of latest completed build" · Give the regular expression ** / *.*. Here's a breakdown of what ** / *.* means : o ** : The double asterisk ( ** ) is a wildcard that matches zero or more directories and subdirectories. It's often used to recursively search through all directories and subdirectories. o / : The forward slash (/) is used as a directory separator, typically on Unix-like systems. o *.* : This part of the regular expression is used to match files with an extension. In regular expressions, the * symbol represents "zero or more" of the preceding character or pattern, and the . character is used to match any character. So, *.* essentially matches any file with any extension. So, when you use the regular expression ** / *.* in a Jenkins build configuration, you are instructing Jenkins to include all files with any extension ( *.* ) located in any directory and subdirectory ( ** ) within the workspace or specified location. Build Steps E Copy artifacts from another project Project name ? App: Which build ? Copy from WORKSPACE of latest completed buld Limitation Note ? Artifacts to copy ? Artifacts not to copy ? : Toget directory CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 32

--- Page 108 ---
Course Code/Title:CS3V15/Devops Unit: III Add one more Build Step and select Execute Windows batch Command. Give the command dir so that the name of files and directories that are copied from App1 to App2 will be displayed on the Console. = Execute Windows batch command ? Command See the list of available environment variables ---- dir ----- Advanced v Add build step * Click Apply and Save. Step 8 : Now Build this project (App2). The output will be as follows - Started by user Anuradha P Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\.jenkins\workspace\App2 Copied 16 artifacts from "App1" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621109248227895.bat C:\ProgramData\Jenkins\,jenkins\workspace\App2>dir Volume in drive C Is OS Volume Serial Number is CGC2-BA9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023* 15:07 <DIR> 01-09-2023 15:07 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 474 test.class 144 test. java 2 File(s) 616 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 33 N32 narf 1750 Rank

--- Page 51 ---
Course Code/Title:CS3V15/Devops Unit: II 7. Maven Profiles: A Build profile is a set of configuration values, which can be used to set or override default values of Maven build. Using a build profile, you can customize build for different environments such as Production v/s Development environments. Types of Build Profile Build profiles are majorly of three types. Type Where it is defined Per Project Defined in the project POM file, pom.xml Per User Defined in Maven settings xml file (%USER_HOME%/.m2/settings.xml) Global Defined in Maven global settings xml file (%M2_HOME%/conf/settings.xml) Profile Activation A Maven Build Profile can be activated in various ways. · Explicitly using command console input. · Through maven settings. · Based on environment variables (User/System variables). · OS Settings (for example, Windows family). · Present/missing files. Now, under src/main/resources, there are three environment specific files - Sl.No. File Name & Description env.properties 1 default configuration used if no profile is mentioned. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 10 N33 nirf 1750 Rank

--- Page 52 ---
Course Code/Title:CS3V15/Devops Unit: II env.test.properties 2 test configuration when test profile is used. 3 env.prod.properties production configuration when prod profile is used. Explicit Profile Activation In the following example, we will attach maven-antrun-plugin:run goal to test the phase. This will allow us to echo text messages for different profiles. We will be using pom.xml to define different profiles and will activate profile at command console using maven command. Assume, we've created the following pom.xml in C:\MVN\project folder. <project xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation = "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.companyname.projectgroup</groupId> <artifactId>project</artifactId> <version>1.0</version> <profiles> <profile> <id>test</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <version>1.1</version> <executions> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 11 N33 nirf 1750 Rank

--- Page 45 ---
Course Code/Title:CS3V15/Devops Unit: II Example for pom.xml file <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> 4. Maven Repository A maven repository is a directory of packaged JAR file with pom.xml file. Maven searches for dependencies in the repositories. There are 3 types of maven repository: 1. Local Repository 2. Central Repository 3. Remote Repository Maven searches for the dependencies in the following order: Local repository then Central repository then Remote repository. Local Central Remote Repository Repository Repository CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 4 N32 nirf 1750 Rank -

--- Page 46 ---
Course Code/Title:CS3V15/Devops Unit: II If dependency is not found in these repositories, maven stops processing and throws an error. 1) Maven Local Repository Maven local repository is located in your local system. It is created by the maven when you run any maven command. By default, maven local repository is %USER_HOME%/.m2 directory. For example: C:\Users\SSS IT\.m2. Update location of Local Repository We can change the location of maven local repository by changing the settings.xml file. It is located in MAVEN_HOME/conf/settings.xml, for example: E:\apache-maven- 3.1.1\conf\settings.xml. 2) Maven Central Repository Maven central repository is located on the web. It has been created by the apache maven community itself. The path of central repository is: http://repo1.maven.org/maven2/. The central repository contains a lot of common libraries that can be viewed by this url http://search.maven.org/#browse. 3) Maven Remote Repository Maven remote repository is located on the web. Most of libraries can be missing from the central repository such as JBoss library etc, so we need to define remote repository in pom.xml file. Let's see the code to add the jUnit library in pom.xml file. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 5 N32 narf 1750 Rank -

--- Page 101 ---
Course Code/Title:CS3V15/Devops Unit: III v => SeleniumDemo src/main/java > src/main/resources > src/test/java > 15 src/test/resources > al JRE System Library [J2SE-1.5] › al Maven Dependencies » drivers v src > @ main v fs test v java v 2> myPackage 10 myTestPage.java 23 resources › › target > > test-output 2 pom.xml > Servers myTestPage.java package myPackage; import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver; import org.testng.annotations.Test; public class myTestPage { @Test public void myFun() { System.setProperty("webdriver.chrome.driver","./drivers/chromedriver.exe"); WebDriver driver = new ChromeDriver(); driver.get("https://google.com"); String title = driver.getTitle(); System.out.println("Hey Anuradha, the tile of this page is: "+title); driver.quit(); } Program explanation : In above program, we have written one test case by the function myFun. In this function, using the Chromedriver which we have installed, we open the web page google.com Then we try to get the tile of this page and display it on the console using System.out statement. Finally we quit the driver. Step 7 : Right click on the Project name in Project Explorer window, Run As-> Maven Test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 26 N3 nirf 1750 Rank

--- Page 102 ---
Unit: III Course Code/Title:CS3V15/Devops Step 8 : Now, locate the Java file myTestPage.java and right click Run As->TestNG Test. It is as follows - 0 Run As # Debug As Profile As Web Services > > 1 TestNG Test > Run Configurations ... > Alt+Shift+X, N Step 9 : We will get the output as follows - Hey Anuradha, the tile of this page is: Google PASSED: myFun Default test Tests run: 1, Failures: 0, Skips: 0 Default suite Total tests run: 1, Failures: 0, Skips: 0 Step 10 : Now, we will open the Jenkin's page and create the HTML Publisher report for the above SeleniumDemo project. The prerequisite to this activity is that the HTML Publisher report plugin must be installed in Jenkins. · Click on New Item, Enter the Item Name and select as Freestyle project, I have given the name HTMLReportDemo. · Then click on configure. Write the suitable description in General section. · The in Build Steps and Add Build steps as Invoke top-level Maven targets provide the Maven version, and under goals write clean test. . Under Post Build Actions, click on Add Post Build Action and select Publish HTML Report. Now give the path of index.html file which is created by Selenium testNG framework in Maven. It is the path of test-output folder. Also give some suitable HTML report title. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 27 N3 nirf 1750 Rank

--- Page 43 ---
Course Code/Title:CS3V15/Devops Unit: II o Generates source code (if auto-generated code is used) o Generates documentation from source code o Compiles source code o Packages compiled code into JAR of ZIP file o Installs the packaged code in local repository, server repository, or central repository 2. Installation of Maven: To install maven on windows, you need to perform following steps: 1. Download maven and extract it 2. Add JAVA_HOME and MAVEN_HOME in environment variable 3. Add maven path in environment variable 4. Verify Maven 1) Download Maven To install maven on windows, you need to download apache maven first. Download Maven latest Maven software from Download latest version of Maven For example: apache-maven-3.1.1-bin.zip 2) Add MAVEN_HOME in environment variable Right click on MyComputer -> properties -> Advanced > Environment variables -> click new button System Settings - Now add MAVEN_HOME in variable name and path of maven in variable value. It must be the home directory of maven i.e. outer directory of bin. For example: E:\apache-maven-3.1.1 3) Add Maven Path in environment variable Click on new tab if path is not set, then set the path of maven. If it is set, edit the path and append the path of maven.Here, we have installed JDK and its path is set by default, so we are going to append the path of maven. The path of maven should be %maven home%/bin. For example, E:\apache-maven- 3.1.1\bin . 4) Verify maven To verify whether maven is installed or not, open the command prompt and write: mvn -version Now it will display the version of maven and jdk including the maven home and java home. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 2 N32 nirf 175º Rank

--- Page 44 ---
Unit: II Course Code/Title:CS3V15/Devops 3. POM FILES: POM is an acronym for Project Object Model. The pom.xml file contains information of project and configuration information for the maven to build the project such as dependencies, build directory, source directory, test source directory, plugin, goals etc. Maven reads the pom.xml file, then executes the goal. Before maven 2, it was named as project.xml file. But, since maven 2 (also in maven 3), it is renamed as pom.xml. Elements of maven pom.xml file For creating the simple pom.xml file, you need to have following elements: Element Description project It is the root element of pom.xml file. modelVersion It is the sub element of project. It specifies the modelVersion. It should be set to 4.0.0. groupId It is the sub element of project. It specifies the id for the project group. artifactId It is the sub element of project. It specifies the id for the artifact (project). An artifact is something that is either produced or used by a project. Examples of artifacts produced by Maven for a project include: JARs, source and binary distributions, and WARs. version It is the sub element of project. It specifies the version of the artifact under given group. Maven pom.xml file with additional elements Here, we are going to add other elements in pom.xml file such as: Element Description packaging defines packaging type such as jar, war etc. name defines name of the maven project. url defines url of the project. dependencies defines dependencies for this project. dependency defines a dependency. It is used inside dependencies. scope defines scope for this maven project. It can be compile, provided, runtime, test and system. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 3 N32 narf 1750 Rank -

--- Page 109 ---
Course Code/Title:CS3V15/Devops Unit: III Note that the git repository and the files from that repository are copied from the workspace of App1 to App2. Also note that test.class and test.java files are copied to the workspace of App2. Started by user Anuradha R Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\. jenkins\workspace\App2 Copied 16 artifacts from "Appl" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621189248227895.bat C:\ProgramData\Jenkins\.jenkins\workspace\App2>dir Volume in drive C is OS Volume Serial Number is C6C2-8A9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 .git 474 test. class 144 test.java 2 File(s) 618 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ Finished: SUCCESS 5. Extended Choice Parameter Overview: The Extended Choice Parameter Plugin provides advanced parameter types for Jenkins jobs, such as multi-select lists, checkboxes, and more complex formats. Key Features: · Parameter Types: Includes multi-select, checkboxes, and other advanced input options. . Dynamic Choices: Allows dynamic generation of choices from scripts or external sources. Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Extended Choice Parameter Plugin," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add an Extended Choice Parameter. ○ Configure the parameter type, choices, and other settings. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 110 ---
Unit: III Course Code/Title:CS3V15/Devops Usage: · Useful for scenarios where complex user input is needed or where multiple selection options are required. Configuring Jenkins to Work with Java, Git, and Maven 1. Configuring Jenkins to Work with Java Overview: Jenkins requires Java to run. Configuring Java in Jenkins involves specifying the Java Development Kit (JDK) installations Jenkins should use. Steps: 1. Install Java Development Kit (JDK): ○ Ensure that JDK is installed on your system. You can download it from the Oracle website or use OpenJDK. 2. Configure JDK in Jenkins: ○ Open Jenkins and go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to JDK section and click Add JDK. ○ Enter a name for the JDK installation (e.g., JDK 11). Check Install automatically to let Jenkins download and install the JDK, or specify the path to an existing JDK installation. ○ If specifying the path manually, provide the JAVA_HOME directory. 3. Example Configuration: ○ Name: JDK 11 ○ JAVA_HOME: /usr/lib/jvm/java-11-openjdk 4. Verify JDK Configuration: You can verify the JDK configuration by creating a simple Jenkins job and adding a build step that prints the Java version using: bash Copy code java -version 2. Configuring Jenkins to Work with Git Overview: The Git Plugin integrates Jenkins with Git repositories, enabling Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 35 1750 Rank

--- Page 13 ---
Course Code/Title:CS3V15/Devops Unit: I 2024. Terraform provides the agility and scalability required to keep up with the dynamic demands of modern applications. Terraform's importance lies in its ability to bring consistency, version control, and automation to infrastructure operations, thereby reducing manual errors, streamlining DevOps workflows, and facilitating applications' rapid and reliable deployment in an increasingly complex and cloud-centric environment. As organizations adopt cloud-native technologies, Terraform remains essential to ensure efficient and consistent infrastructure management. 25. Phantom Phantom enhances security automation and incident response capabilities. In today's rapidly evolving threat landscape, organizations face a constant barrage of cybersecurity incidents, and the ability to respond swiftly and effectively is necessary. It provides a platform for automating security workflows, from detecting and investigating potential threats to orchestrating responses and mitigating risks. Phantom's importance lies in its capacity to reduce response times, increase consistency in incident handling, and free up manual resources from repetitive tasks. With the growing complexity of cyber threats, Phantom empowers security teams to defend against attacks and safeguard critical assets proactively. 26. Nagios Nagios, an open-source monitoring and alerting system, remains vital due to its enduring significance in maintaining the reliability and performance of IT infrastructure and applications. Organizations increasingly rely on complex systems and services. Nagios plays a crucial role by providing real-time monitoring and alerting capabilities, allowing IT teams to detect and address issues before they impact users or cause system outages. Its versatility, extensibility, and support for both on-premises and cloud environments make Nagios a valuable tool for ensuring critical systems' availability, stability, and security, aligning perfectly with the demands of modern IT operations and DevOps practices. 27. Vagrant Vagrant continues to play a crucial role in software development and DevOps. It is a tool that simplifies creating and managing reproducible development environments. Its importance lies in its ability to provide developers and DevOps teams with a consistent and isolated environment for software development, testing, and deployment. With the ever-evolving complexity of software stacks, dependencies, and infrastructure configurations, Vagrant remains essential in ensuring these environments are easily shareable, scalable, and maintainable. It allows developers to work seamlessly across various operating systems and provides a standardized setup that minimizes compatibility issues. 28. Sentry Sentry plays a critical role in modern software development and DevOps practices. With software applications' increasing complexity and scale, identifying and addressing errors and issues has become crucial. Sentry is vital because it provides real-time error tracking and monitoring, allowing development teams to proactively detect and diagnose issues, whether they occur in production or during development. Its importance is minimizing downtime, improving user experience, and maintaining software systems' overall health and reliability. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nirf 1750 Rank

--- Page 14 ---
Course Code/Title:CS3V15/Devops Unit: I 29. Gradle Gradle continues to be a vital tool in software development and DevOps. Gradle is an advanced build automation system that plays a crucial role in managing dependencies, building projects, and orchestrating complex workflows efficiently. Its importance lies in its versatility and scalability, as it caters to various project sizes and types. Gradle's ability to easily handle multi-language, multi-project builds and its support for plugin-based customization make it indispensable in modern software development. As organizations increasingly adopt microservices architectures and cloud-native technologies, Gradle's capabilities are instrumental in managing the complexity of building, testing, and deploying applications across diverse environments. DevOps Architecture DevOps Architecture Plan Release Code Dep Deploy Development and operations both play essential roles in order to deliver applications. The deployment comprises analyzing the requirements, designing, developing, and testing of the software components or frameworks. The operation consists of the administrative processes, services, and support for the software. When both the development and operations cture is the solution to fix the gap between Test Monitor Build Operate ined with collaborating, then the DevOps al www deployment and operation terms; therefore, delivery can be faster. DevOps architecture is used for the applications hosted on the cloud platform and large distributed applications. Agile Development is used in the DevOps architecture so that integration and delivery can be contiguous. When the development and operations team works separately from each other, then it is time- consuming to design, test, and deploy. And if the terms are not in sync with each other, then it may cause a delay in the delivery. So DevOps enables the teams to change their shortcomings and increases productivity. Below are the various components that are used in the DevOps architecture: 1) Build: Without DevOps, the cost of the consumption of the resources was evaluated based on the pre-defined individual usage with fixed hardware allocation. And with DevOps, the usage of cloud, sharing of resources comes into the picture, and the build is dependent upon the user's need, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Build Code Test Plan DevOps Components Moniter Dev Ops Deploy Operate Release 14 N33 nirf 1750 Rank

--- Page 27 ---
Course Code/Title:CS3V15/Devops Unit: I · Servers are maintained easily and there is nearly zero downtime · Users can access cloud data and upload it on the cloud from any device with a solid internet connection · Cloud environments can be modified according to the user's requirements and is easily accessible · Clouds are highly secure, making data breaches more unlikely · Migrating to the cloud eliminates the need to buy on-premises infrastructure . It offers pay-as-you-go pricing, meaning you only pay for the resources you use 1. Infrastructure as a Service: IaaS delivers virtualized computing resources over the Internet. Users can rent virtual machines, storage, and networking infrastructure, allowing for easy scalability without investing in physical hardware. Examples include AWS EC2 and Azure Virtual Machines. 2. Platform as a Service: PaaS offers a robust platform for developers to build, deploy, and manage apps without worrying about the underlying infrastructure. It simplifies application development and deployment, with services like Google App Engine and Heroku leading the way. 3. Software as a Service: SaaS offers software applications on a subscription basis, accessible via a web browser. Users don't need to install or maintain software locally, making it ideal for collaboration tools (e.g., Microsoft 365, Google Workspace) and CRM systems (e.g., Salesforce). 4. Function as a Service: FaaS allows developers to execute code responding to events without managing servers. It's highly scalable and cost-efficient, exemplified by AWS Lambda and Azure Functions. FaaS is also known as serverless computing. 5. Container as a Service: CaaS enables the deployment and management of containerized applications using orchestration tools like Kubernetes. It provides portability and scalability for applications across different cloud environments. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 27 N33 nif 1750 Rank

--- Page 28 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between main cloud computing services Terms Stands for Uses Access Model Technical understanding. Popularity Percentage rise Usage Cloud services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY IAAS Infrastructure as a service. IAAS is used by network architects. IAAS gives access to the resources like virtual machines and virtual storage. It is a service model that provides virtualized computing resources over the internet. It requires technical knowledge. It is popular among developers and researchers. It has around a 12% increment. Used by the skilled developer to develop unique applications. Amazon Web Services, sun, vCloud Express. PAAS Platform as a service. PAAS is used by developers. PAAS gives access to run time environment to deployment and development tools for application. It is a cloud computing model that delivers tools that are used for the development of applications. Some knowledge is required for the basic setup. It is popular among developers who focus on the development of apps and scripts. It has around 32% increment. Used by mid-level developers to build applications. Facebook, and Google search engine. SAAS Software as a service. SAAS is used by the end user. SAAS gives access to the end user. It is a service model in cloud computing that hosts software to make it available to clients. There is no requirement about technicalities company handles everything. It is popular among consumers and companies, such as file sharing, email, and networking. It has about a 27 % rise in the cloud computing model. Used among the users of entertainment. MS Office web, Facebook and Google Apps. 28 N32 nirf 1750 Rank

--- Page 83 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Click "OK" to proceed. 4. Configure the Job General Settings 1. Project Description: ○ Enter a description for your job. 2. Discard Old Builds: ○ Set up a policy to manage the number of builds Jenkins keeps. Source Code Management (SCM) 1. Select SCM: ○ Choose your source code management system (e.g., Git). 2. Configure Repository: ○ Enter the repository URL and provide credentials if needed. 3. Branch Specification: ○ Specify the branch to build (e.g., main, develop). Build Triggers 1. Choose Build Triggers: ○ Poll SCM: Check for changes at intervals. ○ Build Periodically: Schedule builds. ○ GitHub hook trigger: Trigger builds on GitHub push. ○ Build after other projects: Trigger builds based on other jobs. Build Environment 1. Configure Build Environment: ○ Clean workspace: Delete workspace before build starts. ○ Manage secrets: Use secret texts or files. ○ Node selection: Specify where to run the job. Build Steps 1. Add Build Steps: ○ Click "Add build step". ○ Choose the build step type (e.g., Execute shell, Invoke Gradle script). ○ Enter the commands or scripts needed for the build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 nirf 8 1750 Rank

--- Page 84 ---
Course Code/Title:CS3V15/Devops Unit: III Post-build Actions 1. Add Post-build Actions: Click "Add post-build action". ○ Common actions include: Archive artifacts: Store build artifacts. Email notification: Send build status emails. I Publish test results: Aggregate test results. Trigger other jobs: Trigger other jobs based on results. 5. Save and Build 1. Save the Configuration: ○ Click "Save". 2. Build the Job: ○ Click "Build Now" to manually trigger a build. ○ View progress and logs by clicking on the build number. 6. Monitor and Manage Builds 1. Check Build History: ○ View past builds and their statuses. 2. Console Output: ○ Click on a build number to see detailed logs. 3. Build Artifacts: ○ Access any artifacts produced by the build. 7. Advanced Configuration (Optional) 1. Pipeline as Code: 0 For complex workflows, use Jenkins Pipelines. Create a Jenkinsfile in your repository. 2. Plugins: ○ Extend Jenkins functionality with plugins from the Jenkins Plugin Manager. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) 9 N32 nirf 1750 Rank

--- Page 113 ---
Course Code/Title:CS3V15/Devops Unit: III 3. Configure Job Details: ○ General: I Enter a description for your job. Configure options such as discarding old builds if needed. ○ Source Code Management: I Choose Git, Subversion, or another source control system. Enter the repository URL and credentials if the repository is private. Specify the branch or tag to build from. ○ Build Triggers: Set up triggers to start the build. Common options include: Poll SCM: Jenkins will periodically check for changes in the source code repository. . Build periodically: Schedule builds at specific intervals. I GitHub hook trigger for GITScm polling: Trigger builds based on GitHub webhooks. 0 Build Environment: ■ Configure the build environment, such as setting up environment variables or cleaning up before the build starts. ○ Build Steps: Add build steps by clicking Add build step. · Common build steps include: Execute shell: Run shell commands or scripts. . Invoke Gradle script: Use Gradle to build the project. I Invoke Ant: Use Apache Ant for building. ○ Post-build Actions: · Configure actions to perform after the build completes, such as: I Archive the artifacts: Save build artifacts for later use. I Publish JUnit test result report: Display test results in Jenkins. I Send build notifications: Notify stakeholders of build status. 4. Save and Build: ○ Click Save to store the job configuration. o To start a build, click Build Now in the job dashboard. Example Build Configuration: . Job Name: MyApp-Build · Source Code Management: Git o Repository URL: https://github.com/user/myapp.git ○ Branch: main · Build Step: Execute shell ○ Command : myn clean install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 38 nirf 1750 Rank N33

--- Page 114 ---
Course Code/Title:CS3V15/Devops Unit: III · Post-build Action: Archive artifacts ○ Files to archive: target/ *. jar 2. Understanding Jenkins Workspace Overview: The Jenkins workspace is a directory where Jenkins stores files and artifacts related to a particular build. Each job has its own workspace, which is used to perform build operations. Key Aspects of Jenkins Workspace: 1. Workspace Location: o By default, the workspace is located in the Jenkins home directory, typically at /var/lib/jenkins/workspace/ on Linux systems or C:\Program Files (x86) \Jenkins\workspace\ on Windows. 2. Workspace Structure: ○ Each job gets its own subdirectory within the workspace. For example, a job named MyApp-Build will have its workspace at /var/lib/jenkins/workspace/MyApp - Build/. ○ The workspace contains: Source Code: The code pulled from the repository. . Build Artifacts: Files generated during the build process. I Logs: Logs related to the build process. 3. Workspace Usage: ○ Building: During the build process, Jenkins checks out the code into the workspace, executes build steps, and generates artifacts. ○ Archiving Artifacts: After the build, files specified in the post-build actions are archived from the workspace. ○ Cleaning Up: Jenkins may clean up workspaces based on job configurations or policies to save disk space. 4. Customizing Workspace: ○ Configure Custom Workspace Location: In the job configuration, under the Advanced Project Options, you can specify a custom workspace directory. ○ Workspace Cleanup: I Use plugins like the Workspace Cleanup Plugin to manage and clean up workspaces automatically. Example of Workspace Structure: · Workspace Directory: /var/lib/jenkins/workspace/MyApp-Build/ o Source Code: /var/lib/jenkins/workspace/MyApp-Build/src/ o Build Artifacts: /var/lib/jenkins/workspace/MyApp-Build/target/ o Logs: /var/lib/jenkins/workspace/MyApp-Build/logs/ This guide should provide a comprehensive understanding of creating a Jenkins build and managing Jenkins workspaces. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY N32 39 nirf 175º Rank

--- Page 115 ---
Unit: IV Course Code/Title:CS3V15/Devops UNIT IV CONFIGURATION MANAGEMENT USING ANSIBLE Ansible Introduction, Installation, Ansible master/slave configuration, YAML basics, Ansible modules, Ansible Inventory files, Ansible playbooks, Ansible Roles, adhoc commands in ansible. Ansible A SSH ANSIBLE TARGET NODE HOSTS PLAYBOOK Ansible is a powerful DevOps tool designed for automating tasks on remote servers or nodes. In simple terms, it allows you to automate commands and functions on multiple remote machines from a central 'master' node. To illustrate its usefulness, consider a scenario where you need to reboot dozens or even hundreds of remote hosts. You could manually SSH into each one and initiate the reboot, or you can use Ansible to streamline the process, making it efficient and offering a wide range of additional functionalities. In essence, Ansible operates much like the second method mentioned, using SSH (Secure Shell), a secure communication protocol, to control remote nodes in a secure and optimized manner. Ansible Components Ansible Core Components Control node Ansible Inventory Managed node 1 Managed node 2 Managed node 3 1. Control Node - · The central or main node where Ansible is installed. · Used to trigger commands like ansible and ansible-inventory on other nodes. · Acts as the orchestrator for Ansible operations. • TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 1 N33 nirf 1750 Rank

--- Page 116 ---
Course Code/Title:CS3V15/Devops Unit: IV 2. Manage Node- · A remote or slave node where tasks are executed or controlled by Ansible. . These are the servers or devices you want to manage or automate. 3. Inventory - · A list of managed node IPs and configurations. · Logically organized, typically using file formats like YAML or INI. · Created on the control node to describe the deployment of hosts to Ansible. Ansible Additional Components Ansible Architecture CMDB Users Public / Private Cloud Hosts ... ... Inventory .. Modules 000 Ansible Playbook API Plugins Networking www.educba.com 1. Ad-Hoc Commands - . These are one-off commands that you can execute using the ansible command. · Useful for quick tasks or tests on remote nodes. 2. Plugins - · Plugins are pieces of code that extend Ansible's core functionality. · Ansible uses a plugin architecture for flexibility and expandability. · Examples include connectivity plugins for establishing connections and cache plugins. · Learn more about Ansible plugins. 3. Module- · Modules are built-in functions that can be used to perform various tasks. . They eliminate the need to write custom code for common operations. · Examples include modules for package management (apt, yum), service management, and more. CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) CHENNAI INSTITUTE . TECHNOLOGY 2 N33 nirf 1750 Rank

--- Page 157 ---
Select a pipeline run to view information about that run. You can choose to Retain or Delete a run from the context menu. For more information on run retention, see Build and release retention policies. Stages La 51m ago > ... 6 42s Retain Delete Ld 2h ago 0 1m 13s - View pipeline details The details page for a pipeline allows you to view and manage that pipeline. Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. Azure DevOps · Search F 4 FabrikamFiber Edit Run pipeline : + Runs Branches Analytics Y Description Stages Ac #20191209.3 Set up CI with Azure Pipelines Manually triggered { main 2b4b23c #20191209.2 Set up CI with Azure Pipelines & Manually triggered & main 2b4b23c Là 54m ago Č 1m 13s Là 55m ago Č 1m 55s #20191209.1 Set up CI with Azure Pipelines Individual CI 1º main 2b4b23c Lô 56m ago 1m 6s 17

--- Page 158 ---
Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. View pipeline run details From the pipeline run summary you can view the status of your run, both while itis running and when it is complete. #20191210.2 Update azure-pipelines.yml for Azure Pipe ... on FabrikamFiber Summary Environments Triggered by Steve Danielson FabrikamFiber & main b2f795e Today at 12:56 PM Duration: 1m 9s Tests: Get started Changes: ¢ 2 commits Work items: ₾ 1 linked Artifacts: 1 published Stages Jobs Build 1 job completed 415 Run new : Deploy 1 job completed 13s 1 artifact From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, test results, and work Items From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, testresults, and work items Jobs and stages The jobs pane displays an overview of the status of your stages and jobs. This pane may have multiple tabs depending on whether your pipeline has stages and jobs, or just jobs. In this example, the pipeline has two stages named Build and Deploy. You can drill down into the pipeline steps by choosing the job from either the Stages or Jobs pane. 10

--- Page 11 ---
Course Code/Title:CS3V15/Devops Unit: I 17. Selenium It remains a vital tool in software testing and automation due to its enduring relevance in ensuring the quality of web applications. As technology evolves, web applications become increasingly complex, requiring thorough testing across various browsers and platforms. With its robust automation capabilities and extensive browser compatibility, Selenium allows developers and QA teams to automate repetitive testing tasks efficiently, conduct cross- browser testing, and ensure that web applications function flawlessly across diverse environments. Its open-source nature, active community support, and integration with other DevOps tools make Selenium a go-to choice for organizations striving for continuous delivery and the rapid deployment of high-quality software, a cornerstone of modern software development practices. 18. Gremlin Gremlin is an essential tool in chaos engineering, which has become increasingly critical for ensuring the resilience and reliability of modern software systems. As technology advances and complex distributed systems become the norm, the potential for unexpected failures and outages also rises. Gremlin allows organizations to proactively identify weaknesses and vulnerabilities in their infrastructure and applications by simulating controlled failures, such as network disruptions, service outages, and resource constraints. By intentionally inducing chaos and monitoring the system's response, teams can uncover weaknesses before they lead to costly downtime or security breaches. Gremlin facilitates organizations to build more robust, fault-tolerant systems that can withstand real-world challenges and deliver uninterrupted services to users. 19. ServiceNow ServiceNow is a vital platform for organizations seeking to streamline their IT service management and beyond. Its significance lies in its ability to provide a unified, cloud-based solution for automating and optimizing various business processes, including ITSM, ITOM, HR, customer service, and more. Due to the rapid digitization of services, remote work, and the growing complexity of technology infrastructures, ServiceNow offers a comprehensive approach to managing workflows, resolving issues, and delivering services efficiently. Its intelligent automation capabilities, analytics, and AI-driven insights empower organizations to enhance productivity, agility, and customer satisfaction while reducing operational costs. ServiceNow's role in orchestrating and integrating diverse systems and processes makes it an indispensable tool for driving digital transformation and ensuring smooth operations in the ever-evolving business landscape of 2024. 20. Status Service Updates: The Status Page "Status Service Updates: The Status Page" is a critical tool for organizations and businesses of all sizes. In today's world, where online services and applications are integral to operations, ensuring their availability and reliability is essential. It provides real-time information to users and stakeholders about the operational status of services, applications, and infrastructure. The Status Page plays a crucial role in transparency, trust-building, and customer satisfaction by promptly communicating service disruptions, planned maintenance, and incident resolutions. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 1750 Rank

--- Page 12 ---
Course Code/Title:CS3V15/Devops Unit: I Downtime can often lead to significant financial losses and damage to a company's reputation, so having a practical Status Page becomes not just a convenience but a necessity. It allows organizations to showcase their commitment to transparency and responsiveness in addressing service-related issues, ultimately fostering stronger customer relationships and trust. 21. ELK (Elasticsearch, Logstash and Kibana) ELK, which stands for Elasticsearch, Logstash, and Kibana, continues to shine in DevOps and IT operations. This powerful trio of tools remains essential for organizations seeking effective log management, monitoring, and data visualization. Elasticsearch is a highly scalable and fast search engine that enables real-time data indexing and search. Logstash facilitates the collection, processing, and transformation of log data from various sources, making it compatible with Elasticsearch. Kibana, on the other hand, provides a user- friendly interface for visualizing and analyzing data, offering customizable dashboards and powerful data exploration capabilities. ELK's significance in 2024 lies in its ability to empower organizations with comprehensive insights into their systems, applications, and infrastructure. It ultimately facilitates quick problem resolution, proactive monitoring, and data-driven decision-making in an increasingly complex and fast-paced technological landscape. 22. GitLab CI/CD GitLab CI/CD's significance lies in its ability to automate the complete software delivery pipeline, from code changes to deployment, in a single integrated environment. GitLab CI/CD ensures rapid and reliable delivery of software updates. It enables continuous integration (CI) by automatically building and testing code changes, allowing teams to catch issues early in the development cycle. Furthermore, the continuous deployment (CD) aspect automates the release and deployment process, reducing the risk of human errors and enabling organizations to deliver features and updates to users swiftly and confidently. GitLab CI/CD's importance is further accentuated as businesses seek to accelerate digital transformation efforts, respond rapidly to changing market demands, and maintain a competitive edge through efficient and automated software delivery practices. 23. Scripting Scripting remains vital due to its pivotal role in automating and streamlining various aspects of software development, system administration, and DevOps practices. Scripting languages like Python, Bash, and PowerShell empower tech professionals to write code that can execute repetitive tasks, manipulate data, and orchestrate complex processes efficiently. Scripting facilitates rapid prototyping, configuration management, and the creation of automated deployment pipelines. It enhances productivity, ensures consistency and reduces human error in tasks ranging from software testing and deployment to infrastructure provisioning and monitoring. As organizations increasingly embrace DevOps and cloud- native technologies, scripting stays competitive and adaptive in the tech landscape. 24. Terraform Terraform plays a crucial role in modern infrastructure provisioning and management. It allows organizations to define and deploy infrastructure as code, enabling the automated creation and configuration of cloud resources, containers, and other infrastructure components. Cloud computing, microservices, and containerization have become the norm in CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 12 N32 nirf 1750 Rank

--- Page 129 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible ad-hoc Commands The Ansible ad-hoc command uses the /usr/bin/ansible command-line tool to automate a single task on one or more managed nodes. The Ad-hoc commands are quick and easy, but they are not re-usable. The Ad-hoc commands demonstrate the simplicity and power of Ansible. Syntax ansible <hosts> [-m <module_name>] -a <"arguments"> -u <username> [ -- become] Explanation Hosts: It can be an entry in the inventory file. For specifying all hosts in the inventory, use all or "*" . module_name: It is an optional parameter. There are hundreds of modules available in the Ansible, such as shell, yum, apt, file, and copy. By default, it is the command. Arguments: We should pass values that are required by the module. It can change according to the module used. Username: It specifies the user account in which Ansible can execute commands. Become: It's an optional parameter specified when we want to run operations that need sudo privilege. By default, it becomes false. 1. Parallelism and shell commands You can reboot your company server in 12 parallel forks at the same time. For this, you need to set up the SSHagent for connection. $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa To run reboot for all your company servers in the group, 'abc', in 12 parallel forks: $ ansible abc -a "/sbin/reboot" -f 12 By default, Ansible will run the above ad-hoc commands from the current user account. If you want to change then pass the username in ad-hoc command as follows: $ ansible abc -a "/sbin/reboot" -f 12 -u username 2. File Transfer You can use ad-hoc commands for doing SCP (secure copy protocol) which means lots of files in parallel on multiple machines or servers. CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 15 N32 nirf 175º Rank

--- Page 130 ---
Course Code/Title:CS3V15/Devops Unit:IV Transferring file on many machines or servers $ ansible abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf" Creating new directory $ ansible abc -m file - a "dest = /path/user1/new mode = 888 owner = user group = user1 state = directory" Deleting all directory and files $ ansible abc -m file -a "dest = /path/user1/new state = absent" 3. Managing Packages Ad-hoc commands are available for apt and yum module. Here are the following ad-hoc commands using yum. Below command checks, if the yum package is installed or not, but not update it. $ ansible abc -m yum -a "name = demo-tomcat-1 state = present" Below command checks the package is not installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = absent" And below command checks the latest version of package is installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = latest" 4. Managing Users and Groups You can manage, create, and remove a user account on your managed nodes with ad-hoc commands. $ ansible all -m user -a "name=foo password =< crypted password here>" $ ansible all -m user -a "name=foo state=absent" 5. Managing Services Ensure a service is started on all the webservers. $ ansible webservers -m service -a "name=httpd state=started" Alternatively, restart a service on all webservers: $ ansible webservers -m service -a "name=httpd state=restarted" CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 nirf 1750 Rank

--- Page 161 ---
Add a status badge to your repository Many developers like to show that they're keeping their code quality high by displaying a status badge in their repo. Azure Pipelines succeeded To copy the status badge to your clipboard: 1. In Azure Pipelines, go to the Pipelines page to view the list of pipelines.Select the pipeline you created in the previous section. 2. Select , and then select Status badge. 3. Select Status badge. 4. Copy the sample Markdown from the Sample markdown section. Now with the badge Markdown in your clipboard, take the following steps in GitHub: 1. Go to the list of files and select Readme.md. Select the pencil icon to edit. 2. Paste the status badge Markdown at the beginning of the file. 3. Commit the change to the main branch. 4. Notice that the status badge appears in the description of your repository. To configure anonymous access to badges for private projects: 1. Navigate to Project Settings in the bottom left corner of the page 2. Open the Settings tab under Pipelines 3. Toggle the Disable anonymous access to badges slider under General 21

--- Page 162 ---
Azure-Pipeline • Azure Pipelineș provides a YAML pipeline editor that you can use to author and edit your pipelines. • The YAML editor is based on the Monaco Editor. • The editor provides tools like Intellisense support anda task assistant to provide guidance while you edit a pipeline. • You can also edit pipelines by modifying the azure pipelines.yml file directly in your pipeline's repository using a text editor of your choice Edit a YAML pipeline To access the YAML pipeline editor, do the following steps. • Sign in to your organization • (https://dev.azure.com/{yourorganization}). • Select your project, choose Pipelines, and then select the pipeline you want to edit. You can browse pipelines by Recent, All, and Runs. • Choose Edit. • Make edits to your pipeline using Intellisense and the task assistant forguidance. Azure DevOps fabrikam-tailspin FabrikamFiber F FabrikamFiber + Pipelines Overview Recent All Runs Boards Pipelines Recently run pipelines Repos 1 Pipeline Pipelines Pipelines FabrikamFiber 2 3 17

--- Page 117 ---
Course Code/Title:CS3V15/Devops Unit: IV • Explore Ansible modules. 4. Playbook - · Playbooks are a sequence of plays that define the order of tasks executed by Ansible. . A play consists of a list of tasks that target managed nodes in an inventory. · Tasks, in turn, are composed of one or more modules that specify operations. · Playbooks provide a structured way to define and automate complex workflows. 5. Roles - · Roles provide an organized environment for managing complex tasks. · They include templates, playbooks, inventories, error handlers, vars, and meta information. · Ideal for handling larger, multi-step automation processes. 6. Collections - · Collections are distribution formats for Ansible content. · They encompass playbooks, roles, modules, and plugins. · Find Ansible collections. 7. Galaxy - · Ansible Galaxy is a platform for sharing and downloading collections. . It allows the Ansible community to collaborate and exchange automation content. Setting Up the Ansible Control Node Prerequisite: Before you begin setting up Ansible, it's essential to ensure you meet the following prerequisites: · SSH: Make sure SSH is installed on your Linux system. SSH is crucial for secure communication between the control node and managed nodes. Installation Steps: 1. Open your terminal. 2. Run the following command to install Ansible: 1python3 -m pip install -- user ansible 3. SSH Key: o For secure and direct connections to managed nodes, you'll need to provide a public SSH key. You can generate one using the ssh-keygen command. o Follow the on-screen instructions to create your SSH key pair. This key pair is a crucial element of Ansible's secure communications. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 118 ---
Course Code/Title:CS3V15/Devops Unit: IV 4. For specific installation instructions tailored to your Linux distribution, click here. 5. Try to connect ssh using once your mange node is set up. 1ssh <user>@<ip> Note: If you're using a different operating system, find the appropriate installation instructions here. Setting Up the Ansible Mange Node Steps: 1. Install OpenSSH-Server: o To enable remote management of the managed node, you'll need to install the OpenSSH server. Use the following command (assuming you're using a Debian-based Linux distribution like Ubuntu): 1sudo apt install openssh-server o This command installs the OpenSSH server, allowing secure remote access. 2. Create a User: o For convenience and consistency, it's helpful to create a user on the managed node with the same name as the user on the control node. This makes it easier to manage SSH keys and ensures a smoother experience. 3. Configure SSH Key: o To establish secure and direct connections to the managed node, you need to paste the public SSH key from the control node into the authorized_keys file located in the .ssh directory of the user's home folder on the managed node. o The path is typically /home/<user>/.ssh/authorized_keys, where <user> is the username you created or are using on the managed node. You can use the ssh-copy-id command to automate the process of copying your public key to the managed node. For example: 1ssh-copy-id <user>@<managed_node_ip> o This command securely copies your public key to the authorized_keys file, allowing passwordless SSH authentication. Managing Ansible Inventories In the world of Ansible, inventories are like the backbone of your automation infrastructure. They serve as a vital component that lists and organizes the managed nodes, making automation tasks seamless and organized. Inventories use a parent-child concept that allows you to create groupings, which come in handy during specific tasks or scenarios. Why Are Inventories Important? · Inventories provide a comprehensive list of managed nodes, making it easy to interact with and manage them. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 4 N33 nirf 1750 Rank

--- Page 111 ---
Course Code/Title:CS3V15/Devops Unit: III Steps: 1. Install the Git Plugin: 0 Go to Manage Jenkins > Manage Plugins. ○ Under the Available tab, search for Git Plugin and install it. 2. Configure Git in Jenkins: ○ Navigate to Manage Jenkins> Global Tool Configuration. ○ Scroll down to the Git section and click Add Git. ○ Specify the path to the Git executable or use the default path. ○ Optionally, configure additional settings such as Git installation locations. 3. Example Configuration: ○ Name: Git ○ Path to Git executable: /usr/bin/ git (or use the default if Git is installed in a standard location). 4. Configure Job to Use Git: ○ Create or edit a Jenkins job and go to Source Code Management. ○ Select Git. ○ Enter the repository URL (e.g., https://github.com/user/repository.git). ○ Configure credentials if the repository is private. ○ Specify the branch to build (e.g., main). 5. Example Repository URL: ○ Repository URL: https://github.com/example/repo.git o Branch Specifier: main 3. Configuring Jenkins to Work with Maven Overview: The Maven Plugin integrates Jenkins with Apache Maven, allowing Jenkins to use Maven to build projects. Steps: 1. Install Maven: ○ Ensure that Apache Maven is installed on your system. You can download it from the Apache Maven website. 2. Configure Maven in Jenkins: ○ Go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to the Maven section and click Add Maven. ○ Enter a name for the Maven installation (e.g., Maven 3.8.6). ○ Check Install automatically to let Jenkins download Maven, or specify the path to an existing Maven installation. ○ CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 36 1750 Rank

--- Page 112 ---
Unit: III Course Code/Title:CS3V15/Devops 3. Example Configuration: ○ Name: Maven 3.8.6 ○ MAVEN_HOME: / usr / share / maven 4. Configure Maven in a Jenkins Job: ○ Create or edit a Jenkins job and go to Build section. 0 Add a build step and select Invoke top-level Maven targets. o Choose the Maven version configured earlier. ○ Specify the goals to run (e.g., clean install). 5. Example Maven Goals: ○ Goals: clean install 6. Configure Build Environment: ○ Ensure that the job's build environment is set up to use Maven. This might involve setting environment variables or configuring build scripts. 7. Example Environment Variables: ○ MAVEN_OPTS: - Xms512m -Xmx2048m (if you need to set specific JVM options for Maven). By following these steps, Jenkins will be properly configured to work with Java, Git, and Maven, allowing you to set up and manage continuous integration and delivery pipelines effectively. If you need further customization or run into issues, consulting the documentation for each tool or plugin can provide additional guidance. Creating a Jenkins Build and Understanding Jenkins Workspace 1. Creating a Jenkins Build Overview: A Jenkins build is a process where Jenkins executes a series of steps defined in a job configuration. This typically involves compiling code, running tests, and generating artifacts. Steps to Create a Jenkins Build: 1. Access Jenkins Dashboard: ○ Open your Jenkins dashboard, usually found at http://localhost: 8080. 2. Create a New Job: ○ Click on New Item on the left sidebar. Enter a name for your job. ○ Choose a job type. Common types include: I Freestyle project: For simple build jobs with basic configuration. · Pipeline: For more complex build processes using a Jenkinsfile. ○ Click OK to proceed. ○ TECHNOLOGY N33 37 nirf 1750 Rank CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous)

--- Page 127 ---
Course Code/Title:CS3V15/Devops Unit:IV Abbreviation You can also use abbreviation to represent lists. Example Countries: ['America', 'China', 'Canada', 'Iceland'] List inside Dictionaries We can use list inside dictionaries, i.e., value of key is list. Example james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english … List of Dictionaries We can also make list of dictionaries. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english - robert: name: robert richardson rollNo: 53 div: B sex: male CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 13 N32 nirf 1750 Rank

--- Page 128 ---
Course Code/Title:CS3V15/Devops Unit:IV likes: - biology - chemistry … YAML uses "|" to include newlines while showing multiple lines and ">" to suppress newlines while showing multiple lines. Due to this we can read and edit large lines. In both the cases intendentation will be ignored. We can also represent Boolean (True/false) values in YAML. where boolean values can be case insensitive. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english result: maths: 87 chemistry: 45 biology: 56 physics: 70 english: 80 passed: TRUE messageIncludeNewLines: | Congratulation !! You passed with 79% messageExcludeNewLines: > Congratulation !! You passed with 79% CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 14 N33 nirf 1750 Rank

--- Page 143 ---
Unit: V Course Code/Title: CS3V15/Devops Email preferences Send me occasional product updates, announcements, and offers. Verify your account Pick one square that shows two identical objects. wiki How Step 4: Click the green Create account button. It's belowthe form, at the bottom of the page. This will take you to an email verification page. Verify your account Create account By creating an account, you agree to the Terms of Service. For more information about GitHub's privacy practices, see the GitHub Privacy Statement. We'll occasionally send you account-related wikiHow UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 3

--- Page 144 ---
Course Code/Title: CS3V15/Devops Unit: V Carefully review the Terms of Service at https://help.github.com/en/articles/github- terms-of-service and the Privacy Statement at https://help.github.com/en/articles/github- privacy-statement before you continue. You're almost done! We sent a launch code to wikihowneveconcepts@gmail.com + Enter code Didn't get your email? Resend the code or update your email address. wiki How Step 5: Verify your email by entering the code. After clicking Create account, you'll receive an email with a code.Enter this code on the verification page. Entering the code will automatically take you to the welcome page. Step 6: Select your preferences and click Continue. GitHub displays a quick survey that can help you tailor your experience to match what you're looking for. You'll be sent tothe plan selection page after completing the survey. How many team members will be working with you? This will help us guide you to the tools that are best suited for your projects. Just me 2 - 5 5 - 10 10 - 20 20 - 50 50+ Are you a student or teacher? Student Teacher Continue wikiHow CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 4 175° Rank ---

--- Page 135 ---
Course Code/Title:CS3V15/Devops Unit: IV Syntax for writing the module on command line While writing the module we use the options such as -m and -a. The -m is for module and -a is for argument, after the -a option we specify the executable command in double quotes. here 'all' stands for all the remote hosts, otherwise we can mention the group name of the hosts 1 $ ansible all -m command -a "echo 'Hello hosts'" The -m option followed by module name 'command' Command to be executed on remote hosts, given in double quotes Commonly used modules 1. Command : Following is a command module that executes a command on remote hosts $ ansible all -m command -a "echo 'Hello hosts'" 2. User : The user module is used to create, modify or delete users. For example - To create a user named 'new_user' on all the hosts in the 'dev' group using ansible, you can use the user module. Here's the ansible command to do that : $ ansible dev -m user -a "name=new_user state=present" Sometimes the above command fails as there is no privilege to create user on remote host. Then we can issue the command as follows $ ansible all -m user -a "name=new_user state=present" -- become -- ask-become-pass The above command asks for the password for the remote host to login. On supplying the password the user will be created. We can verify. if new user is getting created or not on remote host by issuing following command on each remote host $ less /etc/passwd If we want to create new user without asking for password on all the remote hosts then we can issue the command as - $ ansible all -m user -a "name=new_user1 state=present" -- become -b The -b option is used to become superuser. By this the sudo or su privileges can be granted. .3. Ping : The ping module is used to connectivity to remote hosts $ ansible all -m ping 4. Copy : The copy module is used to copy files from control machine to remote hosts $ ansible dev -m copy -a "src=/home/ansible_user/myfile.txt dest=/home/ansible_user/myfile.txt" The above command will copy the myfile.txt file from control node to all the managed nodes of dev group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 21 N32 nirf 1750 Rank

--- Page 136 ---
Course Code/Title:CS3V15/Devops Unit: IV 5. Package : The package module is used to install, remove or upgrade the packages, For example, the following command installs the nginx web server package : $ ansible package -a "name=nginx state=present" Here name indicates the name of the package to be installed, the state=present means install the package. Normally "present" is for installation, "remove" is for removing the package and "latest" is used to upgrade the package. 6. Shell : The shell module is used to run shell commands with more flexibility on remote hosts. For example - Following shell command executes on remote hosts. $ ansible dev -m shell -a "cmd=is" 7. Service : The service module is used to start, stop or restart a specific service on remote hosts. For example - Following service will start the apache web service on all the remote hosts. $ ansible dev -m service -a "name=apache2 state=started" Note that the state can started for starting the service, stopped for stopping the service. o started/stopped are idempotent actions that will not run commands unless necessary. o restarted will always bounce the service. o reloaded will always reload. I Example 1: Get the Uptime of remote servers We have used command module to run the uptime command and we have given both the ad hoc and the playbook form of execution. as AD-HOC Command $ ansible testservers -m command -a uptime -i ansible_hosts as Playbook - name: Check the remote host uptime hosts: testservers tasks: - name: Execute the Uptime command over Command module register: uptimeoutput command: "uptime" - debug: var: uptimeoutput.stdout_lines CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 22 N32 nirf 1750 Rank

--- Page 139 ---
Course Code/Title:CS3V15/Devops Unit: IV Create the file, if it does NOT exist (or) absent $ ansible testservers -a "touch /tmp/testfile creates=/tmp/testfile" -i ansible_hosts As Playbook - name: "Validate if a file is present or not present using Ansible Command module" hosts: testservers tasks: - name: "Create a file if it does not exist" command: "touch /tmp/latestfile" args: creates: "/tmp/latestfile" register: createif - name: "Display the file to make sure its created" command: "ls -lrt /tmp/latestfile" register: displayif when: createif is changed - debug: var=displayif.stdout - name: "Remove the file if it exist" command: "rm -rf /tmp/latestfile" args: removes: "/tmp/latestfile" register: removeif In the playbook we execute three tasks, • First one is a create a file if it does not exist • Second to display if the creation is successful • Third one is to delete the file if it exists Example 5: Execute or Run the Script when a file exists or not exists Now for this example let us take something relatable to real world scenario. Like Start the Server instance (or) program if the PID file or LOCK file does not exist Here is the sample playbook for you to get started. I am giving a general playbook here as I do not want to pick some server/technology of my own which you cannot relate to. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 140 ---
Course Code/Title:CS3V15/Devops Unit:IV you can feel free to modify the script to your needs as this is just a template. - name: Start of Stop Server instance based on PID/LOCK file availability hosts: appservers tasks: - name: Start the instance when the PID file is not present become: yes become_user: appuser command: "startserver.sh" args: creates: "/path/to/pid/instance.pid" register: startinst - name: Stop the instance when the lock file is present become: yes become_user: appuser command: "stopserver.sh" args: removes: "/path/to/lockfile/instance.lck" register: stopinst CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 26 N33 nirf 1750 Rank

--- Page 87 ---
Course Code/Title:CS3V15/Devops Unit: III 2. Go to Plugin Manager: From the dashboard, click on 'Manage Jenkins' on the left-hand side menu. On the Manage Jenkins page, click on 'Manage Plugins'. This will take you to the Plugin Manager, where you can view, install, and update plugins. 3. Exploring Available Plugins 1. Available Tab: The 'Available' tab lists all plugins that are available for installation. This includes thousands of plugins categorized by functionality (e.g., SCM, build tools, user interface enhancements). 2. Search for Plugins: Use the search box to quickly find the plugin you need. For example, if you're looking to integrate Jenkins with Git, type 'Git' in the search box. 3. Popular Plugins: Jenkins highlights popular plugins at the top of the Available tab. These are commonly used plugins that most users find essential. 4. Installing Plugins 1. Selecting Plugins to Install: Check the box next to each plugin you want to install. You can select multiple plugins at once. 2. Install Without Restart: After selecting your plugins, scroll down and click 'Install without restart'. Jenkins will install the plugins immediately, and you can continue using Jenkins during the installation. 3. Install After Restart: If you prefer, you can choose to 'Download now and install after restart'. This will install the plugins after Jenkins is restarted, ensuring that all changes are applied without interference. 4. Installation Progress: The installation process will show a progress bar for each plugin. Once a plugin is installed, it will move to the 'Installed' tab. 5. Configuring Installed Plugins 1. Plugin Configuration: After installation, some plugins may require additional configuration. Navigate to 'Manage Jenkins' > 'Configure System' or 'Configure Global Security' to adjust settings for your new plugins. 2. Global Tool Configuration: For build tools like Maven or JDKs installed via plugins, you can configure them under 'Global Tool Configuration'. Here, you define the paths to the tools or specify installation options. 6. Updating and Managing Plugins 1. Updating Plugins: Regularly update your plugins to ensure compatibility with the latest version of Jenkins and to benefit from new features or security patches. Go to the 'Updates' tab in the Plugin CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N32 nirf 12 1750 Rank

--- Page 88 ---
Unit: III Course Code/Title:CS3V15/Devops Manager. Jenkins will show you all plugins with available updates. Click 'Update' to install the latest versions. 2. Uninstalling Plugins: If a plugin is no longer needed, you can uninstall it from the 'Installed' tab. Select the plugin and click 'Uninstall'. Uninstallation typically requires a Jenkins restart to complete. 7. Commonly Used Plugins Here are a few essential plugins that you might consider adding to your Jenkins setup: 1. Git Plugin: Integrates Jenkins with Git, allowing you to pull code from GitHub, Bitbucket, or other Git repositories. 2. Pipeline Plugin: Enables the creation and management of Jenkins pipelines, a powerful way to define complex build, test, and deployment processes. 3. Maven Integration Plugin: Adds support for Maven projects, allowing you to build and manage Maven-based projects directly within Jenkins. 4. Email Extension Plugin: Provides advanced email notification options for build statuses, including customizable email templates. 5. Blue Ocean: An alternative user interface for Jenkins that simplifies pipeline creation and offers a modern, user-friendly design. 8. Troubleshooting Plugin Issues 1. Compatibility Issues: Sometimes, plugins might conflict with each other or with the Jenkins version. In such cases, refer to the plugin's documentation or Jenkins logs to diagnose the issue. 2. Rollback: If a plugin update causes problems, you can rollback to a previous version from the 'Installed' tab by selecting the desired version. 3. Restart Jenkins: Some plugin changes require a Jenkins restart. Ensure all jobs are completed before restarting to avoid any interruptions. Commonly Used Jenkins Plugins 1. Git Plugin Overview: The Git Plugin integrates Jenkins with Git repositories, allowing Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nårf 1750 Rank

--- Page 95 ---
Course Code/Title:CS3V15/Devops Unit: III C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha C: \ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>exit e Finished: SUCCESS Step 6 : Now go to GitHub repository and make some changes to your test.java file and commit these changes. BE AnurdhaP Changes in the test java Code Blame 6 lines (6 loc) · 158 Bytes 89 Code 55% faster with GitHub Copilot > public class test { 2 public static void main(String args[]) { for(int 1=1;i <= 5;1++) 3 4 System.out.println("Good Morning Path !!! "); 5 3 6 } Step 7 : Now, if you check on the Jenkins page, it automatically executes the above updated test.java file and creates a build. It is as follows .- C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test. java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Good Morning Path !!! Good Morning Pathi !! Good Morning Path !!! Good Morning Path !!! Good Morning Pathill 2. Parameter Plugin Overview: The Parameter Plugin allows Jenkins jobs to accept parameters at build time, enabling dynamic and flexible builds. Key Features: · Parameter Types: Supports various parameter types such as string, choice, boolean, and more. · Default Values: Provides default values for parameters. · Prompt for Parameters: Users are prompted to enter values for parameters when starting a build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 20

--- Page 96 ---
Course Code/Title:CS3V15/Devops Unit: III Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Parameterized Builds," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add parameters of different types (e.g., string, choice) and configure their options and default values. Usage: · Useful for creating jobs that require user input or need to be customized for different build scenarios. A Example Demo Step 1 : Click on New item and create a job. I have created a job by the name Welcome (Refer section 3.5) Step 2 : Click on the existing job and configure it with parameters. Step 3 : Check the option "This project is parameterized". Select the String Parameter. Create some string parameter. I have created a parameter named Username and default value to it. This project is parameterized ? =String Parameter ? Name ? Username Default Value ? Parth Description ? Plain text Preview Trim the string ? Step 4 : Under the Build Steps section, using echo command I tried to display the parameter value CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 21

--- Page 159 ---
Choose a job to see the steps for that job From the < Jobs in run #20191 ... FabrikamFiber Build O Build Build 40s Initialize job 1s 2 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 Duration: 40s 6 Pool: Azure Pipelines Image: Ubuntu-16.04 7 > Job preparation parameters Checkout 3s CmdLine 25 Component Detect 32s Post-job: Checkout <1s Finalize Job <1s Deploy DeployWeb 10s Finalize build Report build status <1s steps view, you can review the status and details of eachstep. From the Moreactions you can toggle timestamps or view a raw log of all steps in the pipeline. Build 9 Pool: Azure Pipelines 2 Image: Ubuntu-22.04 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 View job raw log Toggle timestamps 6 Duration: 40s Job preparation parameters 10

--- Page 160 ---
Cancel and re-run a pipeline If the pipeline is running, you can cancel it by choosing Cancel. If the run has completed, you can re-run the pipeline by choosing Run new. Pipeline run more actions menu: #20191210.3 Update azure-pipelines.yml for Azure Pip ... on FabrikamFiber Cancel Stages Jobs € Build C Deploy 0/1 compléted 48s Not started Build 48s Cancel From the More actions menu you can download logs, add tags, edit the pipeline, delete the run, and configure retention for the run. cure Pipe ... Run new Download logs Add tags Edit pipeline View retention leases Retain Delete 20

--- Page 141 ---
Course Code/Title: CS3V15/Devops Unit: V UNIT V - BUILDING DEVOPS PIPELINES USING AZURE Create Github Account, Create Repository, Create Azure Organization, Create a new pipeline, Build a sample code, Modify azure-pipelines.yaml file Github Account Creation What is Github? GitHub is a code hosting platform for collaboration and version control. GitHub lets you and others work together on projects fromanywhere. Github is owned by Microsoft, provides access to public(free) and private(paid) repositories. Stepsto create Github Account: Join GitHub · GitHub × + < C https://github.com/join f Facebook - Log In o ... Twitter G Google f Facebook YouTub Product v Solu wiki How Step 1:Go to https://github.com/join in a web browser. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 1 175" Rank

--- Page 142 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: Enter your personal details. In addition to creating a username and entering an email address, you'll also have to create a password. Your password must be at least 15 characters in length or at least 8 characters with at least one number and lowercase letter. Join GitHub First, let's create your user account Username * wikihowneveconcepts Email address * Password * .......... Make sure it's at least 15 characters OR at least 8 characters including a number and a lowercase letter. Learn more. Email preferences Send me occasional product updates, announcements, and offers. Verify your account wiki How Step 3: Click Verify to start the verification puzzle. The instructions vary by puzzle, so just follow the on-screen instructions to confirm that you are a human. A green checkmark will appear after completing the puzzle. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 2

--- Page 119 ---
Course Code/Title:CS3V15/Devops Unit: IV · Groupings in inventories enable you to organize your infrastructure, allowing you to target specific subsets of nodes based on your needs. For instance, when you need to perform tasks in a particular location, groupings become essential. Creating Inventories There are two primary methods to create an inventory on the control node: using a YAML file or an INI file. 1. INI Inventory Example (inventory.ini): 1[virtualmachines] 2host1 ansible_host:192.168.0.1 3192.168.0.2 2. In this INI file, it captures the IP addresses of managed nodes. The default username used for SSH connections is typically the working user of your control node. 3. ansible_host: This parameter is used to specify the IP address, although you can write the IP directly. 4. Inventory.yaml 1 --- 2virtualmachines: # Define a class name to represent a particular group of devices. 1 hosts: # Define the hosts. 2 vm01: # Define a parent name to indicate a class of parent type. 3 ansible_host: 192.168.0.1 # Specify the IP address. 4 http_port: 80 # (optional) Define the HTTP port. 5 ansible_user: auriga # (optional) Provide the username if it's different from the control node's user. o ansible_user: Use this to specify a different user if the managed node's user is not the same as the control node's user. o http_port: Specify a port if necessary. In real-world scenarios, inventory files can be more complex, reflecting the diverse and extensive infrastructure of organizations. You can explore more configurations here. Verifying Your Inventory After creating your inventory, it's a good practice to verify its correctness. This step ensures that the inventory is correctly structured and accessible to Ansible. To do this, use the following command: 1ansible-inventory -i inventory.yaml -list · - i: Indicates the inventory file you want to use. . - list: Requests the listing of the inventory content. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 5 N33 nirf 1750 Rank

--- Page 120 ---
Course Code/Title:CS3V15/Devops Unit:IV output: auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-inventory ·i inventory.yaml -- list "_meta": { "hostvars": { "vm01": "ansible_host": "192.168.0.208", "ansible_user"; "au" "children": [ "ungrouped" "ungrouped": { "hosts": "vn01 1 By following these steps, you have effectively set up and verified your inventory, ensuring a solid foundation for your Ansible automation. Using Ansible Ad-Hoc Commands Introduction: In Ansible, ad-hoc commands are your go-to solution for executing quick, one-off tasks on remote nodes. They provide a straightforward and efficient way to interact with managed nodes without the need for creating full-fledged playbooks. Ad-hoc commands are particularly useful when you need immediate results without the overhead of playbook development. When to Use Ad-Hoc Commands: Ad-hoc commands are best suited for scenarios where the task at hand is simple and doesn't require the complexity of a playbook. They are perfect for tasks like system health checks, package installation, service management, or any other single-operation job. Ad-Hoc Command Syntax: Ad-hoc commands follow a specific syntax that comprises various components, each serving a unique role: . Target Group: This is the group of hosts you intend to target with the ad-hoc command. · Module (-m): Specifies the module to execute. Modules are Ansible's building blocks for performing tasks, and they can range from basic operations like "ping" to more advanced tasks such as package management. · Inventory File (-i): Indicates the location of your inventory file, which defines the list of target devices. Example: Ping All Inventory Devices To illustrate the use of ad-hoc commands, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: 1ansible virtualmachines -m ping -i inventory.yaml virtualmachines is the target group. In this case, it could be any group, or you can use all to target all devices in your inventory. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 47 ---
Course Code/Title:CS3V15/Devops Unit: II <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> Any repository of maven is available in mvnrepository.com 5. Maven Plugins: The maven plugins are central part of maven framework, it is used to perform specific goal. According to Apache Maven, there are 2 types of maven plugins. 1. Build Plugins 2. Reporting Plugins Build Plugins These plugins are executed at the time of build. These plugins should be declared inside the <build> element. Reporting Plugins These plugins are executed at the time of site generation. These plugins should be declared inside the <reporting> element. Maven Core Plugins A list of maven core plugins are given below: Plugin Description clean clean up after build. compiler compiles java source code. deploy deploys the artifact to the remote repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY N32 nirf 6 175º Rank -

--- Page 48 ---
Course Code/Title:CS3V15/Devops Unit: II failsafe runs the JUnit integration tests in an isolated classloader. install installs the built artifact into the local repository. resources copies the resources to the output directory for including in the JAR. site generates a site for the current project. surefire runs the JUnit unit tests in an isolated classloader. verifier verifies the existence of certain conditions. It is useful for integration tests. Example for maven plugin: The compiler plugin is used to compile the source code of a Maven project. This plugin has two goals, which are already bound to specific phases of the default lifecycle: · compile - compile main source files · testCompile - compile test source files Here's the compiler plugin in the POM: <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.12.1</version> <configuration> <source>1.8</source> <target>1.8</target> </configuration> </plugin> 6. Maven Build Lifecycle: A Build Lifecycle is a well-defined sequence of phases, which define the order in which the goals are to be executed. Here phase represents a stage in life cycle. As an example, a typical Maven Build Lifecycle consists of the following sequence of phases. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 7 N33 nirf 175º Rank -

--- Page 133 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible Roles Roles provide a framework for fully independent or interdependent collections of files, tasks, templates, variables, and modules. The role is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks and makes them easier to reuse. The breaking of the playbook allows you to break the playbook into reusable components. Each role is limited to a particular functionality or desired output, with all the necessary steps to provide that result either within the same role itself or in other roles listed as dependencies. Roles are not playbooks. Roles are small functionality that can be used within the playbooks independently. Roles have no specific setting for which hosts the role will apply. Top-level playbooks are the bridge holding the hosts from your inventory file to roles that should be applied to those hosts. Creating a Role The directory structure for roles is essential to creating a new role, such as: Role Structure The roles have a structured layout on the file system. You can change the default structured of the roles as well. For example, let us stick to the default structure of the roles. Each role is a directory tree in itself. So the role name is the directory name within the /roles directory. $ ansible-galaxy -h Usage ansible-galaxy[delete|import|info|init|install|list|login|remove|search|setup][ -- help] [options] ... Options o -h: (help) it shows this help message and exit. o -v: (verbose) Verbose mode (-vvv for more, -vvvv to enable connection debugging). o -- version: it shows program version number and exit. Roles are stored in separate directories and have a particular directory structure CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 19 N33 nirf 1750 Rank

--- Page 134 ---
Course Code/Title:CS3V15/Devops Unit: IV Typical structure of ansible role myweb defaults L main.yml files · handlers L main.yml meta. L main.yml README.md tasks L_ main.yml templates tests inventory test.yml vars L main.yml · defaults : It stores the default variable for the role. For example the default port number is http is 8080, then it can be stored in defaults. · files : This folder contains the files required to transfer or deploy to the target machines. · handlers : It contains handlers, which may be used by some role. · meta : It defines some data or information about the role. · tasks : It contains the main list of tasks to be executed by the role. These tasks might be defined in separate files as per the functionalities. · templates : It contains the templates which can be deployed via a role. Ansible Module Ansible Command module is used to execute commands on a remote node. The Command module, is used mostly to run simple Linux commands on a remote node/server which is part of a host group or Stand alone server mentioned in the host group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 163 ---
‹ FabrikamFiber Runs Branches Analytics Description Edit Run pipeline : Stages #20210609.3 Update azure-pipelines.yml for Azure Pipelines Manually triggered for main 9 61bba8c x2 La Wednesday Ø 215 #20210609.2 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for 1º main 9 bc6c105 x La Wednesday ₾ 305 #20210609.1 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for main 9 53eb92b x • #20210520.1 Updated README.md Individual CI for & new-branch 0 1ada387 ‹ azure-cli-example go main V azure-cli-example / azure-pipelines.yml 30 pool: default 31 32 steps: 33 # Specify python version and install if needed Settings 34 - task: UsePythonVersion@0 35 condition: false 36 inputs: 37 -versionSpec: '3.x' 38 architecture: 'x64' 39 40 # Update pip to latest 41 - bash: python -m pip install -- upgrade pip 42 condition: false 43 displayName: 'Upgrade pip' 44 45 container 46 &continueOnError 47 48 88988 name & parameters & resources & schedules 49 50 51 services 52 strategy 53 & variables 54 & workspace La Wednesday .455 Là May 20 ₾ 135 Variables Run : Tasks 7 Search tasks dotnet .NET Core Build, test, package, or publish a dotnet applicatio. Android signing Sign and align Android APK files Ant Build with Apache Ant 80 App Center distribute Distribute app builds to testers and users via Visu .. App Center test Test app packages with Visual Studio App Center Archive files Compress files into .7z, tar.gz on zip ARM template deployment Deploy an Azure Resource Manager (ARM) templ .. Azure App Service deploy Deploy to Azure App Service a web, mobile, or AP. · The YAML pipeline editor provides several keyboard shortcuts, which we show in the following examples. . Choose Ctrl+Space for Intellisense support while you're editing the YAML pipeline.

--- Page 164 ---
· The task assistant provides a method for adding tasksto your YAML pipeline. · To display the task assistant, edit your YAML pipeline and choose Show assistant. Understand the azure-pipelines.yml file · A pipeline is defined using a YAML file in your repo. Usually, this file is named azure-pipelines.yml and is located at the root of your repo. · Navigate to the Pipelines page in Azure Pipelines, select the pipeline you created, and choose Edit in the context menu of the pipeline to open the YAMLeditor for the pipeline. · This pipeline runs whenever your team pushes a change to the main branch of your repo or creates a pull request. It runs on a Microsoft-hosted Linux machine. · The pipeline process has a single step, which is to run the Maven task. YAML:Code trigger: - main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdk Version: "1.10" jdk11_windows: imageName: "windows-latest" jdk Version: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdk VersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: " ** /TEST -*. xml"goals: "package" Change the platform to build · Navigate to the editor for your pipeline by selectingEdit pipeline action on the build, or by selecting Edit from the pipeline's main page. · To choose a different platform like Windows or Mac, change thevmImage value: · pool: · vmImage: "windows-latest" Add steps . You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can usetasks for building, testing, publishing, or deploying your app. For Java, the

--- Page 165 ---
Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. Customize CI triggers Pipeline triggers cause a pipeline to run. You can use trigger: to cause a pipeline to run whenever you push an update to a branch. YAML pipelines are configured by default with a CI trigger on your default branch (which is usually main). You can set up triggers for specific branches or for pull request validation. For a pull request validation trigger, just replace the trigger: step with pr: as shown in the two examples below. By default, the pipeline runs for each pull request change. . If you'd like to set up triggers, add either of the following snippets at the beginning of your azure-pipelines.yml file. YAML I Copy trigger: - main - releases/* YAML Copy pr: - main - releases/* You can specify the full name of the branch (for example, main) or a prefix-matching · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. Build using multiple versions Add steps You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can use tasks for building, testing, publishing, or deploying your app. For Java, the Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. YAML Copy - task: PublishCodeCoverageResults@1 inputs: codeCoverageTool: "JaCoco" summaryFileLocation: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco/ reportDirectory: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco" failIfCoverageEmpty: true · Select Save and then confirm the changes. . You can view your test and code coverage results by selecting your build and going to the Test and Coverage tabs.

--- Page 166 ---
Build using multiple versions Rename/move pipeline Name ScheduledTriggerTest 1234 Select folder 1 .. . Cancel Save Pipeline settings X Processing of new run requests Enabled Paused Disabled YAML file path azure-pipelines.yml Automatically link work items included in this run Cancel Save YAML trigger: main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdkVersion: "1.10" jdk11_windows: imageName: "windows-latest" jdkversion: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdkVersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: **** /TEST -*. xml" goals: "package"

--- Page 59 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Specify Archetype parameters M Group Id: com.simplileam Artifact Id: mavenproject Version: 0.0.1-SNAPSHOT Package: com,simplilearn.mavenproject . Properties available from archetype: Name Value Add ... Remove Advanced ? < Back Next > Cancel Finish The project is now created. · Open the pom.xml file You can see all the basic information that you have entered on the screen, such as the Artifact ID, Group ID, etc. You can see the junit dependencies have been added. This process takes place by default in Eclipse. There will also be some by default test cases. File Edit Source Navigate Search Project Run Window Help # Package Explorer E Jo JUnit - mavenproject/pom.xml 28 mavenproject > ( src/main/java ( src/test/java > = JRE System Library (|2SE-1:5] i Maven Dependencies target 7 <artifactIdaavenproject</artifactId> <version>0. 0. 1-SNAPSHOT</version>: <packaging>jar</packaging> <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 136 sproperties> 14 <project. build.sourceEncoding>UTF-8</project. build.sourceEncoding> </properties> 15 16 «dependenciess 18- <dependency> 19 <groupId>junit</grouptasI <artifactId=junit</artifactle> sversion>3.8.1</version> <scope>tests/scope> 21 22 223 </dependency> 24 </dependencies> 25 </project> Overview Dependencies Dependency Hierarchy |Effective POM pom.xml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 18 N32 nirf 175º Rank -

--- Page 60 ---
Course Code/Title:CS3V15/Devops Unit: II There you can find AppTest.java to be a default test case. When you click on that, you can see the test cases written in JUnit on your Eclipse screen. # Package Explorer 13 Ju JUnit mavenproject/pom.xml Z App.java AppTest.java 12 1 package com.simplilearn.mavenproject; - fl mavenproject + ( src/main/java - il com.simplileam.mavenproject U App.java = ( src/test/java il com.simplileam.mavenproject , E AppTest.java > > JRE System Library ||2SE-1.5] al Maven Dependencies - target pom.xml 3w import jumit. framework. Test:[] 6 76 / ** . Unit test for simple App. 10 public class AppTest extends TestCase 12 1 14 . Create the test case 15 16 * @param testName name of the test cole 17 18% public AppTest( String testNane ) 19 1 super( testName ); 28 21 1 22 :23% 24 * @return the suite of tests being tested 75 When it comes to adding more test cases, it will depend on the user, but these test cases and commands can easily be added in the workspace. If we try to remove certain dependencies from our file, we will receive error messages. To troubleshoot this, complete the following steps: · Go to another tab: mavenproject/pom.xml · Delete any dependencies · Save the file Immediately, there will be several error messages in the AppTest.java. R mavenproject/pom.xml App.java AppTest.java 83 1 package com.simplilearn.mavenproject; 2 3@ import junit. framework. Test;[] 6 mavenproject/src/test/java/com/simplileam/mavenproject/AppTest java 8 * Unit test for simple App. 9 18 public class AppTest extends TestCase 011 12 { 136 14 . Create the test case 15 16 * @param testName name of the test case 17 188 19 € public AppTest( String testName ) 28 super( testName ); 21 22 - 230 24 * @return the suite of tests being tested Problems E @ Javadoc ) Declaration History 7 errors, 1 warning, 0 others Description + @ Errors (7 items) a Test cannot be resolved to a type Resource Path Location AppTest.java /mavenproject/src/te line 26 Java Problem Type CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 19 N32 nirf 1750 Rank -

--- Page 145 ---
Course Code/Title: CS3V15/Devops Unit: V Step 7:Note the types of plans offered by GitHub. There are a few different plans to choose from, varying in the amount of features provided. Learn to ship software like a pro. GitHub gives students free access to the best developer tools so they can learn by doing. Free Get additional student benefits GitHub Pro Unlimited public/private repositories 2,000 CI/CD minutes/month Free for public repositories 500MB of Packages storage Free for public repositories Protect your branches Ensure that collaborators on your repository cannot make irrevocable chang branches. O Draft pull requests O 120 core-hours of Codespaces compute O Pages and Wikis 15GB of Codespaces storage O Community support ® 3,000 CI/CD minutes/month Free for public repositories 2GB of Packages storage Free for public repositories O 180 core-hours of Codespaces compute 20GB of Codespaces storage Web-based support wikiHow Step 8: Select the free plan. On the plan selection page, scrolldown to click the button for choosing a free plan. This will immediately take you to your GitHub dashboard. Continue for free wiki How UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 5 N33 In 175º Rank ---

--- Page 146 ---
Course Code/Title: CS3V15/Devops Unit: V · If you choose a paid plan, you'll have to enter yourpayment information as requested before you can continue. . If you want to upgrade your Github account in the future, click the menu at the top- right corner, select Settings, andchoose Billing and plans to view your options. GitHub essentials are: · Repositories · Branches · Commits · Pull Requests · Git (the version control software GitHub is built on) Repository: A GitHub repository can be used to store a developmentproject. It can contain folders and any type of files (HTML, CSS,Javascript, Documents, Data, Images). A GitHub repository should also include a licence file and a README file about the project. A GitHub repository can also be used to store ideas, or any resources that you want to share. Github Repository Creation Step 1: Click on the new repository option Marketplace Explore New repository and GitHub without any code! Catch Universe Join a community Universe in San Fr ticket pricing-he Import repository × New gist bird New organization New project llo World guide, you'll create a repository, start a , write comments, and open a pull request. GitHub Sponsors Matching Fund × Ready to support open source? GitHub will match your contribution to developers during their first year in GitHub Sponsors. d the guide Start a project × Welcome to the new dashboard. Get closer to the stuff you care about most. Explore repositories atlas-engineer/next Next browser - Be productive. UnENVIVAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 175° Rank 6

--- Page 31 ---
Course Code/Title:CS3V15/Devops Unit: I our work simpler. The pricing of Azure is also simpler and cost-effective. Popularly termed as "Pay As You Go", which means how much you use, pay only for that. Microsoft Azure Used for · Deployment Of applications: You can develop and deploy the application in the azure cloud by using the service called Azure App Service and Azure Functions after deploying the applications end users can access it. · Identity and Access Managment: The application and data which is deployed and stored in the Microsoft Azure can be secured with the help of Identity and Access Managment. It's commonly used for single sign-on, multi-factor authentication, and identity governance. · Data Storage and Databases: You can store the data in Microsoft azure in service like blob storage for unstructured data, table storage for NoSQL data, file storage, and Azure SQL Database for relational databases. The service can be scaled depending on the amount of data we are getting. · DevOps and Continuous Integration/Continuous Deployment (CI/CD): Azure DevOps will provide some tools like ncluding version control, build automation, release management, and application monitoring Following are some of the services Microsoft Azure offers: 1. Compute: Includes Virtual Machines, Virtual Machine Scale Sets, Functions for serverless computing, Batch for containerized batch workloads, Service Fabric for microservices and container orchestration, and Cloud Services for building cloud-based apps and APIs. 2. Networking: With Azure, you can use a variety of networking tools, like the Virtual Network, which can connect to on-premise data centers; Load Balancer; Application Gateway; VPN Gateway; Azure DNS for domain hosting, Content Delivery Network, Traffic Manager, ExpressRoute dedicated private network fiber connections; and Network Watcher monitoring and diagnostics 3. Storage: Includes Blob, Queue, File, and Disk Storage, as well as a Data Lake Store, Backup, and Site Recovery, among others. 4. Web + Mobile: Creating Web + Mobile applications is very easy as it includes several services for building and deploying applications. 5. Containers: Azure has a property that includes Container Service, which supports Kubernetes, DC/OS or Docker Swarm, and Container Registry, as well as tools for microservices. 6. Databases: Azure also included several SQL-based databases and related tools. 7. Data + Analytics: Azure has some big data tools like HDInsight for Hadoop Spark, R Server, HBase, and Storm clusters 8. AI + Cognitive Services: With Azure developing applications with artificial intelligence capabilities, like the Computer Vision API, Face API, Bing Web Search, Video Indexer, and Language Understanding Intelligent. 9. Internet of Things: Includes IoT Hub and IoT Edge services that can be combined with a variety of machine learning, analytics, and communications services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 31 N32 nirf 1750 Rank

--- Page 32 ---
Unit: I Course Code/Title:CS3V15/Devops 10. Security + Identity: Includes Security Center, Azure Active Directory, Key Vault, and Multi-Factor Authentication Services. 11. Developer Tools: Includes cloud development services like Visual Studio Team Services, Azure DevTest Labs, HockeyApp mobile app deployment and monitoring, Xamarin cross- platform mobile development, and more. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 32 N32 nirf 1750 Rank

--- Page 149 ---
Course Code/Title: CS3V15/Devops Unit: V Step 6: Now you will see that all of our files uploaded in our github. Namanbhatia7 Add initial resume website files Latest commit f47be5b 1 minute ago Display_Picture.png Add initial resume website files 1 minute ago My_Resume.html Add initial resume website files 1 minute ago README.md Initial commit 15 minutes ago Styles.css Add initial resume website files 1 minute ago Branch: · A GitHub branch is used to work with different versions of a repository at the same time. · By default a repository has a master branch (aproduction branch). · Any other branch is a copy of the master branch (as itwas at a point in time). · New Branches are for bug fixes and feature work separate from the master branch. When changes are ready, they can be merged into the master branch. If youmake changes to the master branch while working on a new branch, these updates can be pulled in. Commits: At GitHub, changes are called commits. Each commit (change) has a description explaining why achange was made. Pull Requests : . Pull Requests are the heart of GitHub collaboration. · With a pull request you are proposing that your changesshould be merged (pulled in) with the master. · Pull requests show content differences, changes, additions, and subtractions in colors (green and red). · As soon as you have a commit, you can open a pull request and start a discussion, even before the code isfinished. Git: · Git was created by Linus Torvalds in 2005 to develop Linux Kernel · Git is an open-source distributed version control system. It is designed to handle minor to major projectswith high speed and efficiency. . It is developed to co-ordinate the work among the developers. The version control allows us to track andwork together with our team members at the same workspace. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 9 175" Rank

--- Page 150 ---
CREATE AZURE ORGANIZATION AZURE: Microsoft Azure, often referred to as Azure cloud computing platform run by Microsoft. It offers access, management, and the development of applications and services through global data centers It also provides a range of capabilities, including software as a service (SaaS), platform as a service, and infrastructure as a service (IaaS). It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014 Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Prerequisites: To plan your organizational structure. Microsoft accounts or authenticate users with Microsoft Entra ID. For more information, see Choosing your organization administrator account type. Create an organization: 1. Sign in to Azure DevOps. 2. Select New organization 3. Confirm information, and then select Continue 10

--- Page 147 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: After clicking new repository option, we will have to initialize some things like, naming our project, choosing the visibility etc. After performing these steps click Create Repository button. Create a new repository A repository contains all project files, including the revision history. Already have a project repository elsewhere? Import a repository. Owner Repository name * / Namanbhatia7 - Resume This is going to be name of our project Great repository names are short and memorable. Need inspiration? How about animated-memory? Description (optional) Public Anyone Keep this as public selected ository. You choose who can commit. see this rer Private You choose who can see and commit to this repository. We can add a project description if we want. Skip this step if you're importing an existing repository. Initialize this repository with a README This will let you immediately clone the repository to your comp Tick the README option Add .gitignore: None - Add a license: None ~ Create repository After performing above steps, Click this button Step 3: After clicking the button, we will be directed to below page. Right now the only file we have is a readme file. Namanbhatia7 / Resume o Unwatch - 1 * Star 0 Y Fork 0 <> Code Issues 0 (*) Pull requests 0 ||| Projects 0 BB Wiki Security the Insights # Settings No description, website, or topics provided. Edit Manage topics T 1 commit įº 1 branch > 0 releases 2 1 contributor Branch: master New pull request Create new file Upload files Find File Clone or download TT Namanbhatia7 Initial commit Latest commit 676ac98 now README.md Initial commit now DO README.md Resume UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 7 175" Rank

--- Page 148 ---
Course Code/Title: CS3V15/Devops Unit: V Step 4: Now click on the "Upload files" button. 4 > C https://github.com/avicnotes/html-css-projects ... Search or jump to ... Click to End Session Total: $80.83 Pull requests Issues Marketplace Explore Google" Custom Search @ avicnotes / html-css-projects Unwatch - 1 Star 0 Fork 0 <> Code Issues 1'] Pull requests Actions Projects Wiki O Security ~ Insights ... go main - ¿º 1 branch 0 tags Go to file Add file - { Code About avicnotes screenshots Create new file Upload files 2 24 commits No description, website, or topics provided. README.md Update README.md 2 days ago Readme Screenshot (10).png screenshots 2 days ago Screenshot (11).png screenshots 2 days ago Releases Screenshot (12).png screenshots 2 days ago No releases published Create a new release 19 Screenshot (6).png screenshots 2 days ago accordion.JPG Add files via upload 4 days ago Packages background-dots.png Add files via upload 2 days ago clone-project-1.html Updated navbar items to my original ones 2 days ago index.html Create index.html 's ago No packages published Publish your first package Activate Windows Go to Settings to activate Windows, Environments 1 O Type here to search O W. 11:32 AM 2/1/2021 Step 5:Follow the steps mentioned and click "commitchanges". Drag additional files here to add them to your repository Or choose your files Display_Picture.png Simply drag or add files to be uploaded × My_Resume.html X Styles.css × Commit changes Add initial resume website files Add an optional ended des Give this particular version a name -o- Commit directly to the master branch. Ensure this option is checked !] Create a new branch for this commit and start a pull request. Learn more about pull requests. Commit changes Cancel UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 8

--- Page 55 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Environment Variables Now remove active profile from maven settings.xml and update the test profile mentioned in pom.xml. Add activation element to profile element as shown below. The test profile will trigger when the system property "env" is specified with the value "test". Create an environment variable "env" and set its value as "test". <profile> <id>test</id> <activation> <property> <name>env</name> <value>test</value> </property> </activation> </profile> Let's open command console, go to the folder containing pom.xml and execute the following mvn command. C:\MVN\project>mvn test Profile Activation via Operating System Activation element to include os detail as shown below. This test profile will trigger when the system is windows XP. <profile> <id>test</id> <activation> <os> <name>Windows XP</name> <family>Windows</family> <arch>x86</arch> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 14 N32 narf 1750 Rank -

--- Page 56 ---
Course Code/Title:CS3V15/Devops Unit: II <version>5.1.2600</version> </os> </activation> </profile> Now open command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test Profile Activation via Present/Missing File Now activation element to include OS details as shown below. The test profile will triggerwhen target/generated-sources/axistools/wsdl2java/com/companyname/group is missing. <profile> <id>test</id> <activation> <file> <missing>target/generated-sources/axistools/wsdl2java/ com/companyname/group</missing> </file> </activation> </profile> Now open the command console, go to the folder containing pom.xml and execute the following mvn commands. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test 8. Maven create and build artifacts: The first step is to open Eclipse, which comes with the integrated Maven environment. The Eclipse window opens on the screen. Complete the following steps: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 15 N32 nirf 175º Rank -

--- Page 85 ---
Course Code/Title:CS3V15/Devops Unit: III By following these steps, you can set up a Jenkins job to automate builds, integrate with your source code management system, and perform various actions based on build results. Introduction to Jenkins Plugins What Are Jenkins Plugins? Jenkins plugins are extensions that add extra functionality to Jenkins. They allow you to customize and extend Jenkins to better fit your specific needs. Plugins can provide integrations with other tools, add new features, and improve existing functionalities. Why Use Plugins? . Extend Functionality: Add features not available in the core Jenkins. . Integrate Tools: Seamlessly integrate with other tools and platforms (e.g., GitHub, Docker). . Improve Productivity: Automate more tasks and streamline your workflow. · Customize UI: Tailor the Jenkins interface to better suit your preferences and needs. How to Manage Plugins 1. Accessing the Plugin Manager 1. Go to Jenkins Dashboard: ○ Open your Jenkins dashboard. 2. Navigate to Plugin Manager: ○ Click on "Manage Jenkins". ○ Select "Manage Plugins". 2. Installing Plugins 1. Available Plugins: ○ Go to the "Available" tab to see a list of plugins that can be installed. 2. Search for Plugins: ○ Use the search bar to find specific plugins. 3. Select Plugins: ○ Check the box next to the plugins you want to install. 4. Install Plugins: ○ Click "Install without restart" or "Download now and install after restart". 3. Updating Plugins CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 narf 10 1750 Rank

--- Page 86 ---
Course Code/Title:CS3V15/Devops Unit: III 1. Go to Updates: ○ Navigate to the "Updates" tab to see plugins with available updates. 2. Select Plugins: ○ Check the box next to the plugins you want to update. 3. Update Plugins: ○ Click "Download now and install after restart". 4. Managing Installed Plugins 1. Installed Plugins: ○ Go to the "Installed" tab to see all installed plugins. 2. Uninstall Plugins: ○ Click the "Uninstall" button next to the plugin you want to remove. 3. Check Plugin Versions: ○ View the current version of each installed plugin. Adding Plugins to Jenkins Plugins are a crucial part of Jenkins, allowing you to extend its functionality to suit your project's specific needs. Whether you're integrating Jenkins with other tools, customizing your job configuration, or adding new build steps, plugins provide the flexibility you need. Here's a detailed guide on how to add plugins to Jenkins. 1. Why Plugins Are Important - Extending Jenkins Functionality: Plugins allow Jenkins to integrate with various tools and technologies, such as Git, Maven, Docker, and more. - Customizing Jobs: With plugins, you can add specific steps, triggers, and post-build actions to your jobs, making Jenkins adaptable to any workflow. - Automation: Plugins help automate processes like code quality checks, notifications, and deployments. 2. Accessing the Plugin Manager 1. Navigate to Jenkins Dashboard: Open your Jenkins dashboard in a web browser (e.g., http://localhost:8080). CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 175º Rank

--- Page 3 ---
Course Code/Title:CS3V15/Devops Unit: I Key Goals and Benefits of DevOps Goals of DevOps The fast-paced growth of the IT industry and continuous advancements in technology make it critical to set DevOps goals that are experimental and challenging for companies to compete and thrive in the market. Here are the key goals and principles that every successful DevOps program has in common. 1. Ensures effective collaboration between teams: Effective collaboration in any process relies on shared ownership. During the development process, all those involved should embrace the fact that everyone is equally responsible for the entire development process. Whether it is development, testing, or deployment, each team member should be involved. They should understand that they have an equal stake in the final outcome. In the DevOps paradigm, passing of work from one team to another is completely defined and broken down. This accelerates the entire process of development since collaboration between all the teams involved is streamlined. 2. Creates scalable infrastructure platforms: The primary focus of DevOps is to create a sustainable infrastructure for applications that make them highly scalable. According to the demands of the modern-day business world, scalable apps have become an absolute necessity. In an ideal situation, the process of scaling should be reliable and fully automated. As a result, the app will have the ability to adapt to any situation when a marketing effort goes viral. With the app being scalable, it can adjust itself to large traffic volumes and provide an immaculate user experience. 3. Builds on-demand release capabilities: Companies must focus on keeping their software in a 'releasable' state. Continuous delivery will allow the software to add new features and go live at any stage. DevOps aims to automate the process of release management because it has a plethora of advantages. Automated release management is predictable, fast, and very consistent. Moreover, through automation, companies can release new versions as per their requirements. Automated release management also has complete and thorough audit trials, as these are essential for compliance purposes. 4. Provides faster feedback: Automating monotonous tasks such as testing and reporting will accelerate the process of rapid feedback. Since the development team will know what has to change, it can roll out the updated version faster. In addition, the team can better understand the impact of the changes that it has done in the software lifecycle. A concrete understanding of changes will assist team members in working efficiently in tandem. With rapid feedback, the operations team and developers can make better decisions collectively and enhance the app's performance. Benefits of DevOps DevOps helps organizations deliver added value to their customers. Here are some compelling benefits of DevOps. 1. Smarter work and faster release: With DevOps, your development team can release the required deliverables quickly. Faster release of deliverables will keep you miles ahead of your competitors, which is very important in today's cut-throat business realm. Businesses should understand that if their review cycle is not automated, it will slow down the release process. Moreover, the inclusion of disparate tools will lead to context switching and higher costs. Thus, DevOps can help rectify this worrisome business situation. 2. Quick resolution of issues: In a business world where speed and accuracy are paramount, a fast feedback loop will help you thrive. With DevOps, the communication process becomes seamless, and, as such, it minimizes the time required to solve issues. Without open communication, key issues can slip out of mind, which will have serious repercussions in the long run. DevOps fosters open communication that helps resolve issues, thus unblocking the release pipeline faster. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 3 N32 nirf 1750 Rank

--- Page 4 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Better collaboration between teams: DevOps paves the way for more dynamic and round-the- clock communication between teams. It renders an environment for mutual collaboration and integration among teams that are distributed globally. Eliminating the traditional departmental barriers between teams forms a new sense of ownership, wherein each team member feels equally responsible for meeting delivery timelines. This collaboration contributes to happier and more engaged employees. 4. Fostering innovative mindsets: With DevOps, deployment phases of the application are more relaxed as compared to traditional methods. This is because it streamlines the entire process, ensures that there are no lapses in quality, and allows on-time and efficient release. Thus, as everything is in order, the development team is more at peace. This allows it to think out of the box and provide additional value to the user. Having a development team with an innovative mindset is a boon for any business organization. An innovative approach, in itself, has immense scope and leads to better quality and resolution of issues at hand. Thus, through DevOps, the process of expanding the horizon of an app becomes much easier. 5. Faster threat detection: Automated and continuous testing of the code will make the process of threat detection faster. As developers can locate problem areas at an early stage, they can then resolve them faster. Thus, DevOps is a vital cog in maintaining and enhancing the quality and performance of an app. As the overall build of the app is in capable hands, teams working together are empowered to share feedback as and when necessary. 6. Increased customer satisfaction: Customer satisfaction is paramount in any day and age, irrespective of the business one is involved in. DevOps is known for enhancing customer experience, which ultimately increases the level of customer satisfaction. Dissatisfied customers are never a good sign for any business. Feedback loops are an important component of DevOps. These loops empower end users to track the progress of app development at various stages. 7. In addition, they can suggest changes (if any) or give their inputs to make the app more customer- centric. Due to their dynamic nature, feedback loops help developers and customers remain on the same page. Moreover, DevOps accelerates the process of app development, which eventually lessens the delivery timer. This has a positive impact on the customer satisfaction ratio. 8. Providing the much-needed edge: Along with staying true to their development process, companies need to ensure that they sustain themselves in the cut-throat competition. Implementing DevOps can be your trump card to provide your organization with that much- needed edge. Competitive advantage is necessary, as it can become the deciding factor in the popularity of an application in many cases. Some factors set expert businesses apart from mediocre ones: · Top-quality features · Quicker and timely software releases · Maximizing return on investments · Listening to constructive feedback Difference between Agile and DevOps Agile: Agile program advancement comprises different approaches to computer program improvement beneath which prerequisites and arrangements advance through the collaborative exertion of self- organizing and cross-functional groups and their customer/end client. DevOps: DevOps could be a set of hones that combines program improvement and information- technology operations which points to abbreviating the framework's advancement life cycle and giving nonstop conveyance with tall program quality. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 4 N32 nirf 1750 Rank

--- Page 75 ---
Course Code/Title:CS3V15/Devops Customization This tool is highly customizable as it supports a variety of IDE's . Languages supported Project Configuration Based on Goal of time to get used to it. available. It supports software development in Java, C, C++, and Groovy. For declaring the project configuration, it does not use the XML files. On the phases of the fixed and linear model. Graph of task dependencies that do the work. To add functionality in the project is the main goal of the Gradle. To finish the project in the given timeline is the main goal of the Maven. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY Unit: II This tool serves a limited number of developers and is not that customizable. It supports software development in Java, Scala, C# , and Ruby and it does not natively support C and C+ but can support through plugins like "maven-native- plugin" or we can integrate other build systems like CMake or Makefile. For declaring the project configuration, it uses the XML files. 34 N33 nirf 1750 Rank

--- Page 76 ---
Course Code/Title:CS3V15/Devops Unit: III Unit - III CONTINUOUS INTEGRATION USING JENKINS Install & Configure Jenkins, Jenkins Architecture Overview, creating a Jenkins Job, configuring a Jenkins job, Introduction to Plugins, Adding Plugins to Jenkins, commonly used plugins (Git Plugin, Parameter Plugin, HTML Publisher, Copy Artifact and Extended choice parameters). Configuring Jenkins to work with java, Git and Maven, creating a Jenkins Build and Jenkins workspace. Installing and Configuring Jenkins Overview Jenkins is a widely-used open-source automation server that helps automate the parts of software development related to building, testing, and deploying, facilitating continuous integration and continuous delivery (CI/CD). Below are detailed steps and considerations for installing and configuring Jenkins. Installation 1. System Requirements: ○ Operating System: Jenkins can run on any major operating system. ○ Java: Jenkins requires Java Runtime Environment (JRE) 8 or later. 2. Download Jenkins: ○ Download the latest version of Jenkins from the official Jenkins website. 3. Installation Steps: ○ Windows: I Run the .msi installer and follow the installation wizard. ■ After installation, Jenkins will start automatically as a Windows service. ○ Linux: Add Jenkins repository and import the GPG key. I Use package manager to install Jenkins (e.g., apt-get install jenkins). I Start Jenkins using systemctl (systemctl start jenkins). 4. Initial Setup: ○ After installation, Jenkins can be accessed through a web browser at http : //< your _server _ip_or_domain>: 8080. o Unlock Jenkins by entering the initial admin password, which can be found in the jenkins_home directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 1 N32 nirf 1750 Rank

--- Page 63 ---
Course Code/Title:CS3V15/Devops Unit: II 10. Introduction of Gradle: The list of features that Gradle provides, · Gradle is available with separate Domain Specific Language (DSL) based on Groovy language. · It provides the declarative language elements. Those elements also provide build-by- convention support for Java, Groovy, OSGI, Web and Scala. Language for dependency based programming The declarative language lies on a top of a general purpose task graph, which can be fully supported in the build. Structure your build Gradle allows you to apply common design principles to your build. It will give you a perfect structure for build, so that, you can design well-structured and easily maintained, comprehensible build. Deep API By using this API, you can monitor and customize its configuration and execution behavior to the core. Gradle scales Gradle can easily increase the productivity, from simple and single project builds to huge enterprise multi-project builds. Multi-project builds Gradle supports the multi-project builds and partial builds. If you build a subproject, Gradle takes care of building all the subprojects, that the subproject depends on. Different ways to manage your builds Gradle supports different strategies to manage your dependencies. Gradle is the first build integration tool Gradle is fully supported for your ANT tasks, Maven and lvy repository infrastructure for publishing and retrieving dependencies. It also provides a converter for turning a Maven pom.xml to Gradle script. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 22 N32 nirf 175º Rank -

--- Page 64 ---
Course Code/Title:CS3V15/Devops Unit: II Ease of migration Gradle can easily adapt to any structure. Therefore, you can always develop your Gradle build in the same branch, where you can build live script. Gradle Wrapper Gradle Wrapper allows you to execute the Gradle builds on machines, where Gradle is not installed. This is useful for continuous integration of servers. Free open source Gradle is an open source project, and licensed under the Apache Software License (ASL). Groovy Gradle's build script are written in Groovy programming language. The whole design of Gradle is oriented towards being used as a language and not as a rigid framework. Groovy allows you to write your own script with some abstractions. The whole Gradle API is fully designed in Groovy language. Installation of Gradle: Prerequisites to install Gradle JDK and Groovy are the prerequisites for Gradle installation. Gradle requires JDK version 6 or later to be installed in the system. It uses the JDK libraries which are installed, and sets to the JAVA_HOME environmental variable. Gradle carries its own Groovy library, therefore, we need not install Groovy explicitly. If it is installed, that is ignored by Gradle. The steps to install Gradle in your system are explained below. Step 1 - Verify JAVA Installation First of all, you need to have Java Software Development Kit (SDK) installed on your system. To verify this, execute Java -version command in any of the platform you are working on. In Windows Execute the following command to verify Java installation. I have installed JDK 1.8 in my system. C:\> java - version CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE . TECHNOLOGY 23 N32 narf 1750 Rank

--- Page 57 ---
Course Code/Title:CS3V15/Devops · Go to the File option · In the drop-down menu, select New · Select the Project option Unit: II If you want to create a Java project, you can select the "Java Project" option. Since we are not creating a Java project specifically, we have chosen the "Project" option. eclipse-workspace - Eclipse File Edit Source Refactor Navigate Search Project Run Window Help New Open File ... G) Open Projects from File System ... Shift+Alt+N Close Close All Shift + Cb1+W Save Save As. Save All Shift+Ctrl+S Revert Move .:. Rename 2 Refresh ES Convert Line Delimiters To Print Import ... Export ... Ctrl+P de java Project ui Project ... El Package G Class Interface G Enum @ Annotation 69 Source Folder 18 Java Working Set Folder File Untitled Text File Task JUnit Test Case Example ... Other ... Ctrl+N The dialog box that appears on the screen will display different types of projects. · Select the Maven Project option . Click on Next New Project + Select a wizard Create a Maven Project Wizards: type filter text irroject Gradle Java 2º Java Project #Java Project from Existing Ant Buildfile - @Maven Lu Check out Maven Projects from SCM M& Maven Module Maven Project Examples ? < Back Next > X Cancel Finish A dialog box will appear. Select the default workspace. . Click on "Next" Several Group IDs, Artifact IDs, and Versions will then appear. · Select a plugin there and click on "Next" CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 16 N33 narf 1750 Rank

--- Page 58 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Select an Archetype M Catalog: All Catalogs Configure .. Biter: × Group Id Artifact id Version org.apache.maven.archetypes maven-archetype-archetype 1.0 org.apache.maven.archetypes maven-archetype-|2ee-simple 1.0 org.apache.maven archetypes maven-archetype-plugin 1.2 org.apache.maven.archetypes maven-archetype-plugin-site 1.1 org.apache.maven.archetypes maven-archetype-portlet 1.0.1 org.apache.maven.archetypes maven-archetype-profiles 1.0-alpha-4 org.apache.maven.archetypes maven-archetype-quickstart 1.1 org.apache.maven.archetypes maven-archetype-site 11 org anacha mauan archabenes masan.archchina.cito.cimnia 11 An archetype which contains a sample Maven project. Show the last version of Archetype only Include snapshot archetypes Add Archetype ... > Advanced ? < Back Next > Cancel Finish In the next dialog box that appears, you'll complete the following steps: · Enter the Group ID “com.xyz" · Enter the Artifact ID "mavenproject" · The version will appear on the screen These items can all be modified at a later time if needed. · Click on "Finish" CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 17 N33 nirf 1750 Rank

--- Page 53 ---
Course Code/Title:CS3V15/Devops Unit: II <execution> <phase>test</phase> <goals> <goal>run</goal> </goals> <configuration> <tasks> <echo>Using env.test.properties</echo> <copy file="src/main/resources/env.test.properties" tofile="${project.build.outputDirectory}/env.properties"/> </tasks> </configuration> </execution> </executions> </plugin> </plugins> </build> </profile> </profiles> </project> Now open the command console, go to the folder containing pom.xml and execute the following mvn command. Pass the profile name as argument using -P option. C:\MVN\project>mvn test -Ptest Maven will start processing and displaying the result of test build profile. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 12 N32 nirf 1750 Rank

--- Page 54 ---
Course Code/Title:CS3V15/Devops Unit: II Profile Activation via Maven Settings Open Maven settings.xml file available in %USER_HOME%/.m2 directory where %USER_HOME% represents the user home directory. If settings.xml file is not there, then create a new one. Add test profile as an active profile using active Profiles node as shown below in example. <settings xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd"> <mirrors> <mirror> <id>maven.dev.snaponglobal.com</id> <name>Internal Artifactory Maven repository</name> <url>http://repo1.maven.org/maven2/</url> <mirrorOf> *< /mirrorOf> /mirror </mirrors> <activeProfiles> <activeProfile>test</activeProfile> </activeProfiles> </settings> Now open command console, go to the folder containing pom.xml and execute the following mvn command. Do not pass the profile name using -P option. Maven will display result of test profile being an active profile. C:\MVN\project>mvn test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 13 N33 nirf 1750 Rank

--- Page 65 ---
Course Code/Title:CS3V15/Devops Unit: II Output The output is as follows - java version "1.8.0 66" Java(TM) SE Runtime Environment (build 1.8.0_66-b18) Java HotSpot(TM) 64-Bit Server VM (build 25.66-b18, mixed mode) Step 2 - Download Gradle Build File Download the latest https://gradle.org/install/. version of Gradle from the link available at Step 3 - Set Up Environment for Gradle Setting up environment means, we have to extract the distribution file and copy the library files into proper location. Set up GRADLE_HOME and PATH environmental variables. This step is platform dependent. In Windows Extract the downloaded zip file named gradle-2.11-all.zip and copy the distribution files from Downloads\gradle-2.11\ to C:\gradlel location. After that, add the C: \gradle and C: \gradlelbin directories to the GRADLE_HOME and PATH system variables. Follow the given instructions - Right Click On My Computers -> Click On Properties -> Advanced System Settings -> Click On Environmental Variables. There you will find a dialog box for creating and editing system variables. Click on new button for creating GRADLE_HOME variable (follow the left side screenshot). Click on Edit for editing the existing Path system variable (follow the right side screenshot). Follow the below given screenshots. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 24 N33 nirf 1750 Rank

--- Page 66 ---
Course Code/Title:CS3V15/Devops System Properties X Computer Name Hardware Advanced System Protection Remote Environment Variables x Unit: II System Properties × Computer Name |Hardware Advanced System Protection Remote Environment Variables × New System Variable × Variable name: GRADLE_HOME| Variable value: C:\grade Cancel System variables Variable Value ComSpec C:\Windows\system32\and.exe FP_NO_HOST_C ... NO GRADLE_HOME E:\work\22- Orientdb \gradle-2.11 Variable name: Path Variable value: #C:\gradle bin - System variables Variable Value Edit System Variable × OK Cancel < ORIENTDB_HOME E:\work\22- Orientdblorientdb-communi ... " JAVA_HOME C:\Program Files\Java\jdk1.7.0_60 New ... Edt ... Delete OK Cancel Step 4 - Verify the Gradle installation In windows os Windows_NT Path C:\oraclexe \app \prade\product\11.2.0\, .. PATHEXT .COM ;. EXE ;. BAT ;. CMD ;. VBS ;. VBE ;. JS ;.... v New ... Edit ... Delete OK Cancel You can execute the following command in command prompt. C:\> gradle -v Output Here you will find the Gradle version. Gradle 2.11 Build time: 2016-02-08 07:59:16 UTC Build number: none Revision: 584db1c7c90bdd1de1d1c4c51271c665bfcba978 Groovy: 2.4.4 CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 25 N33 nirf 1750 Rank

--- Page 73 ---
Course Code/Title:CS3V15/Devops Unit: II init - Initializes a new Gradle build. wrapper - Generates Gradle wrapper files. Help tasks Build Environment - Displays all build script dependencies declared in root project 'eclipse-workspace'. components - Displays the components produced by root project 'eclipse-workspace '. [incubating] dependencies - Displays all dependencies declared in root project 'eclipse-works pace'. Dependency Insight - Displays the insight into a specific dependency in root project 'eclipse-workspace.' Dependent Components : It displays the dependent components of components in the root project 'eclipse-workspace.' [incubating] help - Displays a help message. model - Displays the configuration model of root project 'eclipse-workspace.' [incubating] projects - Displays the sub-projects of root project 'eclipse-workspace.' properties - Displays the properties of root project 'eclipse-workspace.' tasks - Displays the tasks runnable from root project 'eclipse-workspace.' To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> To list all the tasks of the project, run the below command: 1. gradle tasks -all CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 32 N33 nirf 1750 Rank

--- Page 74 ---
Course Code/Title:CS3V15/Devops Unit: II To display more details about a task, run the below command: 1. gradle help -- task Listing Dependencies In Gradle, we can list the dependencies which are broken down by the configuration. To list the dependencies, run the below command: 1. gradle -q dependencies Difference between Gradle and Maven Basis Based on Gradle is based on developing Maven is based on developing pure Java language-based software. Configuration It uses a Groovy-based Domain-specific language (DSL) for creating project structure. Focuses on Performance It performs better than maven as it optimized for tracking only current running task. It is necessary to compile. Java Compilation Usability It is a new tool , which requires users to spend a lot This tool is a known tool for many users and is easily Gradle domain-specific language projects. It uses Extensible Markup Language (XML) for creating project structure. Developing applications by adding new features to them . It avoids compilation. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] Maven Developing applications in a given time limit. It does not create local temporary files during software creation and is hence - slower . 33 N33 nirf 1750 Rank -

--- Page 7 ---
Course Code/Title:CS3V15/Devops Unit: I DevOps Tools 1. Git (GitLab, GitHub, Bitbucket) Git remains indispensable in software development and DevOps due to its pivotal role in version control, collaborative coding, and efficient project management. As technology has accelerated, the need for streamlined and organized code management has never been greater. Git empowers developers to collaborate on codebases, effortlessly creating and merging branches for new features and bug fixes. Its distributed nature ensures developers can work seamlessly offline, an increasingly valuable feature in today's remote and distributed work environments. Additionally, Git facilitates the tracking of code modifications, making it easier to identify when and why specific changes were made, a critical aspect of maintaining code quality and security. Software development is essential in driving innovation and advancing progress, and Git maintains its prominent position as the bedrock of efficient, cooperative, and secure coding methodologies. 2. Maven Due to its enduring significance in managing project dependencies, building, and project lifecycle management, Maven remains a pivotal tool in SD and DevOps. As a robust build automation and project management tool, Maven simplifies the complexities of Java-based project development by streamlining the compilation, testing, packaging, and distribution processes. It ensures consistent and reproducible builds, making it easier for development teams to collaborate efficiently and deliver high-quality software. Maven's role in managing dependencies and facilitating continuous integration and deployment remains crucial. Its ability to handle complex build scenarios and integrate seamlessly with modern DevOps practices makes it indispensable for ensuring software projects' reliability, maintainability, and scalability in 2024 and beyond. 3. Jenkins Its importance lies in its role as a powerful automation server that enables continuous integration and continuous delivery (CI/CD) pipelines. Jenkins streamlines software development by automating tasks such as building, testing, and deploying code changes, ensuring that software is delivered quickly and highly. With the growing complexity of modern applications, the need for efficient CI/CD processes has become even more paramount. Jenkins provides flexibility, extensibility, and a vast library of plugins that cater to a wide range of technologies and tools, making it adaptable to diverse development environments. As organizations prioritize speed, reliability, and collaboration in their software development practices, Jenkins stands as a cornerstone tool, enabling teams to achieve seamless automation and efficient delivery of software solutions. 4. Chef Chef, a powerful automation platform, is crucial in managing infrastructure as code. Chef empowers organizations to achieve scalability, reliability, and speed seamlessly. By allowing the automation of server provisioning, configuration, and maintenance, Chef enhances efficiency and consistency across the entire infrastructure, reducing manual errors and ensuring that infrastructure remains desired. Moreover, Chef integrates smoothly with various cloud providers, containerization technologies, and other DevOps tools, making it adaptable to the ever-evolving tech TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 7 N32 nirf 1750 Rank

--- Page 8 ---
Course Code/Title:CS3V15/Devops Unit: I landscape. As organizations prioritize agility and scalability, Chef remains a vital tool in automating complex infrastructure tasks and enabling DevOps teams to focus on innovation and delivery. 5. Puppet Puppet is essential because it simplifies the management and orchestration of complex IT infrastructures by allowing administrators to define infrastructure as code. It ensures consistency and repeatability in configuration across servers, cloud instances, and containers. Businesses increasingly rely on diverse, dynamic, and hybrid infrastructures. Puppet's importance lies in its ability to streamline provisioning, configuration, and continuous compliance, thus reducing operational complexity, minimizing errors, and accelerating software delivery. Puppet continues to empower organizations to efficiently manage and scale their infrastructure while maintaining high levels of security and compliance, making it a crucial tool for DevOps teams. 6. Ansible Ansible is a powerful and widely adopted automation and configuration management tool important in 2024 for several reasons. This tool stands out for its simplicity and versatility. It empowers organizations to automate repetitive tasks, provisioning of infrastructure, and configuration management across diverse environments, making it an invaluable asset for DevOps and IT teams. Furthermore, Ansible's agentless architecture, declarative language, and a vast library of pre- built modules make it accessible to both beginners and seasoned professionals. As organizations prioritize efficiency, scalability, and the rapid deployment of applications and services, Ansible remains an indispensable DevOps toolkit, helping teams streamline operations, enhance security, and maintain infrastructure at scale, all while reducing manual errors and increasing agility in a fast-paced technological landscape. 7. Docker Docker is crucial in modern software development and DevOps practices. It can simplify and streamline the management of applications across various environments. Docker containers encapsulate an app and its dependencies, ensuring consistent and reproducible deployments from development to production. This technology enhances portability and scalability, accelerates development cycles, and reduces the "it works on my machine" problem. In a rapidly evolving software landscape, Docker's containerization approach remains crucial for achieving efficient, isolated, and highly flexible application deployment, making it an essential component of DevOps and continuous delivery pipelines. 8. Kubernetes Kubernetes, often abbreviated as K8s, play a central role in modern software development and operations. Its importance lies in its ability to orchestrate, manage, and automate containerized applications at scale. As organizations increasingly embrace microservices architectures and containerization for their applications, Kubernetes provides the essential infrastructure for deploying, scaling, and maintaining these containers efficiently. The tool's resilience, self-healing capabilities, and support for hybrid and multi-cloud environments make it vital for achieving agility, reliability, and cost-effectiveness in application deployment. It serves as the backbone of cloud-native ecosystems, enabling organizations to accelerate software delivery, improve resource utilization, and respond effectively to the evolving demands of the digital landscape. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N32 nirf 1750 Rank

--- Page 69 ---
Unit: II Course Code/Title:CS3V15/Devops 1 build.gradle gradle wrapper 2 gradle-wrapper.jar - gradle-wrapper.properties 3 gradlew 4 gradlew.bat 5 settings.gradle 6 It is the default structure of a Gradle project. Gradle will generate the following things for us: 1. The gradle file is build script for configuring the current project. 2. An executable JAR file is used as a Gradle wrapper. 3. Configuration properties for Gradle Wrapper. 4. The gradlew is a Gradle wrapper script for UNIX based OS. 5. The bat is the Gradle Wrapper script for Windows. 6. The settings script for configuring the Gradle build. Step3: Create a task Gradle supports APIs for creating and managing tasks through a Groovy-based DSL or Kotlin-based DSL. Every project contains a collection of tasks for some basic operation. Gradle supports a library of tasks that configure the project. For example, there is a Copy task, which copies files from one location to another. The Copy task is one of the most used tasks In Gradle. To use the Copy task in build script, follow the below process. Step1: Create a directory called src C: \Users \HiMaNshU\demo>mkdir src C: \Users \HiMaNshU\demo>cd src Step2: Add a file called myfile.txt in the src directory. Add the single line "Hello, World!" to it, also, we can leave it empty. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 28 N32 nirf 1750 Rank

--- Page 70 ---
Unit: II Course Code/Title:CS3V15/Devops C:\Users \HiMaNshU\demo\src>echo myfile.txt myfile.txt Define a task called Copy in build.gradle file. It will copy the src directory to a new directory called dest. We don't have to create the dest directory; the Copy task will do it for us. 1. task copy(type: Copy, group: "Custom", description: "The sources are copied to dest directory") { 2. from "src" 3. into "dest" 4. } We can provide anything in a group and description. Also, we can omit them, but doing so will also be omitted from the report of the task used later. Now execute our new copy task: C: \Users \HiMaNshU\demo>gradle copy BUILD SUCCESSFUL in 2s 1 actionable task: 1 executed C: \Users \HiMaNshU\demo>_ After the successful execution of the task, we will get BUILD SUCCESSFUL message. The build.gradle file The build.gradle file is build script of a Gradle project. All the tasks and plugins are defined in this file. When we run a gradle command, it looks for a file called build.gradle in the current directory. Although we have called it a build script, strictly, it is a build configuration script. The build script defines a project and its tasks. The default build.gradle file looks like as follows: CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 29 N33 nirf 1750 Rank

--- Page 37 ---
Course Code/Title:CS3V15/Devops Unit: I GIT INSTALLATION Git for Windows stand-alone installer · Download the latest Git for Windows installer. · When you've successfully started the installer, you should see the Git Setup wizard screen. Follow the Next and Finish prompts to complete the installation. The default options are pretty sensible for most users. · Open a Command Prompt (or Git Bash if during installation you elected not to use Git from the Windows Command Prompt). . Run the following commands to configure your Git username and email using the following commands, replacing Emma's name with your own. These details will be associated with any commits that you create: $ git config -- global user.name "CIT_CHENNAI" $ git config -- global user.email CITCHENNAI@atlassian.com • Optional: Install the Git credential helper on Windows Bitbucket supports pushing and pulling over HTTP to your remote Git repositories on Bitbucket. Every time you interact with the remote repository, you must supply a username/password combination. You can store these credentials, instead of supplying the combination every time, with the Git Credential Manager for Windows. BASIC COMMANDS OF GIT: Some basic Git commands along with their syntax and examples: · Initialize a Repository: Syntax: `git init' Example: `git init' · Clone a Repository: Syntax: `git clone <repository_url> Example: `git clone https://github.com/example/repository.git' · Check Repository Status: Syntax: `git status' Example: `git status' · Add Changes to Staging Area: Syntax: `git add <file(s)>' Example: `git add file.txt' · Commit Changes: Syntax:`git commit -m "Commit message" Example:`git commit -m "Add new feature" · Create a New Branch: Syntax: `git branch <branch_name>' Example: `git CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY (Autonomous] N33 37 nirf 1750 Rank

--- Page 38 ---
Course Code/Title:CS3V15/Devops branch feature-branch' Unit: I • Switch to a Branch: Syntax: `git checkout <branch_name>' Example:`git checkout feature-branch' OR Syntax: `git switch <branch_name>' (Git version 2.23 and later) Example: `git switch feature-branch' · Create and Switch to a New Branch: Syntax: `git checkout -b <new_branch name>' Example: `git checkout -b new-feature' OR Syntax: `git switch -c <new_branch_name>' (Git version 2.23 and later) Example: `git switch -c new-feature' . Merge Changes from One Branch to Another: Syntax: `git merge <branch_name>' Example: `git merge feature-branch' · View the Commit History: Syntax: `git log' Example: `git log' · Push Changes to a Remote Repository: Syntax: `git push <remote_name> <branch_name>' Example:`git push origin master' · Pull Changes from a Remote Repository: Syntax: `git pull <remote_name> <branch_name>' Example:`git pull origin master' . Show the Differences Between Working Directory and Staging Area: Syntax: `git diff Example: `git diff · Show the Differences Between Staging Area and Last Commit: Syntax: `git diff -- cached' Example: `git diff -- cached' . Show the Differences Between Working Directory and Last Commit: Syntax: `git diff HEAD' Example: `git diff HEAD' CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 38 N32 nirf 1750 Rank

--- Page 59 ---
Course Code/Title:CS3V15/Devops Unit: II New Maven Project New Maven project Specify Archetype parameters M Group Id: com.simplileam Artifact Id: mavenproject Version: 0.0.1-SNAPSHOT Package: com,simplilearn.mavenproject . Properties available from archetype: Name Value Add ... Remove Advanced ? < Back Next > Cancel Finish The project is now created. · Open the pom.xml file You can see all the basic information that you have entered on the screen, such as the Artifact ID, Group ID, etc. You can see the junit dependencies have been added. This process takes place by default in Eclipse. There will also be some by default test cases. File Edit Source Navigate Search Project Run Window Help # Package Explorer E Jo JUnit - mavenproject/pom.xml 28 mavenproject > ( src/main/java ( src/test/java > = JRE System Library (|2SE-1:5] i Maven Dependencies target 7 <artifactIdaavenproject</artifactId> <version>0. 0. 1-SNAPSHOT</version>: <packaging>jar</packaging> <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 136 sproperties> 14 <project. build.sourceEncoding>UTF-8</project. build.sourceEncoding> </properties> 15 16 «dependenciess 18- <dependency> 19 <groupId>junit</grouptasI <artifactId=junit</artifactle> sversion>3.8.1</version> <scope>tests/scope> 21 22 223 </dependency> 24 </dependencies> 25 </project> Overview Dependencies Dependency Hierarchy |Effective POM pom.xml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 18 N32 nirf 175º Rank -

--- Page 60 ---
Course Code/Title:CS3V15/Devops Unit: II There you can find AppTest.java to be a default test case. When you click on that, you can see the test cases written in JUnit on your Eclipse screen. # Package Explorer 13 Ju JUnit mavenproject/pom.xml Z App.java AppTest.java 12 1 package com.simplilearn.mavenproject; - fl mavenproject + ( src/main/java - il com.simplileam.mavenproject U App.java = ( src/test/java il com.simplileam.mavenproject , E AppTest.java > > JRE System Library ||2SE-1.5] al Maven Dependencies - target pom.xml 3w import jumit. framework. Test:[] 6 76 / ** . Unit test for simple App. 10 public class AppTest extends TestCase 12 1 14 . Create the test case 15 16 * @param testName name of the test cole 17 18% public AppTest( String testNane ) 19 1 super( testName ); 28 21 1 22 :23% 24 * @return the suite of tests being tested 75 When it comes to adding more test cases, it will depend on the user, but these test cases and commands can easily be added in the workspace. If we try to remove certain dependencies from our file, we will receive error messages. To troubleshoot this, complete the following steps: · Go to another tab: mavenproject/pom.xml · Delete any dependencies · Save the file Immediately, there will be several error messages in the AppTest.java. R mavenproject/pom.xml App.java AppTest.java 83 1 package com.simplilearn.mavenproject; 2 3@ import junit. framework. Test;[] 6 mavenproject/src/test/java/com/simplileam/mavenproject/AppTest java 8 * Unit test for simple App. 9 18 public class AppTest extends TestCase 011 12 { 136 14 . Create the test case 15 16 * @param testName name of the test case 17 188 19 € public AppTest( String testName ) 28 super( testName ); 21 22 - 230 24 * @return the suite of tests being tested Problems E @ Javadoc ) Declaration History 7 errors, 1 warning, 0 others Description + @ Errors (7 items) a Test cannot be resolved to a type Resource Path Location AppTest.java /mavenproject/src/te line 26 Java Problem Type CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 19 N32 nirf 1750 Rank -

--- Page 87 ---
Course Code/Title:CS3V15/Devops Unit: III 2. Go to Plugin Manager: From the dashboard, click on 'Manage Jenkins' on the left-hand side menu. On the Manage Jenkins page, click on 'Manage Plugins'. This will take you to the Plugin Manager, where you can view, install, and update plugins. 3. Exploring Available Plugins 1. Available Tab: The 'Available' tab lists all plugins that are available for installation. This includes thousands of plugins categorized by functionality (e.g., SCM, build tools, user interface enhancements). 2. Search for Plugins: Use the search box to quickly find the plugin you need. For example, if you're looking to integrate Jenkins with Git, type 'Git' in the search box. 3. Popular Plugins: Jenkins highlights popular plugins at the top of the Available tab. These are commonly used plugins that most users find essential. 4. Installing Plugins 1. Selecting Plugins to Install: Check the box next to each plugin you want to install. You can select multiple plugins at once. 2. Install Without Restart: After selecting your plugins, scroll down and click 'Install without restart'. Jenkins will install the plugins immediately, and you can continue using Jenkins during the installation. 3. Install After Restart: If you prefer, you can choose to 'Download now and install after restart'. This will install the plugins after Jenkins is restarted, ensuring that all changes are applied without interference. 4. Installation Progress: The installation process will show a progress bar for each plugin. Once a plugin is installed, it will move to the 'Installed' tab. 5. Configuring Installed Plugins 1. Plugin Configuration: After installation, some plugins may require additional configuration. Navigate to 'Manage Jenkins' > 'Configure System' or 'Configure Global Security' to adjust settings for your new plugins. 2. Global Tool Configuration: For build tools like Maven or JDKs installed via plugins, you can configure them under 'Global Tool Configuration'. Here, you define the paths to the tools or specify installation options. 6. Updating and Managing Plugins 1. Updating Plugins: Regularly update your plugins to ensure compatibility with the latest version of Jenkins and to benefit from new features or security patches. Go to the 'Updates' tab in the Plugin CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) N32 nirf 12 1750 Rank

--- Page 88 ---
Unit: III Course Code/Title:CS3V15/Devops Manager. Jenkins will show you all plugins with available updates. Click 'Update' to install the latest versions. 2. Uninstalling Plugins: If a plugin is no longer needed, you can uninstall it from the 'Installed' tab. Select the plugin and click 'Uninstall'. Uninstallation typically requires a Jenkins restart to complete. 7. Commonly Used Plugins Here are a few essential plugins that you might consider adding to your Jenkins setup: 1. Git Plugin: Integrates Jenkins with Git, allowing you to pull code from GitHub, Bitbucket, or other Git repositories. 2. Pipeline Plugin: Enables the creation and management of Jenkins pipelines, a powerful way to define complex build, test, and deployment processes. 3. Maven Integration Plugin: Adds support for Maven projects, allowing you to build and manage Maven-based projects directly within Jenkins. 4. Email Extension Plugin: Provides advanced email notification options for build statuses, including customizable email templates. 5. Blue Ocean: An alternative user interface for Jenkins that simplifies pipeline creation and offers a modern, user-friendly design. 8. Troubleshooting Plugin Issues 1. Compatibility Issues: Sometimes, plugins might conflict with each other or with the Jenkins version. In such cases, refer to the plugin's documentation or Jenkins logs to diagnose the issue. 2. Rollback: If a plugin update causes problems, you can rollback to a previous version from the 'Installed' tab by selecting the desired version. 3. Restart Jenkins: Some plugin changes require a Jenkins restart. Ensure all jobs are completed before restarting to avoid any interruptions. Commonly Used Jenkins Plugins 1. Git Plugin Overview: The Git Plugin integrates Jenkins with Git repositories, allowing Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nårf 1750 Rank

--- Page 27 ---
Course Code/Title:CS3V15/Devops Unit: I · Servers are maintained easily and there is nearly zero downtime · Users can access cloud data and upload it on the cloud from any device with a solid internet connection · Cloud environments can be modified according to the user's requirements and is easily accessible · Clouds are highly secure, making data breaches more unlikely · Migrating to the cloud eliminates the need to buy on-premises infrastructure . It offers pay-as-you-go pricing, meaning you only pay for the resources you use 1. Infrastructure as a Service: IaaS delivers virtualized computing resources over the Internet. Users can rent virtual machines, storage, and networking infrastructure, allowing for easy scalability without investing in physical hardware. Examples include AWS EC2 and Azure Virtual Machines. 2. Platform as a Service: PaaS offers a robust platform for developers to build, deploy, and manage apps without worrying about the underlying infrastructure. It simplifies application development and deployment, with services like Google App Engine and Heroku leading the way. 3. Software as a Service: SaaS offers software applications on a subscription basis, accessible via a web browser. Users don't need to install or maintain software locally, making it ideal for collaboration tools (e.g., Microsoft 365, Google Workspace) and CRM systems (e.g., Salesforce). 4. Function as a Service: FaaS allows developers to execute code responding to events without managing servers. It's highly scalable and cost-efficient, exemplified by AWS Lambda and Azure Functions. FaaS is also known as serverless computing. 5. Container as a Service: CaaS enables the deployment and management of containerized applications using orchestration tools like Kubernetes. It provides portability and scalability for applications across different cloud environments. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 27 N33 nif 1750 Rank

--- Page 28 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between main cloud computing services Terms Stands for Uses Access Model Technical understanding. Popularity Percentage rise Usage Cloud services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY IAAS Infrastructure as a service. IAAS is used by network architects. IAAS gives access to the resources like virtual machines and virtual storage. It is a service model that provides virtualized computing resources over the internet. It requires technical knowledge. It is popular among developers and researchers. It has around a 12% increment. Used by the skilled developer to develop unique applications. Amazon Web Services, sun, vCloud Express. PAAS Platform as a service. PAAS is used by developers. PAAS gives access to run time environment to deployment and development tools for application. It is a cloud computing model that delivers tools that are used for the development of applications. Some knowledge is required for the basic setup. It is popular among developers who focus on the development of apps and scripts. It has around 32% increment. Used by mid-level developers to build applications. Facebook, and Google search engine. SAAS Software as a service. SAAS is used by the end user. SAAS gives access to the end user. It is a service model in cloud computing that hosts software to make it available to clients. There is no requirement about technicalities company handles everything. It is popular among consumers and companies, such as file sharing, email, and networking. It has about a 27 % rise in the cloud computing model. Used among the users of entertainment. MS Office web, Facebook and Google Apps. 28 N32 nirf 1750 Rank

--- Page 23 ---
Course Code/Title:CS3V15/Devops Unit: I application. Amazon EC2 enables you to increase or decrease capacity within minutes. You can use one or hundreds or even thousands of server instances simultaneously. Because this is all controlled with web service APIs, your application can automatically scale itself up and down depending on its needs. Amazon EC2 is integrated with most AWS services, such as Amazon Simple Storage Service (Amazon S3), Amazon Relational Database Service (Amazon RDS), and Amazon Virtual Private Cloud (Amazon VPC) to provide a complete, secure solution for computing applications. Amazon EC2 is an example of Infrastructure as a Service(IaaS). EC2 delivers secure, reliable, cost-effective compute and high-performance compute infrastructure so as to meet the needs of demanding businesses. Amazon EC2 is one of the easiest ways of providing servers on AWS Cloud and also the access to Operating system. · AWS Lambda: AWS Lambda is a serverless, event-driven compute service that allows you to run code without managing servers. You pay only for the compute time you consume and there is no charge when your code is not running. With AWS Lambda, you can run code for any type of application with zero administration. Just upload your code, and Lambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from other AWS services, or you can call it directly from any web or mobile app. But triggering Lambda is possible with over 200 AWS services. You can only pay for what you have used. The compute time that you consume, you are needed to pay for it. You just only need to upload your code and everything required to run will take care of by Lambda and it automatically scales your code with high availability. · AWS Elastic Beanstalk: AWS Elastic Beanstalk is a Platform as a Service that facilitates quick deployment of your applications by providing all the application services that you need for your application. Beanstalk is a plug- and-play platform that allows working with multiple programming languages and environments. Elastic Beanstalk supports a large range of platforms like Node js, Java, PHP, Python, and Ruby. So, you can develop your application to meet your requirements and simply deploy it on Elastic Beanstalk. The main aim to use AWS Elastic Beanstalk is to allow you to focus on the deployment and management of your applications. You can simply upload your code, and AWS Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the underlying resources at any time. 2. Networking · Amazon VPC: Amazon VPC is your network environment in the cloud. It allows you to create a private network within the AWS cloud that uses many of the same concepts and constructs as an on-premises network. Amazon VPC also gives you complete control of the network configuration. Customers can define normal networking configuration items such as IP address ranges, subnet creation, route table creation, network gateways, and security settings. Amazon VPC is an AWS foundational service and integrates with numerous AWS services. For instance, Amazon EC2 instances are deployed into your CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 24 ---
Course Code/Title:CS3V15/Devops Unit: I Amazon VPC. Similarly, Amazon Relational Database Service (Amazon RDS) database instances deploy into your Amazon VPC, where the database is protected by the structure of the network just like your on-premises network. You can easily launch AWS resources into a virtual network by Amazon Virtual Private Cloud. An isolated virtual network environment in the AWS cloud is created by Amazon VPC. . Amazon Route 53: Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service. It is designed to give developers and businesses an extremely reliable and cost-effective way to route end users to Internet applications by translating human-readable names, such as www.geeksforgeeks.com, into the numeric IP addresses that computers use to connect to each other. Amazon Route 53 is fully compliant with IPv6 as well. 2. Storage · Amazon S3 (Simple Storage Service): Amazon Simple Storage Service (Amazon S3) is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the web. It is designed to provide an infinite amount of storage and it is delivered with 99.999999999% durability. You can use Amazon S3 as primary storage for cloud-native applications as a target for backup and recovery and disaster recovery. It offers industry-leading scalability, data availability, security, and performance. It's simple to move large volumes of data into or out of Amazon S3 with Amazon's cloud data migration options. Once data is stored in Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like Amazon S3 Standard - Infrequent Access and Amazon Glacier for archiving. · Amazon Glacier: Amazon Glacier is a secure, durable, and extremely low- cost storage service for data archiving and long-term backup. Data stored in Amazon Glacier takes several hours to retrieve, which is why it's ideal for archiving. The fastest access to your archive data is via Amazon Glacier. 3. Databases · Amazon RDS (Relational Database Service): Amazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost- efficient and resizable capacity while managing time-consuming database administration tasks, freeing you up to focus on your applications and business. You can find Amazon RDS is also available on several database instance types - optimized for memory, performance, or I/O. Amazon RDS provides you with six familiar database engines to choose from, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. · Amazon DynamoDB (Non-Relational Database): Amazon DynamoDB is a fast and flexible NoSQL database service for all applications that need consistent, single-digit millisecond latency at any scale. It is a fully managed database and supports both document and key-value data models. When you create a database table that can store and retrieve any amount of data you can simply use Amazon DynamoDB that will serve any level of requested traffic. Its flexible data model and reliable performance make it a great fit for mobile, web, gaming, Internet of Things (IoT), and many other applications. DynamoDB provides many features like · built-in security CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 24 N32 nirf 1750 Rank

--- Page 21 ---
Unit: I Course Code/Title:CS3V15/Devops · Gaming: AWS has been serving many gaming studios. Combining Amazon EC2 and S3 services with CloudFront enables gaming websites to deliver high-quality gaming experiences to their customers regardless of location. Use Cases of AWS · Netflix Netflix is an entertainment platform that started in the United States, but eventually, it expanded to many countries and soon became popular. However, once Netflix confronted the scalability problem because of the sudden increase in viewers. That made Netflix choose AWS services. Netflix reports that when it started using AWS services like DynamoDB and Cassandra for its distributed databases, it could handle the data easily. So, scalability is a great advantage of AWS. Netflix has adapted around 100,000 server instances from AWS for computing and storage databases, analytics, recommendation engines, and video transcoding as well. . McDonald's McDonald's is the world's largest fast-food company that serves around 64 million people per day. The growth of this company has gone to another level when it started home deliveries. By utilizing AWS services, McDonald's created a platform that integrates local restaurants with delivery partners such as Uber Eats. Scalability is also a reason for the company to choose AWS services. Moreover, with AWS Microservices Architecture, McDonald's platform can scale 20,000 orders per second and integrate with the global partners easily. · Airbnb Airbnb is an international online marketplace for rental homes. This platform connects people who are looking for rental accommodation with those who want to rent out their houses. Quite soon, Airbnb became unable to handle the constant streaming of data on the website from its customers. That is when it started using Amazon EC2 service and Elastic Load Balancing, which distributes incoming traffic to multiple Amazon EC2 instances. In this way, Airbnb could avoid traffic, and customers could use the online platform without any disruption. • Novartis Novartis is the best example for AWS use cases in healthcare. Novartis is one of the world's largest healthcare companies that provides solutions for patients' well-being. It adapted Amazon EC2 services and built a platform using other services such as Amazon Simple Storage Service, Amazon Elastic Block Store, and four availability zones. Data Analysts of Novartis are taking advantage of the AWS services and still implementing new solutions for the patients. · Expedia Expedia is a worldwide online travel agency that has always focused on the constant development and innovation of its platform to offer an extraordinary user experience for its clients. Since 2010, Expedia has been using AWS services to build a standard deployment model for better infrastructure as AWS offers the best data security through different availability zones. • Samsung If you are using Samsung mobile phones, then you may know about the Samsung app store. For setting up the apps stacked in its store, the company started using AWS services. Using AWS app development services, Samsung wanted to provide its customers with the facility to download the apps anywhere without any network traffic. • NASA NASA (National Aeronautics and Space Administration) has always wondered about creating a library to present people with all its achievements through pictures and videos of space. Later on, it created such platforms, but because it had 10 different NASA centers, it couldn't provide the best experience for viewers. So, all it wanted was to create an easy-access platform for TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 21 N32 nirf 1750 Rank

--- Page 22 ---
Unit: I Course Code/Title:CS3V15/Devops people to search for and view images and videos. Then, NASA started adopting many services from AWS to solve this problem, which included Amazon Elastic Compute Cloud, Elastic Load Balancing, Amazon Simple Storage Service, Amazon Simple Queue Service, etc. Among these, Amazon S3 helped the company store all the incoming data such as photos, videos, and audio files without any hassle. • Facebook Facebook, without a doubt, is a widespread social media platform. To build a scalable application, Facebook used services such as Amazon Elastic Compute Cloud, Amazon Simple Storage Service, Amazon Relational Database Service, Amazon SimpleDB, Amazon CloudFront, Amazon Simple Queue Service, etc. Amazon RDS helps the platform to make it easy to set up, operate, and scale the database in the cloud. Various Services offered by AWS · Amazon EC2 (Elastic Cloud computing) · Amazon RDS (Relational Database Services) · Bonus Service: Amazon Connect · Amazon S3 (Simple Storage Service) · Amazon Lambda · Amazon Cognito · Amazon Glacier · Amazon SNS (Simple Notification Service) · Bonus Service: Amazon Lex · Amazon Lightsail · Amazon VPC (Virtual Private Cloud) · Amazon Kinesis · Amazon Inspector · Amazon Auto-scaling · Amazon IAM (Identity and Access Management) · Dynamo DB · Amazon SQS (Simple Queue Service) · Amazon ElastiCache · Amazon Chime · AWS Athena · Code Catalyst · Web Application Firewall · AWS Amplify · AWS Rekognition · AWS QuickSight · AWS CloudFormation · AWS Management Console The Important Cloud Services according to various categories that are provided by AWS are given below : 1. Compute • Amazon EC2: Amazon Elastic Compute Cloud (Amazon EC2) is a web service that provides secure, resizable compute capacity in the cloud. It allows organizations to obtain and configure virtual compute capacity in the cloud. You can select from a variety of operating systems and resource configurations like memory, CPU, and storage that are required for your TECHNOLOGY (Autonomous) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY 22 N33 narf 1750 Rank

--- Page 61 ---
Course Code/Title:CS3V15/Devops Unit: II Return to the previous screen and undo the deletion. The errors that occurred will disappear. *mavenproject/pom.xml & IApp.java AppTest.java 6 <artifactId>mavenproject</artifactId> 7 <version>0.0.1-SNAPSHOT</version> <packaging>jar</packaging> 8 9 16 <name>mavenproject</name> 11 <url>http://maven.apache.org</url> 12 138 <properties> 14 15 16 17% <dependencies> 18 19 <groupId>junit</groupId> I 28 21 22 23 24 <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> </properties> cdependency> <artifactId>junit</artifactid> <version>3.8.1</version> <scope>test</scope> </dependency> </dependencies> 25 </project> 26 9. Dependency Management: The dependencyManagement and dependencies are especially useful for multi-module projects. Dependency Management : This tag consists of a dependencies tag which itself might contain multiple dependency tags. Each dependency is supposed to have at least three maintags: groupId, artifactId, and version. For example: <dependencyManagement> <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 20 N33 nirf 1750 Rank -

--- Page 62 ---
Course Code/Title:CS3V15/Devops Unit: II The above code just declares the new artifact commons-lang3, but it doesn't really add it to the project dependency resource list. Dependencies: This tag contains a list of dependency tags. Each dependency is supposed to have at least two main tags, which are groupId and artifactId. For example: <dependencies> <dependency> <groupId>org.apache.commons</groupId> <artifactId>commons-lang3</artifactId> <version>3.14.0</version> </dependency> </dependencies> The version and scope tags can be inherited implicitly if we have used the dependencyManagement tag before in the POM file. DependencyManagement is just a declaration, and it does not really add a dependency. Dependencies tag adds the actual dependency to the project. An example for adding the JUnit library dependency: <dependencyManagement> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.13.2</version> <scope>test</scope> </dependency> </dependencies> </dependencyManagement> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 21 N33 nirf 175º Rank

--- Page 109 ---
Course Code/Title:CS3V15/Devops Unit: III Note that the git repository and the files from that repository are copied from the workspace of App1 to App2. Also note that test.class and test.java files are copied to the workspace of App2. Started by user Anuradha R Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\. jenkins\workspace\App2 Copied 16 artifacts from "Appl" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621189248227895.bat C:\ProgramData\Jenkins\.jenkins\workspace\App2>dir Volume in drive C is OS Volume Serial Number is C6C2-8A9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 .git 474 test. class 144 test.java 2 File(s) 618 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ Finished: SUCCESS 5. Extended Choice Parameter Overview: The Extended Choice Parameter Plugin provides advanced parameter types for Jenkins jobs, such as multi-select lists, checkboxes, and more complex formats. Key Features: · Parameter Types: Includes multi-select, checkboxes, and other advanced input options. . Dynamic Choices: Allows dynamic generation of choices from scripts or external sources. Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Extended Choice Parameter Plugin," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add an Extended Choice Parameter. ○ Configure the parameter type, choices, and other settings. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 110 ---
Unit: III Course Code/Title:CS3V15/Devops Usage: · Useful for scenarios where complex user input is needed or where multiple selection options are required. Configuring Jenkins to Work with Java, Git, and Maven 1. Configuring Jenkins to Work with Java Overview: Jenkins requires Java to run. Configuring Java in Jenkins involves specifying the Java Development Kit (JDK) installations Jenkins should use. Steps: 1. Install Java Development Kit (JDK): ○ Ensure that JDK is installed on your system. You can download it from the Oracle website or use OpenJDK. 2. Configure JDK in Jenkins: ○ Open Jenkins and go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to JDK section and click Add JDK. ○ Enter a name for the JDK installation (e.g., JDK 11). Check Install automatically to let Jenkins download and install the JDK, or specify the path to an existing JDK installation. ○ If specifying the path manually, provide the JAVA_HOME directory. 3. Example Configuration: ○ Name: JDK 11 ○ JAVA_HOME: /usr/lib/jvm/java-11-openjdk 4. Verify JDK Configuration: You can verify the JDK configuration by creating a simple Jenkins job and adding a build step that prints the Java version using: bash Copy code java -version 2. Configuring Jenkins to Work with Git Overview: The Git Plugin integrates Jenkins with Git repositories, enabling Jenkins to clone, pull, and manage Git-based source code repositories. CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 35 1750 Rank

--- Page 1 ---
Course Code/Title:CS3V15/Devops CHENNAI INSTITUTE OF TECHNOLOGY Transforming Lives CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) NAME OF THE PROGRAMME B. E (CSE) YEAR III SEMESTER V REGULATIONS 2022R COURSE CODE CS3V15 COURSE NAME DEVOPS FACULTY NAME (Prepared by) NAME OF SUBJECT EXPERT (Verified by) M.SUNDHARI CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Unit: I N33 INATIONAL BOARD . ACCREDITATION 100% Accreditation All eligible UG & PG Programs NIRF 151 - 200 Band Engineering 2023 A+ DE NAAC Contact 8681927167 Contact 1 N32 nirf 1750 Rank

--- Page 2 ---
Course Code/Title:CS3V15/Devops Unit: I UNIT - I Introduction to Devops Devops Essentials - Introduction To AWS, GCP, Azure - Version control systems: Git and Github. The word "DevOps" was coined in 2009 by Patrick Debois, who became one of its gurus. The term was formed by combining "development" and "operations," which provides a starting point for understanding exactly what people typically mean when they say "DevOps." Notably, DevOps isn't a process or a technology or a standard. Many devotees refer to DevOps as a "culture"-a viewpoint that New Relic favors. We also use the term "DevOps movement" when talking about topics such as adoption rates and trends for the future, and "DevOps environment" to refer to an IT organization that has adopted a DevOps culture. "DevOps represents a change in IT culture, focusing on rapid IT service delivery through the adoption of agile, lean practices in the context of a system-oriented approach. DevOps emphasizes people (and culture), and seeks to improve collaboration between operations and development teams. DevOps implementations utilize technology- especially automation tools that can leverage an increasingly programmable and dynamic infrastructure from a life cycle perspective." DevOps is defined as a combination of processes and tools created to facilitate organizations in delivering services and applications much faster than they can through conventional software development processes. It helps increase customers' confidence in the applications that an organization offers, thereby allowing the company to flourish and achieve its business goals faster. Development 3 3 DevOps 13 51 Operations Quality Assurance (QA) How DevOps Works? A DevOps process can be summarized as an infinite loop that comprises the following stages - build, test, and release through the delivery pipeline and plan and monitor through feedback, which resets the loop again. With such an amazing combination, teams use tech stack and tooling that assists them in reliably developing apps. Moreover, going away from the Under the DevOps model, development and operations teams work in constant cohesion throughout the entire project lifecycle, starting right from development to deployment. When security is the main focus, the quality assurance team is tightly knitted with the DevOps team throughout the app lifecycle. In this situation, some DevOps teams are also referred to as DevSecOps. Close coordination with the QA team ensures that no loopholes are left unchecked in the provided service/app. COMPANY HOW DEVOPS WORKS DELIVERY PIPELINE BUILD TEST RELEASE PLAN MONITOR FEEDBACK LOOP TOOLBOX" CUSTOMER norm, teams use automated processes here. DevOps tools also allow engineers to complete different tasks independently. Be it provisioning infrastructure or deploying code, they can accomplish these tasks without being dependent on one another. As such, the DevOps model accelerates the overall application development process. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 29 ---
Unit: I Course Code/Title:CS3V15/Devops Enterprise services. AWS virtual private cloud. Outsourced cloud services. Salesforce User Controls Operating System, Runtime, Middleware, and Application data Others It is highly scalable and flexible. Microsoft Azure. Force.com, Gigaspaces. Data of the application It is highly scalable to suit the different businesses according to resources. Google cloud services Google offers a seven wide range of Services: · Compute · Networking · Storage and Databases · Big Data · Machine Learning · Identity & Security · Management and Developer Tools IBM cloud analysis. AWS, Terremark Nothing It is highly scalable to suit the small, mid and enterprise level business 1. Compute: GCP provides a scalable range of computing options you can tailor to match your needs. It provides highly customizable virtual machines. and the option to deploy your code directly or via containers. · Google Compute Engine · Google App Engine · Google Kubernetes Engine · Google Cloud Container Registry · Cloud Functions 2. Networking: The Storage domain includes services related to networking, it includes the following services • Google Virtual Private Cloud (VPC) . Google Cloud Load Balancing · Content Delivery Network · What is Google Cloud Connect · Google Cloud DNS · What is Google Cloud Web Hosting 3. Storage and Databases: The Storage domain includes services related to data storage, it includes the following services · Google Cloud Storage . Cloud SQL · Cloud Bigtable · Google Cloud Datastore • Persistent Disk TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 29 N32 nirf 1750 Rank

--- Page 30 ---
Unit: I Course Code/Title:CS3V15/Devops 4. Big Data: The Storage domain includes services related to big data, it includes the following services · Google BigQuery · Google Cloud Dataproc · Google Cloud Datalab · Google Cloud Pub/Sub 5. Cloud AI: The Storage domain includes services related to machine learning, it includes the following services · Cloud Machine Learning · Vision API · Speech API · Natural Language API · Translation API • Jobs API 6. Identity & Security: The Storage domain includes services related to security, it includes the following services · Cloud Resource Manager · Cloud IAM · Cloud Security Scanner · Cloud Platform Security 7. Management Tools: The Storage domain includes services related to monitoring and management, it includes the following services • Stackdriver · Monitoring · Logging · Error Reporting · Trace · Cloud Console 8. Developer Tools: The Storage domain includes services related to development, it includes the following services . Cloud SDK · Deployment Manager · Cloud Source Repositories · Cloud Test Lab AZURE Azure is Microsoft's cloud platform, just like Google has its Google Cloud and Amazon has its Amazon Web Service or AWS.000. Generally, it is a platform through which we can use Microsoft's resources. For example, to set up a huge server, we will require huge investment, effort, physical space, and so on. In such situations, Microsoft Azure comes to our rescue. It will provide us with virtual machines, fast processing of data, analytical and monitoring tools, and so on to make TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 30 N33 narf 1750 Rank

--- Page 41 ---
Unit: I Course Code/Title:CS3V15/Devops S.No. Git 1 Git is a software. 2 Git is a command-line tool GitHub is a graphical user interface 3 Git is installed locally on the system GitHub is hosted on the web 4 Git is maintained by linux. GitHub is maintained by Microsoft. 5 Git is focused on version control and code sharing. GitHub is a hosting service for Git repositories. 6 Git is a version control system to manage source code history. 7 Git was first released in 2005. GHub was launched in 2008. 8 Git has no user management feature. 9 Git is open-source licensed. GitHub includes a free-tier and pay- for- use tier. 10 Git has minimal external tool configuration. 11 Git provides a Desktop interface named Git Gui. GitHub provides a Desktop interface named GitHub Desktop. 12 Git competes with CVS, Azure DevOps Server, Subversion, Mercurial, etc. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY GitHub GitHub is a service. GitHub is focused on centralized source code hosting. GitHub has a built-in user management feature. GitHub has an active marketplace for tool integration. GitHub competes with GitLab, Bit Bucket, AWS Code Commit, etc. 41 N32 nirf 1750 Rank

--- Page 42 ---
Course Code/Title:CS2V15/Devops Unit: II UNIT - II COMPILE AND BUILD USING MAVEN AND GRADLE Introduction, Installation of Maven, POM files, Maven Build lifecycle, Build phases(compile build, test, package) Maven Profiles, Maven repositories(local, central, global),Maven plugins, Maven create and build Artifacts, Dependency management, Installation of Gradle, Understand build using Gradle. 1. Introduction of Maven: Apache Maven is an automation tool. The tool is written in Java. It was initially released on 13 July 2004. It is developed by the Apache software foundation. It is part of the Jakarta Project. It is working on two aspects: how software is built, and its dependencies. It was created by Jason van Zyl. It is built by using a plugin-based architecture that allows it to make the use of any application controllable by standard input. It dynamically downloads Java libraries. Understanding the problem without Maven There are many problems that we face during the project development. They are discussed below: 1) Adding set of Jars in each project: In case of struts, spring, hibernate frameworks, we need to add set of jar files in each project. It must include all the dependencies of jars also. 2) Creating the right project structure: We must create the right project structure in servlet, struts etc, otherwise it will not be executed. 3) Building and Deploying the project: We must have to build and deploy the project so that it may work. Maven simplifies the above mentioned problems. It does mainly following tasks. 1. It makes a project easy to build 2. It provides uniform build process (maven project can be shared by all the maven projects) 3. It provides project information (log document, cross referenced sources, mailing list, dependency list, unit test reports etc.) 4. It is easy to migrate for new features of Maven Apache Maven helps to manage o Builds Documentation o Reporing o SCMs o Releases o Distribution What is Build Tool? A build tool is used for building a process. It does following: CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] CHENNAI INSTITUTE TECHNOLOGY N32 nirf 1 1750 Rank

--- Page 49 ---
Course Code/Title:CS3V15/Devops Unit: II Validate Deploy Compile Install Test Verify Package Integration Test · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. There are always pre and post phases to register goals, which must run prior to, or after a particular phase. When Maven starts building a project, it steps through a defined sequence of phases and executes goals, which are registered with each phase. Maven has the following three standard lifecycles - • default: This is the main lifecycle, as it's responsible for project deployment. CHENNAI CHENNAI INSTITUTE . TECHNOLOGY 8 clean: Handles project cleaning, ensuring that all artifacts generated by previous INSTITUTE OF TECHNOLOGY 175º Rank (Autonomous]

--- Page 50 ---
Course Code/Title:CS3V15/Devops Unit: II builds are removed. · site: Manages the creation of the project's site documentation. Default Lifecycle (default) Most Maven users will be familiar with the default lifecycle. It is a general model of a build process for a software application. The first phase is validate and the last phase is deploy. · Validate: This step validates if the project structure is correct. For example - It checks if all the dependencies have been downloaded and are available in the local repository. · Compile: It compiles the source code, converts the .java files to .class, and stores the classes in the target/classes folder. · Test: It runs unit tests for the project. · Package: This step packages the compiled code in a distributable format like JAR or WAR. · Integration test: It runs the integration tests for the project. · Verify: This step runs checks to verify that the project is valid and meets the quality standards. · Install: This step installs the packaged code to the local Maven repository. · Deploy: It copies the packaged code to the remote repository for sharing it with other developers. Clean Lifecycle (clean) The first lifecycle in Maven. Running mvn clean invokes the clean lifecycle which consists of three lifecycle phases: · pre-clean : execute processes needed prior to the actual project cleaning · clean : remove all files generated by the previous build · post-clean : execute processes needed to finalize the project cleaning Site Lifecycle (site) Maven does more than build software artifacts from project, it can also generate project documentation and reports about the project, or a collection of projects. Project documentation and site generation have a dedicated lifecycle which contains four phases: 1. pre-site : execute processes needed prior to the actual project site generation 2. site : generate the project's site documentation 3. post-site : execute processes needed to finalize the site generation, and to prepare for site deployment 4. site-deploy: deploy the generated site documentation to the specified web server CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 39 ---
Course Code/Title:CS3V15/Devops Unit: I GITHUB GitHub is an increasingly popular programming resource used for code sharing. It's a social networking site for programmers that many companies and organizations use to facilitate project management and collaboration. According to statistics collected in October 2020, it is the most prominent source code host, with over 60 million new repositories created in 2020 and boasting over 56 million total developers. GitHub is a Git repository hosting service that provides a web-based graphical interface. It is the world's largest coding community. Putting a code or a project into GitHub brings it increased, widespread exposure. Programmers can find source codes in many different languages and use the command-line interface, Git, to make and keep track of any changes. GitHub helps every team member work together on a project from any location while facilitating collaboration. You can also review previous versions created at an earlier point in time. GitHub's Features? 1. Easy Project Management: GitHub is a place where project managers and developers come together to coordinate, track, and update their work so that projects are transparent and stay on schedule. 2. Increased Safety With Packages Packages can be published privately, within the team, or publicly to the open-source community. The packages can be used or reused by downloading them from GitHub. 3. Effective Team Management GitHub helps all the team members stay on the same page and organized. Moderation tools like Issue and Pull Request Locking help the team to focus on the code. 4. Improved Code Writing Pull requests help the organizations to review, develop, and propose new code. Team members can discuss any implementations and proposals through these before changing the source code. 5. Increased Code Safety GitHub uses dedicated tools to identify and analyze vulnerabilities to the code that other tools tend to miss. Development teams everywhere work together to secure the software supply chain, from start to finish. 6. Easy Code Hosting All the code and documentation are in one place. There are millions of repositories on GitHub, and each repository has its own tools to help you host and release code. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 39 N32 nirf 1750 Rank

--- Page 40 ---
Course Code/Title:CS3V15/Devops Unit: I HOSTING SERVICE FOR GIT REPOSITORY When it comes to hosting Git repositories, various platforms provide a robust infrastructure for collaborative development, version control, and project management. Choosing the right hosting service depends on factors like ease of use, collaboration features, and integration capabilities. Here are some popular Git hosting services widely utilized in the software development community: 1. GitHub: GitHub stands out as one of the most prevalent Git hosting platforms, offering a user- friendly interface, powerful collaboration features, and seamless integration with various tools. It serves as an ideal choice for open-source projects, private repositories, and team collaboration. 2. GitLab: GitLab is a comprehensive web-based Git repository manager that not only provides source code management but also includes features like continuous integration. It caters to both cloud-based and self-hosted solutions, giving users flexibility in hosting their repositories. 3. Bitbucket: Owned by Atlassian, Bitbucket is another popular Git repository hosting service. Supporting both Git and Mercurial repositories, it offers features like code collaboration, issue tracking, and continuous integration. Bitbucket is often preferred by teams using other Atlassian tools such as Jira and Confluence. 4. GitKraken Glo Boards: GitKraken Glo Boards is an integrated task and issue tracking service linked with GitKraken, a Git client. This platform allows teams to manage tasks directly associated with their Git repositories and provides a visual approach to monitoring project progress. 5. SourceForge: SourceForge, with a long history, hosts open-source software projects and offers version control, bug tracking, and project management tools. While not as prominent as some other options, it remains a viable choice for numerous projects. 6. AWS CodeCommit: As part of Amazon Web Services (AWS), AWS CodeCommit is a fully managed source control service. It seamlessly integrates with other AWS services and provides a secure and scalable environment for hosting Git repositories. Selecting the most suitable Git hosting service depends on your team's requirements, project size, and preferences for cloud-based or self-hosted solutions. Each platform has its strengths, catering to specific use cases within the software development landscape. Difference between GIT and GITHUB Git and GitHub are related concepts but serve different purposes in the context of version control and collaborative software development. Git: Git is a distributed version control system for tracking changes in source code during software development. It is designed for coordinating work among programmers, but it can be used to track changes in any set of files. Its goals include speed, data integrity, and support for distributed, non-linear workflows. GitHub: GitHub is a web-based Git repository hosting service, which offers all of the distributed revision control and source code management (SCM) functionality of Git as well as adding its own features. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 40 N32 nirf 1750 Rank

--- Page 9 ---
Course Code/Title:CS3V15/Devops Unit: I 9. Slack Slack is a crucial tool for businesses and organizations worldwide. Its significance lies in facilitating seamless communication and collaboration among teams, whether working in the same office or remotely. Slack's real-time messaging, file sharing, and integration capabilities streamline workflow, enhance productivity and keep teams connected across different time zones and locations. As the work landscape evolves, with more companies embracing hybrid and remote work models, Slack is a vital hub for quick decision-making, project coordination, and knowledge sharing. With an ever- expanding ecosystem of integrations and features, Slack remains at the forefront of modern workplace communication, making it essential for businesses to stay agile, efficient, and competitive. 10. AWS Cloud Computing and Storage in DevOps AWS (Amazon Web Services) Cloud Computing and Storage are crucial in DevOps because they provide scalable, flexible, and cost-effective infrastructure for DevOps practices. AWS offers many services, including compute resources, databases, container orchestration, and serverless computing, which align perfectly with modern software development and deployment demands. Organizations adopt DevOps to accelerate software delivery. AWS provides the foundation for rapidly deploying and scaling applications, supporting continuous integration and continuous delivery (CI/CD) pipelines, and automating infrastructure provisioning through tools like AWS CloudFormation. Furthermore, AWS's storage solutions enable efficient data management, backup, and recovery, ensuring the resilience and reliability required for DevOps operations. As cloud technology evolves, AWS remains at the forefront, enabling DevOps teams to focus on innovation and efficiency. 11. Azure Cloud Computing and Storage in DevOps Azure Cloud Computing and Storage will be pivotal in DevOps practices in 2024 and beyond. Azure offers a comprehensive cloud ecosystem that enables organizations to scale their infrastructure, deploy applications, and store data efficiently. Azure provides essential services for continuous integration and continuous deployment (CI/CD), automation, monitoring, and security. Its cloud computing capabilities facilitate the provisioning of resources on demand, ensuring that development and testing environments are readily available. Azure's storage solutions, including Azure Blob Storage, Azure Files, and Azure SQL Database, enable secure data storage and retrieval, supporting the data-driven aspects of DevOps. Besides, Azure's integration with DevOps tools like Azure DevOps Services streamlines the software development lifecycle, enhancing collaboration and automation. 12. GCP Cloud Computing and Storage in DevOps Google Cloud Platform (GCP) offers robust cloud computing and storage solutions. GCP provides a scalable, reliable, and highly available infrastructure essential for modern DevOps practices. With its comprehensive set of services, including Google Compute Engine, Google Kubernetes Engine, Cloud Storage, and BigQuery, GCP empowers DevOps teams to build, deploy, and manage applications easily. Its emphasis on automation, infrastructure as code, and container orchestration aligns seamlessly with DevOps principles. Moreover, GCP's cutting-edge technologies, such as AI and machine learning capabilities, provide DevOps practitioners with advanced tools for monitoring, analytics, and automation, making it a powerful choice for organizations seeking to optimize their software development and delivery processes. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 10 ---
Course Code/Title:CS3V15/Devops Unit: I 13. Monitoring, Alerting, and Incident Response Tools: SignalFx Monitoring, alerting, and incident response tools like SignalFx are pivotal in DevOps and software development. As software systems become complex and distributed, the need for real-time visibility into performance and the ability to respond swiftly to incidents is significant. SignalFx excels in this regard by providing advanced monitoring and observability solutions that enable organizations to detect anomalies, trace issues across microservices proactively, and set up intelligent alerts. As applications scale, cloud-native architectures become the norm, and user expectations for reliability grow, SignalFx's capabilities are crucial. It empowers DevOps teams to ensure high availability, optimize resource utilization, and maintain a seamless user experience by identifying and addressing performance issues before they impact end-users. It is one of the most essential tools for modern software operations. 14. Appdynamics AppDynamics, a leading application performance management and monitoring platform, remains critically important as it ensures the optimal performance of modern digital businesses. As organizations rely on complex and distributed software systems, proactively monitoring, troubleshooting, and optimizing these applications becomes essential. AppDynamics provides real-time visibility into application performance, allowing businesses to swiftly identify bottlenecks, latency issues, and errors. With the ever-growing complexity of applications, the importance of AppDynamics lies in its ability to empower organizations to deliver exceptional user experiences, maintain application reliability, and swiftly respond to performance issues, thereby ensuring the continued success and competitiveness of digital businesses. 15. Raygun It is a crucial tool in software development and DevOps because it ensures application reliability and performance. Raygun is an application monitoring and error-tracking platform that empowers development teams to identify, diagnose, and resolve real-time issues. With software systems growing in complexity and the increased demand for seamless user experiences, Raygun's importance lies in providing actionable insights into application errors and performance bottlenecks. It enables organizations to proactively address issues, reduce downtime, and enhance user satisfaction, leading to higher software quality and improved customer experiences. Software is central to businesses across industries. Raygun's role in maintaining application health and facilitating rapid issue resolution makes it a fundamental tool for DevOps professionals and software developers. 16. Splunk Cloud Splunk Cloud helps organizations gain critical insights from the ever-expanding volume of data generated in today's digital landscape. As businesses increasingly rely on data- driven decision-making, Splunk Cloud stands out as a robust and scalable platform for monitoring, searching, analyzing, and visualizing machine-generated data. Its importance lies in providing real-time visibility into the health and performance of complex systems, applications, and infrastructures, enabling rapid incident detection and response. As cybersecurity threats evolve, Splunk Cloud's advanced security analytics and threat detection capabilities remain indispensable for safeguarding against cyberattacks and ensuring data integrity. In a world where data is a strategic asset, Splunk Cloud's role in harnessing the power of data for operational excellence and security cannot be overstated. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 10 N32 nirf 1750 Rank

--- Page 25 ---
Course Code/Title:CS3V15/Devops Unit: I · backups · automated multi-region replication in-memory caching · data export tools. Global Infrastructure of AWS · AWS is a cloud computing platform which is globally available. . Global infrastructure is a region around the world in which AWS is based. Global infrastructure is a bunch of high-level IT services which is shown below: AWS is available in 19 regions, and 57 availability zones in December 2018 and 5 more regions 15 more availability zones for 2019. The following are the components that make up the AWS infrastructure: o Availability Zones o Region o Edge locations o Regional Edge Caches Compnents of Global Infrastructure Availability zones Region Edge Locations Regional Edge Caches Availability zone as a Data Center An availability zone is a facility that can be somewhere in a country or in a city. Inside this facility, i.e., Data Centre, we can have multiple servers, switches, load balancing, firewalls. The things which interact with the cloud sits inside the data centers. o An availability zone can be a several data centers, but if they are close together, they are counted as 1 availability zone. Region o A region is a geographical area. Each region consists of 2 more availability zones. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 26 ---
Course Code/Title:CS3V15/Devops Unit: I o A region is a collection of data centers which are completely isolated from other regions. · A region consists of more than two availability zones connected to each other through links. AZ-a AZ-b AZ-c Availability zones are connected through redundant and isolated metro fibers. Edge Locations o Edge locations are the endpoints for AWS used for caching content. o Edge locations consist of CloudFront, Amazon's Content Delivery Network (CDN). o Edge locations are more than regions. Currently, there are over 150 edge locations. Edge location is not a region but a small location that AWS have. It is used for caching the content. o Edge locations are mainly located in most of the major cities to distribute the content to end users with reduced latency. o For example, some user accesses your website from Singapore; then this request would be redirected to the edge location closest to Singapore where cached data can be read. Regional Edge Cache o AWS announced a new type of edge location in November 2016, known as a Regional Edge Cache. o Regional Edge cache lies between CloudFront Origin servers and the edge locations. o A regional edge cache has a large cache than an individual edge location. o Data is removed from the cache at the edge location while the data is retained at the Regional Edge Caches. o When the user requests the data, then data is no longer available at the edge location. Therefore, the edge location retrieves the cached data from the Regional edge cache instead of the Origin servers that have high latency. What Is Cloud Computing? Cloud computing is the use of hardware and software components in an off-premises location to deliver a service to a network. Users can access files and applications from any device that can access the internet. Some features and capabilities include: · Cloud providers can pull the computing resources to provide services to multiple customers with the help of a multi-tenant model · Cloud computing proves an on-demand self-service, which helps administrators monitor performance CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 26 N32 nirf 1750 Rank -

--- Page 5 ---
Unit: I Course Code/Title:CS3V15/Devops Example: Facebook's mobile app which is updated every two weeks effectively tells users you can have what you want and you can have it. Now ever wondered how Facebook was able to do social smoothing? It's the DevOps philosophy that helps facebook and sure that apps aren't outdated and that users get the best experience on Facebook. Facebook accomplishes this true code ownership model that makes its developers responsible that includes testing and supporting through production and delivery for each kernel of code. They write and update their true policies like this but Facebook has developed a DevOps culture and has successfully accelerated its development lifecycle. Difference between Agile and DevOps: S. No. Agile 1 It started in the year 2001. It started in the year 2007. 2 Invented by John Kern, and Martin Fowler. Invented by John Allspaw and Paul Hammond at Flickr, and the Phoenix Project by Gene Kim. 3 Agile is a method for creating software. It is not related to software development. Instead, the software that is used by DevOps is pre-built, dependable, and simple to deploy. 4 An advancement and administration approach. Typically a conclusion of administration related to designing. 5 The agile handle centers on consistent changes. DevOps centers on steady testing and conveyance. 6 A few of the finest steps embraced in Agile are recorded underneath - 1. Backlog Building 2.Sprint advancement DevOps to have a few best hones that ease the method - 1. Focus on specialized greatness. 2. Collaborate straightforwardly with clients and join their feedback 7 Agile relates generally to the way advancement is carried of, any division of the company can be spry in its hones. This may be accomplished through preparation. 8 All the group individuals working in a spry hone have a wide assortment of comparable ability sets. This is often one of the points of interest of having such a group since within the time of requirement any of the group individuals can loan help instead of holding up for the group leads or any pro impedances. 9 Spry accepts "smaller and concise". Littler the group superior it would be to convey with fewer complexities. 10 Since Agile groups are brief, a foreordained sum of time is there which are sprints. Tough, it happens that a sprint has endured longer than a month but regularly a week long. 11 A big team for your project is not required. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY DevOps DevOps centers more on program arrangement choosing the foremost dependable and most secure course. DevOps features a diverse approach and is very viable, most of the time it takes after "Divide and Conquer". Work partitioned among the improvement and operation groups. DevOps, on the other hand, accepts that "bigger is better". DevOps, on the other hand, prioritizes reliabilities. It is since of this behavior that they can center on a long-term plan that minimizes commerce's unsettling influences. It demands collaboration among different teams for the completion of work. 5 N33 nirf 1750 Rank

--- Page 6 ---
Unit: I Course Code/Title:CS3V15/Devops 12 Some of the Tools- · Bugzilla · JIRA · Kanboard and more. 13 It is suitable for managing complex projects in any department. It centers on the complete engineering process. 14 It does not focus on the automation. It focusses on automation. 15 Working system gets more significance in Agile than documentation. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Some of the Tools- · Puppet • Ansible • AWS · Chef · team City OpenStack and more. The process documentation is significant in DevOps. 6 N32 nirf 1750 Rank

--- Page 71 ---
Unit: II Course Code/Title:CS3V15/Devops build.gradle &3 I 1 /* 2 * This file was generated by the Gradle 'init' task. 3 * 4 * This generated file contains a sample Java Library project to get you started. 5 * For more details take a look at the Java Libraries chapter in the Gradle 6 * User Manual available at https://docs.gradle.org/6.0.1/userguide/java_library_plugin.html 7 */ 8 9 plugins { 10 // Apply the java-library plugin to add support for Java Library 11 id 'java-library' 12 } 13 14 repositories { 15 // Use jcenter for resolving dependencies. 16 // You can declare any Maven/Ivy/file repository here. 17 jcenter() 18 } 19 20 dependencies { 21 // This dependency is exported to consumers, that is to say found on their compile classpath. 22 api 'org. apache. commons : commons-math3: 3.6.1' 23 24 // This dependency is used internally, and not exposed to consumers on their own compile classpath. 25 implementation 'com. google. guava: guava: 28.0-jre' 26 27 // Use JUnit test framework 30 28 testImplementation 'junit : junit: 4.12' 29 } 30 The build.gradle file contains three default sections. They are as follows: o plugins: In this section, we can apply the java-library plugin to add support for java library. o Repositories: In this section, we can declare internal and external repository for resolving dependencies. We can declare the different types of repository supported by Gradle like Maven, Ant, and Ivy. · Dependencies: In this section, we can declare dependencies that are necessary for a particular subject. Additionally, we can declare other project-related modules like a task in this file. Display the Information of the Gradle project To understand the structure, dependencies and debugging problems of a build, Gradle provides many built-in features that display information on a project. Following are some basic commands to display the information of the project: Listing projects In Gradle, all the sub-projects of a project in the workspace can be listed in a hierarchy. To do so, run the below command from the root directory of the project. 1. gradle -q projects CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] N32 nirf 1750 Rank

--- Page 72 ---
Course Code/Title:CS3V15/Devops Unit: II To see all tasks and more detail, run gradle tasks -- all To see more detail about a task, run gradle help -- task <task> C:\Users \HiMaNshU\eclipse-workspace >gradle -q projects Root project Root project 'eclipse-workspace' No sub-projects To see a list of the tasks of a project, run gradle <project-path> : tasks For example, try running gradle :tasks C:\Users\HiMaNshU\eclipse-workspace> Listing Tasks Gradle allows us to list all the essential tasks of the project. To list the task, run the below command: 1. gradle -q tasks Output: C:\Users\HiMaNshUleclipse-workspace>gradle -q tasks Tasks runnable from root project Build Setup tasks CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 31 N33 nirf 1750 Rank

--- Page 33 ---
Course Code/Title:CS3V15/Devops Unit: I Difference between AWS (Amazon Web Services), Google Cloud, and Azure Technology EC2 (Elastic Compute Cloud) AWS Databases Supported Pricing Models Difficulties Many enterprises find it difficult to understand the company cost AWS fully relational and supports NoSQL databases and Big Data. Per hour - rounded up. On demand, reserved spot. structure. Google Cloud Google Engine(GCE) Compute Technologies pioneered by Google, like Big Query, Big Table, and Hadoop, are databases, and Big Data,naturally fully supported. Per minute - rounded up On demand sustained use. — Fewer services. features Azure VHD Disk) (Virtual Hard Azure supports relational and both NoSQL Windows and through AzureTable HDInsight. Per minute - rounded up. Per minute- rounded up commitments(Pre-paid or monthly) and Less "Enterprise-ready. Storage Services Simple Storage Service(S3) Elastic Block Storage. Elastic File storage. Machine Learing Sage maker. Lex. polly.And many more Blob Storage Queue Storage. File Storage Disk Storage. Data Lake Store Machine learning Azure Bot service Cognitive service GIT Cloud storage. Persistent Disk Transfer appliance. Cloud speech AI Cloud Video Intelligence. Cloud Machine learning engine Git is a distributed version control system (DVCS) that helps manage and track changes in source code during software development. It was created by Linus Torvalds in 2005 and has become one of the most widely used version control systems in the software development industry. Some key concepts and features of Git: · Version Control: Git allows developers to keep track of changes made to their code over time. This includes modifications, additions, and deletions of files. · Distributed System: Git is a distributed version control system, meaning that each developer has a complete copy of the entire repository, including its full history. This allows developers to work independently and merge their changes when necessary. · Branching: Git enables developers to create branches, which are essentially separate lines of development. This allows for the parallel development of features or bug fixes without affecting the main codebase. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) LEN 33 nirf 1750 Rank

--- Page 34 ---
Course Code/Title:CS3V15/Devops Unit: I · Merging: Git provides tools for merging changes from one branch into another. This is essential when multiple developers are working on different branches and need to bring their changes together. · History Tracking: Git maintains a complete history of changes made to the codebase. Developers can view, revert, or analyze changes made over time. · Remote Repositories: Git supports remote repositories, allowing developers to collaborate with others by pushing and pulling changes to and from a shared repository. Platforms like GitHub, GitLab, and Bitbucket provide hosting services for Git repositories. · Staging Area: Git uses a staging area (also known as the index) to prepare and review changes before committing them to the repository. This allows developers to selectively include or exclude specific changes. · Open Source: Git is an open-source tool, and its source code is freely available for modification and distribution. Various GIT Components: Git is composed of several key components that work together to enable version control and collaborative development. Here are the main components of Git: · Repository (Repo): A repository is a directory or storage space where your project and its version history are stored. It contains all the files and directories associated with your project, along with the metadata and configuration information. · Working Directory: The working directory is the directory on your local machine where you manipulate files and make changes to your project. It is essentially your local copy of the repository. · Index (Staging Area): The index, also known as the staging area, is a middle ground where changes are prepared before being committed to the repository. It allows you to selectively stage changes, which means you can choose which modifications to include in the next commit. · Commit: A commit is a snapshot of the changes made to the files in the repository. It represents a specific point in the project's history and is accompanied by a commit message that describes the changes. · Branch: A branch is a parallel line of development within a repository. It allows developers to work on different features or bug fixes simultaneously without affecting the main codebase. Branches can be merged to incorporate changes into other branches. . Head: HEAD is a reference to the latest commit in the currently checked-out branch. It essentially points to the tip of the branch you are currently on. · Remote: A remote is a version of the repository stored on a different server. Git supports collaboration by allowing developers to push and pull changes between their local repository and remote repositories. Platforms like GitHub, GitLab, and Bitbucket are examples of remote repositories. · Clone: Cloning is the process of creating a copy of a remote repository on your local machine. This allows you to start working on your own copy of a project. · Fetch: The fetch operation retrieves changes from a remote repository but does not automatically merge them into your working directory. It is useful for reviewing changes before deciding to merge. · Pull: Pull is a combination of fetch and merge. It retrieves changes from a remote repository and automatically merges them into your working directory. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 34 N32 nirf 1750 Rank

--- Page 107 ---
Course Code/Title:CS3V15/Devops Unit: III Step 8 : Then configure the Build Steps by selecting the option Copy artifacts from another project as follows - . . First of all give the name of the project from which you want to copy the files. In my case it is App1. . Then select the option "copy from WORKSPACE of latest completed build" · Give the regular expression ** / *.*. Here's a breakdown of what ** / *.* means : o ** : The double asterisk ( ** ) is a wildcard that matches zero or more directories and subdirectories. It's often used to recursively search through all directories and subdirectories. o / : The forward slash (/) is used as a directory separator, typically on Unix-like systems. o *.* : This part of the regular expression is used to match files with an extension. In regular expressions, the * symbol represents "zero or more" of the preceding character or pattern, and the . character is used to match any character. So, *.* essentially matches any file with any extension. So, when you use the regular expression ** / *.* in a Jenkins build configuration, you are instructing Jenkins to include all files with any extension ( *.* ) located in any directory and subdirectory ( ** ) within the workspace or specified location. Build Steps E Copy artifacts from another project Project name ? App: Which build ? Copy from WORKSPACE of latest completed buld Limitation Note ? Artifacts to copy ? Artifacts not to copy ? : Toget directory CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 32

--- Page 108 ---
Course Code/Title:CS3V15/Devops Unit: III Add one more Build Step and select Execute Windows batch Command. Give the command dir so that the name of files and directories that are copied from App1 to App2 will be displayed on the Console. = Execute Windows batch command ? Command See the list of available environment variables ---- dir ----- Advanced v Add build step * Click Apply and Save. Step 8 : Now Build this project (App2). The output will be as follows - Started by user Anuradha P Running as SYSTEM Building in workspace C:\ProgramData\Jenkins\.jenkins\workspace\App2 Copied 16 artifacts from "App1" build number 1 [App2] $ cmd /c call C:\WINDOWS\TEMP\jenkins13582621109248227895.bat C:\ProgramData\Jenkins\,jenkins\workspace\App2>dir Volume in drive C Is OS Volume Serial Number is CGC2-BA9A Directory of C:\ProgramData\Jenkins\.jenkins\workspace\App2 01-09-2023* 15:07 <DIR> 01-09-2023 15:07 01-09-2023 15:07 <DIR> 01-09-2023 14:30 01-09-2023 14:30 474 test.class 144 test. java 2 File(s) 616 bytes 3 Dir(s) 116,317,384,704 bytes free C:\ProgramData\Jenkins\.jenkins\workspace\App2>exit @ CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 33 N32 narf 1750 Rank

--- Page 17 ---
Course Code/Title:CS3V15/Devops Unit: I that failed in a test suite gets simpler. Also, we can schedule the execution of the test cases at predefined times. After testing, the code is continuously integrated with the existing code. 4) Continuous Monitoring Monitoring is a phase that involves all the operational factors of the entire DevOps process, where important information about the use of the software is recorded and carefully processed to find out trends and identify problem areas. Usually, the monitoring is integrated within the operational capabilities of the software application. 5) Continuous Feedback The application development is consistently improved by analyzing the results from the operations of the software. This is carried out by placing the critical phase of constant feedback between the operations and the development of the next version of the current software application. The continuity is the essential factor in the DevOps as it removes the unnecessary steps which are required to take a software application from development, using it to find out its issues and then producing a better version. It kills the efficiency that may be possible with the app and reduce the number of interested customers. 6) Continuous Deployment In this phase, the code is deployed to the production servers. Also, it is essential to ensure that the code is correctly used on all the servers. The new code is deployed continuously, and configuration management tools play an essential role in executing tasks frequently and quickly. Here are some popular tools which are used in this phase, such as Chef, Puppet, Ansible, and SaltStack. Containerization tools are also playing an essential role in the deployment phase. Vagrant and Docker are popular tools that are used for this purpose. These tools help to produce consistency across development, staging, testing, and production environment. They also help in scaling up and scaling down instances softly. Containerization tools help to maintain consistency across the environments where the application is tested, developed, and deployed. There is no chance of errors or failure in the production environment as they package and replicate the same dependencies and packages used in the testing, development, and staging environment. It makes the application easy to run on different computers. 7) Continuous Operations All DevOps operations are based on the continuity with complete automation of the release process and allow the organization to accelerate the overall time to market continuingly. It is clear from the discussion that continuity is the critical factor in the DevOps in removing steps that often distract the development, take it longer to detect issues and produce a better version of the product after several months. With DevOps, we can make any software product more efficient and increase the overall count of interested customers in your product. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 17 N32 nirf 1750 Rank

--- Page 18 ---
Course Code/Title:CS3V15/Devops Unit: I AWS AWS stands for Amazon Web Services, It is an expanded cloud computing platform provided by Amazon Company. AWS provides a wide range of services with a pay-as-per-use pricing model over the Internet such as Storage, Computing power, Databases, Machine Learning services, and much more. AWS facilitates for both businesses and individual users with effectively hosting the applications, storing the data securely, and making use of a wide variety of tools and services improving management flexibility for IT resources. Advantages & Features of AWS: 1. Cost savings: One of the biggest benefits of AWS is that it can help businesses save money. As mentioned previously, businesses can avoid the high upfront costs of traditional infrastructure with AWS and pay only for the resources they use. Traditionally, businesses had to invest in hardware and software upfront, which often led to overspending. Let's look at this for example - if a business needs to run a website that gets 1000 visitors per day, they would need to purchase and maintain enough servers to support this traffic. With AWS, the business only pays for the compute resources they use when someone visits their website. This can result in significant cost savings. 2. Flexibility: Another key benefit of AWS is its flexibility. Businesses are able to customize their virtual environment - whether the operating system, database, programming language, or something else - to meet their specific needs. Especially in today's climate, the migration process to the cloud should be as frictionless as possible - and AWS makes that possible. Regardless of your use case or industry, AWS can be tailored to fit your needs, whether you're looking for a single cloud- hosted application or an entire suite of integrated solutions. 3. Reliability: AWS is known for being reliable, with an uptime of 99.9%. This makes it a great platform for mission-critical applications that need to be available 24/7. AWS also offers the ability to deploy resources across multiple availability zones for even greater reliability. The cloud platform also has a number of features that make it easier to ensure reliability, such as autoscaling and auto-healing. Autoscaling allows businesses to automatically scale their resources up or down based on demand, while auto-healing enables them to quickly identify and replace any faulty components. 4. Security: Businesses can take advantage of advanced security features, such as identity and access management, to help protect their data. Their tough infrastructure with an end-to-end approach is designed to withstand attacks and AWS provides customers with tools to help them monitor and respond to threats. When it comes to storage, Amazon S3 provides customers with a secure and reliable way to store and access data. The service is designed to be highly scalable and resilient, with built-in redundancy. Fine-grain identity and access controls can be applied to S3 buckets and objects, giving customers control over who has access to their data. Security tasks can be automated with AWS CloudFormation, making it easier for businesses to manage their security policies. And, you can rest easy knowing that AWS takes privacy seriously, with comprehensive customer data protection and compliance measures. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 18 N32 nirf 1750 Rank

--- Page 11 ---
Course Code/Title:CS3V15/Devops Unit: I 17. Selenium It remains a vital tool in software testing and automation due to its enduring relevance in ensuring the quality of web applications. As technology evolves, web applications become increasingly complex, requiring thorough testing across various browsers and platforms. With its robust automation capabilities and extensive browser compatibility, Selenium allows developers and QA teams to automate repetitive testing tasks efficiently, conduct cross- browser testing, and ensure that web applications function flawlessly across diverse environments. Its open-source nature, active community support, and integration with other DevOps tools make Selenium a go-to choice for organizations striving for continuous delivery and the rapid deployment of high-quality software, a cornerstone of modern software development practices. 18. Gremlin Gremlin is an essential tool in chaos engineering, which has become increasingly critical for ensuring the resilience and reliability of modern software systems. As technology advances and complex distributed systems become the norm, the potential for unexpected failures and outages also rises. Gremlin allows organizations to proactively identify weaknesses and vulnerabilities in their infrastructure and applications by simulating controlled failures, such as network disruptions, service outages, and resource constraints. By intentionally inducing chaos and monitoring the system's response, teams can uncover weaknesses before they lead to costly downtime or security breaches. Gremlin facilitates organizations to build more robust, fault-tolerant systems that can withstand real-world challenges and deliver uninterrupted services to users. 19. ServiceNow ServiceNow is a vital platform for organizations seeking to streamline their IT service management and beyond. Its significance lies in its ability to provide a unified, cloud-based solution for automating and optimizing various business processes, including ITSM, ITOM, HR, customer service, and more. Due to the rapid digitization of services, remote work, and the growing complexity of technology infrastructures, ServiceNow offers a comprehensive approach to managing workflows, resolving issues, and delivering services efficiently. Its intelligent automation capabilities, analytics, and AI-driven insights empower organizations to enhance productivity, agility, and customer satisfaction while reducing operational costs. ServiceNow's role in orchestrating and integrating diverse systems and processes makes it an indispensable tool for driving digital transformation and ensuring smooth operations in the ever-evolving business landscape of 2024. 20. Status Service Updates: The Status Page "Status Service Updates: The Status Page" is a critical tool for organizations and businesses of all sizes. In today's world, where online services and applications are integral to operations, ensuring their availability and reliability is essential. It provides real-time information to users and stakeholders about the operational status of services, applications, and infrastructure. The Status Page plays a crucial role in transparency, trust-building, and customer satisfaction by promptly communicating service disruptions, planned maintenance, and incident resolutions. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 11 N32 nirf 1750 Rank

--- Page 12 ---
Course Code/Title:CS3V15/Devops Unit: I Downtime can often lead to significant financial losses and damage to a company's reputation, so having a practical Status Page becomes not just a convenience but a necessity. It allows organizations to showcase their commitment to transparency and responsiveness in addressing service-related issues, ultimately fostering stronger customer relationships and trust. 21. ELK (Elasticsearch, Logstash and Kibana) ELK, which stands for Elasticsearch, Logstash, and Kibana, continues to shine in DevOps and IT operations. This powerful trio of tools remains essential for organizations seeking effective log management, monitoring, and data visualization. Elasticsearch is a highly scalable and fast search engine that enables real-time data indexing and search. Logstash facilitates the collection, processing, and transformation of log data from various sources, making it compatible with Elasticsearch. Kibana, on the other hand, provides a user- friendly interface for visualizing and analyzing data, offering customizable dashboards and powerful data exploration capabilities. ELK's significance in 2024 lies in its ability to empower organizations with comprehensive insights into their systems, applications, and infrastructure. It ultimately facilitates quick problem resolution, proactive monitoring, and data-driven decision-making in an increasingly complex and fast-paced technological landscape. 22. GitLab CI/CD GitLab CI/CD's significance lies in its ability to automate the complete software delivery pipeline, from code changes to deployment, in a single integrated environment. GitLab CI/CD ensures rapid and reliable delivery of software updates. It enables continuous integration (CI) by automatically building and testing code changes, allowing teams to catch issues early in the development cycle. Furthermore, the continuous deployment (CD) aspect automates the release and deployment process, reducing the risk of human errors and enabling organizations to deliver features and updates to users swiftly and confidently. GitLab CI/CD's importance is further accentuated as businesses seek to accelerate digital transformation efforts, respond rapidly to changing market demands, and maintain a competitive edge through efficient and automated software delivery practices. 23. Scripting Scripting remains vital due to its pivotal role in automating and streamlining various aspects of software development, system administration, and DevOps practices. Scripting languages like Python, Bash, and PowerShell empower tech professionals to write code that can execute repetitive tasks, manipulate data, and orchestrate complex processes efficiently. Scripting facilitates rapid prototyping, configuration management, and the creation of automated deployment pipelines. It enhances productivity, ensures consistency and reduces human error in tasks ranging from software testing and deployment to infrastructure provisioning and monitoring. As organizations increasingly embrace DevOps and cloud- native technologies, scripting stays competitive and adaptive in the tech landscape. 24. Terraform Terraform plays a crucial role in modern infrastructure provisioning and management. It allows organizations to define and deploy infrastructure as code, enabling the automated creation and configuration of cloud resources, containers, and other infrastructure components. Cloud computing, microservices, and containerization have become the norm in CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 12 N32 nirf 1750 Rank

--- Page 161 ---
Add a status badge to your repository Many developers like to show that they're keeping their code quality high by displaying a status badge in their repo. Azure Pipelines succeeded To copy the status badge to your clipboard: 1. In Azure Pipelines, go to the Pipelines page to view the list of pipelines.Select the pipeline you created in the previous section. 2. Select , and then select Status badge. 3. Select Status badge. 4. Copy the sample Markdown from the Sample markdown section. Now with the badge Markdown in your clipboard, take the following steps in GitHub: 1. Go to the list of files and select Readme.md. Select the pencil icon to edit. 2. Paste the status badge Markdown at the beginning of the file. 3. Commit the change to the main branch. 4. Notice that the status badge appears in the description of your repository. To configure anonymous access to badges for private projects: 1. Navigate to Project Settings in the bottom left corner of the page 2. Open the Settings tab under Pipelines 3. Toggle the Disable anonymous access to badges slider under General 21

--- Page 162 ---
Azure-Pipeline • Azure Pipelineș provides a YAML pipeline editor that you can use to author and edit your pipelines. • The YAML editor is based on the Monaco Editor. • The editor provides tools like Intellisense support anda task assistant to provide guidance while you edit a pipeline. • You can also edit pipelines by modifying the azure pipelines.yml file directly in your pipeline's repository using a text editor of your choice Edit a YAML pipeline To access the YAML pipeline editor, do the following steps. • Sign in to your organization • (https://dev.azure.com/{yourorganization}). • Select your project, choose Pipelines, and then select the pipeline you want to edit. You can browse pipelines by Recent, All, and Runs. • Choose Edit. • Make edits to your pipeline using Intellisense and the task assistant forguidance. Azure DevOps fabrikam-tailspin FabrikamFiber F FabrikamFiber + Pipelines Overview Recent All Runs Boards Pipelines Recently run pipelines Repos 1 Pipeline Pipelines Pipelines FabrikamFiber 2 3 17

--- Page 127 ---
Course Code/Title:CS3V15/Devops Unit:IV Abbreviation You can also use abbreviation to represent lists. Example Countries: ['America', 'China', 'Canada', 'Iceland'] List inside Dictionaries We can use list inside dictionaries, i.e., value of key is list. Example james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english … List of Dictionaries We can also make list of dictionaries. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english - robert: name: robert richardson rollNo: 53 div: B sex: male CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 13 N32 nirf 1750 Rank

--- Page 128 ---
Course Code/Title:CS3V15/Devops Unit:IV likes: - biology - chemistry … YAML uses "|" to include newlines while showing multiple lines and ">" to suppress newlines while showing multiple lines. Due to this we can read and edit large lines. In both the cases intendentation will be ignored. We can also represent Boolean (True/false) values in YAML. where boolean values can be case insensitive. Example - james: name: james john rollNo: 34 div: B sex: male likes: - maths - physics - english result: maths: 87 chemistry: 45 biology: 56 physics: 70 english: 80 passed: TRUE messageIncludeNewLines: | Congratulation !! You passed with 79% messageExcludeNewLines: > Congratulation !! You passed with 79% CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 14 N33 nirf 1750 Rank

--- Page 151 ---
Azure DevOps Taking you to your Azure DevOps organization ... Congratulations, you're an organization owner! Sign in to your organization at any time, https://dev.azure.com/{yourorganization}. With your organization, the following aspects are included in the free tier: First five users free (Basic license): Azure Pipelines: o One Microsoft-hosted CI/CD (one concurrent job, upto 30 hours per month) o One self-hosted CI/CD concurrent job Azure Boards: Work item tracking and Kanban boards Azure Repos: Unlimited private Git repos Azure Artifacts: Two GB free per organization Build applications with Azure: Azure DevOps enables you to build, test, and deploy anyapplication to any cloud or on premises To configure build pipelines that continuously build, test, and verify your applications. 11

--- Page 152 ---
Part 1: Get started with Azure DevOps Part 2: Build applications with Azure DevOps Part 3: Deploy applications with Azure DevOps Create a build pipeline with Azure Pipelines: Prerequisites: Familiarity with forking and cloning a GitHub repo Account requirements: An Azure DevOps organization o To use Microsoft-hosted agents, your Azure DevOps organization must have access to Microsoft- hosted parallel jobs. Check your parallel jobs and request a free grant. O You can use GitHub Code spaces to complete the module, even if your Azure DevOps organization doesn't have any parallel jobs. A GitHub account Software requirements: If using GitHub Code spaces to complete the module, there are no software requirements as all software is included in the Codespace If using a local development environment withMicrosoft-hosted agents, you must have the following software installed: o Visual Studio Code o .NET 6.0 SDK o Git 12

--- Page 47 ---
Course Code/Title:CS3V15/Devops Unit: II <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> Any repository of maven is available in mvnrepository.com 5. Maven Plugins: The maven plugins are central part of maven framework, it is used to perform specific goal. According to Apache Maven, there are 2 types of maven plugins. 1. Build Plugins 2. Reporting Plugins Build Plugins These plugins are executed at the time of build. These plugins should be declared inside the <build> element. Reporting Plugins These plugins are executed at the time of site generation. These plugins should be declared inside the <reporting> element. Maven Core Plugins A list of maven core plugins are given below: Plugin Description clean clean up after build. compiler compiles java source code. deploy deploys the artifact to the remote repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY N32 nirf 6 175º Rank -

--- Page 48 ---
Course Code/Title:CS3V15/Devops Unit: II failsafe runs the JUnit integration tests in an isolated classloader. install installs the built artifact into the local repository. resources copies the resources to the output directory for including in the JAR. site generates a site for the current project. surefire runs the JUnit unit tests in an isolated classloader. verifier verifies the existence of certain conditions. It is useful for integration tests. Example for maven plugin: The compiler plugin is used to compile the source code of a Maven project. This plugin has two goals, which are already bound to specific phases of the default lifecycle: · compile - compile main source files · testCompile - compile test source files Here's the compiler plugin in the POM: <plugin> <artifactId>maven-compiler-plugin</artifactId> <version>3.12.1</version> <configuration> <source>1.8</source> <target>1.8</target> </configuration> </plugin> 6. Maven Build Lifecycle: A Build Lifecycle is a well-defined sequence of phases, which define the order in which the goals are to be executed. Here phase represents a stage in life cycle. As an example, a typical Maven Build Lifecycle consists of the following sequence of phases. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 7 N33 nirf 175º Rank -

--- Page 153 ---
Create a pipeline in azure What is pipeline Azure Pipelines supports continuous integration (CI)and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline. The latest way to build pipelines is with the YAML pipeline editor. You can also use Classic pipelines with the Classic editor. code release build CI plan continuous testing CD deploy monitor operate Create your first pipeline This is a step-by-step guide to using Azure Pipelines to build a sample application from a Git repository. This guide uses YAML pipelines configured with the YAML pipeline editor. If you'd like to use Classic pipelines instead, see Define your Classic pipeline. For guidance on using TFVC, see Build TFVC repositories. Prerequisites - Azure DevOps A GitHub account where you can create are pository. An Azure DevOps organization. Create one for free. If your team already has one, then make sure you're an administrator of the Azure Devops. 12

--- Page 154 ---
Get the Java sample code To get started, fork the following repository into your GitHub account. https://github.com/MicrosoftDocs/pipelines-java Create your first Java pipeline 1. Sign-in to your Azure DevOps organization and go to your project. 2. Go to Pipelines, and then select New pipeline. 3. Do the steps of the wizard by first selecting GitHub as the location of your source code. 4. You might be redirected to GitHub to sign in. If so, enter your GitHub credentials. 5. When you see the list of repositories, select your repository. 6. You might be redirected to GitHub to install the Azure Pipelines app. If so,select Approve & install. 7. Azure Pipelines will analyze your repository and recommend the Maven pipeline template. 8. When your new pipeline appears, take a look at the YAML to see what it does. When you're ready, select Save and run. 9. You're prompted to commit a new azure pipelines.yml file to your repository. After you're happy with the message, select Save and run again. If you want to watch your pipeline in action, select the build job. You just created and ran a pipeline that we automatically created for you, because your code appeared to be a good match for the Maven template. You now have a working YAML pipeline (azure-pipelines.yml) in your repository that's ready for you to customize! 14

--- Page 67 ---
Course Code/Title:CS3V15/Devops Unit: II Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013 JVM: 1.7.0_60 (Oracle Corporation 24.60-b09) OS: Windows 8.1 6.3 amd64 Understanding build using Gradle: The Gradle build is a process of creating a Gradle project. When we run a gradle command, it will look for a file called build.gradle in the current directory. This file is also called the Gradle build script. The build configuration, tasks, and plugins are described in this file. The build script describes a project and its tasks. Let's create a small Gradle project, run some of the basic Gradle commands, and understand how Gradle manages the project. Follow the steps below to create and test a Gradle project. Step1: Open the command line and create a directory First, Open the command line and create a directory for the project and change directory to it. Let's create a demo directory. C: \Users \HiMaNshU>mkdir demo C: \Users \HiMaNshU>cd demo Step2: Initialize a Gradle project To generate a Gradle project, run the gradle init command. It will generate a simple project. With this project, we will explore and understand everything that is generated. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 26 N33 nirf 1750 Rank

--- Page 68 ---
Course Code/Title:CS3V15/Devops Unit: II When we run the gradle init command, it will ask for some basic requirements. First, it will ask the type of project that we want to create. It will give four options: 1. 1: basic 2. 2: application 3. 3: library 4. 4: Gradle plugin Select our requirements. Hence, we are just making a demo project so that we will select the basic option. To select basic option, press 1 and Enter key. Consider the below output: C: \Users \HiMaNshU\demo>gradle init Starting a Gradle Daemon, 1 incompatible and 1 stopped Daemons could ed, use -- status for details Select type of project to generate: 1: basic 2: application 3: library 4: Gradle plugin Enter selection (default: basic) [1 .. 4] 1 Next, it will ask for DSL. There are two options that are available for DSL: 1. 1: Groovy 2. 2: Kotlin Groovy is the default DSL provided by Gradle. Select build script DSL. Select build script DSL: 1: Groovy 2: Kotlin Enter selection (default: Groovy) [1 .. 2] 1 Next, it will ask for the project name. Type the project name and press Enter key. It will take a while to build a project. After the successful execution of the project, we will get a message BUILD SUCCESSFUL. Project name (default: demo): First_Gradle > Task : init Get more help with your project: https: //guides. gradle.org/creating- uilds BUILD SUCCESSFUL in 2m 56s 2 actionable tasks: 2 executed C: \Users\HiMaNshU\demo>mkdir src Now we have successfully created a Gradle project. Now, what will happen to our specified directory? Consider the below structure of the Gradle project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 27 N33 narf 1750 Rank

--- Page 143 ---
Unit: V Course Code/Title: CS3V15/Devops Email preferences Send me occasional product updates, announcements, and offers. Verify your account Pick one square that shows two identical objects. wiki How Step 4: Click the green Create account button. It's belowthe form, at the bottom of the page. This will take you to an email verification page. Verify your account Create account By creating an account, you agree to the Terms of Service. For more information about GitHub's privacy practices, see the GitHub Privacy Statement. We'll occasionally send you account-related wikiHow UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 3

--- Page 144 ---
Course Code/Title: CS3V15/Devops Unit: V Carefully review the Terms of Service at https://help.github.com/en/articles/github- terms-of-service and the Privacy Statement at https://help.github.com/en/articles/github- privacy-statement before you continue. You're almost done! We sent a launch code to wikihowneveconcepts@gmail.com + Enter code Didn't get your email? Resend the code or update your email address. wiki How Step 5: Verify your email by entering the code. After clicking Create account, you'll receive an email with a code.Enter this code on the verification page. Entering the code will automatically take you to the welcome page. Step 6: Select your preferences and click Continue. GitHub displays a quick survey that can help you tailor your experience to match what you're looking for. You'll be sent tothe plan selection page after completing the survey. How many team members will be working with you? This will help us guide you to the tools that are best suited for your projects. Just me 2 - 5 5 - 10 10 - 20 20 - 50 50+ Are you a student or teacher? Student Teacher Continue wikiHow CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 4 175° Rank ---

--- Page 45 ---
Course Code/Title:CS3V15/Devops Unit: II Example for pom.xml file <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> <dependencies> <dependency> <groupId>junit</groupId> <artifactId>junit</artifactId> <version>4.8.2</version> <scope>test</scope> </dependency> </dependencies> </project> 4. Maven Repository A maven repository is a directory of packaged JAR file with pom.xml file. Maven searches for dependencies in the repositories. There are 3 types of maven repository: 1. Local Repository 2. Central Repository 3. Remote Repository Maven searches for the dependencies in the following order: Local repository then Central repository then Remote repository. Local Central Remote Repository Repository Repository CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 4 N32 nirf 1750 Rank -

--- Page 46 ---
Course Code/Title:CS3V15/Devops Unit: II If dependency is not found in these repositories, maven stops processing and throws an error. 1) Maven Local Repository Maven local repository is located in your local system. It is created by the maven when you run any maven command. By default, maven local repository is %USER_HOME%/.m2 directory. For example: C:\Users\SSS IT\.m2. Update location of Local Repository We can change the location of maven local repository by changing the settings.xml file. It is located in MAVEN_HOME/conf/settings.xml, for example: E:\apache-maven- 3.1.1\conf\settings.xml. 2) Maven Central Repository Maven central repository is located on the web. It has been created by the apache maven community itself. The path of central repository is: http://repo1.maven.org/maven2/. The central repository contains a lot of common libraries that can be viewed by this url http://search.maven.org/#browse. 3) Maven Remote Repository Maven remote repository is located on the web. Most of libraries can be missing from the central repository such as JBoss library etc, so we need to define remote repository in pom.xml file. Let's see the code to add the jUnit library in pom.xml file. pom.xml <project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.javatpoint.application1</groupId> <artifactId>my-application1</artifactId> <version>1.0</version> <packaging>jar</packaging> <name>Maven Quick Start Archetype</name> <url>http://maven.apache.org</url> CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous] 5 N32 narf 1750 Rank -

--- Page 101 ---
Course Code/Title:CS3V15/Devops Unit: III v => SeleniumDemo src/main/java > src/main/resources > src/test/java > 15 src/test/resources > al JRE System Library [J2SE-1.5] › al Maven Dependencies » drivers v src > @ main v fs test v java v 2> myPackage 10 myTestPage.java 23 resources › › target > > test-output 2 pom.xml > Servers myTestPage.java package myPackage; import org.openqa.selenium.WebDriver; import org.openqa.selenium.chrome.ChromeDriver; import org.testng.annotations.Test; public class myTestPage { @Test public void myFun() { System.setProperty("webdriver.chrome.driver","./drivers/chromedriver.exe"); WebDriver driver = new ChromeDriver(); driver.get("https://google.com"); String title = driver.getTitle(); System.out.println("Hey Anuradha, the tile of this page is: "+title); driver.quit(); } Program explanation : In above program, we have written one test case by the function myFun. In this function, using the Chromedriver which we have installed, we open the web page google.com Then we try to get the tile of this page and display it on the console using System.out statement. Finally we quit the driver. Step 7 : Right click on the Project name in Project Explorer window, Run As-> Maven Test CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 26 N3 nirf 1750 Rank

--- Page 102 ---
Unit: III Course Code/Title:CS3V15/Devops Step 8 : Now, locate the Java file myTestPage.java and right click Run As->TestNG Test. It is as follows - 0 Run As # Debug As Profile As Web Services > > 1 TestNG Test > Run Configurations ... > Alt+Shift+X, N Step 9 : We will get the output as follows - Hey Anuradha, the tile of this page is: Google PASSED: myFun Default test Tests run: 1, Failures: 0, Skips: 0 Default suite Total tests run: 1, Failures: 0, Skips: 0 Step 10 : Now, we will open the Jenkin's page and create the HTML Publisher report for the above SeleniumDemo project. The prerequisite to this activity is that the HTML Publisher report plugin must be installed in Jenkins. · Click on New Item, Enter the Item Name and select as Freestyle project, I have given the name HTMLReportDemo. · Then click on configure. Write the suitable description in General section. · The in Build Steps and Add Build steps as Invoke top-level Maven targets provide the Maven version, and under goals write clean test. . Under Post Build Actions, click on Add Post Build Action and select Publish HTML Report. Now give the path of index.html file which is created by Selenium testNG framework in Maven. It is the path of test-output folder. Also give some suitable HTML report title. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 27 N3 nirf 1750 Rank

--- Page 91 ---
Course Code/Title:CS3V15/Devops Unit: III Now we will create a repository on GitHub and then push the above committed Git repository on the GitHub. Open a web browser, and login to www.GitHub.com. Create a repository by the name MyJavaPrograms. Start a new repository A repository contains all of your project's files, revision history, and collaborator discussion. AnurdhaP / MyJavaPrograms Public Anyone on the internet can see this repository Private You choose who can see and commit to this repository Create a new repository 1 then copy the URL of this repository. HTTPS SSH https://github.com/AnurdhaP/MyJavaPrograms.git Now open the command prompt window and go to the MyJavaPrograms directory and issue the following command. git remote add origin https://github.com/AnurdhaP/MyJavaPrograms.git then push the Git to GitHub repository using the command push -u origin master CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 16

--- Page 92 ---
Course Code/Title:CS3V15/Devops Unit: III It is illustrated as follows - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git push -u origin master Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 8 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 719 bytes | 719.00 KiB/s, done. Total 4 (delta 0), reused 0 (delta 0), pack-reused 0 To https://github.com/AnurdhaP/MyJavaPrograms.git * [new branch] master -> master branch 'master' set up to track 'origin/master'. E: \MyJavaPrograms> · Now it we refresh the repository on GitHub then we can see our test.java and test.class programs are present in that repository. MyJavaPrograms Public !" master - 1º 1 branch 0 tags Go to f AnurdhaP First Commit for Java program .4cb test.class First Commit for Java program test.java First Commit for Java program Help people interested in this repository understand your project by adding a README. Step 4 : Now create a job in Jenkins that makes use of GitHub plugin in order to access the GitHub repository. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 17 N3 nirf 1750 Rank

--- Page 97 ---
Course Code/Title:CS3V15/Devops Unit: III Build Steps Execute Windows batch command ? Command See the list of available environment variables javac welcome. java java welcome Advanced = Execute Windows batch command 7 Command See the list of avalable chuironment variables acho User name is Nuernane% Advanced Step 5 : Click on Apply and then Save. Step 6 : Select Build with parameters option. Dashboard > welcome > Status Changes Workspace Build with Parameters Configure Delete Project 0 Rename CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 22 N3 nirf 1750 Rank

--- Page 98 ---
Course Code/Title:CS3V15/Devops Unit: III Project welcome This build requires parameters: Username Parth D Build Cancel We can change the username as well. Suppose I changed the Username as Anand then click the Build, we get the console output as follows - C: \ProgramData\Jenkins\.jenkins\workspace\welcome>javac welcome.java C:\ProgramData\Jenkins\.jenkins\workspace\welcomeyjava welcome Welcome to first Job of Jenkins [welcome] $ cmd /c call C:\WINDOWS\TEMP\jenkins16582464065687933173.bat C:\ProgramData\Jenkins\.jenkins\workspace\welcome>echo User name is Anand User name is Anand C:\ProgramData\Jenkins\.jenkins\workspace\welcome>exit 0 Finished: SUCCESS Thus we set variety of parameters to the Jenkins job. We can set the Boolean parameter, choice parameter, credentials, file parameter, password parameter and so on. 3. HTML Publisher Filter Plain Boolean Parameter Choice Parameter Credentials Parameter File Parameter Multi-line String Parameter Password Parameter Run Parameter String Parameter Add Parameter Overview: The HTML Publisher Plugin allows Jenkins to publish HTML reports and artifacts generated during the build process. Key Features: · Report Publishing: Publishes HTML reports, dashboards, or other HTML artifacts. · Report Directory: Allows specifying the directory containing HTML reports. · Index Page: Sets a default HTML file to be displayed when accessing the report. Configuration: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 23 nirf 1750 Rank N33

--- Page 81 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Credentials: Add credentials if required. ○ Branch Specifier: Specify the branch (e.g., */main). 7. Build Triggers: ○ Build Periodically: Use CRON syntax to schedule builds. ○ Poll SCM: Jenkins will check the repository for changes at specified intervals. ○ Other Triggers: Configure other triggers such as GitHub hooks, upstream projects, etc. 8. Build Environment: o Configure the build environment settings like setting environment variables, running scripts before the build, etc. 9. Build Steps: ○ Click on "Add Build Step" and choose the appropriate build step (e.g., "Execute Shell", "Invoke Ant", "Invoke Gradle script"). ○ Example: For executing a shell script, add the necessary shell commands. 10. Post-build Actions: ○ Click on "Add post-build action" and choose the appropriate action (e.g., "Archive the artifacts", "Publish JUnit test result report"). ○ Configure the settings for each post-build action. 11. Save the Configuration: ○ Once you have configured all the necessary settings, click "Save" at the bottom of the page. 12. Run the Job: ○ On the job's main page, click "Build Now" to run the job immediately. 13. Monitor the Job: ○ Click on the build number in the "Build History" to view the detailed output and logs of the build process. ○ Check the console output for any errors or warnings. Practical Example Setting up a Freestyle Project for a Maven Build: 1. Create New Item: ○ Name: MyMavenProject o Type: Freestyle project 2. Source Code Management: ○ Git: CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 82 ---
Unit: III Course Code/Title:CS3V15/Devops I Repository URL: https://github.com/example/my-maven-project.git · Branch Specifier: * /main 3. Build Triggers: ○ Poll SCM: H/15 * (poll every 15 minutes) 4. Build Steps: ○ Invoke Top-Level Maven Targets: Goals: clean install 5. Post-build Actions: ○ Publish JUnit test result report: I Test report XMLs: * /target/surefire-reports/ *. xml 6. Save and Build: ○ Click "Save". ○ Click "Build Now" on the job's main page. By following these steps, you can set up a Jenkins job to automate your build process for various types of projects. Jenkins provides extensive customization options, allowing you to tailor the job to meet your specific requirements. Configuring a Jenkins Job 1. Install Jenkins First, make sure Jenkins is installed. You can download it from the official Jenkins website. 2. Access the Jenkins Dashboard Open your web browser and go to http://your-server-ip: 8080 to access the Jenkins dashboard. 3. Create a New Job 1. Start a New Job: ○ Click on "New Item" on the left-hand menu. 2. Name the Job: ○ Enter a name for your job in the "Enter an item name" field. 3. Select Job Type: ○ Choose the type of job (e.g., Freestyle project, Pipeline). CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) - TECHNOLOGY N32 nirf 7 1750 Rank

--- Page 163 ---
‹ FabrikamFiber Runs Branches Analytics Description Edit Run pipeline : Stages #20210609.3 Update azure-pipelines.yml for Azure Pipelines Manually triggered for main 9 61bba8c x2 La Wednesday Ø 215 #20210609.2 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for 1º main 9 bc6c105 x La Wednesday ₾ 305 #20210609.1 Update azure-pipelines.yml for Azure Pipelines & Manually triggered for main 9 53eb92b x • #20210520.1 Updated README.md Individual CI for & new-branch 0 1ada387 ‹ azure-cli-example go main V azure-cli-example / azure-pipelines.yml 30 pool: default 31 32 steps: 33 # Specify python version and install if needed Settings 34 - task: UsePythonVersion@0 35 condition: false 36 inputs: 37 -versionSpec: '3.x' 38 architecture: 'x64' 39 40 # Update pip to latest 41 - bash: python -m pip install -- upgrade pip 42 condition: false 43 displayName: 'Upgrade pip' 44 45 container 46 &continueOnError 47 48 88988 name & parameters & resources & schedules 49 50 51 services 52 strategy 53 & variables 54 & workspace La Wednesday .455 Là May 20 ₾ 135 Variables Run : Tasks 7 Search tasks dotnet .NET Core Build, test, package, or publish a dotnet applicatio. Android signing Sign and align Android APK files Ant Build with Apache Ant 80 App Center distribute Distribute app builds to testers and users via Visu .. App Center test Test app packages with Visual Studio App Center Archive files Compress files into .7z, tar.gz on zip ARM template deployment Deploy an Azure Resource Manager (ARM) templ .. Azure App Service deploy Deploy to Azure App Service a web, mobile, or AP. · The YAML pipeline editor provides several keyboard shortcuts, which we show in the following examples. . Choose Ctrl+Space for Intellisense support while you're editing the YAML pipeline.

--- Page 164 ---
· The task assistant provides a method for adding tasksto your YAML pipeline. · To display the task assistant, edit your YAML pipeline and choose Show assistant. Understand the azure-pipelines.yml file · A pipeline is defined using a YAML file in your repo. Usually, this file is named azure-pipelines.yml and is located at the root of your repo. · Navigate to the Pipelines page in Azure Pipelines, select the pipeline you created, and choose Edit in the context menu of the pipeline to open the YAMLeditor for the pipeline. · This pipeline runs whenever your team pushes a change to the main branch of your repo or creates a pull request. It runs on a Microsoft-hosted Linux machine. · The pipeline process has a single step, which is to run the Maven task. YAML:Code trigger: - main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdk Version: "1.10" jdk11_windows: imageName: "windows-latest" jdk Version: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdk VersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: " ** /TEST -*. xml"goals: "package" Change the platform to build · Navigate to the editor for your pipeline by selectingEdit pipeline action on the build, or by selecting Edit from the pipeline's main page. · To choose a different platform like Windows or Mac, change thevmImage value: · pool: · vmImage: "windows-latest" Add steps . You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can usetasks for building, testing, publishing, or deploying your app. For Java, the

--- Page 117 ---
Course Code/Title:CS3V15/Devops Unit: IV • Explore Ansible modules. 4. Playbook - · Playbooks are a sequence of plays that define the order of tasks executed by Ansible. . A play consists of a list of tasks that target managed nodes in an inventory. · Tasks, in turn, are composed of one or more modules that specify operations. · Playbooks provide a structured way to define and automate complex workflows. 5. Roles - · Roles provide an organized environment for managing complex tasks. · They include templates, playbooks, inventories, error handlers, vars, and meta information. · Ideal for handling larger, multi-step automation processes. 6. Collections - · Collections are distribution formats for Ansible content. · They encompass playbooks, roles, modules, and plugins. · Find Ansible collections. 7. Galaxy - · Ansible Galaxy is a platform for sharing and downloading collections. . It allows the Ansible community to collaborate and exchange automation content. Setting Up the Ansible Control Node Prerequisite: Before you begin setting up Ansible, it's essential to ensure you meet the following prerequisites: · SSH: Make sure SSH is installed on your Linux system. SSH is crucial for secure communication between the control node and managed nodes. Installation Steps: 1. Open your terminal. 2. Run the following command to install Ansible: 1python3 -m pip install -- user ansible 3. SSH Key: o For secure and direct connections to managed nodes, you'll need to provide a public SSH key. You can generate one using the ssh-keygen command. o Follow the on-screen instructions to create your SSH key pair. This key pair is a crucial element of Ansible's secure communications. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 118 ---
Course Code/Title:CS3V15/Devops Unit: IV 4. For specific installation instructions tailored to your Linux distribution, click here. 5. Try to connect ssh using once your mange node is set up. 1ssh <user>@<ip> Note: If you're using a different operating system, find the appropriate installation instructions here. Setting Up the Ansible Mange Node Steps: 1. Install OpenSSH-Server: o To enable remote management of the managed node, you'll need to install the OpenSSH server. Use the following command (assuming you're using a Debian-based Linux distribution like Ubuntu): 1sudo apt install openssh-server o This command installs the OpenSSH server, allowing secure remote access. 2. Create a User: o For convenience and consistency, it's helpful to create a user on the managed node with the same name as the user on the control node. This makes it easier to manage SSH keys and ensures a smoother experience. 3. Configure SSH Key: o To establish secure and direct connections to the managed node, you need to paste the public SSH key from the control node into the authorized_keys file located in the .ssh directory of the user's home folder on the managed node. o The path is typically /home/<user>/.ssh/authorized_keys, where <user> is the username you created or are using on the managed node. You can use the ssh-copy-id command to automate the process of copying your public key to the managed node. For example: 1ssh-copy-id <user>@<managed_node_ip> o This command securely copies your public key to the authorized_keys file, allowing passwordless SSH authentication. Managing Ansible Inventories In the world of Ansible, inventories are like the backbone of your automation infrastructure. They serve as a vital component that lists and organizes the managed nodes, making automation tasks seamless and organized. Inventories use a parent-child concept that allows you to create groupings, which come in handy during specific tasks or scenarios. Why Are Inventories Important? · Inventories provide a comprehensive list of managed nodes, making it easy to interact with and manage them. CHENNAI CHENNAI TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 4 N33 nirf 1750 Rank

--- Page 157 ---
Select a pipeline run to view information about that run. You can choose to Retain or Delete a run from the context menu. For more information on run retention, see Build and release retention policies. Stages La 51m ago > ... 6 42s Retain Delete Ld 2h ago 0 1m 13s - View pipeline details The details page for a pipeline allows you to view and manage that pipeline. Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. Azure DevOps · Search F 4 FabrikamFiber Edit Run pipeline : + Runs Branches Analytics Y Description Stages Ac #20191209.3 Set up CI with Azure Pipelines Manually triggered { main 2b4b23c #20191209.2 Set up CI with Azure Pipelines & Manually triggered & main 2b4b23c Là 54m ago Č 1m 13s Là 55m ago Č 1m 55s #20191209.1 Set up CI with Azure Pipelines Individual CI 1º main 2b4b23c Lô 56m ago 1m 6s 17

--- Page 158 ---
Choose Edit to edit your pipeline. For more information, see YAML pipeline editor. You can also edit your pipeline by modifying the azure-pipelines.yml file directly in the repository that hosts the pipeline. View pipeline run details From the pipeline run summary you can view the status of your run, both while itis running and when it is complete. #20191210.2 Update azure-pipelines.yml for Azure Pipe ... on FabrikamFiber Summary Environments Triggered by Steve Danielson FabrikamFiber & main b2f795e Today at 12:56 PM Duration: 1m 9s Tests: Get started Changes: ¢ 2 commits Work items: ₾ 1 linked Artifacts: 1 published Stages Jobs Build 1 job completed 415 Run new : Deploy 1 job completed 13s 1 artifact From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, test results, and work Items From the summary pane you can view job and stage details, download artifacts, and navigate to linked commits, testresults, and work items Jobs and stages The jobs pane displays an overview of the status of your stages and jobs. This pane may have multiple tabs depending on whether your pipeline has stages and jobs, or just jobs. In this example, the pipeline has two stages named Build and Deploy. You can drill down into the pipeline steps by choosing the job from either the Stages or Jobs pane. 10

--- Page 131 ---
Course Code/Title:CS3V15/Devops Unit: IV Ensure a service is stopped: $ ansible webservers -m service -a "name=httpd state=stopped" 6. Gathering Facts Fact represents the discovered variables about a system. You can use the facts to implement conditional execution of tasks, and also used to get ad-hoc information about your systems. To see all the facts: $ ansible all -m setup Ansible Playbooks Playbooks are the files where the Ansible code is written. Playbooks are written in YAML format. YAML means "Yet Another Markup Language," so there is not much syntax needed. Playbooks are one of the core features of Ansible and tell Ansible what to execute, and it is used in complex scenarios. They offer increased flexibility. Playbooks contain the steps which the user wants to execute on a particular machine. And playbooks are run sequentially. Playbooks are the building blocks for all the use cases of Ansible. Ansible playbooks tend to be more configuration language than a programming language. Through a playbook, you can designate specific roles to some of the hosts and other roles to other hosts. By doing this, you can orchestrate multiple servers in very different scenarios, all in one playbook. Playbook Structure Each playbook is a collection of one or more plays. Playbooks are structured by using Plays. There can be more than one play inside a playbook. Playbook Play Task Module Target Environment O Task Module Play Task Target Environment Play Module O Task CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY 17 N33 nirf 1750 Rank

--- Page 132 ---
Course Code/Title:CS3V15/Devops Unit:IV name: install and configure DB hosts: testServer become: yes vars: oracle_db_port_value : 1521 tasks: -name: Install the Oracle DB yum: < code to install the DB> -name: Ensure the installed service is enabled and running service: name: < your service name> The Different YAML Tags name This tag specifies the name of the Ansible playbook. As in what this playbook will be doing. Any logical name can be given to the playbook. hosts This tag specifies the lists of hosts or host group against which we want to run the task. The hosts field/tag is mandatory. It tells Ansible on which hosts to run the listed tasks. The tasks can be run on the same machine or on a remote machine. One can run the tasks on multiple machines and hence hosts tag can have a group of hosts' entry as well. vars Vars tag lets you define the variables which you can use in your playbook. Usage is similar to variables in any programming language. tasks All playbooks should contain tasks or a list of tasks to be executed. Tasks are a list of actions one needs to perform. A tasks field contains the name of the task. This works as the help text for the user. It is not mandatory but proves useful in debugging the playbook. Each task internally links to a piece of code called a module. A module that should be executed, and arguments that are required for the module you want to execute. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 18 N33 nirf 1750 Rank

--- Page 111 ---
Course Code/Title:CS3V15/Devops Unit: III Steps: 1. Install the Git Plugin: 0 Go to Manage Jenkins > Manage Plugins. ○ Under the Available tab, search for Git Plugin and install it. 2. Configure Git in Jenkins: ○ Navigate to Manage Jenkins> Global Tool Configuration. ○ Scroll down to the Git section and click Add Git. ○ Specify the path to the Git executable or use the default path. ○ Optionally, configure additional settings such as Git installation locations. 3. Example Configuration: ○ Name: Git ○ Path to Git executable: /usr/bin/ git (or use the default if Git is installed in a standard location). 4. Configure Job to Use Git: ○ Create or edit a Jenkins job and go to Source Code Management. ○ Select Git. ○ Enter the repository URL (e.g., https://github.com/user/repository.git). ○ Configure credentials if the repository is private. ○ Specify the branch to build (e.g., main). 5. Example Repository URL: ○ Repository URL: https://github.com/example/repo.git o Branch Specifier: main 3. Configuring Jenkins to Work with Maven Overview: The Maven Plugin integrates Jenkins with Apache Maven, allowing Jenkins to use Maven to build projects. Steps: 1. Install Maven: ○ Ensure that Apache Maven is installed on your system. You can download it from the Apache Maven website. 2. Configure Maven in Jenkins: ○ Go to Manage Jenkins > Global Tool Configuration. ○ Scroll down to the Maven section and click Add Maven. ○ Enter a name for the Maven installation (e.g., Maven 3.8.6). ○ Check Install automatically to let Jenkins download Maven, or specify the path to an existing Maven installation. ○ CHENNAI INSTITUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 36 1750 Rank

--- Page 112 ---
Unit: III Course Code/Title:CS3V15/Devops 3. Example Configuration: ○ Name: Maven 3.8.6 ○ MAVEN_HOME: / usr / share / maven 4. Configure Maven in a Jenkins Job: ○ Create or edit a Jenkins job and go to Build section. 0 Add a build step and select Invoke top-level Maven targets. o Choose the Maven version configured earlier. ○ Specify the goals to run (e.g., clean install). 5. Example Maven Goals: ○ Goals: clean install 6. Configure Build Environment: ○ Ensure that the job's build environment is set up to use Maven. This might involve setting environment variables or configuring build scripts. 7. Example Environment Variables: ○ MAVEN_OPTS: - Xms512m -Xmx2048m (if you need to set specific JVM options for Maven). By following these steps, Jenkins will be properly configured to work with Java, Git, and Maven, allowing you to set up and manage continuous integration and delivery pipelines effectively. If you need further customization or run into issues, consulting the documentation for each tool or plugin can provide additional guidance. Creating a Jenkins Build and Understanding Jenkins Workspace 1. Creating a Jenkins Build Overview: A Jenkins build is a process where Jenkins executes a series of steps defined in a job configuration. This typically involves compiling code, running tests, and generating artifacts. Steps to Create a Jenkins Build: 1. Access Jenkins Dashboard: ○ Open your Jenkins dashboard, usually found at http://localhost: 8080. 2. Create a New Job: ○ Click on New Item on the left sidebar. Enter a name for your job. ○ Choose a job type. Common types include: I Freestyle project: For simple build jobs with basic configuration. · Pipeline: For more complex build processes using a Jenkinsfile. ○ Click OK to proceed. ○ TECHNOLOGY N33 37 nirf 1750 Rank CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous)

--- Page 99 ---
Unit: III Course Code/Title:CS3V15/Devops 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "HTML Publisher Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Post-build Actions section. ○ Select Publish HTML reports. ○ Specify the directory containing HTML reports and the index page. Usage: . Ideal for displaying build reports such as test results or code coverage metrics in a user-friendly HTML format. DEMO: Step 1 : We will create a Maven project using Selenium and TestNG framework. Open Eclipse : IDE and Create a Maven Project. Select the Create Simple Project (Skip archetype selection) and enter the Group Id and Artifact Id as per your choice. My project name is SeleniumDemo. Step 2 : Add the Selenium dependency in your pom.xml file as follows - <dependency> <groupId>org.seleniumhq.selenium</groupId> <artifactId> selenium-java</artifactId> <version>3.141.59</version> </dependency > Save the pom.xml file. Step 3 : Now install testNG plugin. For that, from Menu select Help-> Eclipse Marketplace Help Welcome 7 Help Contents Search Show Context Help Show Active Keybindings ... Ctrl+Shift+L Tip of the Day Tips and Tricks ... Cheat Sheets ... @ Eclipse User Storage Open the Eclipse Marketpla ** Perform Setup Tasks ... % Check for Updates Install New Software ... Eclipse Marketplace ... = About Eclipse IDE · Donate Contribute Type TestNG in the search window. The testNG installation option will be available. click Install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 24 N32 nirf 1750 Rank ---

--- Page 100 ---
Course Code/Title:CS3V15/Devops Unit: III C Eclipse Meketplace × Eclipse Marketplace Select solutions to initiit, Prese leuralt Now to proceed with lutallation. Press the "more info" link to learn more about a solution Search Recent Popular Favorites itwled - Research at the Eclipse × All Categories Go TestNG for Eclipse . This plug-in late you run your TextNG wits from Erfiore. You can run sulter, groupe er individuel methods. Errors are reported in a pepwane tab that lets you. more info by Chat Bent Apache 20 tantog zielt testingunit integration functional selection * 737 ** * stal: 1,83M (25,021 last month) Install MoreUnit 3.3.0 Morelinit is an Solipoe plugin that should mult you in wilting mory unk with. It supports ali programming languages (witching between teds and clanset under. mone.info Install Infinitest 5.4.0.3 Infinitest is a continuont tout runner for tova,'and is valuable to developers valing a Marketplaces (7) Cancel Then confirm the selected features by clicking on Confirm button. x Confirá Selected Features These Confirm to gorthus with the induction. Cs go back te chocie bis sules it CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 25 N3 nirf 1750 Rank

--- Page 103 ---
Course Code/Title:CS3V15/Devops Unit: III =Publish HTML reports ? Reports HTML directory to archive ? E:\Eclipse_workplace\SeleniumDemo\test-output Index page[s] ? index.html Index page title[s] (Optional) Report title ? HTML Report Publishing options Step 11 : Click on Apply and then Save. Now build this project. Step 12 : On successful build, it creates the HTML Report as follows - x + € > C @ localhost:0000/juby/HTML/\portDemo/HTML_20Report/ Back to HTMLBepsatDemo Test results All suites Default suite 11 D myPackage.myTestPage Info . Repetir output Results . Thus using HTML Publish Report Plugin we can successfully generate HTML report of some testing framework application. · Troubleshooting : Sometimes, the raw file is displayed as HTML report. To get the HTML formatted report we must click on Manage Jenkins section, scroll down and locate Script Console section. Click on it. Tools and Actions C Broad Configuration Sium Disk Discard all the loaded alla lo memory and reloud Jerking CLI Script Console Ewecubes arbitrary suyt lor and run below given command : System.setProperty("hudson.model.DirectoryBrowserSupport.CSP", ") . Then restart the Jenkins Page and Build the project once again. The HTML report can then be displayed as given in above step. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 28 N32 nirf 1750 Rank

--- Page 104 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Copy Artifact Overview: The Copy Artifact Plugin allows Jenkins jobs to copy build artifacts from other jobs, facilitating artifact reuse across different jobs. Key Features: · Artifact Copying: Copies files from one build to another, which can be from a specific build or the latest successful build. · Triggering Builds: Can be used in downstream jobs to fetch artifacts from upstream jobs. Configuration: 1. Install the Plugin: Go to Manage Jenkins > Manage Plugins > Available tab, search for "Copy Artifact Plugin," and install it. 2. Configure Job: ○ In the job configuration, go to the Build section. ○ Add a Copy artifacts from another project build step. o Specify the project name, build number or criteria, and the target directory for copied artifacts. Usage: · Commonly used in multi-job pipelines where artifacts need to be shared between different jobs or stages. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 29 N3 nirf 1750 Rank

--- Page 35 ---
Unit: I Course Code/Title:CS3V15/Devops Push: Push is the operation that sends your committed changes to a remote repository, making them accessible to others. Git workflow v0.1 v0.2 v1.0 Master Hotfix Release Develop Feature Feature Git Flow is a structured branching model designed for projects with well- defined release cycles and a need for strict quality control. Branches: The branching model described is commonly known as the Gitflow Workflow. It's a branching strategy that defines a strict branching model designed to facilitate collaboration and streamline the release process. Let's go into detail about each branch: clone operation Working Copy edit, add, move files update operation Modify working copy status and diff operation Review changes commit and push operation push operation Commit changes push operation amend and push operation Review changes 1. Master Branch: The `master'branch represents the main codebase and contains production- ready code. This branch is typically stable and should only include thoroughly tested and approved changes. Each commit on the 'master' branch represents a new version or release of the software. 2. Develop Branch: The 'develop' branch is an integration branch where various feature branches are merged. It serves as a staging area for testing new features and ensuring they work well together before merging into the 'master' branch. This branch may have ongoing development work and is not necessarily always in a production- ready state. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 35 nirf 1750 Rank N33

--- Page 36 ---
Course Code/Title:CS3V15/Devops Unit: I 3. Feature Branches: Feature branches are created for developing new features or implementing changes. These branches are typically based on the 'develop'branch. Once a feature is complete, the branch is merged back into the 'develop' branch. Feature branches allow developers to work on specific tasks without affecting the main codebase. 4. Release Branch: The 'release' branch is created when the 'develop' branch reaches a point where it is ready for a production release. This branch is used for final testing, bug fixes, and preparing the code for deployment. No new features should be added to the release branch. Once the release is deemed stable, it is merged into both the 'master' branch and the 'develop' branch. 5. Hotfix Branch: The 'hotfix' branch is used to quickly address critical issues or bugs in the production code. It is created directly from the 'master' branch. Hotfixes are intended to be small and focused on resolving the specific issue at hand. Once the hotfix is complete, it is merged into both the 'master'branch and the `develop'branch to ensure that the fix is applied to future releases. Here is the typical flow: · Developers work on feature branches based on the 'develop' branch. · Completed features are merged into the 'develop' branch. · When ready for a release, a 'release' branch is created from 'develop". · The release branch undergoes testing and bug fixes. · The release branch is merged into both 'master' and 'develop' once it's stable. · If a critical issue arises in production, a 'hotfix' branch is created from 'master'. · The hotfix is merged into both 'master' and 'develop to keep both branches in sync. · This Gitflow Workflow helps maintain a structured development process, ensuring that features are developed, tested, and released in a controlled manner. Example Scenario: · Imagine you're working on a large software project with a team of 10 developers. You have a major release planned for every six months. · You create a "feature/Ticket-Id" branch to develop a new login system. · Once the feature is complete, it's merged into the "develop" branch for integration and testing. · As the release date approaches, you create a "release/v1.0" branch to freeze code for the upcoming release. · Any critical issues discovered in the production environment are fixed in "hotfix" branch and merged into "master" and "develop." CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 36 N33 nirf 1750 Rank

--- Page 145 ---
Course Code/Title: CS3V15/Devops Unit: V Step 7:Note the types of plans offered by GitHub. There are a few different plans to choose from, varying in the amount of features provided. Learn to ship software like a pro. GitHub gives students free access to the best developer tools so they can learn by doing. Free Get additional student benefits GitHub Pro Unlimited public/private repositories 2,000 CI/CD minutes/month Free for public repositories 500MB of Packages storage Free for public repositories Protect your branches Ensure that collaborators on your repository cannot make irrevocable chang branches. O Draft pull requests O 120 core-hours of Codespaces compute O Pages and Wikis 15GB of Codespaces storage O Community support ® 3,000 CI/CD minutes/month Free for public repositories 2GB of Packages storage Free for public repositories O 180 core-hours of Codespaces compute 20GB of Codespaces storage Web-based support wikiHow Step 8: Select the free plan. On the plan selection page, scrolldown to click the button for choosing a free plan. This will immediately take you to your GitHub dashboard. Continue for free wiki How UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 5 N33 In 175º Rank ---

--- Page 146 ---
Course Code/Title: CS3V15/Devops Unit: V · If you choose a paid plan, you'll have to enter yourpayment information as requested before you can continue. . If you want to upgrade your Github account in the future, click the menu at the top- right corner, select Settings, andchoose Billing and plans to view your options. GitHub essentials are: · Repositories · Branches · Commits · Pull Requests · Git (the version control software GitHub is built on) Repository: A GitHub repository can be used to store a developmentproject. It can contain folders and any type of files (HTML, CSS,Javascript, Documents, Data, Images). A GitHub repository should also include a licence file and a README file about the project. A GitHub repository can also be used to store ideas, or any resources that you want to share. Github Repository Creation Step 1: Click on the new repository option Marketplace Explore New repository and GitHub without any code! Catch Universe Join a community Universe in San Fr ticket pricing-he Import repository × New gist bird New organization New project llo World guide, you'll create a repository, start a , write comments, and open a pull request. GitHub Sponsors Matching Fund × Ready to support open source? GitHub will match your contribution to developers during their first year in GitHub Sponsors. d the guide Start a project × Welcome to the new dashboard. Get closer to the stuff you care about most. Explore repositories atlas-engineer/next Next browser - Be productive. UnENVIVAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 175° Rank 6

--- Page 43 ---
Course Code/Title:CS3V15/Devops Unit: II o Generates source code (if auto-generated code is used) o Generates documentation from source code o Compiles source code o Packages compiled code into JAR of ZIP file o Installs the packaged code in local repository, server repository, or central repository 2. Installation of Maven: To install maven on windows, you need to perform following steps: 1. Download maven and extract it 2. Add JAVA_HOME and MAVEN_HOME in environment variable 3. Add maven path in environment variable 4. Verify Maven 1) Download Maven To install maven on windows, you need to download apache maven first. Download Maven latest Maven software from Download latest version of Maven For example: apache-maven-3.1.1-bin.zip 2) Add MAVEN_HOME in environment variable Right click on MyComputer -> properties -> Advanced > Environment variables -> click new button System Settings - Now add MAVEN_HOME in variable name and path of maven in variable value. It must be the home directory of maven i.e. outer directory of bin. For example: E:\apache-maven-3.1.1 3) Add Maven Path in environment variable Click on new tab if path is not set, then set the path of maven. If it is set, edit the path and append the path of maven.Here, we have installed JDK and its path is set by default, so we are going to append the path of maven. The path of maven should be %maven home%/bin. For example, E:\apache-maven- 3.1.1\bin . 4) Verify maven To verify whether maven is installed or not, open the command prompt and write: mvn -version Now it will display the version of maven and jdk including the maven home and java home. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE . TECHNOLOGY 2 N32 nirf 175º Rank

--- Page 44 ---
Unit: II Course Code/Title:CS3V15/Devops 3. POM FILES: POM is an acronym for Project Object Model. The pom.xml file contains information of project and configuration information for the maven to build the project such as dependencies, build directory, source directory, test source directory, plugin, goals etc. Maven reads the pom.xml file, then executes the goal. Before maven 2, it was named as project.xml file. But, since maven 2 (also in maven 3), it is renamed as pom.xml. Elements of maven pom.xml file For creating the simple pom.xml file, you need to have following elements: Element Description project It is the root element of pom.xml file. modelVersion It is the sub element of project. It specifies the modelVersion. It should be set to 4.0.0. groupId It is the sub element of project. It specifies the id for the project group. artifactId It is the sub element of project. It specifies the id for the artifact (project). An artifact is something that is either produced or used by a project. Examples of artifacts produced by Maven for a project include: JARs, source and binary distributions, and WARs. version It is the sub element of project. It specifies the version of the artifact under given group. Maven pom.xml file with additional elements Here, we are going to add other elements in pom.xml file such as: Element Description packaging defines packaging type such as jar, war etc. name defines name of the maven project. url defines url of the project. dependencies defines dependencies for this project. dependency defines a dependency. It is used inside dependencies. scope defines scope for this maven project. It can be compile, provided, runtime, test and system. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] 3 N32 narf 1750 Rank -

--- Page 165 ---
Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. Customize CI triggers Pipeline triggers cause a pipeline to run. You can use trigger: to cause a pipeline to run whenever you push an update to a branch. YAML pipelines are configured by default with a CI trigger on your default branch (which is usually main). You can set up triggers for specific branches or for pull request validation. For a pull request validation trigger, just replace the trigger: step with pr: as shown in the two examples below. By default, the pipeline runs for each pull request change. . If you'd like to set up triggers, add either of the following snippets at the beginning of your azure-pipelines.yml file. YAML I Copy trigger: - main - releases/* YAML Copy pr: - main - releases/* You can specify the full name of the branch (for example, main) or a prefix-matching · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. Build using multiple versions Add steps You can add more scripts or tasks as steps to your pipeline. A task is a pre-packaged script. You can use tasks for building, testing, publishing, or deploying your app. For Java, the Maven task we used handles testing and publishing results, however, you can use a task to publish code coverage results too. · Open the YAML editor for your pipeline. . Add the following snippet to the end of your YAML file. YAML Copy - task: PublishCodeCoverageResults@1 inputs: codeCoverageTool: "JaCoco" summaryFileLocation: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco/ reportDirectory: "$(System.DefaultWorkingDirectory)/ ** /site/jacoco" failIfCoverageEmpty: true · Select Save and then confirm the changes. . You can view your test and code coverage results by selecting your build and going to the Test and Coverage tabs.

--- Page 166 ---
Build using multiple versions Rename/move pipeline Name ScheduledTriggerTest 1234 Select folder 1 .. . Cancel Save Pipeline settings X Processing of new run requests Enabled Paused Disabled YAML file path azure-pipelines.yml Automatically link work items included in this run Cancel Save YAML trigger: main strategy: matrix: jdk10_linux: imageName: "ubuntu-latest" jdkVersion: "1.10" jdk11_windows: imageName: "windows-latest" jdkversion: "1.11" maxParallel: 2 pool: vmImage: $(imageName) steps: - task: Maven@4 inputs: mavenPomFile: "pom.xml" mavenOptions: "-Xmx3072m" javaHomeOption: "JDKVersion" jdkVersionOption: $(jdkVersion) jdkArchitectureOption: "x64" publishJUnitResults: true testResultsFiles: **** /TEST -*. xml" goals: "package"

--- Page 83 ---
Course Code/Title:CS3V15/Devops Unit: III ○ Click "OK" to proceed. 4. Configure the Job General Settings 1. Project Description: ○ Enter a description for your job. 2. Discard Old Builds: ○ Set up a policy to manage the number of builds Jenkins keeps. Source Code Management (SCM) 1. Select SCM: ○ Choose your source code management system (e.g., Git). 2. Configure Repository: ○ Enter the repository URL and provide credentials if needed. 3. Branch Specification: ○ Specify the branch to build (e.g., main, develop). Build Triggers 1. Choose Build Triggers: ○ Poll SCM: Check for changes at intervals. ○ Build Periodically: Schedule builds. ○ GitHub hook trigger: Trigger builds on GitHub push. ○ Build after other projects: Trigger builds based on other jobs. Build Environment 1. Configure Build Environment: ○ Clean workspace: Delete workspace before build starts. ○ Manage secrets: Use secret texts or files. ○ Node selection: Specify where to run the job. Build Steps 1. Add Build Steps: ○ Click "Add build step". ○ Choose the build step type (e.g., Execute shell, Invoke Gradle script). ○ Enter the commands or scripts needed for the build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE . TECHNOLOGY N32 nirf 8 1750 Rank

--- Page 84 ---
Course Code/Title:CS3V15/Devops Unit: III Post-build Actions 1. Add Post-build Actions: Click "Add post-build action". ○ Common actions include: Archive artifacts: Store build artifacts. Email notification: Send build status emails. I Publish test results: Aggregate test results. Trigger other jobs: Trigger other jobs based on results. 5. Save and Build 1. Save the Configuration: ○ Click "Save". 2. Build the Job: ○ Click "Build Now" to manually trigger a build. ○ View progress and logs by clicking on the build number. 6. Monitor and Manage Builds 1. Check Build History: ○ View past builds and their statuses. 2. Console Output: ○ Click on a build number to see detailed logs. 3. Build Artifacts: ○ Access any artifacts produced by the build. 7. Advanced Configuration (Optional) 1. Pipeline as Code: 0 For complex workflows, use Jenkins Pipelines. Create a Jenkinsfile in your repository. 2. Plugins: ○ Extend Jenkins functionality with plugins from the Jenkins Plugin Manager. CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) 9 N32 nirf 1750 Rank

--- Page 79 ---
Course Code/Title:CS3V15/Devops Unit: III 4. Plugins: ○ Role: Extend Jenkins functionality without modifying the core software. ○ Types: I Source Control Plugins: Git, SVN, Mercurial. Build Tools Plugins: Maven, Gradle, Ant. Notification Plugins: Email, Slack, HipChat. · Reporting Plugins: JUnit, Cobertura, Checkstyle. ○ Management: Plugins can be installed, updated, and configured via the Jenkins web interface. 5. Master-Slave Architecture: ○ Concept: The Jenkins master distributes build tasks to multiple agents, enabling distributed builds. ○ Benefits: Scalability: Handle more builds simultaneously. I Resource Management: Assign builds to agents with the necessary resources. Isolation: Run builds in isolated environments to avoid conflicts. Workflow and Data Flow 1. Job Trigger: ○ Jobs can be triggered by various events: Manual Trigger: Initiated by a user through the Jenkins interface. Scheduled Trigger: Using CRON-like syntax to schedule jobs. · SCM Trigger: Automatically triggered by changes in the source code repository. Upstream/Downstream Trigger: Triggered by the completion of other jobs. 2. Build Execution: ○ Job Assignment: The master assigns the job to an available agent based on labels, availability, and resource requirements. Build Environment Setup: The agent sets up the environment, including checking out the code, installing dependencies, and configuring the workspace. ○ Execution: The agent runs the build steps as defined in the job configuration. 3. Build Results: ○ Logs: Captures console output and logs from the build process. ○ Artifacts: Stores build artifacts like binaries, packages, and reports. ○ Test Results: Collects and displays test results and code coverage reports. ○ Build History: Maintains a history of all builds, including status, duration, and changes. 4. Notification and Reporting: ○ Notifications: Jenkins can send notifications through various channels (email, chat, etc.) upon build completion or failure. ○ Dashboards: Provides dashboards for visualizing build status, trends, and metrics. ○ Reports: Generates and displays reports on test results, code quality, and other metrics. INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) N32 nirf 4 1750 Rank

--- Page 80 ---
Unit: III Course Code/Title:CS3V15/Devops Summary Jenkins' architecture is designed to be flexible and scalable. The master-agent model allows for efficient distribution of build tasks, while plugins provide extensive customization options. By leveraging Jenkins' comprehensive job configuration and robust notification and reporting capabilities, teams can streamline their CI/CD pipelines and improve software quality and delivery speed. Creating a Jenkins Job Creating a Jenkins job involves setting up a new project within Jenkins to automate various tasks such as building code, running tests, and deploying applications. Here is a step-by-step guide to creating a Jenkins job. Prerequisites · Jenkins installed and running. · Basic understanding of the Jenkins interface. . Access to the Jenkins dashboard. Step-by-Step Guide 1. Access Jenkins Dashboard: . Open your web browser and navigate to your Jenkins instance URL (e.g., http://localhost:8080). 2. Create a New Job: ○ On the Jenkins dashboard, click on the "New Item" link on the left-hand side menu. 3. Enter Job Name: ○ Enter a name for your new job in the "Enter an item name" field. o Choose the type of job you want to create. For most uses, "Freestyle project" is a good starting point. ○ Click "OK". 4. Configure the Job: ○ After clicking "OK", you will be directed to the job configuration page. 5. General Configuration: ○ Description: Provide a brief description of the job. ○ Discard Old Builds: Optionally set this to limit the number of old builds Jenkins keeps. ○ Select Version Control System: Choose the version control system (e.g., Git, Subversion). 6. Source Code Management: ○ Repository URL: Enter the repository URL. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY 5 nirf 175 Rank N33

--- Page 129 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible ad-hoc Commands The Ansible ad-hoc command uses the /usr/bin/ansible command-line tool to automate a single task on one or more managed nodes. The Ad-hoc commands are quick and easy, but they are not re-usable. The Ad-hoc commands demonstrate the simplicity and power of Ansible. Syntax ansible <hosts> [-m <module_name>] -a <"arguments"> -u <username> [ -- become] Explanation Hosts: It can be an entry in the inventory file. For specifying all hosts in the inventory, use all or "*" . module_name: It is an optional parameter. There are hundreds of modules available in the Ansible, such as shell, yum, apt, file, and copy. By default, it is the command. Arguments: We should pass values that are required by the module. It can change according to the module used. Username: It specifies the user account in which Ansible can execute commands. Become: It's an optional parameter specified when we want to run operations that need sudo privilege. By default, it becomes false. 1. Parallelism and shell commands You can reboot your company server in 12 parallel forks at the same time. For this, you need to set up the SSHagent for connection. $ ssh-agent bash $ ssh-add ~/.ssh/id_rsa To run reboot for all your company servers in the group, 'abc', in 12 parallel forks: $ ansible abc -a "/sbin/reboot" -f 12 By default, Ansible will run the above ad-hoc commands from the current user account. If you want to change then pass the username in ad-hoc command as follows: $ ansible abc -a "/sbin/reboot" -f 12 -u username 2. File Transfer You can use ad-hoc commands for doing SCP (secure copy protocol) which means lots of files in parallel on multiple machines or servers. CHENNAI CHENNAI INSTITUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY (Autonomous) 15 N32 nirf 175º Rank

--- Page 130 ---
Course Code/Title:CS3V15/Devops Unit:IV Transferring file on many machines or servers $ ansible abc -m copy -a "src = /etc/yum.conf dest = /tmp/yum.conf" Creating new directory $ ansible abc -m file - a "dest = /path/user1/new mode = 888 owner = user group = user1 state = directory" Deleting all directory and files $ ansible abc -m file -a "dest = /path/user1/new state = absent" 3. Managing Packages Ad-hoc commands are available for apt and yum module. Here are the following ad-hoc commands using yum. Below command checks, if the yum package is installed or not, but not update it. $ ansible abc -m yum -a "name = demo-tomcat-1 state = present" Below command checks the package is not installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = absent" And below command checks the latest version of package is installed. $ ansible abc -m yum -a "name = demo-tomcat-1 state = latest" 4. Managing Users and Groups You can manage, create, and remove a user account on your managed nodes with ad-hoc commands. $ ansible all -m user -a "name=foo password =< crypted password here>" $ ansible all -m user -a "name=foo state=absent" 5. Managing Services Ensure a service is started on all the webservers. $ ansible webservers -m service -a "name=httpd state=started" Alternatively, restart a service on all webservers: $ ansible webservers -m service -a "name=httpd state=restarted" CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 nirf 1750 Rank

--- Page 93 ---
Course Code/Title:CS3V15/Devops Unit: III And configure the Jenkins job as follows - General Description In this job we have used GitHub plugin Plain text Preview Discard old builds ? GitHub project This project is parameterized ? Throttle builds Execute concurrent builds if necessary Advanced V Source Code Management None Git ? Repositories ? Repository URL https://github.com/AnurdhaP/MyJavaPrograms.gil Please enter Git repository. Credentials ? - none - Add Advanced CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 18 N3 nirf 1750 Rank

--- Page 94 ---
Course Code/Title:CS3V15/Devops Unit: III Build Triggers Trigger builds remotely (e.g. from scripts) ? Build after other projects are built ? Build periodically ? GitHub hook trigger for GITSem polling ? Poll SCM ? Schedule ? A Do you really mean "every minute" when you say "" "" " ""? Perhaps you meant "H" "" "" to poll once per hour Would last have run at Wednesday, 30 August, 2023 at 4:11:02 pm india Standard Time; would'nest run at Wednesday, 30.A Ignore post-commit hooks ? Build Steps Execute Windows batch command ? Command See the list of available environment variables javas test.java java test. java ----- Advanced V Add build sten Save this configuration. Step 5 : Now if we build this job the output that we see is as follows on Console Output. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 19 N3 nirf 1750 Rank

--- Page 19 ---
Course Code/Title:CS3V15/Devops Unit: I 5. Compliance: By compliance, we mean that certain businesses are required to follow specific regulations. Financial services companies in the United States, for example, must comply with the Sarbanes-Oxley Act, while healthcare, education, and energy companies must comply with HIPAA and other regulations. AWS provides a number of compliance-related features and services, such as data encryption and identity and access management, to help businesses meet these requirements. 6. High-Performance: Interested in delivering your applications quickly and efficiently? Taking advantage of AWS features such as auto-scaling and load balancing will help ensure your applications are always available and running optimally. AWS can help businesses improve their performance by offering a variety of cloud-based services, including Amazon Elastic Compute Cloud (EC2), which provides high-performance computing resources, and Amazon CloudFront, which delivers content quickly and securely to users around the world. Others include machine learning (ML) and analytics services, such as Amazon SageMaker and Amazon Athena. These services provide the tools businesses need to quickly and easily analyze their data for insights. Fast networking in the cloud is also possible with AWS, thanks to its Elastic Load Balancing (ELB) and Amazon Virtual Private Cloud (VPC). With ELB, businesses can balance their workloads across multiple instances for increased performance, while VPC allows businesses to create isolated private networks in the cloud. 7. Developer Tools: Developer tools are designed to make it easier for developers to create, deploy, and manage applications - and AWS provides developers with what they need to build applications quickly and easily. By leveraging developer tools, developers can save time and money by automating tedious tasks. They also benefit from access to AWS's extensive library of pre-built applications that can help them get their projects off the ground quickly. Services such as Amazon Elastic Beanstalk and Amazon CloudFormation can help them automate the process of creating and deploying applications. Other ways developers can improve productivity with AWS include using AWS Code Commit to store and manage source code. 8. Integration: Thanks to its many integrations with other Amazon services, as well as third-party services, AWS makes it easy for businesses to get started with cloud computing. AWS provides a wide range of services that can be easily integrated into existing business infrastructure. This allows businesses to add new features and capabilities without having to make major changes or invest in new hardware or software. For instance, if a business wants to add mobile capabilities to its website, it can take advantage of Amazon's Mobile SDK and Web Services. These tools allow businesses to quickly develop and deploy mobile apps that connect directly with their existing infrastructure. 9. Management Console: The AWS management console is a web-based interface that provides users with a simple way to interact with and manage their AWS resources - essentially a place where you can access and manage everything on the cloud. It provides a graphical view of all the resources associated with an account, as well as tools for creating and configuring new resources. Compared to traditional command- line interfaces, the AWS management console saves time and makes it easier for users to get the most out of their AWS services. Not only that, but your business gets access to 350+ free digital training courses through the AWS Academy, covering topics such as cloud fundamentals, DevOps, security, and big data. This means you can train your employees on how to use AWS, and in turn, help them become more efficient at their jobs. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 19 N32 nirf 1750 Rank

--- Page 20 ---
Course Code/Title:CS3V15/Devops Unit: I 10. Scalability: With an on-demand service, businesses can quickly spin up new servers as needed with just a few clicks. This makes it much easier to scale resources up or down as demand changes, allowing businesses to save costs and maintain performance even during peak periods. For example, if a business is expecting a sudden surge in traffic due to an advertising campaign or seasonal event, they can easily add more capacity to their server infrastructure to handle the increased load. Bru Textiles, a specialty textile company in Belgium, was able to quickly scale its infrastructure by leveraging AWS. Bru Textiles went digital to grow and offer new services. Embracing technology, they brought in digital twin technology to give their customers an idea of the texture and essence of their physical fabrics. AWS Applications · Storage and Backup: Storage and backup are important for any Cloud Computing service. AWS provides you with reliable storage services like Amazon Simple Storage Service to store large- scale data and backup services like AWS Backup to take backups of this data, which is stored in other AWS services. AWS stores the data in three different availability zones so that if one fails, you can still access your data. This makes AWS storage reliable and easily accessible. Therefore, companies with huge application data to store and backup securely can use AWS. · Big Data: One of the biggest challenges faced by companies these days is Big Data. The companies are struggling to store their large amounts of data using traditional methods. With AWS Big Data storage services, they can manage to store their data even if the data limit increases unexpectedly as AWS provides virtually unlimited data storage with scale-in and scale- out options. AWS offers easy access and faster data retrieval as well. For data processing, it offers services like EMR, with which the companies can easily set up, operate, and scale their big data. Therefore, efficiently storing and managing Big Data is among the top AWS applications. · Enterprise IT: AWS is a one-stop solution for any IT business. Many features of it such as secure storage, scalability, flexibility, and elasticity support companies to innovate faster than ever before. Using AWS for IT enterprises makes them profitable in terms of both money and time. As AWS maintains its cloud architecture, it need not waste time and money on professionals to do the same. · Social Networking: Social networking is essential for businesses in the present-day scenario where Digital Marketing is key, and it is easier with AWS. Companies can connect with customers and stakeholders and communicate through social networking sites and develop their business. Services like AWS social networking engine, which is powered by Turn Key GNU/Linux (HVM) AMI stack, are used for performance and scalability to help companies build a suitable social networking site and gain profits. · Mobile Apps: Mobile applications are embedded with day-to-day life. With AWS, you have the facility to create an app in your desired programming language. You can also keep up the applications that are consistently accessible and solid with high compute, storage, database, and application services. You can take advantage of AWS auto-scaling and managed relational database service for the better performance of your apps. · Websites: AWS offers a wide range of website hosting options to create the best website for customers. Its services like Amazon Light sail have everything, such as a virtual machine, SSD- based storage, data transfer, DNS management, and a static IP, to launch a website in such a way that the user can manage the website easily. Amazon EC2, AWS Lambda, Elastic Load Balancing, AWS Amplify, Amazon S3, etc. also help users build reliable and scalable websites. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 147 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: After clicking new repository option, we will have to initialize some things like, naming our project, choosing the visibility etc. After performing these steps click Create Repository button. Create a new repository A repository contains all project files, including the revision history. Already have a project repository elsewhere? Import a repository. Owner Repository name * / Namanbhatia7 - Resume This is going to be name of our project Great repository names are short and memorable. Need inspiration? How about animated-memory? Description (optional) Public Anyone Keep this as public selected ository. You choose who can commit. see this rer Private You choose who can see and commit to this repository. We can add a project description if we want. Skip this step if you're importing an existing repository. Initialize this repository with a README This will let you immediately clone the repository to your comp Tick the README option Add .gitignore: None - Add a license: None ~ Create repository After performing above steps, Click this button Step 3: After clicking the button, we will be directed to below page. Right now the only file we have is a readme file. Namanbhatia7 / Resume o Unwatch - 1 * Star 0 Y Fork 0 <> Code Issues 0 (*) Pull requests 0 ||| Projects 0 BB Wiki Security the Insights # Settings No description, website, or topics provided. Edit Manage topics T 1 commit įº 1 branch > 0 releases 2 1 contributor Branch: master New pull request Create new file Upload files Find File Clone or download TT Namanbhatia7 Initial commit Latest commit 676ac98 now README.md Initial commit now DO README.md Resume UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 7 175" Rank

--- Page 148 ---
Course Code/Title: CS3V15/Devops Unit: V Step 4: Now click on the "Upload files" button. 4 > C https://github.com/avicnotes/html-css-projects ... Search or jump to ... Click to End Session Total: $80.83 Pull requests Issues Marketplace Explore Google" Custom Search @ avicnotes / html-css-projects Unwatch - 1 Star 0 Fork 0 <> Code Issues 1'] Pull requests Actions Projects Wiki O Security ~ Insights ... go main - ¿º 1 branch 0 tags Go to file Add file - { Code About avicnotes screenshots Create new file Upload files 2 24 commits No description, website, or topics provided. README.md Update README.md 2 days ago Readme Screenshot (10).png screenshots 2 days ago Screenshot (11).png screenshots 2 days ago Releases Screenshot (12).png screenshots 2 days ago No releases published Create a new release 19 Screenshot (6).png screenshots 2 days ago accordion.JPG Add files via upload 4 days ago Packages background-dots.png Add files via upload 2 days ago clone-project-1.html Updated navbar items to my original ones 2 days ago index.html Create index.html 's ago No packages published Publish your first package Activate Windows Go to Settings to activate Windows, Environments 1 O Type here to search O W. 11:32 AM 2/1/2021 Step 5:Follow the steps mentioned and click "commitchanges". Drag additional files here to add them to your repository Or choose your files Display_Picture.png Simply drag or add files to be uploaded × My_Resume.html X Styles.css × Commit changes Add initial resume website files Add an optional ended des Give this particular version a name -o- Commit directly to the master branch. Ensure this option is checked !] Create a new branch for this commit and start a pull request. Learn more about pull requests. Commit changes Cancel UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 8

--- Page 77 ---
Unit: III Course Code/Title:CS3V15/Devops 5. Customize Jenkins: ○ Install suggested plugins or select the required plugins manually. ○ Create the first admin user. Configuration 1. Global Configuration: ○ Navigate to Manage Jenkins > Configure System. ○ Set up global environment variables, email notifications, and other system-wide settings. 2. Security Configuration: ○ Configure security settings under Manage Jenkins > Configure Global Security. o Use the built-in user database or integrate with external authentication mechanisms like LDAP. 3. Node Configuration: ○ Jenkins can distribute build loads to multiple nodes. ○ Configure nodes by navigating to Manage Jenkins > Manage Nodes and Clouds. 4. Job Configuration: ○ Create new jobs by clicking New Item on the Jenkins dashboard. o Configure job-specific settings such as source code management, build triggers, and post-build actions. 5. Pipeline Configuration: ○ Jenkins supports pipeline as code using Jenkinsfile. ○ Define your pipeline stages in a Jenkinsfile placed in the root of your project repository. 6. Plugins: ○ Enhance Jenkins functionality by installing plugins from the Manage Jenkins > Manage Plugins section. 7. Backup and Restore: ○ Regularly back up the jenkins_home directory, which contains Jenkins configurations, job configurations, and build history. ○ Use plugins like ThinBackup for automated backups. Ref: https://youtube.com/playlist?list=PL6flErFppaj35spJjPy41-IruDjw2kRV-&si=Vzd9AAMv53xwWB S Ref: https://bit.ly/youtube_jenkins (click here) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 2 N32 nirf 1750 Rank

--- Page 78 ---
Course Code/Title:CS3V15/Devops Unit: III Jenkins Architecture Overview Jenkins is a robust automation server widely used for continuous integration and continuous delivery (CI/CD) processes. Here's a detailed breakdown of the Jenkins architecture and its core components. Core Components of Jenkins Architecture 1. Jenkins Master: ○ Role: The Jenkins master is the central control unit. 0 Functions: Scheduling Jobs: Assigns build jobs to the appropriate agents. Monitoring Agents: Keeps track of the state and health of agents. I Job Execution: Executes build jobs directly if no agents are available. I User Interface: Provides a web-based interface for users to configure jobs, view build results, and manage the Jenkins environment. 2. Build Agents (Nodes): ○ Role: Agents are responsible for executing the build jobs assigned by the master. ○ Types: Agents can run on different operating systems and hardware configurations. ○ Configuration: Static Agents: Permanently assigned to the Jenkins master. . Dynamic Agents: Created and destroyed as needed, often used in cloud environments. ○ Communication: Agents communicate with the master using the Jenkins Remoting protocol. 3. Job Configuration: ○ Types of Jobs: I Freestyle Projects: Basic job type with simple configurations. Pipeline Jobs: Scripted or declarative pipelines that define the entire build process as code. I Multi-Configuration (Matrix) Projects: Allows testing across different environments and configurations. ○ Components: I Source Code Management: Integration with version control systems like Git, SVN, etc. . Build Triggers: Conditions that start the job, such as code commits, scheduled times, or manual triggers. Build Steps: Actions performed during the build, such as compiling code, running tests, and packaging artifacts. . Post-Build Actions: Steps executed after the build, like deploying artifacts, sending notifications, or archiving results. TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 3 N32 nirf 1750 Rank

--- Page 95 ---
Course Code/Title:CS3V15/Devops Unit: III C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha C: \ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>exit e Finished: SUCCESS Step 6 : Now go to GitHub repository and make some changes to your test.java file and commit these changes. BE AnurdhaP Changes in the test java Code Blame 6 lines (6 loc) · 158 Bytes 89 Code 55% faster with GitHub Copilot > public class test { 2 public static void main(String args[]) { for(int 1=1;i <= 5;1++) 3 4 System.out.println("Good Morning Path !!! "); 5 3 6 } Step 7 : Now, if you check on the Jenkins page, it automatically executes the above updated test.java file and creates a build. It is as follows .- C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>javac test. java C:\ProgramData\Jenkins\.jenkins\workspace\MyJavaPrograms>java test.java Good Morning Path !!! Good Morning Pathi !! Good Morning Path !!! Good Morning Path !!! Good Morning Pathill 2. Parameter Plugin Overview: The Parameter Plugin allows Jenkins jobs to accept parameters at build time, enabling dynamic and flexible builds. Key Features: · Parameter Types: Supports various parameter types such as string, choice, boolean, and more. · Default Values: Provides default values for parameters. · Prompt for Parameters: Users are prompted to enter values for parameters when starting a build. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 20

--- Page 96 ---
Course Code/Title:CS3V15/Devops Unit: III Configuration: 1. Install the Plugin: 0 Go to Manage Jenkins > Manage Plugins > Available tab, search for "Parameterized Builds," and install it. 2. Configure Job: ○ In the job configuration, check This project is parameterized. 0 Add parameters of different types (e.g., string, choice) and configure their options and default values. Usage: · Useful for creating jobs that require user input or need to be customized for different build scenarios. A Example Demo Step 1 : Click on New item and create a job. I have created a job by the name Welcome (Refer section 3.5) Step 2 : Click on the existing job and configure it with parameters. Step 3 : Check the option "This project is parameterized". Select the String Parameter. Create some string parameter. I have created a parameter named Username and default value to it. This project is parameterized ? =String Parameter ? Name ? Username Default Value ? Parth Description ? Plain text Preview Trim the string ? Step 4 : Under the Build Steps section, using echo command I tried to display the parameter value CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY - N3 nirf 1750 Rank 21

--- Page 139 ---
Course Code/Title:CS3V15/Devops Unit: IV Create the file, if it does NOT exist (or) absent $ ansible testservers -a "touch /tmp/testfile creates=/tmp/testfile" -i ansible_hosts As Playbook - name: "Validate if a file is present or not present using Ansible Command module" hosts: testservers tasks: - name: "Create a file if it does not exist" command: "touch /tmp/latestfile" args: creates: "/tmp/latestfile" register: createif - name: "Display the file to make sure its created" command: "ls -lrt /tmp/latestfile" register: displayif when: createif is changed - debug: var=displayif.stdout - name: "Remove the file if it exist" command: "rm -rf /tmp/latestfile" args: removes: "/tmp/latestfile" register: removeif In the playbook we execute three tasks, • First one is a create a file if it does not exist • Second to display if the creation is successful • Third one is to delete the file if it exists Example 5: Execute or Run the Script when a file exists or not exists Now for this example let us take something relatable to real world scenario. Like Start the Server instance (or) program if the PID file or LOCK file does not exist Here is the sample playbook for you to get started. I am giving a general playbook here as I do not want to pick some server/technology of my own which you cannot relate to. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 25 N32 nirf 1750 Rank

--- Page 140 ---
Course Code/Title:CS3V15/Devops Unit:IV you can feel free to modify the script to your needs as this is just a template. - name: Start of Stop Server instance based on PID/LOCK file availability hosts: appservers tasks: - name: Start the instance when the PID file is not present become: yes become_user: appuser command: "startserver.sh" args: creates: "/path/to/pid/instance.pid" register: startinst - name: Stop the instance when the lock file is present become: yes become_user: appuser command: "stopserver.sh" args: removes: "/path/to/lockfile/instance.lck" register: stopinst CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 26 N33 nirf 1750 Rank

--- Page 115 ---
Unit: IV Course Code/Title:CS3V15/Devops UNIT IV CONFIGURATION MANAGEMENT USING ANSIBLE Ansible Introduction, Installation, Ansible master/slave configuration, YAML basics, Ansible modules, Ansible Inventory files, Ansible playbooks, Ansible Roles, adhoc commands in ansible. Ansible A SSH ANSIBLE TARGET NODE HOSTS PLAYBOOK Ansible is a powerful DevOps tool designed for automating tasks on remote servers or nodes. In simple terms, it allows you to automate commands and functions on multiple remote machines from a central 'master' node. To illustrate its usefulness, consider a scenario where you need to reboot dozens or even hundreds of remote hosts. You could manually SSH into each one and initiate the reboot, or you can use Ansible to streamline the process, making it efficient and offering a wide range of additional functionalities. In essence, Ansible operates much like the second method mentioned, using SSH (Secure Shell), a secure communication protocol, to control remote nodes in a secure and optimized manner. Ansible Components Ansible Core Components Control node Ansible Inventory Managed node 1 Managed node 2 Managed node 3 1. Control Node - · The central or main node where Ansible is installed. · Used to trigger commands like ansible and ansible-inventory on other nodes. · Acts as the orchestrator for Ansible operations. • TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 1 N33 nirf 1750 Rank

--- Page 116 ---
Course Code/Title:CS3V15/Devops Unit: IV 2. Manage Node- · A remote or slave node where tasks are executed or controlled by Ansible. . These are the servers or devices you want to manage or automate. 3. Inventory - · A list of managed node IPs and configurations. · Logically organized, typically using file formats like YAML or INI. · Created on the control node to describe the deployment of hosts to Ansible. Ansible Additional Components Ansible Architecture CMDB Users Public / Private Cloud Hosts ... ... Inventory .. Modules 000 Ansible Playbook API Plugins Networking www.educba.com 1. Ad-Hoc Commands - . These are one-off commands that you can execute using the ansible command. · Useful for quick tasks or tests on remote nodes. 2. Plugins - · Plugins are pieces of code that extend Ansible's core functionality. · Ansible uses a plugin architecture for flexibility and expandability. · Examples include connectivity plugins for establishing connections and cache plugins. · Learn more about Ansible plugins. 3. Module- · Modules are built-in functions that can be used to perform various tasks. . They eliminate the need to write custom code for common operations. · Examples include modules for package management (apt, yum), service management, and more. CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) CHENNAI INSTITUTE . TECHNOLOGY 2 N33 nirf 1750 Rank

--- Page 51 ---
Course Code/Title:CS3V15/Devops Unit: II 7. Maven Profiles: A Build profile is a set of configuration values, which can be used to set or override default values of Maven build. Using a build profile, you can customize build for different environments such as Production v/s Development environments. Types of Build Profile Build profiles are majorly of three types. Type Where it is defined Per Project Defined in the project POM file, pom.xml Per User Defined in Maven settings xml file (%USER_HOME%/.m2/settings.xml) Global Defined in Maven global settings xml file (%M2_HOME%/conf/settings.xml) Profile Activation A Maven Build Profile can be activated in various ways. · Explicitly using command console input. · Through maven settings. · Based on environment variables (User/System variables). · OS Settings (for example, Windows family). · Present/missing files. Now, under src/main/resources, there are three environment specific files - Sl.No. File Name & Description env.properties 1 default configuration used if no profile is mentioned. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] INSTITUTE TECHNOLOGY 10 N33 nirf 1750 Rank

--- Page 52 ---
Course Code/Title:CS3V15/Devops Unit: II env.test.properties 2 test configuration when test profile is used. 3 env.prod.properties production configuration when prod profile is used. Explicit Profile Activation In the following example, we will attach maven-antrun-plugin:run goal to test the phase. This will allow us to echo text messages for different profiles. We will be using pom.xml to define different profiles and will activate profile at command console using maven command. Assume, we've created the following pom.xml in C:\MVN\project folder. <project xmlns = "http://maven.apache.org/POM/4.0.0" xmlns:xsi = "http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation = "http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"> <modelVersion>4.0.0</modelVersion> <groupId>com.companyname.projectgroup</groupId> <artifactId>project</artifactId> <version>1.0</version> <profiles> <profile> <id>test</id> <build> <plugins> <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-antrun-plugin</artifactId> <version>1.1</version> <executions> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 11 N33 nirf 1750 Rank

--- Page 125 ---
Course Code/Title:CS3V15/Devops Unit: IV This command executes the specified playbook while prompting for the necessary privilege escalation password. Output and Result: Activities Terminal - Nov 2 13:00 . auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook-eg2.yaml PLAY [Install Nginx] TASK [Gathering Facts] Total: [vnet]: FAILEDT .. ['msg": "Missing sudo password') PLAY RECAP : ok=0 changed=0 unreachable=0 Fatled =! skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory. yaml playbook-eg2.yaml - - ask-becone-pass BECOME password: PLAY [Install Nginx] TASK [Gathering Facts] ok: [vri01] TASK [Install Nginx using apt] ok: [vri01] PLAY RECAP * VMD1 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ [] As shown in the example image above, the first command encountered an error due to the absence of the -ask-become-pass flag. This flag is essential when your task requires elevated privileges, as it prompts Ansible to request the sudo password for authentication. The corrected command includes the -ask-become-pass flag, ensuring that the necessary privileges are obtained before executing the task. This is particularly important when working with tasks that require administrative access, such as package installations or system configurations. Note: In this blog, we primarily focus on the core and fundamental components of Ansible, providing an introductory overview and understanding of its key concepts. While Ansible offers advanced features such as "Collections," "Ansible Galaxy," "Roles," "Dynamic Inventory," and "Custom Modules and Plugins," we recognize that these topics are extensive and may require separate dedicated discussions. For the purpose of this blog, we aim to establish a strong foundation of Ansible's core components. If you're interested in diving deeper into these advanced topics, we recommend exploring Ansible's official documentation and additional resources dedicated to each subject. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 11 N32 nirf 1750 Rank

--- Page 126 ---
Course Code/Title:CS3V15/Devops Unit:IV Understanding YAML Ansible uses YAML syntax for expressing Ansible playbooks. This chapter provides an overview of YAML. Ansible uses YAML because it is very easy for humans to understand, read and write when compared to other data formats like XML and JSON. Every YAML file optionally starts with " --- " and ends with " ... ". In this section, we will learn the different ways in which the YAML data is represented. key-value pair YAML uses simple key-value pair to represent the data. The dictionary is represented in key: value pair. Note - There should be space between : and value. Example: A student record --- #Optional YAML start syntax james: name: james john rollNo: 34 div: B sex: male ... #Optional YAML end syntax Abbreviation You can also use abbreviation to represent dictionaries. Example James: {name: james john, rollNo: 34, div: B, sex: male} Representing List We can also represent List in YAML. Every element(member) of list should be written in a new line with same indentation starting with "- " (- and space). Example countries: - America - China - Canada - Iceland CHENNAI INSTITUTE OF TECHNOLOGY CHENNAI INSTITUTE . TECHNOLOGY (Autonomous) 12 N33 nirf 1750 Rank

--- Page 31 ---
Course Code/Title:CS3V15/Devops Unit: I our work simpler. The pricing of Azure is also simpler and cost-effective. Popularly termed as "Pay As You Go", which means how much you use, pay only for that. Microsoft Azure Used for · Deployment Of applications: You can develop and deploy the application in the azure cloud by using the service called Azure App Service and Azure Functions after deploying the applications end users can access it. · Identity and Access Managment: The application and data which is deployed and stored in the Microsoft Azure can be secured with the help of Identity and Access Managment. It's commonly used for single sign-on, multi-factor authentication, and identity governance. · Data Storage and Databases: You can store the data in Microsoft azure in service like blob storage for unstructured data, table storage for NoSQL data, file storage, and Azure SQL Database for relational databases. The service can be scaled depending on the amount of data we are getting. · DevOps and Continuous Integration/Continuous Deployment (CI/CD): Azure DevOps will provide some tools like ncluding version control, build automation, release management, and application monitoring Following are some of the services Microsoft Azure offers: 1. Compute: Includes Virtual Machines, Virtual Machine Scale Sets, Functions for serverless computing, Batch for containerized batch workloads, Service Fabric for microservices and container orchestration, and Cloud Services for building cloud-based apps and APIs. 2. Networking: With Azure, you can use a variety of networking tools, like the Virtual Network, which can connect to on-premise data centers; Load Balancer; Application Gateway; VPN Gateway; Azure DNS for domain hosting, Content Delivery Network, Traffic Manager, ExpressRoute dedicated private network fiber connections; and Network Watcher monitoring and diagnostics 3. Storage: Includes Blob, Queue, File, and Disk Storage, as well as a Data Lake Store, Backup, and Site Recovery, among others. 4. Web + Mobile: Creating Web + Mobile applications is very easy as it includes several services for building and deploying applications. 5. Containers: Azure has a property that includes Container Service, which supports Kubernetes, DC/OS or Docker Swarm, and Container Registry, as well as tools for microservices. 6. Databases: Azure also included several SQL-based databases and related tools. 7. Data + Analytics: Azure has some big data tools like HDInsight for Hadoop Spark, R Server, HBase, and Storm clusters 8. AI + Cognitive Services: With Azure developing applications with artificial intelligence capabilities, like the Computer Vision API, Face API, Bing Web Search, Video Indexer, and Language Understanding Intelligent. 9. Internet of Things: Includes IoT Hub and IoT Edge services that can be combined with a variety of machine learning, analytics, and communications services. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 31 N32 nirf 1750 Rank

--- Page 32 ---
Unit: I Course Code/Title:CS3V15/Devops 10. Security + Identity: Includes Security Center, Azure Active Directory, Key Vault, and Multi-Factor Authentication Services. 11. Developer Tools: Includes cloud development services like Visual Studio Team Services, Azure DevTest Labs, HockeyApp mobile app deployment and monitoring, Xamarin cross- platform mobile development, and more. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 32 N32 nirf 1750 Rank

--- Page 105 ---
Course Code/Title:CS3V15/Devops Unit: III How to install copy artifact plugin ? Step 1 : Start the Jenkins tool by entering the login and password. Step 2 : Click on Manage Jenkins. Click on Plugins. Step 3 : In the search window, type the name copy Artifact. Step 4 : The plugin name will be displayed if it is not already installed. Select it. Name Adde a build atop to copy artifacts from another project. Thủa plugin le up for adoption! We are looking for naw maintainent. Velt our Adopt a Plugin intuitive for mort information. then click on Install. The plugin gets installed. Example Demo · Let us discuss how to use Copy Artifact plugin. Step 1 : Create a new Job by clicking on item New Item. Give the some name to your job. I have given the name App1 and selected the Freestyle Project. Step 2: Give suitable description in the description box. Step 3 : Under the source code management, click on Git option and specify the repository URL of GitHub. Source Code Management None Git ? Repositories 1 Repository URL + ·hong · CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 30 N3 nirf 1750 Rank

--- Page 106 ---
Unit: III Course Code/Title:CS3V15/Devops Step 4 : Under the Build Steps section, choose the Execute Windows batch command option. And type the javac and java commands. Build Steps = Execute Windows batch command Command See the list of available environment variables javac test.java java test Advanced Add bulld stop * Step 5 : Click in Apply and Save. Step 6 : Select the Build Now option. On successful built, the test, class file gets generated in the Jenkins workspace of the App1 job. C:\ProgramData\Jenkins\.jenkins\workspace\Appl>javac test.java C:\ProgramData\Jenkins\.jenkins\workspace\Appi>java test Good Morning Pathill Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! Good Morning Path !!! C:\ProgramData\Jenkins\.jenkins\workspace\Appl>exit @ Finished: SUCCESS Step 7 : Now we have to copy test.class file from the workspace of Appl to another job. For that' purpose we will create another Job named App2 by clicking New Item. Select the Freestyle Project. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY - TECHNOLOGY 31 N3 nirf 1750 Rank

--- Page 155 ---
10. When you're ready to make changes to your pipeline, select it in the Pipelines page, and then Edit the azure-pipelines.yml file. View and manage your pipelines You can view and manage your pipelines by choosing Pipelines from the left-hand menu to go to the pipelines landing page. Azure DevOps : FabrikamFiber 1 Pipelines 0 Search F FabrikamFiber + Pipelines New pipeline Overview Recent All Runs Y Filter pipelines Boards Repos Pipelines Pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... La 32m ago & Manually triggered 8º main & 42s Environments Releases FabrikamFiber #20191209.3 · Set up CI with Az ... La 1h ago & Manually triggered & main 1m 13s Library Task groups *** Deployment groups From the pipelines landing page you can view pipelines and pipeline runs, create and import pipelines, manage security, and drill down into pipeline and run details. Choose Recent to view recently run pipelines (the default view), or choose All to view all pipelines. Pipelines New pipeline : Recent All Runs Filter pipelines Recently run pipelines Pipeline Last run pipelines-dotnet-core #20191209.2 · Set up CI with Az ... & Manually triggered & main Là 36m ago ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Az ... Là 1h ago 1m 13s & Manually triggered & main 15

--- Page 156 ---
Select a pipeline to manage that pipeline and view the runs. Select the build number for the last run to view the results of that build, select the branch name toview the branch for that run, or select the context menu to run the pipeline and perform other management actions. Recently run pipelines Pipeline Last run #20191209.2 · Set up CI with Azure Pip ... > pipelines-dotnet-core 8 Manually triggered 12º main Là 1h ago * ₾ 425 FabrikamFiber #20191209.3 · Set up CI with Azure Pip ... 試 2h 8 Manually triggered 8 main Edit in Run pipeline Manage security Rename/move Delete Select Runs to view all pipeline runs. You canoptionally filter the displayed runs. Pipelines Recent All Runs Filter by keywords State V Repository V New pipeline Requested forV Tags X All pipeline runs Description Stages Set up CI with Azure Pipelines & #20191209.2 on pipelines-dotnet-core & main d4964 .. Set up CI with Azure Pipelines #20191209.1 on pipelines-dotnet-core åº main d4964 ... Set up CI with Azure Pipelines & #20191209.3 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines & #20191209.2 on FabrikamFiber { main 2b4b23c Set up CI with Azure Pipelines #20191209.1 on FabrikamFiber & main 2b4b23c La 44m ago 42s La 53m ago ₾ 46s Là 1h ago 0 1m 13s Ld 1h ago 1m 555 Lo 1h ago 1m 6s 16

--- Page 89 ---
Course Code/Title:CS3V15/Devops Unit: III Key Features: · Source Code Management: Configures Jenkins to use Git repositories as the source code for jobs. Example Demo Step 1 : Create a simple Java program. I have created a folder named MyJavaPrograms and inside it created a simple Java program as follows - e.g., SSH test.java public class test { public static void main(String args[I) { for(int i=1;i <= 5;1++) System.out.printin("Welcome Anuradha"); } Plugin," Step 2 : Open the command prompt, switch to that folder and execute the above Java program. It is illustrated by following screenshot. 1 C:\Windows\System32\cmd.e X + Microsoft Windows [Version 10.0.22621.2134] (c) Microsoft Corporation. All rights reserved. 0 X before E:\MyJavaPrograms>javac test. java E: \MyJavaPrograms>java test Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha Welcome Anuradha E: \MyJavaPrograms> Step 3 : Now we will create a Git repository and push this repository on GitHub. First of all we will initialise the Git repository my using git init command. CHENNAI INSTITUTE TECHNOLOGY 14 - N32 nirf 1750 Rank CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt)

--- Page 90 ---
Unit: III Course Code/Title:CS3V15/Devops C:\Windows\System32\cmd.e x X E:\MyJavaPrograms>git init Initialized empty Git repository in E:/MyJavaPrograms/.git/ E:\MyJavaPrograms>git status On branch master No commits yet Untracked files: (use "git add <file> ... " to include in what will be committed) test.class test. java nothing added to commit but untracked files present (use "git add" to track) E:\MyJavaPrograms> then add the java and class files to git repository - C:\Windows\System32\cmd.e x 0 X E:\MyJavaPrograms>git add . E:\MyJavaPrograms>git status On branch master No commits yet Changes to be committed: (use "git rm -- cached <file> ... " to unstage) new file: test.class new file: test. java E: \MyJavaPrograms> Now we will commit the changes - C:\Windows\System32\cmd.e X E:\MyJavaPrograms>git commit -m "First Commit for Java program" [master (root-commit) 4cb04db] First Commit for Java program 2 files changed, 6 insertions(+) create mode 100644 test.class create mode 100644 test. java E:\MyJavaPrograms> CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) - INSTITUTE . TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 159 ---
Choose a job to see the steps for that job From the < Jobs in run #20191 ... FabrikamFiber Build O Build Build 40s Initialize job 1s 2 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 Duration: 40s 6 Pool: Azure Pipelines Image: Ubuntu-16.04 7 > Job preparation parameters Checkout 3s CmdLine 25 Component Detect 32s Post-job: Checkout <1s Finalize Job <1s Deploy DeployWeb 10s Finalize build Report build status <1s steps view, you can review the status and details of eachstep. From the Moreactions you can toggle timestamps or view a raw log of all steps in the pipeline. Build 9 Pool: Azure Pipelines 2 Image: Ubuntu-22.04 3 Agent: Hosted Agent 4 Started: Today at 1:13 PM 5 View job raw log Toggle timestamps 6 Duration: 40s Job preparation parameters 10

--- Page 160 ---
Cancel and re-run a pipeline If the pipeline is running, you can cancel it by choosing Cancel. If the run has completed, you can re-run the pipeline by choosing Run new. Pipeline run more actions menu: #20191210.3 Update azure-pipelines.yml for Azure Pip ... on FabrikamFiber Cancel Stages Jobs € Build C Deploy 0/1 compléted 48s Not started Build 48s Cancel From the More actions menu you can download logs, add tags, edit the pipeline, delete the run, and configure retention for the run. cure Pipe ... Run new Download logs Add tags Edit pipeline View retention leases Retain Delete 20

--- Page 133 ---
Course Code/Title:CS3V15/Devops Unit:IV Ansible Roles Roles provide a framework for fully independent or interdependent collections of files, tasks, templates, variables, and modules. The role is the primary mechanism for breaking a playbook into multiple files. This simplifies writing complex playbooks and makes them easier to reuse. The breaking of the playbook allows you to break the playbook into reusable components. Each role is limited to a particular functionality or desired output, with all the necessary steps to provide that result either within the same role itself or in other roles listed as dependencies. Roles are not playbooks. Roles are small functionality that can be used within the playbooks independently. Roles have no specific setting for which hosts the role will apply. Top-level playbooks are the bridge holding the hosts from your inventory file to roles that should be applied to those hosts. Creating a Role The directory structure for roles is essential to creating a new role, such as: Role Structure The roles have a structured layout on the file system. You can change the default structured of the roles as well. For example, let us stick to the default structure of the roles. Each role is a directory tree in itself. So the role name is the directory name within the /roles directory. $ ansible-galaxy -h Usage ansible-galaxy[delete|import|info|init|install|list|login|remove|search|setup][ -- help] [options] ... Options o -h: (help) it shows this help message and exit. o -v: (verbose) Verbose mode (-vvv for more, -vvvv to enable connection debugging). o -- version: it shows program version number and exit. Roles are stored in separate directories and have a particular directory structure CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY 19 N33 nirf 1750 Rank

--- Page 134 ---
Course Code/Title:CS3V15/Devops Unit: IV Typical structure of ansible role myweb defaults L main.yml files · handlers L main.yml meta. L main.yml README.md tasks L_ main.yml templates tests inventory test.yml vars L main.yml · defaults : It stores the default variable for the role. For example the default port number is http is 8080, then it can be stored in defaults. · files : This folder contains the files required to transfer or deploy to the target machines. · handlers : It contains handlers, which may be used by some role. · meta : It defines some data or information about the role. · tasks : It contains the main list of tasks to be executed by the role. These tasks might be defined in separate files as per the functionalities. · templates : It contains the templates which can be deployed via a role. Ansible Module Ansible Command module is used to execute commands on a remote node. The Command module, is used mostly to run simple Linux commands on a remote node/server which is part of a host group or Stand alone server mentioned in the host group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 20 N32 nirf 1750 Rank

--- Page 141 ---
Course Code/Title: CS3V15/Devops Unit: V UNIT V - BUILDING DEVOPS PIPELINES USING AZURE Create Github Account, Create Repository, Create Azure Organization, Create a new pipeline, Build a sample code, Modify azure-pipelines.yaml file Github Account Creation What is Github? GitHub is a code hosting platform for collaboration and version control. GitHub lets you and others work together on projects fromanywhere. Github is owned by Microsoft, provides access to public(free) and private(paid) repositories. Stepsto create Github Account: Join GitHub · GitHub × + < C https://github.com/join f Facebook - Log In o ... Twitter G Google f Facebook YouTub Product v Solu wiki How Step 1:Go to https://github.com/join in a web browser. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 1 175" Rank

--- Page 142 ---
Course Code/Title: CS3V15/Devops Unit: V Step 2: Enter your personal details. In addition to creating a username and entering an email address, you'll also have to create a password. Your password must be at least 15 characters in length or at least 8 characters with at least one number and lowercase letter. Join GitHub First, let's create your user account Username * wikihowneveconcepts Email address * Password * .......... Make sure it's at least 15 characters OR at least 8 characters including a number and a lowercase letter. Learn more. Email preferences Send me occasional product updates, announcements, and offers. Verify your account wiki How Step 3: Click Verify to start the verification puzzle. The instructions vary by puzzle, so just follow the on-screen instructions to confirm that you are a human. A green checkmark will appear after completing the puzzle. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 175" Rank 2

--- Page 149 ---
Course Code/Title: CS3V15/Devops Unit: V Step 6: Now you will see that all of our files uploaded in our github. Namanbhatia7 Add initial resume website files Latest commit f47be5b 1 minute ago Display_Picture.png Add initial resume website files 1 minute ago My_Resume.html Add initial resume website files 1 minute ago README.md Initial commit 15 minutes ago Styles.css Add initial resume website files 1 minute ago Branch: · A GitHub branch is used to work with different versions of a repository at the same time. · By default a repository has a master branch (aproduction branch). · Any other branch is a copy of the master branch (as itwas at a point in time). · New Branches are for bug fixes and feature work separate from the master branch. When changes are ready, they can be merged into the master branch. If youmake changes to the master branch while working on a new branch, these updates can be pulled in. Commits: At GitHub, changes are called commits. Each commit (change) has a description explaining why achange was made. Pull Requests : . Pull Requests are the heart of GitHub collaboration. · With a pull request you are proposing that your changesshould be merged (pulled in) with the master. · Pull requests show content differences, changes, additions, and subtractions in colors (green and red). · As soon as you have a commit, you can open a pull request and start a discussion, even before the code isfinished. Git: · Git was created by Linus Torvalds in 2005 to develop Linux Kernel · Git is an open-source distributed version control system. It is designed to handle minor to major projectswith high speed and efficiency. . It is developed to co-ordinate the work among the developers. The version control allows us to track andwork together with our team members at the same workspace. UnENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 9 175" Rank

--- Page 150 ---
CREATE AZURE ORGANIZATION AZURE: Microsoft Azure, often referred to as Azure cloud computing platform run by Microsoft. It offers access, management, and the development of applications and services through global data centers It also provides a range of capabilities, including software as a service (SaaS), platform as a service, and infrastructure as a service (IaaS). It was officially launched as Windows Azure in February 2010 and later renamed Microsoft Azure on March 25, 2014 Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. Prerequisites: To plan your organizational structure. Microsoft accounts or authenticate users with Microsoft Entra ID. For more information, see Choosing your organization administrator account type. Create an organization: 1. Sign in to Azure DevOps. 2. Select New organization 3. Confirm information, and then select Continue 10

--- Page 123 ---
Course Code/Title:CS3V15/Devops Unit: IV Ad-Hoc Limitations: While ad-hoc commands are ideal for single, immediate tasks, they are not designed for complex, multi- step automation. Playbooks come to the rescue when tasks have interdependencies and need to be executed in a coordinated manner. Readability and Reusability: Playbooks are authored in YAML, a human-readable and straightforward format. This not only makes them easy to write but also facilitates sharing and collaboration. You can reuse playbooks across various scenarios, saving time and effort. Conditional and Looping Logic: Playbooks offer advanced features, including conditional statements and looping, that allow you to adapt automation to different situations. This flexibility makes playbooks versatile and capable of handling a wide range of automation needs. In essence, playbooks are your tool of choice when automation tasks become multi-faceted and require a structured and logical approach. They provide the power to streamline and automate complex workflows with precision and efficiency. Example: Ping All Inventory Devices To illustrate the use of a playbook, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: playbook.yaml 1 --- 2 -name : My First Play #Name of play 3 hosts: virtualmachines #defining host can be all 4 tasks: 5 -name:Ping My Hosts #Name of task 6 ansible.builtin.ping: # can also write ping To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 9 N32 nirf 1750 Rank

--- Page 124 ---
Course Code/Title:CS3V15/Devops Unit: IV Output and Result: Activities Terminal - Nov 2 12:29 . O auriga@auriga-Latitude-E7470: - /Desktop/ansible X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-playbook -i inventory.yaml playbook.eg1.yaml PLAY [My First Play] TASK [Gathering Facts] ok: [ vri01] TASK [Ping Devices] ok: [vm01] PLAY RECAP vn01 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 auriga@auriga-Latitude-E7470 :- /Desktop/ansible$[] rescued=0 ignored=0 X As you can see there is gathering facts task which we didn't create is running it is the default task which ping the connection. Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: playbook.yaml 1 --- 2- name: Install Nginx 3 hosts: all 4 become: yes 5 tasks: 6 - name: Install Nginx using apt 7 apt: 8 name: nginx 9 state: present To run a playbook, use the following command: 1ansible-playbook -i inventory.yaml playbook.yaml -- ask-become-pass CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 10 N33 nirf 1750 Rank

--- Page 119 ---
Course Code/Title:CS3V15/Devops Unit: IV · Groupings in inventories enable you to organize your infrastructure, allowing you to target specific subsets of nodes based on your needs. For instance, when you need to perform tasks in a particular location, groupings become essential. Creating Inventories There are two primary methods to create an inventory on the control node: using a YAML file or an INI file. 1. INI Inventory Example (inventory.ini): 1[virtualmachines] 2host1 ansible_host:192.168.0.1 3192.168.0.2 2. In this INI file, it captures the IP addresses of managed nodes. The default username used for SSH connections is typically the working user of your control node. 3. ansible_host: This parameter is used to specify the IP address, although you can write the IP directly. 4. Inventory.yaml 1 --- 2virtualmachines: # Define a class name to represent a particular group of devices. 1 hosts: # Define the hosts. 2 vm01: # Define a parent name to indicate a class of parent type. 3 ansible_host: 192.168.0.1 # Specify the IP address. 4 http_port: 80 # (optional) Define the HTTP port. 5 ansible_user: auriga # (optional) Provide the username if it's different from the control node's user. o ansible_user: Use this to specify a different user if the managed node's user is not the same as the control node's user. o http_port: Specify a port if necessary. In real-world scenarios, inventory files can be more complex, reflecting the diverse and extensive infrastructure of organizations. You can explore more configurations here. Verifying Your Inventory After creating your inventory, it's a good practice to verify its correctness. This step ensures that the inventory is correctly structured and accessible to Ansible. To do this, use the following command: 1ansible-inventory -i inventory.yaml -list · - i: Indicates the inventory file you want to use. . - list: Requests the listing of the inventory content. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 5 N33 nirf 1750 Rank

--- Page 120 ---
Course Code/Title:CS3V15/Devops Unit:IV output: auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible-inventory ·i inventory.yaml -- list "_meta": { "hostvars": { "vm01": "ansible_host": "192.168.0.208", "ansible_user"; "au" "children": [ "ungrouped" "ungrouped": { "hosts": "vn01 1 By following these steps, you have effectively set up and verified your inventory, ensuring a solid foundation for your Ansible automation. Using Ansible Ad-Hoc Commands Introduction: In Ansible, ad-hoc commands are your go-to solution for executing quick, one-off tasks on remote nodes. They provide a straightforward and efficient way to interact with managed nodes without the need for creating full-fledged playbooks. Ad-hoc commands are particularly useful when you need immediate results without the overhead of playbook development. When to Use Ad-Hoc Commands: Ad-hoc commands are best suited for scenarios where the task at hand is simple and doesn't require the complexity of a playbook. They are perfect for tasks like system health checks, package installation, service management, or any other single-operation job. Ad-Hoc Command Syntax: Ad-hoc commands follow a specific syntax that comprises various components, each serving a unique role: . Target Group: This is the group of hosts you intend to target with the ad-hoc command. · Module (-m): Specifies the module to execute. Modules are Ansible's building blocks for performing tasks, and they can range from basic operations like "ping" to more advanced tasks such as package management. · Inventory File (-i): Indicates the location of your inventory file, which defines the list of target devices. Example: Ping All Inventory Devices To illustrate the use of ad-hoc commands, let's consider a simple task: pinging all devices listed in your inventory. Here's the command: 1ansible virtualmachines -m ping -i inventory.yaml virtualmachines is the target group. In this case, it could be any group, or you can use all to target all devices in your inventory. CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 6 N32 nirf 1750 Rank

--- Page 137 ---
Course Code/Title:CS3V15/Devops Unit:IV I Example 2: Get the Hostname and Version of remote servers with UNAME we have used the command module and executing an uname -a command as AD-HOC command $ ansible testservers -m command -a "uname -a" -i ansible hosts As Playbook - name: Check the remote host Hostname, Version, Distribution with UNAME hosts: testservers tasks: - name: Execute the UNAME command register: unameout command: "uname -a" - debug: var: unameout.stdout lines I Example 3: Check the Disk Usage of Remote server To get the disk usage, we are using df -h , here -h is human readable As Ad Hoc command $ ansible testservers -m command -a "df -h" -i ansible hosts As Playbook - name: Check the disk usage of all the file system in the remote servers hosts: testservers tasks: - name: Execute the df command register: dfout command: "df -h" - debug: var: dfout.stdout lines CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 23 N32 nirf 1750 Rank

--- Page 138 ---
Course Code/Title:CS3V15/Devops Unit:IV Example 4: Restart Apache Server using Ansible Command Module So far, In all the sample we have seen we are using one application server and one web server. Totally two servers. But in this example, we need to limit our execution only to the web server as we are going to check the restart the apache web server. this is done using -- limit parameter As Ad hoc command $ ansible testservers -m command -a "httpd -k restart" -i ansible_hosts -b - limit As Playbook - name: restart apache web server hosts: testservers tasks: - name: restartapache register: httpdresout become: yes command: "httpd -k restart" when: ansible_hostname == "mwiweb02" - debug: var: httpdresout.stdout_lines I Example 5: Execute a command when a file exists or not exists There are two most useful parameters in ansible command module such as removes and creates removes - used to tell ansible to Execute the command only if the file exist creates - used to tell ansible to Execute the specified command only if the file does not exist Here we are going to do a very simple file creation and removal based on the file availability or existence. In General using the ansible command module, This method of creating and removing the file is not recommended as Ansible has a dedicated module named file to do the same effortlessly. As Ad hoc command Remove the file, if it does exist (or) present $ ansible testservers -a "rm -rf /tmp/testfile removes=/tmp/testfile" -i ansible_hosts CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 24 N33 nirf 1750 Rank

--- Page 15 ---
Course Code/Title:CS3V15/Devops Unit: I which is a mechanism to control the usage of resources or capacity. 2) Code: Many good practices such as Git enables the code to be used, which ensures writing the code for business, helps to track changes, getting notified about the reason behind the difference in the actual and the expected output, and if necessary reverting to the original code developed. The code can be appropriately arranged in files, folders, etc. And they can be reused. 3) Test: The application will be ready for production after testing. In the case of manual testing, it consumes more time in testing and moving the code to the output. The testing can be automated, which decreases the time for testing so that the time to deploy the code to production can be reduced as automating the running of the scripts will remove many manual steps. 4) Plan: DevOps use Agile methodology to plan the development. With the operations and development team in sync, it helps in organizing the work to plan accordingly to increase productivity. 5) Monitor: Continuous monitoring is used to identify any risk of failure. Also, it helps in tracking the system accurately so that the health of the application can be checked. The monitoring becomes more comfortable with services where the log data may get monitored through many third-party tools such as Splunk. 6) Deploy: Many systems can support the scheduler for automated deployment. The cloud management platform enables users to capture accurate insights and view the optimization scenario, analytics on trends by the deployment of dashboards. 7) Operate: DevOps changes the way traditional approach of developing and testing separately. The teams operate in a collaborative way where both the teams actively participate throughout the service lifecycle. The operation team interacts with developers, and they come up with a monitoring plan which serves the IT and business requirements. 8) Release: Deployment to an environment can be done by automation. But when the deployment is made to the production environment, it is done by manual triggering. Many processes involved in release management commonly used to do the deployment in the production environment manually to lessen the impact on the customers. DevOps Lifecycle DevOps defines an agile relationship between operations and Development. It is a process that is practiced by the development team and operational engineers Learning DevOps is not complete without understanding the DevOps lifecycle phases. The DevOps lifecycle includes seven phases as given below: CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 15 N32 nirf 1750 Rank

--- Page 16 ---
Unit: I Course Code/Title:CS3V15/Devops together from beginning to the final stage of the product. Development Operations Integration Deployment DevOps Lidecycle Feedback 2) Continuous Integration Testing Monitering 1) Continuous Development This phase involves the planning and coding of the software. The vision of the project is decided during the planning phase. And the developers begin developing the code for the application. There are no DevOps tools that are required for planning, but there are several tools for maintaining the code. This stage is the heart of the entire DevOps lifecycle. It is a software development practice in which the developers require to commit changes to the source code more frequently. This may be on a daily or weekly basis. Then every commit is built, and this allows early detection of problems if they are present. Building code is not only involved compilation, but it also includes unit testing, integration testing, code review, and packaging. The code supporting new functionality is continuously integrated with the existing code. Therefore, there is continuous development of software. The updated code needs to be integrated continuously and smoothly with the systems to reflect changes to the end-users. Commit Build O O Test O Stage Deploy Dev/QA O O > Continuous Integration/Delivery Development </> git Code Commit = production server. 3) Continuous Testing Production Jenkins is a popular tool used in this phase. Whenever there is a change in the Git repository, then Jenkins fetches the updated code and prepares a build of that code, which is an executable file in the form of war or jar. Then this build is forwarded to the test server or the This phase, where the developed software is continuously testing for bugs. For constant testing, automation testing tools such as TestNG, JUnit, Selenium, etc are used. These tools allow QAs to test multiple code- bases thoroughly in parallel to ensure that there is no flaw in the functionality. In this phase, Docker Containers can be used for simulating the test environment. It may occur in the form of documentation files or maybe produce large-scale data about the application parameters when it is in a continuous use position. The system errors such as server not reachable, low memory, etc are resolved in this phase. It maintains the security and availability of the service. Automation testing saves a lot of time and effort for executing the tests instead of doing this manually. Apart from that, report generation is a big plus. The task of evaluating the test cases INSTITUTE TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) 16 N33 narf 1750 Rank

--- Page 13 ---
Course Code/Title:CS3V15/Devops Unit: I 2024. Terraform provides the agility and scalability required to keep up with the dynamic demands of modern applications. Terraform's importance lies in its ability to bring consistency, version control, and automation to infrastructure operations, thereby reducing manual errors, streamlining DevOps workflows, and facilitating applications' rapid and reliable deployment in an increasingly complex and cloud-centric environment. As organizations adopt cloud-native technologies, Terraform remains essential to ensure efficient and consistent infrastructure management. 25. Phantom Phantom enhances security automation and incident response capabilities. In today's rapidly evolving threat landscape, organizations face a constant barrage of cybersecurity incidents, and the ability to respond swiftly and effectively is necessary. It provides a platform for automating security workflows, from detecting and investigating potential threats to orchestrating responses and mitigating risks. Phantom's importance lies in its capacity to reduce response times, increase consistency in incident handling, and free up manual resources from repetitive tasks. With the growing complexity of cyber threats, Phantom empowers security teams to defend against attacks and safeguard critical assets proactively. 26. Nagios Nagios, an open-source monitoring and alerting system, remains vital due to its enduring significance in maintaining the reliability and performance of IT infrastructure and applications. Organizations increasingly rely on complex systems and services. Nagios plays a crucial role by providing real-time monitoring and alerting capabilities, allowing IT teams to detect and address issues before they impact users or cause system outages. Its versatility, extensibility, and support for both on-premises and cloud environments make Nagios a valuable tool for ensuring critical systems' availability, stability, and security, aligning perfectly with the demands of modern IT operations and DevOps practices. 27. Vagrant Vagrant continues to play a crucial role in software development and DevOps. It is a tool that simplifies creating and managing reproducible development environments. Its importance lies in its ability to provide developers and DevOps teams with a consistent and isolated environment for software development, testing, and deployment. With the ever-evolving complexity of software stacks, dependencies, and infrastructure configurations, Vagrant remains essential in ensuring these environments are easily shareable, scalable, and maintainable. It allows developers to work seamlessly across various operating systems and provides a standardized setup that minimizes compatibility issues. 28. Sentry Sentry plays a critical role in modern software development and DevOps practices. With software applications' increasing complexity and scale, identifying and addressing errors and issues has become crucial. Sentry is vital because it provides real-time error tracking and monitoring, allowing development teams to proactively detect and diagnose issues, whether they occur in production or during development. Its importance is minimizing downtime, improving user experience, and maintaining software systems' overall health and reliability. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 13 N32 nirf 1750 Rank

--- Page 14 ---
Course Code/Title:CS3V15/Devops Unit: I 29. Gradle Gradle continues to be a vital tool in software development and DevOps. Gradle is an advanced build automation system that plays a crucial role in managing dependencies, building projects, and orchestrating complex workflows efficiently. Its importance lies in its versatility and scalability, as it caters to various project sizes and types. Gradle's ability to easily handle multi-language, multi-project builds and its support for plugin-based customization make it indispensable in modern software development. As organizations increasingly adopt microservices architectures and cloud-native technologies, Gradle's capabilities are instrumental in managing the complexity of building, testing, and deploying applications across diverse environments. DevOps Architecture DevOps Architecture Plan Release Code Dep Deploy Development and operations both play essential roles in order to deliver applications. The deployment comprises analyzing the requirements, designing, developing, and testing of the software components or frameworks. The operation consists of the administrative processes, services, and support for the software. When both the development and operations cture is the solution to fix the gap between Test Monitor Build Operate ined with collaborating, then the DevOps al www deployment and operation terms; therefore, delivery can be faster. DevOps architecture is used for the applications hosted on the cloud platform and large distributed applications. Agile Development is used in the DevOps architecture so that integration and delivery can be contiguous. When the development and operations team works separately from each other, then it is time- consuming to design, test, and deploy. And if the terms are not in sync with each other, then it may cause a delay in the delivery. So DevOps enables the teams to change their shortcomings and increases productivity. Below are the various components that are used in the DevOps architecture: 1) Build: Without DevOps, the cost of the consumption of the resources was evaluated based on the pre-defined individual usage with fixed hardware allocation. And with DevOps, the usage of cloud, sharing of resources comes into the picture, and the build is dependent upon the user's need, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY Build Code Test Plan DevOps Components Moniter Dev Ops Deploy Operate Release 14 N33 nirf 1750 Rank

--- Page 113 ---
Course Code/Title:CS3V15/Devops Unit: III 3. Configure Job Details: ○ General: I Enter a description for your job. Configure options such as discarding old builds if needed. ○ Source Code Management: I Choose Git, Subversion, or another source control system. Enter the repository URL and credentials if the repository is private. Specify the branch or tag to build from. ○ Build Triggers: Set up triggers to start the build. Common options include: Poll SCM: Jenkins will periodically check for changes in the source code repository. . Build periodically: Schedule builds at specific intervals. I GitHub hook trigger for GITScm polling: Trigger builds based on GitHub webhooks. 0 Build Environment: ■ Configure the build environment, such as setting up environment variables or cleaning up before the build starts. ○ Build Steps: Add build steps by clicking Add build step. · Common build steps include: Execute shell: Run shell commands or scripts. . Invoke Gradle script: Use Gradle to build the project. I Invoke Ant: Use Apache Ant for building. ○ Post-build Actions: · Configure actions to perform after the build completes, such as: I Archive the artifacts: Save build artifacts for later use. I Publish JUnit test result report: Display test results in Jenkins. I Send build notifications: Notify stakeholders of build status. 4. Save and Build: ○ Click Save to store the job configuration. o To start a build, click Build Now in the job dashboard. Example Build Configuration: . Job Name: MyApp-Build · Source Code Management: Git o Repository URL: https://github.com/user/myapp.git ○ Branch: main · Build Step: Execute shell ○ Command : myn clean install CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous] TECHNOLOGY 38 nirf 1750 Rank N33

--- Page 114 ---
Course Code/Title:CS3V15/Devops Unit: III · Post-build Action: Archive artifacts ○ Files to archive: target/ *. jar 2. Understanding Jenkins Workspace Overview: The Jenkins workspace is a directory where Jenkins stores files and artifacts related to a particular build. Each job has its own workspace, which is used to perform build operations. Key Aspects of Jenkins Workspace: 1. Workspace Location: o By default, the workspace is located in the Jenkins home directory, typically at /var/lib/jenkins/workspace/ on Linux systems or C:\Program Files (x86) \Jenkins\workspace\ on Windows. 2. Workspace Structure: ○ Each job gets its own subdirectory within the workspace. For example, a job named MyApp-Build will have its workspace at /var/lib/jenkins/workspace/MyApp - Build/. ○ The workspace contains: Source Code: The code pulled from the repository. . Build Artifacts: Files generated during the build process. I Logs: Logs related to the build process. 3. Workspace Usage: ○ Building: During the build process, Jenkins checks out the code into the workspace, executes build steps, and generates artifacts. ○ Archiving Artifacts: After the build, files specified in the post-build actions are archived from the workspace. ○ Cleaning Up: Jenkins may clean up workspaces based on job configurations or policies to save disk space. 4. Customizing Workspace: ○ Configure Custom Workspace Location: In the job configuration, under the Advanced Project Options, you can specify a custom workspace directory. ○ Workspace Cleanup: I Use plugins like the Workspace Cleanup Plugin to manage and clean up workspaces automatically. Example of Workspace Structure: · Workspace Directory: /var/lib/jenkins/workspace/MyApp-Build/ o Source Code: /var/lib/jenkins/workspace/MyApp-Build/src/ o Build Artifacts: /var/lib/jenkins/workspace/MyApp-Build/target/ o Logs: /var/lib/jenkins/workspace/MyApp-Build/logs/ This guide should provide a comprehensive understanding of creating a Jenkins build and managing Jenkins workspaces. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) INSTITUTE TECHNOLOGY N32 39 nirf 175º Rank

--- Page 135 ---
Course Code/Title:CS3V15/Devops Unit: IV Syntax for writing the module on command line While writing the module we use the options such as -m and -a. The -m is for module and -a is for argument, after the -a option we specify the executable command in double quotes. here 'all' stands for all the remote hosts, otherwise we can mention the group name of the hosts 1 $ ansible all -m command -a "echo 'Hello hosts'" The -m option followed by module name 'command' Command to be executed on remote hosts, given in double quotes Commonly used modules 1. Command : Following is a command module that executes a command on remote hosts $ ansible all -m command -a "echo 'Hello hosts'" 2. User : The user module is used to create, modify or delete users. For example - To create a user named 'new_user' on all the hosts in the 'dev' group using ansible, you can use the user module. Here's the ansible command to do that : $ ansible dev -m user -a "name=new_user state=present" Sometimes the above command fails as there is no privilege to create user on remote host. Then we can issue the command as follows $ ansible all -m user -a "name=new_user state=present" -- become -- ask-become-pass The above command asks for the password for the remote host to login. On supplying the password the user will be created. We can verify. if new user is getting created or not on remote host by issuing following command on each remote host $ less /etc/passwd If we want to create new user without asking for password on all the remote hosts then we can issue the command as - $ ansible all -m user -a "name=new_user1 state=present" -- become -b The -b option is used to become superuser. By this the sudo or su privileges can be granted. .3. Ping : The ping module is used to connectivity to remote hosts $ ansible all -m ping 4. Copy : The copy module is used to copy files from control machine to remote hosts $ ansible dev -m copy -a "src=/home/ansible_user/myfile.txt dest=/home/ansible_user/myfile.txt" The above command will copy the myfile.txt file from control node to all the managed nodes of dev group. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 21 N32 nirf 1750 Rank

--- Page 136 ---
Course Code/Title:CS3V15/Devops Unit: IV 5. Package : The package module is used to install, remove or upgrade the packages, For example, the following command installs the nginx web server package : $ ansible package -a "name=nginx state=present" Here name indicates the name of the package to be installed, the state=present means install the package. Normally "present" is for installation, "remove" is for removing the package and "latest" is used to upgrade the package. 6. Shell : The shell module is used to run shell commands with more flexibility on remote hosts. For example - Following shell command executes on remote hosts. $ ansible dev -m shell -a "cmd=is" 7. Service : The service module is used to start, stop or restart a specific service on remote hosts. For example - Following service will start the apache web service on all the remote hosts. $ ansible dev -m service -a "name=apache2 state=started" Note that the state can started for starting the service, stopped for stopping the service. o started/stopped are idempotent actions that will not run commands unless necessary. o restarted will always bounce the service. o reloaded will always reload. I Example 1: Get the Uptime of remote servers We have used command module to run the uptime command and we have given both the ad hoc and the playbook form of execution. as AD-HOC Command $ ansible testservers -m command -a uptime -i ansible_hosts as Playbook - name: Check the remote host uptime hosts: testservers tasks: - name: Execute the Uptime command over Command module register: uptimeoutput command: "uptime" - debug: var: uptimeoutput.stdout_lines CHENNAI INSTITUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY (Autonomout) 22 N32 nirf 1750 Rank

--- Page 121 ---
Course Code/Title:CS3V15/Devops Unit: IV · - m ping specifies the "ping" module, a basic module that checks the reachability of the managed nodes. . - i inventory.yaml points to the inventory file containing the list of devices. · The "ping" module sends a test command to the target devices and reports their status, confirming whether they are responsive. Output and Result: auriga@auriga-Latitude-E7470 :- /Desktop/ansibleŞ ansible all -[ inventory.yaml -m ping VOOS | UNREACHABLET => ( "changed's false. "mg's "Failed to connect to the host via s "unreachable": true auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ton denied (publickey_ password) In the above example the ssh host is not reachable. Activities Terminal Nov 2 12:13 . auriga@auriga-Latitude-E7470: - /Desktop/ansible C auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -i inventory.yaml -m ping vn81 | SUCCESS => 'ansible_facts": { "discovered_interpreter_python": "/usr/bin/python3" "changed": false, "ping": "pong" auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ In the above output image it is working fine Example: Installing Nginx Using Apt Module For a more practical example, let's use the "apt" module to install Nginx on your managed nodes. Here's the command: 1ansible all -i inventory.yaml -m apt -a "name=nginx state=present" -- become -- ask-become-pass · - m apt specifies the "apt" module, which is responsible for package management. · - a "name=nginx state=present" includes variables for the module to act upon, such as specifying that Nginx should be installed (state=present). · - become signifies that the command should run with elevated privileges (sudo). · - ask-become-pass prompts for the sudo password of the managed node to ensure the installation proceeds smoothly. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomovt) TECHNOLOGY 7 N32 nirf 1750 Rank

--- Page 122 ---
Course Code/Title:CS3V15/Devops Unit:IV Output and Result: Activities Terminal * Nov 2 12:24 · auriga@auriga-Latitude-E7470: - /Desktop/ansible F X auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470: - /Desktop/ansible auriga@auriga-Latitude-E7470 :- /Desktop/ansible$ ansible all -[ inventory.yaml -m apt -a "name=ansible state=present" -- become - - ask-become-pass BECOME password: vm01 | CHANGED => { "ansible_facts": { "discovered_interpreter_python": * /usr/bin/python3" "cache_update_time": 1698985056, "cache_updated": false, "changed": true, "stderr": "" 'stderr_Lines": []. stdout": "Reading package lists ... \nBuilding dependency tree ... \nReading state information ... \nThe following packages were automatically installed and are no longer required:\n gtr1.2-goa-1.0 hplip-data libcgt-fast-perl libcgt-pm-perl\n libdouble-conversion3 Libevent-core-2.1-7 libevent-pthreads-2.1-7\n \ibfcgt-perl libfprint-2-todi libfwupdplugin1 libht ml-template-perl\n Libpcre2-16-0 libqtScoresa libqtsdbuss libqtsguis libqt5network5\n libqtSpositionings libqt5printsupports libqt5qm15 Libqtsquick5\n libqtSsensors5 libqt5svg5 Li bqt5webchannel5 libqt5webkit5 libqt5widgets5\n Libxcb-xinerama@ Libxcb-xinpute Libxmlbi printer-driver-postscript-hp\n python3-renderpm python3-reportlab python3-reportlab-accel\n qt5-gtk-platformthene qttranslations5-lien shim\nUse 'sudo apt autoremove' to remove them. InThe following additional packages will be installed:\n teee-data python3-argcomplete pyt hon3-crypto python3-dnspython\n python3-jinja2 python3 . jmespath python3-kerberos python3-libcloud\n python3-netaddr python3-ntin-auth python3-requests-kerberos\n python3-requests- ntlm python3-selinux python3-winrm python3-xmltodict\nSuggested packages: \n cowsay sshpass python- jinja2-doc ipython3 python-netaddr-docs\nThe following NEW packages will be install ed:\n ansible leee-data python3-argcomplete python3-crypto python3-dnspython\n python3- jinja2 python3-jmespath python3-kerberos python3 . Libcloud\n python3-netaddr python3-ntlm-aut h python3-requests-kerberos\n python3-requests-ntIm python3-selinux python3-winrm python3-xmltodict\n0 upgraded, 16 newly installed, 0 to remove and 50 not upgraded. InNeed to get 97 25 kb of archives. \nAfter this operation, 98.6 MB of additional disk space will be used. \nGet:1 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-jinja2 all 2.10.1-2 [95.5 KB]\nGet:2 http://in.archive.ubuntu.com/ubuntu focal/main amd64 python3-crypto amd64 2.6.1-13ubuntu2 [237 KB]\nGet:3 http://in.archive.ubuntu.com/ubuntu focal-updates/main amd64 pyt hon3-dnspython all 1.16.0-1ubuntu1 [89.2 KB]\nGet:4 http://in.archive.ubuntu.com/ubuntu focal/main amd64 ieee-data all 20180805.1 [1589 KB]\nGet: 5 http://in.archive. ubuntu.com/ubuntu focal-updates/main amd64 python3-netaddr all 0.7.19-3ubuntu1 [236 KB]\nGet:6 http://in.archive.ubuntu.com/ubuntu focal/universe and64 ansible all 2.9.6+dfsg-1 [5794 KB]\nGet:7 http: //\n.archive.ubuntu.com/ubuntu focal/universe amd64 python3-argcomplete all 1.8.1-1.3ubuntu1 [27.2 kB]\nGet:8 http://in.archive.ubuntu.com/ubuntu focal-updates/main and64 python3- jne spath all 0.9.4-2ubuntu1 [21.5 kB]\nGet:9 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-kerberos amd64 1.1.14-3.1build1 [22.6 k8]\nGet: 10 http://in.archive.ubuntu. con/ubuntu focal/universe amd64 python3-libcloud all 2.8.0-1 [1483 KB]\nGet:11 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-ntlm-auth all 1.1.0-1 [19.6 KB]\nGet: 1 2 http://in.archive.ubuntu.com/ubuntu focal/universe and64 python3-requests-kerberos all 0.12.0-2 [11.9 KB]\nGet: 13 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-r equests-ntlm all 1.1.0-1 [6004 B]\nGet: 14 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-selinux amd64 3.0-1build2 [139 KB]\nGet : 15 http://in.archive.ubuntu.com/ubu ntu focal/universe and64 python3-xmltodict all 0.12.0-1 [12.6 KB]\nGet:16 http://in.archive.ubuntu.com/ubuntu focal/universe amd64 python3-winrm all 0.3.0-2 [21.7 KB]\nFetched 9725 k B in 7s (1454 KB/s)\nSelecting previously unselected package python3-jinja2.\r\n(Reading database ... \r(Reading database ... 5*\r (Reading database .. . 10%\r(Reading database ... 15% \r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database . .. 45%\r (Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 68%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading da tabase ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 212542 files and directories currently installed.)\r\nPreparin g to unpack ... /00-python3-jinja2_2.10.1-2_all.deb ... \r\nUnpacking python3-jinja2 (2.10.1-2) ... \r\nSelecting previously unselected package python3-crypto. \r\nPreparing to unpack .. ./01-python3-crypto_2.6.1-13ubuntu2_amd64.deb ... \r\nUnpacking python3-crypto (2.6. 1-13ubuntu2) ... \r\nSelecting previously unselected package python3-dnspython. \r\nPreparing to unpa ck ... /02-python3-dnspython_1.16.0-lubuntu1_all.deb ... \r\nUnpacking python3-dnspython (1. 16.0-1ubuntu1) ... \r\nSelecting previously unselected package leee-data. \r\nPreparing to unp ack ... /03-ieee-data_20180805.1_all.deb ... \r\nUnpacking teee-data (20188805.1) ... \r\nSelecting previously unselected package python3-netaddr. \r\nPreparing to unpack ... /04-python3. netaddr_0.7.19-3ubuntu1_all.deb ... \r\nUnpacking python3-netaddr (0.7.19-3ubuntu1) ... \r\nSelecting previously unselected package ansible. \r\nPreparing to unpack .. . /05-ansible_2.9.6 +dfsg-1_all.deb ... \r\nUnpacking ansible (2.9.6+dfsg-1) ... \r\nSelecting previously unselected package python3-argcomplete. \r\nPreparing to unpack .. . /06-python3-argcomplete_1.8.1-1. 3ubuntu1_all.deb ... \r\nUnpacking python3-argcomplete (1.8.1-1.3ubuntu1) ... \r\nSelecting previously unselected package python3- jnespath. \r\nPreparing to unpack ... /07-python3- jnespa th_0.9.4-2ubuntul_all.deb ... \r\nUnpacking python3-jmespath (0.9.4-2ubuntu1) ... \r\nSelecting previously unselected package python3-kerberos. \r\nPreparing to unpack ... /08-python3-ke rberos_1.1.14-3.1build1_and64.deb ... \r\nUnpacking python3-kerberos (1.1.14-3.1build1) ... \r\nSelecting previously unselected package python3-libcloud. \r\nPreparing to unpack .. . /09- python3-libcloud_2.8.0-1_all.deb ... \r\nUnpacking python3-libcloud (2.8.0-1) ... \r\nSelecting previously unselected package python3-ntlm-auth. \r\nPreparing to unpack ... /10-python3-n tim-auth_1.1.0-1_all.deb .. . \r\nUnpacking python3-ntlm-auth (1.1.0-1) ... \r\nSelecting previously unselected package python3-requests-kerberos. \r\nPreparing to unpack ... /11-python3. requests-kerberos 0.12.8-2 all. deb .... \r\nUnpacking python3-requests-kerberos (0.12.0-2) ... \r\nSelecting previously unselected package python3-requests-ntln. \r\nPreparing to unpack By using these ad-hoc commands, you can perform quick, task-specific operations on your managed nodes, saving time and effort in your automation tasks. CommonModules: Ansible offers an extensive library of modules for various tasks, from system administration to application deployment. You can explore the full list of Ansible modules in the official documentation to find the most suitable module for your specific requirements. BestPractices: When working with ad-hoc commands, consider using SSH key-based authentication for secure, passwordless access. It streamlines the authentication process and enhances the security of your automation tasks. Using Ansible Playbooks Introduction: In the world of Ansible, playbooks are the cornerstone of structured and complex automation. While ad- hoc commands are perfect for quick and isolated tasks, playbooks step in when you need a well- orchestrated sequence of operations. They allow you to tackle multi-step automation scenarios where tasks depend on each other, and conditional actions are required. Let's delve deeper into the importance of playbooks: The Role of Playbooks: Playbooks serve as the framework for orchestrating automation tasks that involve a sequence of operations. Whether it's configuring servers, deploying applications, or managing infrastructure, playbooks provide a structured way to define how these tasks are executed. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY (Autonomous) TECHNOLOGY 8 N33 nirf 1750 Rank

--- Page 35 ---
Course Code/Title: CS3401/Theory of Computation Step 4: The NFA will be, Here qo, qı and q2 is a final state because & closure (qo), ¿ - closure (q1) and ¿ - closure (q2) contains final state q2. 0 1 0,1 90 91 0,1,2 1,2 92 2 Fig. 1.9.2 Example 1.9.2 Convert the following NFA with e to NFA without &. E a 5 Start 0 E 3 a 1 b 2 6 b 7 E Fig. 1.9.3 E 8 3 g Solution: We will first obtain & - closure of every state. The E - closure is basically an & - transition from one state to other. Hence ¿ - closure (0) = {0} ¿ - closure (1) = {1} ¿ - closure (2) = {2, 3, 4, 6, 9} ¿ - closure (3) = {3, 4, 6} ¿ - closure (4) = {4} & - closure (5) = {5, 8, 3, 4, 6, 9} = {3, 4, 5, 6, 8, 9} sorted it! ¿ - closure (6) = {6} ¿ - closure (7) = {7, 8, 3, 4, 6, 9} CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN 35

--- Page 36 ---
Course Code/Title: CS3401/Theory of Computation = {3, 4, 6, 7, 8, 9} ¿ - closure (8) = {8, 3, 4, 6, 9} = {3, 4, 6, 8, 9} ¿ - closure (9) = {9} Now we will obtain 8' transitions for each state and for each input symbol. 8' (0, a) = & - closure (8 (8' (0, ¿), a)) = 8 - closure (8 (¿ - closure(0), a)) = 8 - closure (8 (0, a)) = 8 - closure (1) {1} § ' (0, b) = & - closure (8 (8' (0,E), b)) = 8 - closure (8 (& - closure (0), b)) = 8 - closure (8 (0, b)) = 8 - closure ($) =¢ 8' (1,a) = & - closure (8 (8' (1,E), a)) = 8 - closure (8 (& - closure (1), a)) = 8 - closure (8 (1, a)) = 8 - closure ($) = ¢ 8 ' (1, b) = & - closure (8 (8 (1,c), b)) = 8 - closure (8 (& - closure (1), b)) = 8 - closure (8 (1, b)) = 8 - closure (2) = {2, 3, 4, 6, 9} § ' (2, a) = & - closure (8 (8' (2,8), a)) = 8 - closure (8 (¿ - closure (2), a)) = 8 - closure (8 (2, 3, 4, 6, 9), a) = 8 - closure (8 (2, a) U ô (3, a) U ô (4, a) U ô (6, a) U ô (9, a)) = 8 - closure (¢ U ¢ U 5 U ¢ U $ ) = 8 - closure (5) 36 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank N39

--- Page 51 ---
Course Code/Title: CS3401/Theory of Computation Input 0 1 State =>[p] [p, q] [p] [q] [r] [+] [r] [s] [s] [s] [s] [p. q] [p, q, r] . [p. r] [p, q, r] [p, q, r, s] [p, r] [p, r] [p, q, s] [p] [p, q, r, s] [p, q, r, s] [p, r, s] [p, q. s] [p, q, r, s] [p, r, s] [p, r, s] [p, q, s] [p, s] [p, s] [p, q, s] [p, s] The final state F' = { [s], [p, q, r, s], [p, q, s], [p, r, s], [p, s] } The transition graph shows two disconnected parts. But part I will be accepted as final DFA because it consists of start state and final states, in part II there is no start state. Refer Fig. 1.10.3 (See Fig. 1.10.3 on next page): Example 1.10.4 Convert the following NFA to a DFA . AU: May-13, Marks 10 8 a b P [ p, q1 9 r 51 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 52 ---
Course Code/Title: CS3401/Theory of Computation 1 0 [p] [p.q] 0 [p,q,r] 1 1 1 Part I [p.r.s] q 1 [p,q,s] 0 0 0 1 0,1 1 0 [p.r] 0 [p,q,r,s] [p.s] 1 0 T S 0,1 Part 11 Fig. 1.10.3 Solution: We will apply & transitions on each state for each input. 8 ([p], a) = [p] 8 ([p], b) = {p, q} = [p, q] -> new state 8 ([q], a) = [r] 8 ([q], b) = [r] 8 ([r], a) = ¢ 8 ([r], b) = ¢ The 8 transition on newly generated states 8 ([p, q], a) = {p, r} = [p, r] -> new state 8 ([p, q], b) = {p, q, r) = [p, q, r] -> new state ò ([p, r], a) = {p} U $ = [p] o ([p, r], b) = {p, q} U i.e. [p, q] 8 ([p, q, r], a) = {p} U {r} U $ = [p, r] 8 ([p, q, r], b) = {p, q} U {r} U $ = {p, q, r} i.e. [p, q, r] As no new state is getting generated in above transitions, the transition table can be constructed as follows - CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN 52

--- Page 15 ---
Course Code/Title: CS3401/Theory of Computation · Logic: The binary number is made up of 0's and 1's when any binary number ends with 0 it is always even and when a binary number ends with 1 it is always odd. For example, 0100 is a even number, it is equal to 4. 0011 is a odd number, it is equal to 3. · Design: While designing FA we will assume one start state one state ending in 0 and other state for ending with 1. Since we want to check whether given binary number is even or not, we will make the state for 0, as the final state. (Refer Fig. 1.6.2) Simulation: The FA indicates clearly S1 is a state which handles all the 1's and S2 is a state which handles all the 0's. Let us take some input. 0 S2 0 So 0 1 1 S1 1 Fig. 1.6.2 01000 => 0S21000 01S1 000 010S2 00 0100S2 0 01000S2 Another idea to represent FA with the help of transition table. States Input 0 1 So S2 S1 S1 S2 SI S2 S2 S1 Transition table Example 1.6.2 Design FA which checks whether the given unary number is divisible by 3. Solution: 15 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank

--- Page 16 ---
Course Code/Title: CS3401/Theory of Computation · Logic : The unary number is made up of ones. The number 3 can be written in unary form as 111, number 5 can be written as 11111 and so on. The unary number which is divisible by 3 can be 111 or 111111 or 111111111 and so on. · Design Transition Table Input 1 91 92 - 90 91 92 92 93 93 91 Fig. 1.6.3 · Simulation Consider a number 111111 which is equal to 6 i.e. divisible by 3. So after complete scan of this number we reach to final state q3. Start 111111 State q0 1q1 11111 11q2 1111 111q3 111 1111q1 11 11111q2 1 111111q3 -> Now we are in final state. Example 1.6.3 Design FA to check whether given decimal number is divisible by three. Solution: · Logic : To determine whether the given decimal number is divisible by three, we need to take the input number digit by digit. Also, while considering its divisibility by three, we have to consider that the possible remainders could be 1, 2 or 0. The remainder 0 means, it is divisible by 3. Hence from input set {0, 1, 2, .... 9} (since decimal number is a input), we will get either remainder 0 or 1 or 2 while testing its divisibility by 3. So we need to group these digits according to their remainders. 16 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank CEN

--- Page 5 ---
Course Code/Title: CS3401/Theory of Computation Now, 2) Induction hypothesis - Now we will assume n = k and will obtain the result for it. The equation then becomes, 1+2+3+ ... + k = (k(k+1)/2 3) Inductive step - Now we assume that equation is true for n = k. And we will then check if it is also true for n = k + 1 or not. Consider the equation assuming n = k + 1 L.H.S. = 1+2+3 ... + k + k + 1 equation (1) k(k+1)/2 +k+1 =k(k+1)+2(k+1)/2 =(k+ 1) (k + 2)/2 taking common factor i.e. = (k+ 1) (k+ 1 + 1)/2 = R.H.S. Example 1.4.2 Prove: n!>= 2n-1 Solution: Consider, 1. Basis of induction - Let n = 1 then L.H.S. = 1 R.H.S. = 21 -1 =2º =1 hence n!>= 2"-1 is proved. 2. Induction hypothesis - Let n = n + 1 then k! = 2k-1 where k > = 1 then (k + 1)! = (k + 1)k! by definition of n! = (k+1) 2k -1 = 2 × 2k-1 = 2k Example 1.4.3 Prove that 12 + 22 + 32 + ... + n2 = >"i=1 i2 = n(n + 1) (2n + 1)/6 using mathematical induction. AU: May-07, 16, Marks 6, May-14, Marks 8 Solution: We will first prove it by basis of induction and then will consider induction hypothesis. 1. Basis of induction - Let n = 1 L.H.S. = 12 = 1 R.H.S. = 1(1+1)(2.1+1) /6 = 6/6 R.H.S. = 1 As L.H.S. = R.H.S it is proved for n = 1. 2. Induction hypothesis - Let, n = k. 5 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 6 ---
Course Code/Title: CS3401/Theory of Computation Then 12 + 22 + + k2 = >k ;= 1 i2 = k(k+1)(2k+1) /6 is true Now, we will try to prove it for n = k + 1. Hence we will substitute n by k + 1. 12 + 22 + 32 +. .+ (k+1)2 = >k+1 ;= 1 i2 = (k+1)(k+1+1)(2(k+1)+1) /6 L.H.S. = 12 + 22 + 32 + ...... + k2 + (k+1)2 We will substitute equation (1) and we will get, = (k+1) [k(2k+1)+6k+6 /6] = (k+1) [2k2+k+6k+6 /6] = (k+1) [2k2 +7k+6 /6] = (k+1)(2k2+4k+3k+6) /6 =(k+1)(2k(k+2)+3(k+2)) /6=(k+1)((2k+3)(k+2)) /6 =(k+1)((k+2)(2k+3)) /6 == (k+1)(k+1+1)(2(k+1)+1) /6 = R.H.S. As L.H.S. = R.H.S., it is true that using n = k + 1 the given expression can be proved. Example 1.4.4 Prove 1+4+ 7+ ....... + (3n-2) = n(3n-1)/2 for n > 0 Solution: Consider, 1. Basis of induction - Let, n = 1, L.H.S. (3.1-2) = 1 R.H.S. = 1(3.1-1) /2 = 2/2 = 1 As L.H.S. = R.H.S. given expression is true for n = 1. 2. Induction hypothesis - We assume that the given expression is true for n = k. i.e. 1+4+7+ ....... +(3k-2) = k(3k-1) is true. .. .... (1) Now we will prove it for n = k + 1. L.H.S. 1+4 +7 + ...... + (3k-2)+(3(k+1)-2) We will substitute R.H.S. of equation (1). =k(3k-1) /2+(3(k+1)-2)=k(3k-1)+2[3(k+1)-2] /2 = 3k2-k+6k+2 /2 = 3k2+5k+2 /2 = 3k2+2k+3k+2 /2 = (2k+2)+(3k2 +3k) /2 =2(k+1)+3k(k+1)/2=(k+1)(3k+2)/2=(k+1)(3(k+1)-1) /2 = R.H.S. 6 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 67 ---
Course Code/Title: CS3401/Theory of Computation = 8 - closure (6) = C 8' (C, letter) = C 8' (C, digit) = & - closure(8 (4, 5, 6, 7, 8, 10), digit) = 8 - closure (9) = D 8' (C, digit) = D 8' (D, letter) = & - closure(8 (4, 5, 7, 8, 9, 10), letter) = 8 - closure (6) =C 8' (D, letter) = C 8' (D, digit) = & - closure(8 (4, 5, 7, 8, 9, 10), digit) = 8 - closure (9) = D 8' (D, digit) = D Transition Table Transition Diagram Letter Digit A T A * B * C B 0 C D C D letter B digit letter digit D D C D digit letter C letter The two states C and D are equivalent. Similarly state B and C are equivalent B = C = D. The minimized DFA will then be letter A letter / digit B Example 1.11.5 Convert the following & - NFA to NFA and then convert the resultant NFA to DFA. AU Dec .- 18, Marks 13 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank 67

--- Page 68 ---
Course Code/Title: CS3401/Theory of Computation D 1 1 0 A B 1 C Fig. 1.11.10 Solution: Conversion of NFA to NFA We will obtain & - closure of each state ¿ - closure (A) = {A, B, C} ¿ - closure (B) = {B, C} ¿ - closure (C) = {C} ¿ - closure (D) = {D} Now we will obtain 8' transitions for each state on each input symbol. 8' (A, 0) = & - closure (8 (& - closure(A), 0)) = 8 - closure (8 ((A, B, C), 0)) = 8 - closure (§ (A, 0) U ô (B, 0), U ô (C,0)) = & - closure (¢ U ¢ U ¢) = ϕ 8' (A, 0) = ¢ 8' (A, 1) = & - closure (8 (& - closure(A), 1)) = 8 - closure (8 (A, B, C), 1) = 8 - closure (A, D, C) = 8 - closure (A) U & - closure (D) U & - closure (C) = {A, B, C, D} 8' (A, 1) = {A, B, C, D} 8' (B, 0) = & - closure (8 (& - closure (B), 0)) = 8 - closure (8 (B, C), 0) 8' (B, 0) = ¢ 8' (B, 1) = & - closure (8 (& - closure (B), 1)) = 8 - closure (8 ((B, C), 1) = 8 - closure (D, C) 68 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 59 ---
Course Code/Title: CS3401/Theory of Computation = & - closure {8 (qo, b) U 8 (q1, b) U ô (q2, b)} = & - closure {qo} = {q0, q1, q2} i.e. A. Hence we can state that, 8' (A, a) = B §' (A, b) = A Step 3: Now let us find transitions for state B = {q1, q2} 8' (B, a) = & - closure (8 (q1, q2), a) = & - closure {q1} = {q1, q2} i.e. B 6' (B, b) = & - closure (8 ( q1, q2), b) = 8 - closure {8 (q1, b) U ò (q2, b)} = & - closure {qo} = (q0, q1, q2) i.e. A. Step 4: Hence the generated DFA is a b b a a A B A A B B A b Fig. 1.11.2 Example 1.11.2 Convert the given NFA into its equivalent DFA - 0 1 2 E 91 42 Fig. 1.11.3 Solution: Let us obtain & - closure of each state. E - closure (qo) = {qo, q1, q2} E - closure (q1)= {q1, q2} E - closure (q2) = {q2} Now we will obtain d' transition. Let e-closure (q0) = {q0, q1, q2} call it as state A. 8' (A, 0) = & - closure { 8 ((qo, q1, q2), 0)} = 8 - closure {8 (qo, 0) U ò (q1, 0) U ò (q2, 0)} 59 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY nirf 175" Rank -- LEN

--- Page 60 ---
Course Code/Title: CS3401/Theory of Computation = & - closure {qo} = {q0, q1, q2} i.e. state A 8' (A, 1) = & - closure { 8 ((qo, q1, q2), 1)} = & - closure {(q0,1) U 8 (q1,1) U & (q2,1)} = & - closure {q1, q2} = {q1, q2} Call it as state B 8' (A, 2) = & - closure {8 ((qo, q1, q2), 2)} = 8 - closure {8 (qo, 2) U 8 (q1, 2) U 8 (q2, 2)} = & - closure {q2} = {q2} Call it as state C. Thus we have obtained, 8' (A, 0) = A 8' (A, 1) = B 8' (A,2) = C i.e. Now we will find transitions on states B and C for each input. 0 1 A B 2 C Fig. 1.11.4 Hence, 8' (B, 0) = & - closure {8 (q1, q2), 0} = 8 - closure {8 (q1, 0) U ò (q2, 0)} = 8 - closure {0} = ¢ 8' (B, 1) = & - closure {8 (q1, q2), 1} = 8 - closure {8 (q1, 1) U ò (q2, 1)} = & - closure {q1} 60 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY pou LEN 175" Rank

--- Page 23 ---
Course Code/Title: CS3401/Theory of Computation Solution: The DFA will be 0 0 0 0 1 90 1 92 1 93 Example 1.6.11 Give deterministic finite automata accepting the following language over the alphabet. 1) Number of 1's is a multiple of 3. 2) Number of 1's is not a multiple of 3. AU: Dec .- 13, Marks 8 Solution: 1) We assume alphabet as input set E = {1} 1 q1 1 92 1 q3 1 1 2) For recognizing number of 1's not a multiple of 3, we will use the DFA in step (1). That means we will make non final states as final and final state as non-final ones. The DFA will then be as shown in figure. 1 1 1 92 93 1 1 Example 1.6.12 Given 2 = (a, b), construct a DFA which recognize the language L= {b" ab" :m,n>0}. AU Dec .- 16, Marks 6 Solution: . The m and n are > 0 and there is no restriction on number of m and n. In other words any number of m and n are allowed except null. · Hence we can rewrite given L as L = b+ ab+ · From this L we can construct DFA as 23 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 24 ---
Course Code/Title: CS3401/Theory of Computation b b b a 91 92 b 93 Example 1.6.13 Construct a DFA to accept strings over (a, b), such that every block of length five contains at least two a's. Use extended transition of function to trace a string W = aabba. Solution: The following DFA contains block of five symbols. The DFA loops back when the block ends. At each state both the a and b symbols are considered. The state names indicate the total number of a's so far visited. For instance if the state name is 1 then it indicates that there is single a being read. When two a's are read then it leads to a final state. The DFA will be shown in Fig. 1.6.12. b b b b 0 b 0 b 0 b 0 0 b 0 a 1 a b 1 a b 1 a b 1 b a a a a 1 a 2 a,b a,b 2 a,b 2 2 a a a Fig. 1.6.12 Example 1.6.14 Design a DFA which accept strings of O's and 1's which when interpreted as a binary integer is multiple of 5. Also give the sequence of states that DFA is in while processing the input string: 1001011. Solution: The required DFA can be drawn as follows. 1 0 1 1 91 0 q2 0 0 1 1 0 Processing of 1001011 24 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY nirf 175" Rank -- N39

--- Page 57 ---
Course Code/Title: CS3401/Theory of Computation 0 1 a {b,d} b c {b,c} C d a d - a Solution: Refer similar example 1.10.2 by assuming p = a, q=b, r = c and s = d. Example 1.10.8 Construct NFA that accepts all string that end in 01. Give its transition table and extended transition function for the input string 00101. Also construct a DFA for the above NFA using subset construction method. AU May-16, Marks 10 Solution: The NFA will be designed using r.e. = (0+1)* 01 0,1 0 1 92 The transition table will be as follows: i/p 0 1 State 91 - ( 92] 92 0 0 Refer example 1.10.6 for conversion of NFA to DFA. Review Question 1. Prove that "A language L is accepted by some DFA if and only if L is accepted by some NFA". AU Dec .- 15, Marks 10 Equivalence of NFA with & to DFA AU: May-05, 11, Dec .- 06,13,15,18, Marks 16 Prove that there exists a DFA for every & - NFA. AU: May-11, Marks 8 Step 1: Consider M = (Q, 2, 8, qo, F) is a NFA with s. We have to convert this NFA with & to equivalent DFA denoted by 57 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank --

--- Page 58 ---
Course Code/Title: CS3401/Theory of Computation MD =(QD, Σ, δρ, qo, FD) Then obtain, & - closure (qo) = {P1, P2, P3 ... Pn} then [P1, P2, P3 , ... Pn] becomes a start state of DFA. Now [P1, P2, P3 ... Pn] E QD Step 2: We will obtain & transitions on [P1, P2, P3, ... Pn ] for each input. OD ([P1, P2, Pn], a) = & - closure (ò (P1, a) U 8 (P2, a) U ... o (Pn, a)) = U"i=1 8 - closure d (Pi, a) where a is input € Σ. Step 3: The states obtained [P1, P2, P3, ... Pn] E QD. The states containing final state Pi is a final state in DFA. Now let us see some examples of conversion based on this procedure. Example 1.11.1 Convert the following NFA with & to equivalent DFA. b a Start E 92 a b Fig. 1.11.1 Solution: Step 1: To convert this NFA we will first find &-closures. E - closures {qo} = {qo, q1, q2} ¿ - closures {q1} = {q1, q2} E - closures {q2} = {q2} Step 2: Let us start from & - closure of start state ¿ - closure {qo} = {qo, q1, q2} we will call this state as A. Now let us find transitions on A with every input symbol. 8 (A, a) = &-closure (8 (A, a)) = 8 - closure (8 (qo, q1, q2), a) = 8 - closure {(qo, a) U ò (qı, a) U ò (q2, a)} = 8 - closure {q1} = {q1, q2). Let us call it as state B. 8 (A, b) = c-closure (8 (A, b)) = 8 - closure (8 (qo, q1, q2), b) 58 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 37 ---
Course Code/Title: CS3401/Theory of Computation = {3, 4, 5, 6, 8, 9} 8' (2, b) = & - closure: (8 (8 (2, ¿), b)) = 8 - closure (8 (E - closure (2), b)) = 8 - closure (8 (2, 3, 4, 6, 9), b) = 8 - closure (8(2, b) U ô(3, b) U ô(4, b) U ô(6, b) U ô(9, b)) = 8 - closure (¢ U ¢ U ¢ U 7 U ¢) = 8 - closure (7) = {3, 4, 6, 7, 8, 9} 8'(3, a) = & - closure (8 (8'(3,c), a)) = 8 - closure (8 (& - closure (3), a)) = 8 - closure (8 (3, 4, 6), a). = 8 - closure (8 (3, a) U ô (4, a) U ô (6, a)) = 8 - closure (¢ U 5 U ¢) = 8 - closure (5) = {3, 4, 5, 6, 8, 9} 8'(3, b) = & - closure (8 (d'(3, a), b)) = 8 - closure (8 (& - closure (3), b)) = 8 - closure (8 (3, 4, 6), b) = 8 - closure (8 (3, b) U ô (4, b) U ô (6, b)) = 8 - closure (¢ U ¢ U 7) = 8 - closure (7) = {3, 4, 6, 7, 8, 9} 8' (4, a) = & - closure (8 (8' (4, ¿), a)) = 8 - closure (8 (¿ - closure (4), a)) = 8 - closure (8 (4, a)) = 8 - closure (5) = {3, 4, 5, 6, 8, 9} 8'(4, b) = & - closure (8 (8' (4, ¿), b)) = 8 - closure (8 (& - closure (4, b)) = 8 - closure (8 (4, b)) = 8 - closure (¢) 37 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 38 ---
Course Code/Title: CS3401/Theory of Computation = ¢ § ' (5, a) = & - closure (8 (8'(5, ¿), a)) = 8 - closure (8 (& - closure (5), a)) = 8 - closure (8 (3, 4, 5, 6, 8, 9), a) = e-closure (8 (3, a) U (4, a) U ô (5, a) U ô (6, a) U ô (8, a) U ô (9,a)) = & - closure (¢ U 5 U ¢ U ¢ U $ U ¢ U ¢) = 8 - closure (5) = {3, 4, 5, 6, 8, 9} 8' (5, b) = & - closure (8 (8'(5, ¿), b)) = & - closure (8 (& - closure (5), b)) = 8 - closure (8 (3, 4, 5, 6, 8, 9), b) = 8 - closure (§ (3, b) U ô (4, b) U ô (5, b) U ô (6, b) U ô (8, b) U õ (9, b)) = 8 - closure (¢ U ¢ U ¢ U 7 U ¢ U ¢) = 8 - closure (7) = {3, 4, 6, 7, 8, 9} 8' (6, a) = & - closure (8 (8' (6, E), a)) = 8 - closure (8 (& - closure (6), a)) = 8 - closure (8 (6, a)) = 8 - closure (¢) = ϕ 8' (6, b) = & - closure (8 (8 (6, E), b)) = 8 - closure (8 (& - closure (6), b)) = 8 - closure (8 (6, b)) = 8 - closure (7) = {3, 4, 6, 7, 8, 9} 8' (7, a) = & - closure (8 (8'(7, ¿), a)) = 8 - closure (8 (& - closure (7), a)) = 8 - closure (8 (3, 4, 6, 7, 8, 9), a) = 8 - closure (8 (3, a) U ô (4, a) U ô (6, a) U ô (7, a) U ô (8, a) U ô (9, a)) = & - closure ($ U 5 U ¢ U ¢ U ¢ U ¢) = 8 - closure (5) 38 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank

--- Page 25 ---
Course Code/Title: CS3401/Theory of Computation qo H 1001011 1 q1 H 001011 10 q2 H 01011 100 q4 H 1011 1001 q4 H 011 10010 q3 H 11 100101 q2 H 1 1001011 qo H Accept. Example 1.6.15 Design a FA for following language L = {WWW is Binary word of length 4i (where i >= 1) such that each consecutive block of 4 bits contains atleast 2 0s} = {0000, 0110, 01101100, ... ) AU: Dec 19, Marks 13 Solution : 0 1 1 0000 0 0001 0010 0011 1 0 1 0 1 0 1 0100 0 1 0101 0 0 0110 1 1 0 1 0 0111. 1011, 1101 a, b 1000 1001 1010 1100 1 0 Non-deterministic Finite Automata (NFA) AU: May-09,12, Marks 8 · The concept of Non-deterministic Finite Automata is exactly reverse of Deterministic Finite Automata. The Finite Automata is called NFA when there exists many paths for a specific input from current state to next state. The NFA can be shown as in Fig. 1.7.1 . Note that the NFA shows from qo for input a there are two next states qi and q2. Similarly, from qo for input b the next states are q0 and q1. 25 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank

--- Page 26 ---
Course Code/Title: CS3401/Theory of Computation · Thus it is not fixed or determined that Fig. 1.7.1 Non-deterministic finite automata with a particular input where to go next. Hence this FA is called non-deterministic finite automata. b b Fig. 1.7.1 Non-deterministic finite automata · Consider the input string bba. This string can be derived as Input b b a Path q0 q0 q1 Input b b a Path q0 q0 q2 Input b b a Path q0 q1 q1 Thus you cannot take the decision of which path has to be followed for deriving the given string. Definition of NFA The NFA can be formally defined as a collection of 5-tuples. 1) Q is a finite set of states. 2) ≥ is a finite set of inputs. 3) 8 is called next state or transition function. 4) q0 is initial state. 5) F is a final state where F= Q. There can be multiple final states. Thus the next question might be what is the use of NFA. The NFA is basically used in theory of computations because they are more flexible and easier to use than the DFAs. Difference between NFA and DFA 26 INGSTOUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 47 ---
Course Code/Title: CS3401/Theory of Computation Let, M=(Q, E, δ, qo, F) is a NFA which accepts the language L(M). There should be equivalent DFA denoted by M' = (Q', Σ', δ', qo', F') such that L(M) = L(M'). The conversion method will follow following steps - 1) The start state of NFA M will be the start for DFA M'. Hence add q0 of NFA (start state) to Q'. Then find the transitions from this start state. 2) For each state [q1, q2, q3, ... qi] in Q' the transitions for each input symbol 2 can be obtained as, i) &' ([q1,q2 ... qi], a) = 8 (q1, a) U 8 (q2, a) U ......... (qi, a) = [q1, q2, .... qk ] may be some state. ii) Add the state [q1, q2, .... qk] to DFA if it is not already added in Q'. iii) Then find the transitions for every input symbol from 2 for state [q1, q2, .... ,qk ]. If we get some state [q1, q2, ... qn ] which is not in Q' of DFA then add this state to Q'. iv) If there is no new state generating then stop the process after finding all the transitions. 3) For the state [q1, q2, .... qn] E Q' of DFA if any one state qi is a final state of NFA then [q1, q2, ...... qn ] becomes a final state. Thus the set of all the final states E F' of DFA. Example 1.10.1 Determine the DFA from a given NFA. M = ((qo, qı), (a, b), 8, qo, {q1}) with the state table diagram for d given below. 8 a b 90 91 (90.41) AU Dec .- 16, Marks 10, May-18, Marks 13 Solution: Let the DFA M'= (Q', Σ, δ ' , q0, F') Now, the 8' function will be computed as follows - As & (qo, 0) = {qo, q1} 8' ([qo], 0) = [qo, q1] As in NFA the initial state is q0, the DFA will also contain the initial state [q0]. Let us draw the transition table for 8 function for a given NFA. Input 0 1 State 8 (90, 0) {90, 9} = ô (90,1) 8 (91, 0) {90, 9} = 8 ( q1. 1) 8 Function for NFA 47 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank

--- Page 48 ---
Course Code/Title: CS3401/Theory of Computation From the transition table we can compute that there are [q0], [q1], [q0, q1] states for its equivalent DFA. We need to compute the transition from state [q0, q1]. ô ({qo, q1},0) = > (qo, 0) U 8 (q1, 0) = {qo, q1} U ¢ = {qo, qı} So, 8' ([qo, q1], 0) = [qo, q1] Similarly, d' ({qo, q1}, 1) = 8 (q0, 1) U 8 (q1,1) = {q1} U {qo, qı} = {q0, q1} So, d' ([qo, q1], 1) = [qo, q1] As in the given NFA q1 is a final state, then in DFA wherever q1 exists that state becomes a final state. Hence in the DFA final states are [q1] and [q0, q1]. Therefore set of final states F = {[q1], [q0, q1]} Input 0 1 0 State 0 [90.91] 1 1 [90. 91] [41] 1 0 [9]] [90. 91] [90. 91] [90 91] Transition table for equivalent DFA Fig. 1.10.1 Transition diagram for DFA We can even change the names of the states of DFA. A = [q0] B = [q1] C= [q0, q1] With these new names the DFA will be as in Fig. 1.10.2. 0 A C 0,1 1 1 B Fig. 1.10.2 An equivalent DFA CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN 48

--- Page 49 ---
Course Code/Title: CS3401/Theory of Computation Example 1.10.2 Construct DFA equivalent to the NFA M = ((p, q, r), (0, 1), 8, p {q, s}) Where & is defined in the following table. 8 0 1 P [q, s) łql q H (q, r] r [s] S (p] AU: Dec .- 04, Marks 8 Solution: To construct DFA, ô {p, 0} = {q, s} new state generated 8 {p, 1} = {q} ô {q, 0} = {r} 8 {q, 1} = {q, r} new state 8 {r, 0} = {s} 8 (r, 1) = {p} 8 (s, 0} = - 8 {s, 1} = {p} 8 {{q, s}, 0} = {r} 8 {{q, s}, 1} = {p, q, r} new state 8 { {q, r), 0} = {r, s} new state 8 {{q, r}, 1} = {p, q, r} 8 {{p, q, r}, 0} = {q, r, s} new state 6 {{p, q, r}, 1} = {p, q, r} ô {{r, s), 0} = {s} § {{r, s), 1} = {p} ô {{q, r, s}), 0} = {r, s} 8 {{q, r, s), 1} = {p, q, r} The transition table is as shown below. 49 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 50 ---
Course Code/Title: CS3401/Theory of Computation 8 0 > p (q, s] 1 {r} |q, r) r (s) [g, s] (pl (p) [r] (p, q, r] (r, s] (p, q, r) {q, r, s) fr, s] {s] [q, r, s] [r, s) (p, q, rl (p, q, r} (p, q, r) Example 1.10.3 Construct DFA equivalent to the given NFA. Input 0 State - p (p, q) T S S S 1 P S AU: Dec .- 06, Marks 8, Dec .- 19, Marks 13 Solution: The NFA M = {{p, q, r, s}, {0, 1}, 8 (p), {s} } The equivalent DFA will be constructed. Input 0 State -[p] [p, q] [g] [x] [r] [s] [s] [s] [p, q] [p, q, r] 1 [p] [r] t [s] [p, r] Continuing with the generated new states. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 50 LEN nirf 175" Rank

--- Page 17 ---
Course Code/Title: CS3401/Theory of Computation The groups are as given below - remainder 0 group: S0: (0, 3, 6, 9) remainder 1 group: Sı: (1, 4, 7) remainder 2 group: S2: (2,5,8) We have named out these states as S0, S1 and S2. The state So will be the final state as it is remainder 0 state. · Design: (2,5,8) S (0,3,6,9) So (0,3,6,9) · Simulation: (2,5,8) S2 (0,3,6,9) (1,4,7) (1,4,7) (1,4,7) S1 (2,5,8) (2,5,8) (0,3,6,9) (1,4,7) Fig. 1.6.4 Let us test the above FA, if the number is 36 then it will proceed by reading the number digit by digit. From start 3 6 go to state So 3 6 from Sp to So 36 input ends and we are in final state So Fig. 1.6.5 Hence the number is divisible 3, isn't it ? Similarly if number is 121 which is not divisible by 3, it will be processed as S 121 1 S1 21 17 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank N39

--- Page 18 ---
Course Code/Title: CS3401/Theory of Computation 12 S0 1 121 S1 which is remainder 1 state. Example 1.6.4 Design FA which checks whether a given binary number is divisible by three. Solution: · Logic: We will consider the input digit by digit. While considering the divisibility by 3, we should think of three types of states, namely- remainder 0 i.e. S0 state remainder 1 i.e. S1 state remainder 2 i.e. S2 state · Design: S · Simulation: 0 0 So 1 1 0 1 S1 S2 1 0 Fig. 1.6.6 Let us take some number 010 which is equivalent to 2. We will scan this number from MSB to LSB. 010 TTT So Si S2 Then we will reach to state S2 which is remainder 2 state. Similarly for input 1001 which is equivalent to 9 we will be in final state after scanning the complete input. 1001 Sı S2 Sı So Thus the number is really divisible by 3. We can also represent the simulation in following manner - 18 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank

--- Page 11 ---
Course Code/Title: CS3401/Theory of Computation R.H.S. = k4 +6k3 +13k2 +12k+4 /4 ..... (3) From equation (2) and equation (3) we get L.H.S. = R.H.S. is proved for n = k +1 Thus given expression is true. Finite Automata (FA) Definition A finite automata is a collection of 5-tuple (Q, 2, 8, qo, F) where, Q is a finite set of states, which is non empty. 2 is input alphabet, indicates input set. Qo is an initial state and qo is in Q i.e. qo E Q. F is a set of final states. 8 is a transition function or a mapping function. Using this function the next state can be determined. Finite Automata Model · The finite automata can be represented using i) Input tape - It is a linear tape having some number of cells. Each input symbol is placed in each cell. ii) Finite control - The finite control decides the next state on receiving particular input from input tape. The tape reader reads the cells one by one from left to right and at a time only one input symbol is read. (Refer Fig. 1.5.1) abababa Input tape Tape reader reading the input symbol Finite control Fig. 1.5.1 Model for finite automata · For example: suppose current state is q, and suppose reader is reading the symbol 'a' then it is finite control which decides what will be the next state at input 'a'. The transition from current state q with input w to next state q' producing w' will be represented as, (q, w) + (q', w') If w is a string and M is a finite automata, then w is accepted by the FA iff (w, s) H (q, ¿) with q as final state. 11 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 12 ---
Course Code/Title: CS3401/Theory of Computation · The set of strings accepted by a FA given by M then M is accepted by language L. The acceptance of M by some language L is denoted by L (M). · A machine M accepts a language L iff, L= L (M). Representation of Finite Automata The strings and languages can be accepted by a finite automata, when it reaches to a final state. There are two preferred notations for describing automata : 1. Transition diagram A transition diagram or transition graph can be defined as collection of - 1) Finite set of states K. 2) Finite set of symbols 2. 3) A non empty set S of K. It is called start state. 4) A set F S K of final states. 5) A transition function K×A ->K with K as state and A as input from 2* S1 Represents the state Represents transition from one state to another Số OR Start state Start So ST 1 Final state S. The notations used in transition diagram are - For example: The FA can be represented using transition graph. The machine initially is in start state S0 then on receiving input 0 it changes to state S1. From S0 receiving input 1 the machine changes its state to S4. The state S2 is a final state or accept state. When we trace the input for transition diagram and reach to a final state at end of input string then it is said that the given input is accepted by transition diagram. 12 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 21 ---
Course Code/Title: CS3401/Theory of Computation 0 1 90 0 0 91 1 q2 1,0 1 0 93 1 Fig. 1.6.10 Example 1.6.8 Define DFA. Design a DFA to accept the binary numbers which are divisible by 5. Solution: i) DFA - Refer section 1.6. ii) DFA for divisibility by 5: For designing DFA as a divisibility by 5 tester for a binary string we will consider following states. q0 - remainder 0 state q1 - remainder 1 state q2 - remainder 2 state q3 - remainder 3 state q4 - remainder 4 state Now we will design DFA by considering above states and finding next states with input 0 and 1. The DFA will be 0 1 1 q1 0 q3 1 04 D 0 1 0 q2 Simulation: Consider the string (1111)2 = (15)10 qo H 1111 1q1 H 111 11q3 H 11 111q2 H 1 1111 qo H Divisible by 5. q0 is a final state. 21 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank --

--- Page 22 ---
Course Code/Title: CS3401/Theory of Computation Consider input string (1110) = (14) 10 q0 H 1110 1 q1 H 110 11 q3 H 10 111 q2 H 0 1110 q4 H Remainder 4. q4 is remainder 4 state. Example 1.6.9 Draw transition diagram for recognizing the set of all operators in C language. AU: Dec .- 07, Marks 10 Solution: · Logic: In C language there are various operators, such as relational operators bitwise operators and arithmetic operators. Various relational operators are <, <= , >=, != , ==. The bitwise operators are &&, |. Let us draw the transition diagram for these operators. · Design: -- S14 ! S15 return (operator, OR) & S12 & S13 return (operator, AND) SO < S1 S2 return (operator, LE) Other S3 return (operator, LT) 1 = Other S4 S5 return (operator, EQ) > S9 S6 S7 )return (operator, GE) = Other $16 return (operator, NOT) Other S8 return (operator, GT) S10 Other S1 return (operator, NE) Fig. 1.6.11 Example 1.6.10 Construct a DFA accepting all string w over {0, 1}, such that number of 1's in w is 3 mod 4. AU: Dec .- 11, Marks 8 22 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 7 ---
Course Code/Title: CS3401/Theory of Computation As L.H.S. = R.H.S. the given expression is true for n = k + 1. Example 1.4.5 Prove: 2" > n3 where n ≥ 10. Solution: 1) Basis of induction - Initially we assume, n = 10 then 210 > (10)3 is true 2) Induction hypothesis - Now assume, n = k then 2K > k3 3) Inductive step- Now we will assume k = k + 1 then the equation becomes L.H.S. = 2(k+1) = 2.2k ≥(1 + 1/10)3.2k ≥(1 + 1/k)3. 2k But as 2k > k3 we will replace 2k by k3 then ≥ (1+ 1/k)3. k3 ≥ (k+1 /k)3. k3 ≥ (k+1)3 /k3. k3 ≥ (k+1)3 = R.H.S. Hence 2k+1 ≥ (k + 1)3 is also true. This proves 2" > n3 for n ≥ 10. Example 1.4.6 Prove: 2" > n using principle of mathematical induction. Solution: 1) Basis of induction - Assume n = 1 then, 2" > n becomes 21 > 1 2>1 This also implies 2" - n > 0 7 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 8 ---
Course Code/Title: CS3401/Theory of Computation 2) Induction hypothesis - Assume n = k is true i.e. 2k > k i.e. 2k - k > 0 Thus 2k - k implies a positive number. i.e. 2k - k =t 3) Inductive step - Assume k = k + 1 then 2k+1 _ (k+ 1) 2k.2 - (k+1) But from equation (1) 2k = k + t then (k+t).2 - k - 1 i.e. 2k + 2t - k - 1 k+ 2t - 1 > 0 i.e. as positive number. This implies 2k+1 > k + 1. Hence 2" > n true. Example 1.4.7 Prove 2" <= n! for all n > = 4. Solution: 1. Basis of induction - Let n = 4 then L.H.S. 24 = 16 R.H.S .= n! (4)! (1 x 2 x 3 x 4) = 24 i.e. L.H.S <= R.H.S. Hence 2 <= n! is proved. 2. Induction hypothesis We assume n = k and 2k <= k! is also true where k >= 4. Now we have to consider n = k + 1, then L.H.S. = 2k+1 = 2k 21 R.H.S. = (k + 1) != (k+ 1) k! By induction hypothesis we have k! > 2k (k + 1) k! > 2k 21 where k >= 4 8 CHENNAI INGSTOUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY LEN nirf 175" Rank --

--- Page 3 ---
Course Code/Title: CS3401/Theory of Computation Example 1.3.1 Prove that V2 is not rational. AU: Dec .- 11, Marks 8 Solution: Proof: We will assume that, V2 is a rational number. That means we can write V2 = a/b ... (1) where a and b are integers and a/b is irreducible and can be simplified to lowest term. Squaring on both sides of equation (1), 2 = a2/b2 i.e. 2b2 = a2 This shows that L.H.S. is even (i.e. multiple of two). Hence R.H.S. is also even. Now if we write a = 2 k then, 2b2 = (2 k)2 = 4k2 b2 = 2 k2 This means, even b is an even number. This is contradiction our assumption that a/b simplified to lowest term because a and b are both even. So V2 cannot be rational. Example 1.3.2 Prove that if n is a positive integer such that n mod 4 is 2 or 3 then n is not a perfect square. AU: Dec .- 12, Marks 6 Solution: The method of contra positive is used. According to method of contra position for proving "If A then B" we prove if not A then not B. Here we prove - IF n is a perfect square then n mod (4) must be 0 or 1. Consider that there exists a positive integer n such that n mod 4 is 0 or 1. Then n is a perfect square i.e. n = m2, where m is another positive integer. Following are some cases - 1) If m mod 4 = 0 then m = 4i. For instance - IF i = 1 then m is perfect square Here i = 1, 4, 9, ... 2) If m mod 4 = 1, then m = 4i + 1. Here i = 2, 6, 12, 20, ... 3) If m mod 4 = 2, then m = 4i + 2. Here we fail to identify value of i that will make m as perfect square. 4) If m mod 4 = 3 then m = 4i + 3. Again there does not exist any value of i which lead to m as perfect square. This proves that n is a positive integer such that n mod 4 is 2 to 3 and n is not a perfect square. 3 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY nirf 175" Rank LEN

--- Page 4 ---
Course Code/Title: CS3401/Theory of Computation Proof by Counter Example In order to prove certain statements, we need to see all possible conditions in which that statement remains true. There are some situations in which the statement can not be true. For example - There is no such pair of integers such that a mod b = b mod a Proof: Consider a = 2 and b = 3. Then clearly 2 mod 3 3 mod 2 Thus the given pair is true for any pair of integers but if a = b then naturally a mod b = b mod a Thus we need to change the statement slightly. We can say a mod b = b mod a, when a = b. (This type of proof is called counter example. Such proof is true only at some specific condition. Inductive Proof AU: May-07,08,12,14,16,17, Dec .- 12, Marks 16 Inductive proofs are special proofs based on some observations. It is used to prove recursively defined objects. This type of proof is also called as proof by mathematical induction. The proof by mathematical induction can be carried out using following steps 1. Basis: In this step we assume the lowest possible value. This is an initial step in the proof of mathematical induction. For example, we can prove that the result is true for n = 0 or n = 1. 2. Induction hypothesis: In this step we assign value of n to some other value k. That mean we will check whether the result is true for n = k or not. 3. Inductive step: In this step, if n = k is true then we check whether the result is true for n = k + 1 or not. If we get the same result at n = k + 1 then we can state that given proof is true by principle of mathematical induction. Example 1.4.1 Prove by induction on n that 2"i=0 i = n(n+1)/2 AU: May-12, Marks 6 Solution: Initially, 1) Basis of induction - Assume, n = 1. Then, L.H.S= n, = 1 R.H.S = n(n+1)/2 = 1(1+1)/2 = 2/2 = 1 4 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 55 ---
Course Code/Title: CS3401/Theory of Computation State Input 0 1 - [do] [9] [92] [2] [92] [9] [9] - [90, 9+] [90. 92] [40,92] [90, 91,92] [90.92, 91] [40, 91,92, q] [9%] . [90,92] [90, 91,42] * [90. 92. 4] [90.97] [90,91.9A [90. 91/92, 9%] [90.92- 9/1 [90 91.92.9d Example 1.10.6 Construct a non-deterministic finite automaton accepting the set of strings over {a,b} ending in aba. Use it to construct a DFA accepting the same set of strings. AU: May-14, Marks 10 Solution: NFA for accepting the strings ending with aba is, a, b a b a 92 93 Fig. 1.10.4 The transition table will be Input a b State {90,91} 92 92 93 93 0 We will obtain 8 transitions for each input on each state. 8 (qo, a) = {qo, q1} = [qo, q1] new state. 55 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 56 ---
Course Code/Title: CS3401/Theory of Computation ò (qo, b) = {qo} 8 (qı, a) = ¢ o (q1, b) = {q2} 8 (q2, a) = {q3} 8 (q2, b) = ¢ 8 (q3, a) = ¢ 8 (q3, b) = ¢ The & transitions on newly generated states is 8 ([qo, q1], a) = {qo, q1} i.e. [qo, q1] 8 ([qo, q1], b) = {qo, q2} = [qo, q2] new state 8 ([qo, q2], a) = {qo, q1, q3} = [qo, q1, q3] new state 8 ([qo, q2], b) = {qo} 8 ([qo, q1, q3], a) = [qo, q1] 8 ([qo, q1, q3], b) = [qo, q2] As no new state is getting generated. We will build transition table is as follows - Input a State b b [40] [40] [9]] 6 a a [90,91] [42] b [92] [43] [90,41] [90,92] [90,92] [90,91,93] [%] [90/91/93] [90,91] The DFA will be as shown in Fig. 1.10.5 [40,92] a a [90.91.93] [90,92] b Fig. 1.10.5 The states q1, q2 and q3 are non-reachable. Hence they will be eliminated. Example 1.10.7 Construct a DFA equivalent to the NFA. M=({a, b, c, d}, {0,1} 8, a {b, d}) where 8 is a defined as AU Dec .- 14, Marks 6 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY pou LEN 175" Rank 56

--- Page 45 ---
Course Code/Title: CS3401/Theory of Computation = 8 - Closure (8 (& - Closure (q1), 1)) = 8 - Closure (8 (q1, 1)) = 8 - Closure (q1) d' (q1, 1) = {q1} The transation table for NFA without & will be, 0 1 T * 91 e The transition diagram will be - 0 0,1 q1 1 Review Question 1. If L is accepted by a NFA with & transition then show that L is accepted by an NFA without & transition. AU: May-04, Dec .- 09, Marks 8; May-09, Marks 12, Dec .- 12, Marks 8 Equivalence of NFA and DFA AU: May-05,12,13,14,16,18, Dec .- 04,06,14,15,16,19, Marks 13 NFA can be converted to equivalent DFA. Following theorem illustrates this concept. Theorem: Let L be a set accepted by non-deterministic finite automation. Then there exists a deterministic finite automation that accepts L. OR Prove that "A language L is accepted by some DFA if and only if L is accepted by some NFA". AU: Dec .- 15, Marks 10, AU: May-05, Dec .- 14, Marks 10 Proof :Let M = (Q, Σ, δ, q0, F) be an NFA for language L. Then define DFA M' such that, Μ' = (Q', Σ', δ', qo', F') The states of M' are all the subset of M'. The Q'=2º. F' be the set of all the final states in M. The elements in Q' will be denoted by [q1, q2, q3, ... qi] and the elements in Q are denoted by {qo, q1, q2, ... }The [q1, q2, ... qi] will be assumed as one state in Q' if in the NFA q0 is a initial state it is denoted in DFA as qo' = [qo]. We define, 45 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 46 ---
Course Code/Title: CS3401/Theory of Computation 8' ([q1, q2, q3, ... qi], a) = [P1, P2, P3, ... Pj] if and only if, 8({q1, q2 ,q3, .... qi}, a) = {P1, P2, P3, .... Pj} This means that whenever in NFA, at the current states (q1, q2, q3, ... qi} if we get input a and it goes to the next states [P1, P2, ... Pj] then while constructing DFA for it the current state is assumed to be [q1, q2, q3, ... qi, a]. At this state, the input is a and the next is assumed to be [P1, P2, ... Pj]. On applying & function on each of the states q1, q2, q3, ... qi the new states may be any of the states from [P1, P2, ... Pj]. The theorem can be proved with the induction method by assuming length of input string x 8' (qo, x) = [q1, q2, ... qi]. if and only if, 8 (qo, x) = {q1, q2, q3, ... qi} Basis: If length of input string is 0 i.e. |x| = 0, that means x is & then qo' = [qo] Induction: If we assume that the hypothesis is true for the input string of length m or less than m. Then if x a is a string of length m+1. Then the function &' could be written as, 8' (qo, xa) = d' (d' (qo, x), a) 8' (qo, x) = [P1, P2, ... Pj] if and only if, 8 (qo, x) = {P1, P2, P3, ... Pj} By definition of 8' 8' ([P1, P2, ... Pj], a) = [r1, r2, .... rk] if and only if, 8 ({P1, P2, ... Pj} a) = {r1, r2, ... rk} Thus 8' (qo, x a) = [r1, r2, .... [k] if and only if 8 (qo, x a) = {r1, r2, ... k} is shown by inductive hypothesis. Thus L (M) = L (M') With the help of this theorem, let us try to solve few examples. Conversion from NFA to DFA We will discuss the method of converting NFA to its equivalent DFA. 46 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 61 ---
Course Code/Title: CS3401/Theory of Computation = {q1, q2) i.e state B itself. 6' (B, 2) = & - closure {8 (q1, q2), 2} = & - closure {8 (q1, 2) U 8 (q2, 2)} = 8 - closure {q2} = {q2} i.e state C. Hence, §' (B, 0) = ¢ §' (B, 1) = B §' (B, 2) = C 0 1 1 A B 2 2 c . Fig. 1.11.5 The partial transition diagram will be, Now we will obtain transitions for C: 8' (C, 0) = & - closure {8 (q2, 0)} = 8 - closure {0} = ¢ 8' (C, 1) = & - closure {8 (q2, 1)} = 8 - closure {0} = 0 8' (C,2) = & - closure {8 (q2, 2)} = q2 61 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 62 ---
Course Code/Title: CS3401/Theory of Computation 0 1 A B 1 2 2 C 2 Fig. 1.11.6 Hence the DFA is, As A = {q0, q1, q2} in which final state q2 lies hence A is final state in B = {q1, q2} the state q2 lies hence B is also final state in C = {q2}, the state q2 lies hence C is also a final state. Example 1.11.3 Consider following NFA with ¿. AU: May-11, Marks 6 Input a b C State T P {p} {q} 00 {r} o q {a} {r} {p) {r} {p} Convert it to its equivalent DFA. Solution: We will first compute & - closure for start state p. & closure (p) = {p} call it as state A. Now we will obtain 8 transitions on state A. a a b p q 8 C b C E a Fig. 1.11.7 State A CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGTOUTE . TECHNOLOGY LEN {a} nirf 175" Rank 62

--- Page 53 ---
Course Code/Title: CS3401/Theory of Computation Assuming [p] as start state and [r] as final state Input a b State [p] [p] [p, q] [q] [r] [x] . [r] 0 0 [p, q] [p, r] [p, q, r] * [p, r] [p] [p, q] [p, q, r] [p, r] [p, q, r] The states [p, r] and [p, q, r] are final states as they contain [r]. The DFA can be represented by the transition diagarm as shown - a b [p] [p,q] b b a b [p,q,r] a a [p.r] The states [q] and [r] are eliminated as they are dead states. Example 1.10.5 Construct a DFA accepting binary strings such that the third symbol from the right end is 1. AU May-12, Marks 10 Solution: The strings in such a language are - either 0 or 1 1 0 or 1 0 or 1 The NFA will be, 0,1 1 0,1 0,1 92 The transition table can be - 53 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 54 ---
Course Code/Title: CS3401/Theory of Computation Input 0 1 States {40,91} 91 92 92 92 Af - We will obtain the 8 transitions on each input and state. 8 (qo, 0) = [qo] 8 (qo, 1) = [qo, q1] i.e. [qo, q1] -> New state 8 (q1, 0) = [q2], 8 (q1, 1) = [q2] 8 (q2, 0) = [qf], o (q2, 1) = [qf] 8 ([qo, q1], 0) = {qo, q2}, = [qo, q2] i.e. New state 8 ([qo, q1], 1) = {qo, q1, q2} = [qo, q1, q2] -> New state o ([qo, q2], 0) = {qo, qF} = {qo, qf} -> New state 8 ([qo, q2], 1) = {qo, q1, 9f} = [qo, q1, qf] -> New state 8 ([qo, q1, q2], 0) = {qo, q2, qf} -> New state 8 ([qo, q1, q2], 1) = {qo, q1, q2, qf } -> New state ô ([qo, qf], 0) = [qo] 8 ([qo, qf], 1) = [qo, qı] 8 ([qo, q1, qr], 0) = [qo, q2] 8 ([qo, q1, qf], 1) = [qo, q1, q2] 8 ([qo, q2, qf], 0) = [qo, qf] 8 ([qo, q2, qf], 1) = [qo, q1, qf] 8 ([qo, q1, q2, qf], 0) = [qo, q2, qf] 8 ([qo, q1, q2, qf], 1) = [qo, q1, q2, 9f] The transition table for above computation is as given below - The states q1, q2 and qf are not reachable states. Hence they can be eliminated. 54 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 13 ---
Course Code/Title: CS3401/Theory of Computation S 0 1 So S2 1 0 S4 0 S3 Fig. 1.5.2 Transition diagram Another example: a So a S b S2 S3 b We have drawn a transition diagram for the input aabb. S4 Note that the start state is So and final state is S4. The input set is 2 = {a,b}. The states S1, S2, S3 are all intermediate states. 2. Transition table This is a tabular representation of finite automata. For transition table the transition function is used. For example: Input a b States - 92 q2 The rows of the table corresponds to states and columns of the table correspond to inputs. Here q0 is start state, q2 is final state. Types of Automata Finite Automata Deterministic Finite Automata (DFA) Non Deterministic Finite Automata (NFA) Fig. 1.5.3 DFA and NFA CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE & TECHNOLOGY nirf 175" Rank N39 13

--- Page 14 ---
Course Code/Title: CS3401/Theory of Computation There are two types of finite automata - i) Deterministic Finite Automata. ii) Non Deterministic Finite Automata. Deterministic Finite Automata (DFA) AU: Dec .- 07, 10, 11, 13, 16, 19, May-07, 14, Marks 13 The finite automata is called Deterministic Finite Automata if there is only one path for a specific input from current state to next state. For example, the DFA can be shown in Fig. 1.6.1. Sp S. b SI Fig. 1.6.1 Deterministic finite automata From state S0 for input 'a' there is only one path, going to S2. Similarly from S0 there is only one path for input b going to S1. The DFA can be represented by the same 5-tuples described in the definition of FSM. Definition of DFA A deterministic finite automation is a collection of following things - 1) The finite set of states which can be denoted by Q. 2) The finite set of input symbols 2. 3) The start state qo such that qo E Q. 4) A set of final states F such that F E Q. 5) The mapping function or transition function denoted by 8. Two parameters are passed to this transition function: One is current state and other is input symbol. The transition function returns a state which can be called as next state. For example, q1 = 8 (qo, a) means from current state qo, with input a the next state transition is q1. In short, the DFA is a five tuple notation denoted as : Α= (Q, Σ, δ, qo, F) The name of DFA is A which is a collection of above described five elements. Example 1.6.1 Design a FA which checks whether the given binary number is even. Solution: 14 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank

--- Page 41 ---
Course Code/Title: CS3401/Theory of Computation = {q1, q2} 8' (qo, b) = & - closure (8 (8' (qo, ¿), b)) = 8 - closure (8 (& - closure (qo), b)) = 8 - closure (8 (qo, b)). = ¢ 8' (q1, a) = & - closure (8 (8' (q1, ¿), a)) = 8 - closure (8 (& - closure (q1), a)) = 8 - closure (8 (q1, q2), a) = 8 - closure (8 (q1, a) U ò (q2, a)) = 8 - closure (¢ U ¢) = 0 8' (q1, b) = & - closure (8 (8' (q1, ¿), b)) = 8 - closure (8 (& - closure (q1), b)) = 8 - closure (8 (q1, q2), b) = 8 - closure (8 (q1, b) U ò (q2, b)) = 8 - closure (q2) = 8 - closure (q2) = {q2} 8' (q2, a) = & - closure (8 (8' (q2, ¿), a)) = 8 - closure (8 (& - closure (q2), a)) = 8 - closure (8 (q2, a)) = 8 - closure (¢) = 0 8' (q2, b) = 8 - closure (8 (8'(q2, ¿), b)) = 8 - closure (8 (& - closure (q2), b)) = 8 - closure (8 (q2, b)) = 8 - closure (q2) = {q2} The transition table can be - 41 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 42 ---
Course Code/Title: CS3401/Theory of Computation Input a b State T {41 92) 0 {2} 92 0 {2} States qı and q2 becomes the final as closures of q1 and q2 contains the final state q2. The NFA can be shown by transition diagram as shown in Fig. 1.9.5. b a b 92 a Fig. 1.9.5 Example 1.9.4 Convert the given NFA with & to ordinary NFA. b a E 92 0 a b Fig. 1.9.6 Solution: We will first obtain & - closure of each state. & - closure (qo) = {qo, q1, q2} - closure (q1) = {q1, q2) - closure (q2) = {q2} Now &' transitions for each state on each input. 8' (qo, a) = & - closure (8 (8' (qo, ¿), a)) = 8 - closure (8 (& - closure (qo), a)) = & closure (8 ((qo, q1, q2), a)) = & closure (8 (qo, a) U ò (q1, a) U ô (q2, a)) = 8 - closure (§ U qı U qı). = 8 - closure (q1) = (q1, q2) 42 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 73 ---
Course Code/Title: CS3401/Theory of Computation C X × D E G X A BCDEFG Step 2: The states that need to be marked are (A, B), (A, D), (A, E), (A, F), (A, G), (A, H), (B, D), (B, E), (B, F), (B, G), (B, H), (D, E), (D, F), (D, G), (D, H), (E, F), (E, G), (E, H), (F,G), (F, H), (G, H). If we observe transition table then 8 (B, 0) = G 8 (B, 1) = C 8 (H, 0) = G 8 (H, 1) = C Similarly 8 (D, 0) = C 8 (D, 1) = G 8 (F, 0) = C 8 (F, 1) = G Thus pairs (B, H) and (D, F) are equivalent Step 3: C X X D E G H V × A B CDE F G Step 4: Now, consider pair (A, E) 73 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY nirf 175" Rank LEN

--- Page 74 ---
Course Code/Title: CS3401/Theory of Computation 8 (A, 0) = | B 8 (A, 1) = F 8 (E, 0) = H 8 (E, 1) = F Equivalent pairs Pair (A, E) is equivalent Step 5: i) Pair (A, B) 8 (A, 0) =B 8 (A, 1) = F -> Final, Non Final states § (B, 0)=G ô (B, 1) = C -> Final, Non Final states Pair (A, B) is not equivalent ii) Pair (A, D) 8 (A, 0) = | B 8 (D. 0) = C 8 (A, 1) = F 8 (D. 1) = G Final, non-final state .. Pair (A, D) is not equivalent iii) Pair (A, F) 8 (A, 0) = | B ô (A, 1) = F 8 (F, 0) = C ô (F, 1) = G Final, non-final pair Pair (A, F) is not equivalent. iv) Pair (A, G) §(A, 0)=B & (A, 1) = F 8(G, 0)=G ô (G, 1) = E To decide equivalence of pair (A, G) we need to find equivalence of pair (B, G) and (E, F) § (B, 0) =G ô (B, 1) = C -> Final, Non-Final pair 8 (G, 0) = G 8 (G, 1) = E -> Final, Non Final pair Pair (B, G) is not equivalent. Similarly we find pair (E, F) and ultimately pair (A, G) are not equivalent. v) Pair (A, H) 8 (A, 0) = B 8 (A, 1) = F -> Final, Non-Final pair 8 (H, 0) = G 8 (H, 1) = C -> Final, Non-Final pair 74 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank

--- Page 63 ---
Course Code/Title: CS3401/Theory of Computation 8' (A, a) = ¿ - closure (8 (A, a)) = 8 - closure (8 (p, a)) = 8 - closure (p) = {p} i.e. state A only. 8' (A, b) = & - closure (8 (A, b)) = 8 - closure (8 (p, b)) = 8 - closure (q) = {q, p} i.e. {p, q} Let us call it state B. 8' (A, b) = B 8' (A, c) = & - closure (8 (A, c)) = 8 - closure (8 (p, c)) = 8 - closure (r) = {q, r} Call it as state C. State B = {p, q} 8' (B, a) = 8 - closure (8 (B, a)) = 8 - closure (8 (p, q), a) = & - closure (8 (p, a) U ô (q, a)) = 8 - closure (p U q) = 8 - closure (p, q) = & - closure(p) Ų ¿ - closure(q) = {P} U {q} = {p, q} i.e. state B only. §' (B, a) = B. 8' (B, b) = & - closure (§ (B, b)) = 8 - closure (8 (p, q), b) = 8 - closure (8 (p, b) U ò (q, b)) = 8 - closure (q U r) = 8 - closure (q, r) 8' (B, b) = & - closure(q) U ¿ - closure(r) = {p, q} U {q, r} = {p, q, r} i.e. state D §(B, b) = D. 8' (B, c) = & - closure (8 (B, C)) 63 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN

--- Page 64 ---
Course Code/Title: CS3401/Theory of Computation = 8 - closure (8 (p, q), c) = 8 - closure (8 (p, c) U ô (q, c)) = 8 - closure (r U ¢) = & - closure (r) = {q, r} 8' (B, c) = C State C = {q, r} 8' (C, a) = & - closure (8 (C, a)) = 8 - closure (8 (q, r), a) = 8 - closure (8 (q, a) U ò (r, a)) = 8 - closure (q U r) = 8 - closure (q) U ¿ - closure(r) = {p, q} U {q, r} = (p, q, r) i.e. state D. 8' (C, a) = D 8' (C, b) = & - closure (8 (C, b)) = 8 - closure (8 (q, r), b) = 8 - closure (8 (q, b) U ò (r, b)). = 8 - closure (r U ) = 8 - closure (r) = {q, r) i.e. state C. 8' (C, b) = C 8' (C, c) = & - closure (8 (C, c)) = 8 - closure (8 (q, r), c) = 8 - closure (8 (q, c) U ò (r, c)) = 8 - closure (§ U p) = 8 - closure (p) = {p} i.e. state A. 8' (C, c) = A State D = {p, q, r} 8' (D, a) = & - closure (8 (D, a)) 64 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank

--- Page 69 ---
Course Code/Title: CS3401/Theory of Computation = 8 - closure (D) U ¿ - closure(C) = DUC = {D, C} §' (B, 1) = {D, C} 8' (C, 0) = & - closure(8 (8 - closure(C), 0) = 8 - closure (8 (C, 0) = 8 - closure (¢) 8' (C, 0) = ¢ 8' (C, 1) = & - closure (8 (& - closure (C), 1) = 8 - closure (8 (C, 1) = 8 - closure (C) = {C} 8' (C, 1) = C 8' (D, 0) = & closure (8 (& - closure(D), 0)) = 8 - closure (8 (D, 0) = 8 - closure (B) 8' (D, 0) = (B, C) 8' (D, 1) = & - closure (8 (e-closure(D), 1)) = 8 - closure (8 (D, 1) = 8 - closure () 8' (D, 1) = ¢ The transition table will be Input 0 1 State A ĮA. B. C. D] B C D The NFA will be: 69 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 70 ---
Course Code/Title: CS3401/Theory of Computation 1 A 8 0 1 1 0 Conversion of NFA to DFA 8 (A, 0) = 0 8 (A, 1) = {A, B, C, D} -> call it as state P 8 (A, 1) = p 8 (P, 0) = P = 8 ((A, B, C, D), 0) = ô (A, 0) U ô (B, 0) U ô (C, 0) U õ (D, 0) {B, C} -> call it as state Q S (P, 0) = Q 8 (P, 1) = ô ((A, B, C, D), 1) = ô (A, 1) U ô (B, 1) U ô (C, 1) U ô (D, 1) = {A, B, C, D} 8 (P,1) = P § (Q, 0) = > ((B, C,), 0) = ô (B, 0) U § (C, 0) 8 (Q, 0) = ¢ ô (Q, 1)=((B, C), 1) = ô (B, 1) U 8 (C, 1) = {C, D} U {C} = {C, D} -> call it as state R 8 (Q, 1) = R § (R, 0)={8 (C, D), 0} = {B, C} 8 (R, 0) = Q 8 (R, 1)=> ((C, D), 1) 70 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 43 ---
Course Code/Title: CS3401/Theory of Computation 8' (qo, b) = & - closure (8 (8' (qo, ¿), b)) = & closure (8 (& - closure (qo), b)) = 8 - closure (8 ((qo, q1, q2), b)) = 8 - closure (§ (qo, b) U 8 (q1, b) U ò (q2, b)) = 8 - closure (qo U ¢ U qo) = 8 - closure (qo) = (q 0, q 1, q 2 ) 8' (q1, a) = & - closure (8 (8' (q1, ¿), a)) = 8 - closure (8 ( - closure (q1), a)) = 8 - closure (8 ({q1, q2), a)) = 8 - closure (8 (q1, a) U ò (q2, a)) = 8 - closure (qı U qı) = 8 - closure (q1) = {q1, q2} 8' (q1, b) = & - closure (8 (8' (q1, ¿), b)) = 8 - closure (8 (& - closure (q1), b)) = 8 - closure (8 (q1, q2), b)) = 8 - closure (8 (q1, b) U 8 (q2, b)) = & closure (¢ U qo) = {q 0, q 1, q 2 } 8' (q2, a) = & - closure (8 (8' (q2, ¿), a)) = 8 - closure (8 ( - closure (q2), a)) = 8 - closure (8 (q2, a)) = 8 - closure (q1) = (q1, q2) 8' (q2, b) = & - closure (8 (8' (q2, E), b)) = & closure (8 (& - closure (q2), b)) = 8 - closure (8 (q2, b)) = 8 - closure (qo) = {q 0, q 1, q 2 } The transition table can be - 43 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY LEN nirf 175" Rank

--- Page 44 ---
Course Code/Title: CS3401/Theory of Computation a b {9 0, 91 92) {q 1,92} 92 {9 0. 9192} Example 1.9.5 Construct an NFA without & transitions for the NFA given in Fig. 1.9.7. AU May-12, Marks 8 0 1 90 91 Fig. 1.9.7 Solution: We will first obtain & - closures of qo and qı as follows - & - Closure (qo) = {qo, q1} E - Closure (q1) = {q1} Now &' transitions on each input symbol is obtained as - 8'(qo, 0) = & - Closure (8 (8' (qo, E), O)) = 8 - Closure (8 (& - Closure (qo), 0)) = 8 - Closure (8 ((qo, q1), 0)) = 8 - Closure (qo) d' (qo, 0) = {qo, q1} 8' (qo, 1) = & - Closure (8 (8' (qo, ¿), 1)) = 8 - Closure (8 (& - Closure (qo), 1)) = 8 - Closure (8 ((qo, q1), 1)) = 8 - Closure (q1) d' (qo, 1) = {q1} 8' (q1, 0) = & - Closure (8 (8' (q1, E), O)) = 8 - Closure (8 (& - Closure (q1), 0)) = 8 - Closure (8 (q1, 0)) = 8 - Closure (¢) 8' (q1, 0) = ¢ 8' (q1, 1) = & - Closure (8 (8' (q1, E), 1)) 44 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY N39 nirf 175" Rank

--- Page 31 ---
Course Code/Title: CS3401/Theory of Computation F' = FU{qo} if &-closure contains state off F otherwise M' is a NFA without & moves. The 8' function can be denoted by 8" with some input. For example, 8' (q, a) = 8" (q, a) for some q in Q and a from 2. We will apply the method of induction with input x. The x will not be ¿ because d' (qo, ¿) = {qo} 8" (qo, ¿) = c-closure (qo). Therefore we will assume length of string to be 1. Basis: |x| = 1. Then x is a symbol a. 8'(qo,a)= 8" (qo,a) Induction |x| > 1 Let x = wa 8'(qo, w) = d'(d'(qo, w), a) By inductive hypothesis, d'(qo, w) = 8" (qo, w) = p Now we will show that &' (p, a) = 8 (qo, wa) But 8'(p, a) = U d'(q, a) = U 8" (q, a) qinp qinp As p= 8" (qo, w) We have, U 8" (q, a) = 8" (qo, wa) q in p Thus by definition 8" d'(qo, wa) = 8" (qo, wa) Rule for conversion 8'(q, a) = & - closure (8 (8 (q, ¿), a)) where 8(q,c) = & - closure (q) Before solving some examples based on conversion of NFA with & to NFA without & above rule should be remembered. Subset construction algorithm: 1. Initially & - closure (qo) is in D states of DFA. 2. While there are unmarked states qi in D states { 31 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 32 ---
Course Code/Title: CS3401/Theory of Computation 3. Marks qi 4. for each symbol a { 5. U = & - closure (ò(qi, a)) 6. if (U is not in D states) { 7. Add U as unmarked state in D states. } 8. Dtran [qi, a] = U } } Example for Understanding Example 1.9.1 Convert the given NFA with & to NFA without &. AU: Dec .- 09, Marks 8 0 1 2 E q1 8 q2 Fig. 1.9.1 Solution: Step 1: We will first obtain & - closure of each state i.e. we will find out & - reachable states from current state. Hence E - closure (qo) = {qo, q1, q2} E - closure (q1) = {q1, q2} & - closure (q2) = {q2} As & - closure (qo) means with null input (no input symbol) we can reach to qo, qı or q2. In a similar manner for qı and q2 & - closures are obtained. Step 2: Now we will obtain 8' transitions for each state on each input symbol. 8' (qo, 0) = & - closure (8 (8' (qo, E), O)) = 8 - closure (8(& - closure(qo), 0)) = 8 - closure (8(qo, q1, q2), 0). = 8 - closure (8(qo, 0) U ò(q1, 0) U & (q2,0)) = & - closure (qo U ¢ U ¢) = 8 - closure (qo) = {q0, q1, q2} 32 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 39 ---
Course Code/Title: CS3401/Theory of Computation = {3, 4, 5, 6, 8, 9} § ' (7, b) = & - closure (8 (8' (7, ¿), b)) = 8 - closure (8 (& - closure (7), b)) = 8 - closure (8 (3, 4, 6, 7, 8, 9), b) = 8 - closure (8 (3, b) U ô (4, b) U ô (6, b) U ô (7, b) U ô (8, b) U õ (9, b)) = 8 - closure (¢ U ¢ U 7 U ¢ U ¢ U ¢) = 8 - closure (7) = {3, 4, 6, 7, 8, 9} 8' (8, a) = & - closure (8 (8'(8, ¿), a)) = & - closure (ò (& - closure (8), a)) = 8 - closure (ò (3, 4, 6, 8, 9), a) = 8 - closure (8 (3, a) U ô (4, a) U ô (6, a) U ô (8, a) U ô (9, a)) = 8 - closure (¢ U 5 U ¢ U ¢ U ¢) = 8 - closure (5) = {3, 4, 5, 6, 8, 9} 8' (8, b) = & - closure (8 (d' (8, ¿), b)) = 8 - closure (8 (¿ - closure (8), b)) = 8 - closure (8 (3, 4, 6, 8, 9), b) = & - closure (8 (3, b) U ô (4, b) U ô (6, b) U ô (8, b) U ô (9, b)) = 8 - closure (¢ U ¢ U 7 U ¢ U ¢) = 8 - closure (7) = {3, 4, 6, 7, 8, 9} 8' (9, a) = & - closure (8 (8' (9, ¿), a)) = 8 - closure (8 (& - closure (9), a)) = 8 - closure (8 (9, a)) = 8 - closure (¢) = ¢ 8' (9, b) = & - closure (8 (d' (9, ¿), b)) = 8 - closure (8 (& - closure (9), b)) = 8 - closure (8 (9, b)) = 8 - closure (¢) 39 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 40 ---
Course Code/Title: CS3401/Theory of Computation = ϕ Now we will build the transition table using above calculated 8' transitions. Input a b State [0] [1] ¢ [1] (2, 3, 4, 6, 9) (2) (3, 4, 5, 6, 8, 9] (3, 4, 6, 7, 8, 9) (3) (3, 4, 5, 6, 8, 9) (3, 4, 6, 7, 8, 9] (4) (3, 4, 5, 6, 8, 9) (5) (3, 4, 5, 6, 8, 9] (3, 4, 6, 7, 8, 9} [6] (3, 4, 6, 7, 8, 9) 171 (3, 4, 5, 6, 8, 9) {3, 4, 6, 7, 8, 9) (8) (3, 4, 5, 6, 8, 9) (3, 4, 6, 7, 8, 9] [9] 0 State {2} is a final state since & - closure (2) contains 9 which is actually a final state in given NFA. Similarly state 5, 7, 8 and 9 are final states. Example 1.9.3 Convert the following NFA with & to NFA without E. b a 91 E 92 Fig. 1.9.4 Solution: We will first obtain & - closures of q0, q1 and q2 as follows. E - closure (qo) = {qo} & - closure (q1) = {q1, q2} & - closure (q2) = {q2} Now the &' transitions on each input symbol is obtained as, 8' (qo, a) = & - closure (8 (8' (qo, ¿), a)) = 8 - closure (8 (& - closure (qo), a)) = 8 - closure (8 (qo, a)) = 8 - closure (q1) 40 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 65 ---
Course Code/Title: CS3401/Theory of Computation = 8 - closure (8 (p, q, r), a) = 8 - closure (8 (p, a) U ò (q, a) U ò (r, a)) = 8 - closure (p U q U r) = 8 - closure (p) U ¿ - closure(q) U ¿ - closure (r) = (p, q, r) i.e. state D. 8'(D, a) = D 8' (D, b) = & - closure (8 (D, b)) = 8 - closure (8 (p, q, r), b) = 8 - closure (8 (p, b) U ô (q, b) U ô (r, b)) = 8 - closure (q Ur U ¢) = 8 - closure (q, r) = 8 - closure (q) U ¿ - closure (r) = (p, q, r) i.e. state D. 8' (D, b) = D 8' (D, c) = & - closure (8 (D, c)) = 8 - closure (8 (p, q, r), c) = 8 - closure (8 (p, c) U ò (q, c) U ò (r, c)) = 8 - closure (r U ¢ U p) = 8 - closure (r) U & - closure (p) = {q, r} U {P} = {p, q, r} i.e. state D. 8' (D, c) = D The transition table from above calculations can be obtained as, Input a State A A B B C D D D CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY b C B C D C C A D D 65 N39 nirf 175" Rank -

--- Page 66 ---
Course Code/Title: CS3401/Theory of Computation As state A = {p} it is a start state and states C and D contain final state r, hence these are final states. The transition diagram for the DFA is, a A c C C b b C a B b D a Fig. 1.11.8 a,b,c Example 1.11.4 Consider the following & - NFA for an identifier. Consider the & -closure of each state and find it's equivalent DFA. AU: Dec .- 15, Marks 10 Solution: ¿ - closure (1) = {1} Call it as state A 8' (A, letter) = & - closure(8 (1, letter) = 8 - closure(2) = {2, 3, 4, 5, 8, 10} Call it as state B 8' (A, letter) = B 8' (A, digit) = & - closure(8 (1, digit)) = $ 8' (A digit) = ¢ 8' (B, letter) = & - closure(8 (2, 3, 4, 5, 8, 10), letter) = 8 - closure (6) = {6, 7, 4, 5, 8, 10) i.e. = {4, 5, 6, 7, 8, 10} Call it as state C 8' (B, letter) = C 8' (B, digit) = & - closure(&(2, 3, 4, 5, 8, 10), digit) = 8 - closure (9) = {9, 7, 4, 5, 8, 10} i.e. = {4, 5, 7, 8, 9, 10} Call it as state D 8' (B, digit) = D 8' (C, letter) = & - closure(8 (4, 5, 6, 7, 8, 10), letter) 66 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY LEN nirf 175" Rank

--- Page 71 ---
Course Code/Title: CS3401/Theory of Computation ô (R, 1) = {C} 8 (C, 0) = ¢ 8 (C, 1) = C The transition table will be Input 0 1 State T P Q ₱ R 0 C C 1 A 0 1 R 1 0 1 Review Questions 1. Let L be a set accepted by NFA then prove that there exists a deterministic finite automaton that accepts L. Is the converse true? AU: May-05, Marks 10 2. Prove that a language L is accepted by some & - NFA if and only if L is accepted by some DFA. AU: Dec .- 06, Marks 8 3. Prove the equivalence of NFA and DFA using subset construction. AU: Dec .- 13, Marks 8 Minimization of DFA AU Dec .- 18, May-13, 14, 16, Marks 10 The minimization of FSM means reducing the number of states from given FA. Thus we get the FSM with redundant states after minimizing the FSM. While minimizing FSM we first find out which two states are equivalent we can represent those two states by one representative state. 71 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY CEN nirf 175" Rank

--- Page 72 ---
Course Code/Title: CS3401/Theory of Computation The two states q1 and q2 are equivalent if both 8 (q1, x) and 8 (q2, x) are final states or both of them are non final states for all x € 2* (2* indicate any string of any length) we can minimize the given FSM by finding equivalent states. Example 1.12.1 Minimize the DFA as given below. AU May-16, Marks 8 0 Start A 1 8 0 D 1 0 1 E F 1 0 G H 1 Q Fig. 1.12.1 Solution : We will build the transition table for given DFA, as follows: Input 0 1 State T < B F B G A c D C G B H F F C G G G E H G C Step 1: We will build a table in following format for finding equivalent states and mark final and non-final states with x 72 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 33 ---
Course Code/Title: CS3401/Theory of Computation 8' (qo, 1) = & - closure (8 (8'(qo,E), 1)) = 8 - closure (8 (qo, q1, q2), 1) = 8 - closure (8 (qo, 1) U ò (q1, 1) U 8 (q2,1)) = & closure (§ U q1 U ¢) = 8 - closure (qı) d' (qo, 1) = {q1, q2} 8' (q1,0) = & - closure (8 (8'(q1,8), 0)) = 8 - closure (8 (& - closure (q1), 0)) = & closure (8 (q1,q2), 0) = & closure (8 (q1, 0) U ò (q2, 0)) = & closure (¢U¢) = 8 - closure (¢) = ¢ 8' (q1, 1) = & - closure (8 (8' (q1, ¿), 1)) = 8 - closure (8 (& - closure (q1), 1)) = 8 - closure (ò (q1, q2), 1) = 8 - closure (8 (q1, 1) U ò (q2, 1)) = 8 - closure (q1 U ¢) = 8 - closure (q1) = {q1, q2} 8'(q2, 0) = c- closure (8 (8' (q2, E), 0)) = 8 - closure (8 (& - closure (q2), 0)) = 8 - closure (8 (q2, 0)) = 8 - closure () = 0 8' (q2, 1) = & - closure (8 (8' (q2, &), 1)) = & closure (8 (& - closure (q2), 1)) = 8 - closure (8 (q2,1)) = 8 - closure (¢) 8' (q2, 1) = 8' (qo, 2) = & - closure (8 (8' (q2, ¿), 2)) 33 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank

--- Page 34 ---
Course Code/Title: CS3401/Theory of Computation = 8 - closure (8 (& -closure (qo), 2)) = 8 - closure (8 (qo, q1, q2), 2). = 8 - closure (8 (qo, 2) U 8 (q1, 2) U 8 (q2, 2)) = 8 - closure (q2) = 8 - closure (q2) = {q2} 8' (q1,2) = & - closure (8 (8' (q1, E), 2)) = 8 - closure (8 (& - closure (q1), 2)) = 8 - closure (8 (q1, q2), 2) = &- closure (8 (q1, 2) U 8 (q2, 2)) = & closure (¢ U q2) = {q2} 8' (q2, 2) = & - closure (8 (8' (q2, ¿), 2)) = 8 - closure (8 (& - closure (q2), 2)) = 8 - closure (8 (q2, 2)) = 8 - closure (q2) = {q2} Now we will summarize all the computed &' transitions - 8'(qo, 0) = {qo, q1, q2}, d'(qo, 1) = {q1, q2), d'(qo, q2) = {q2} 8'(q1, 0) = ¢ d'(q1, 1) = {q1, q2}, d'(q1, 2) = {q2} 8'(q2, 0) = ¢ 8'(q2, 1) = ¢ 8'(q2, 2) = {q2} Step 3: From this we can write the transition table as - Input 0 1 2 State {41, 92} {92] 91 {91. 92} {92} 92 0 CHENNAIC CHENNAI INGSTOUTE . INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank { 2} 34

--- Page 1 ---
Course Code/Title: CS3401/Theory of Computation UNIT I - AUTOMATA AND REGULAR EXPRESSIONS Syllabus: Need for automata theory - Introduction to formal proof - Finite Automata (FA) - Deterministic Finite Automata (DFA) - Non-deterministic Finite Automata (NFA) - Equivalence between NFA and DFA - Finite Automata with Epsilon transitions - Equivalence of NFA and DFA- Equivalence of NFAs with and without &-moves- Conversion of NFA into DFA - Minimization of DFAs. Need for Automata Theory · In automata theory and computability the word computability is for the computation in mathematical modeling. Hence this subject deals with the study of various mathematical models that are required for some computations. · In this study, various models, their behavior, properties, kind of languages they accept, their limitations are studied. · The theory of computation provides a set of abstract structures that are useful for solving certain classes of problems. These abstract structures can be implemented on available hardware or software platform. · Using these abstract structures the required design efforts can be determined for the actual model. · Theory of computation plays an important role in compiler design. · In switching theory, design and analysis of digital circuits automata theory can be applied. · Many times, to prove the correctness of the program, automata theory is used. Introduction to Formal Proof The formal proof can be using deductive proof and inductive proof. The deductive proof consists of sequence of statements given with logical reasoning in order to prove the first or initial statement. · The formal proof can be using deductive proof and inductive proof. · The deductive proof consists of sequence of statements given with logical reasoning in order to prove the first or initial statement. The initial statement is called hypothesis. · The inductive proof is a recursive kind of proof which consists of sequence of parameterized statements that use the statement itself with lower values of its parameter. . In short, formal proofs are the proofs in which we try to prove that statement B is true because statement A is true. The statement A is called hypothesis and B is called conclusion statement. In other words, "if A then B" we say that B is deduced from A. Additional Forms of Proof We will discuss additional forms of proofs with the help of some examples. We will discuss 1. Proofs about sets. 2. Proofs by contradiction. 3. Proofs by counter example. A Proof about Sets 1 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 2 ---
Course Code/Title: CS3401/Theory of Computation The set is a collection of elements or items. By giving proofs about the sets we try to prove certain properties of the sets. For example if there are two expressions A and B and we want to prove that both the expressions A and B are equivalent then we need to show that the set represented by expression A is same as the set represented by expression B. Let PUQ = QUR if we map expression A with PUQ and expression B with QUR then to prove A = B we need to prove PUQ = QUP. This proof is of the kind "if and only if" that means an element x is in A if and only if it is in B. We will make use of sequence of statements along with logical justification in order to prove this equivalence. Let us prove PUQ = QUP. Proving LHS Sr. No. Statement 1. x is in PUQ 2. x is in P or x is in Q 3. x is in Q or x is in P 4. x is in QU P Proving RHS Sr. No. Statement 1. x is in Q UP 2. x is in Q or x is in P 3. x is in Por x is in Q 4. x is in PU Q Justification Given 1) And by definition of union 2) And by definition of union 3) Rule (2) And by definition of union Justification Given 1) And by definition of union 2) And by definition of union 3) Rule (2) And by definition of union Hence PUQ = QUP. Thus A = B is true as element x is in B if and only if x is in A. Proof by Contradiction In this type of proof, for the statement of the form if A then B we start with statement A is not true and thus by assuming false A we try to get the conclusion of statement B. When it becomes impossible to reach to statement B we contradict ourself and accept that A is true. For example - Prove PUQ = QUP. Proof · Initially we assume that PUQ = QUP is not true. That is PUQ # QUP. · Now consider that x is in Q, or x is in P. Hence we can say x is in PUQ (according to definition of union). · But this also implies that x is in QUP according to definition of union. · Hence the assumption which we made initially is false. · Thus PUQ = QUP is proved. 2 CHENNAI CHENNAI INGSTOUTE TECHNOLOGY INSTITUTE OF TECHNOLOGY nirf 175" Rank -- LEN

--- Page 75 ---
Course Code/Title: CS3401/Theory of Computation Pair (A, H) is not equivalent Step 6: In this way, we find pairs (A, E), (B, H) and (D, E) are only equivalent pairs and all other remaining pairs are not equivalent. B × 0 X D × x × E × x × F × x x x G × x x × X X H × × X x X x A B CDE F G Thus we get equivalent pairs as (A, E), (B, H), (D, F). Hence the minimized DFA will be, 0 1 A B D B G C C A C D C G G G A Example 1.12.2 Minimize the 75 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 76 ---
Course Code/Title: CS3401/Theory of Computation b a 2 b b 5 6 3 b b 7 AU Dec .- 18, Marks 7 Solution: We will construct table as follows We will mark final and non-final state by x 2 3 X 4 5 6 7 × × x × 1 2 3 4. 5 6 Now we will find the equivalence of the pairs (3, 4), (3, 5), (3, 6), (4, 5), (4, 6),(5, 6), Pair (3, 4) 8 (3, a) = 6 8 (3, b) = 7 -> Final, Non Final Pair 8 (4, a) = 5 8 (4, b) = 4 -> Final, Non Final Pair Hence put x in (3, 4) Pair (3, 5) 76 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 85 ---
Course Code/Title: CS3401/Theory of Computation Q'1 = {q4} Q'2 = {q0} Q'3 = {q1, q2, q3} Hence, II1 = {{q4}, {q0}, {q1, q2, q3}} Now in set (q1, q2, q3) we compare qı with q2 under 0-column we get q2 and q1 which are in the same set, and under 1 - column we get q4. And for state qı and q3 we get same entries under 0 column and under 1 - column. Hence we can group them {q2, q3}. The equivalence class becomes, II2 = {{q4}, {qo}, {q1}, {q2, q3} } Now further we cannot partition any of the set. We get II3 = { {qo}, {q2}, {q4}, {q1, q3} } Hence minimized DFA is - Input 0 The transition diagram will be - 1 0.1 State T Q1 91 92 02 91 94 94 Review Question 0 91 0 Q4 1 94 0.1 Fig. 1.12.3 1. Discuss on the relation between DFA and minimal DFA. AU: May-13, Marks 6 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY 85 nirf 175" Rank N39

--- Page 86 ---
Course Code/Title: CS3401/Theory of Computation Two Marks Questions with Answers Q.1 List any four ways of theorem proving. Ans. : 1) Proof by contradiction. 2) Proof by contrapositive. 3) Proof by counter example. 4) Proof by principle of mathematical induction. Q.2 Differentiate between proof by contradiction and proof by contrapositive. AU: May-11 Ans. The difference between proof by contradiction and contrapositive Sr. No. Proof by contradiction 1. If "A is true then B is also true" is to be proved then by this method we assume A is true and B is false. When we get some sort of contradiction then we declare that given statement is true. 2. In this method the primary goal is to prove contradiction. But what should be the contradiction is not definite. Q.3 Define automata. Proof by contrapositive If "A is true then B is also true" is to be proved then by this method we assume B is false and prove that A is also false. This ultimately proves the given statement. In this method, the primary goal is clear. Ans .: Automata are the kind of machines which take some string as input. This input goes through a finite number of states and may enter in the final state. Q.4 List the applications of automata theory. Ans .: 1. Automata theory is the base for the formal languages and these formal languages are useful of the programming languages. 2. Automata theory plays an important role in compiler design. 3. To prove the correctness of the program automata theory is used. 4. In switching theory and design and analysis of digital circuits automata theory is applied. 5. Automata theory deals with the design finite state machines. Q.5 What is structural induction? AU: Dec .- 11 Ans .: Structural induction is a kind of proof by induction. It is used to prove that there exists a relationship between some function of integers and a given formula. For this proof technique - 1. Prove the pattern is true for smallest number we care about. 2. Assume it holds for an arbitrary number n. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank 86

--- Page 29 ---
Course Code/Title: CS3401/Theory of Computation a C Start E 92 Fig. 1.8.2 8 Input a b c 8 State 91 91 0 92 92 92 G We can parse the string aabbcc as follows - 8 (qo, aabbcc) H & (qo, abbcc) H ô (qo, bbcc) Hồ (qo, abbcc) H ô (qi, bbcc) Hồ (q1, bcc) Hồ (q1, cc) Hồ (q1, Ecc) Hồ (q2, cc) Hồ (q2, c) Hồ (q2, E) Thus we reach to accept state, after scanning the complete input string. Definition of & - closure The & - closure (p) is a set of all states which are reachable from state p on & - transitions such that: i) ¿ - closure (p) = p where p E Q. ii) If there exists & closure (p) = {q} and 8(q,c) = r then ¿ - closure (p) = {q, r} Example 1.8.2 Find & - closure for the following NFA with &. 29 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 30 ---
Course Code/Title: CS3401/Theory of Computation C E 90 8 Fig. 1.8.3 Solution: ¿ - closure (qo) = {qo, q1, q2} means self state + ¿ - reachable states. ¿ - closure (q1) = {q1, q2} means qı is a self state and q2 is a state obtained from qı with & input. & - closure (q2) = {q2} Example 1.8.3 Build a non-deterministic FSM with & that accepts L = {w = {a, b, c) *: w contains at least one string that consists of three identical symbolds in a row). For example · The following strings are in L are aabbb, baabbbccc · The following strings are not in L are &, aba, abababab, abcbcab. Solution: The NDFSM with & is a 92 a a 94 a,b,c 00 95 b 96 b q7 98 a,b,c a,b,c q10 911 912 a,b,c Equivalence of NFA with & to NFA without & AU May-04, 09, 12, Dec .- 09, 12, 13, Marks 12 In this method we try to remove all the & transitions from given NFA. The method will be 1. Find out all the & transitions from each state from Q. That will be called as &-closure {q} where qi E Q. 2. Then 8' transitions can be obtained. The 8' transitions means an E-closure on & moves. 3. Step-2 is repeated for each input symbol and for each state of given NFA. 4. Using the resultant states the transition table for equivalent NFA without & can be built. Theorem: If L is accepted by NFA with & transitions, then there exist L which is accepted by NFA without & transitions. AU May-09, Marks 12; Dec .- 09, 13, Marks 8 Proof : Let, M=(Q, E, δ, qo, F) be an NFA with & transitions. Construct M' = (Q, Σ, δ', q0, F') where 30 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY pou LEN 175" Rank

--- Page 19 ---
Course Code/Title: CS3401/Theory of Computation Consider input 1001 SH 001 1 H S1 001 10 H S2 01 100 H S1 1 1001 H So i.e. ACCEPT state Example 1.6.5 Consider the finite automata transition table shown below with F = {q0} AU: Dec.-10, Marks 10 States Inputs 0 1 92 ៛ 9 93 q2 93 93 41 92 Find the language accepted by the finite automata. Solution: The transition diagram for the given transition table will be as shown in Fig. 1.6.7. 1 91 1 0 0 0 0 1 92 93 1 Fig. 1.6.7 · The sets accepted by this language are (0011, 00, 11, 1010, 0101, ... } · This shows that all the strings contain even number of 0's and even number of 1's. Hence the language accepted by this finite automata contains "all the strings having even number of O's and even number of 1's". Example 1.6.6 Design FA to accept L, where L = {Strings in which a always appears trippled) over the set 2 = {a, b}. AU: Dec .- 10, Marks 10 Solution: · Logic: For this particular language the valid strings are aaab, baaaaaa, bbaaab and so on. The a always appears in a clump of 3. 19 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY N39 nirf 175" Rank --

--- Page 20 ---
Course Code/Title: CS3401/Theory of Computation Note that in Fig. 1.6.8 the sequence of tripple a is maintained to reach to the final state. · Design b b a 91 a 92 93 a Fig. 1.6.8 Example 1.6.7 Construct a DFA that accepts all the strings on {0, 1} except those containing the substring 101. AU: May-07, Marks 6, May-14, Marks 8 Solution : · The simplest method to construct such DFA is to construct DFA for the language containing the substring 101. · Now just change the non-final states to final states and make final state as non-final state. The DFA then becomes shown in Fig. 1.6.9. 0 0 1,0 1 0 1 92 94 1 0 93 1 Fig. 1.6.9 · is a DFA that does not accept the language containing substring 101. 20 INGSTOUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 93 ---
Course Code/Title: CS3401/Theory of Computation b b Fig. 1.7.1 Non-deterministic finite automata Q.23 Draw the transition diagram (automata) for an identifier. AU: Dec .- 13 Ans. : letter | digit Tetter Q.24 Define deductive proof. AU Nov .- 14 Ans .: The deductive proof consists of sequence of statements given with logical reasoning in order to prove the first or initial statements. The initial statement is called hypothesis. Q.25 Design DFA to accept strings over 2 = (0,1) with two consective O's. AU: Nov .- 14 Ans .: 0, 1 0 0 1 Fig. 1.13.7 Q.26 Define Deterministic Finite Automata (DFA). AU: Dec .- 16,19 Ans .: The Deterministic Finite Automata is a collection of following things: 1) Finite set of states - Q 2) Finite set of input symbols - ≥ 3) The start state qo such that qo E Q. 4) The set of final states F such that F E Q 5) The mapping function 8. The DFA is denoted by, M = (Q, Σ, δ, q0, F). 93 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 94 ---
Course Code/Title: CS3401/Theory of Computation Q.27 Draw a non-deterministic automata to accept a string containing substring 0101. AU: May-16 Ans .: 1 0 D 91 1 0 1 1 0 Fig. 1.13.8 Q.28 Obtain the DFA equivalent to the following NFA. AU: Dec .- 03 0,1 0 1 Fig. 1.13.9 Ans .: The transition table for given NFA can be drawn as follows. States 0 1 1901, 1911 1901 - 1921 [q2) - - To construct equivalent DFA. 8 (qo, 0) = {qo, qı} a new state 8 {qo, 1} = {qo} 8 {q1, 1) = - 8 (q1, 1} = {q2} 8 {q2, 0} = - 8 {q2, 1} =- & {{qo, q1), 0} = {qo, qı} 8 {{qo, q1), 1} = {qo, q2} a new state 8 {{qo, q2}, 0} = {qo, q1} 8 {{qo, q2}, 1} = {qo} Hence the DFA is 94 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY N39 nirf 175" Rank

--- Page 9 ---
Course Code/Title: CS3401/Theory of Computation i.e. 2k+1 <= (k+1)! is true. As it is true for n = k + 1, the given expression is proved. Example 1.4.8 Prove that for every integer n>0 the number 42n+1 + 3+2 is multiple of 13. AU: May-08, 14, Marks 10 Solution: We will prove this using method of induction. Basis - Assume n= 1 P(1)= 42(1)+1+31+2 = 43+33 = 91 which is = 13×7 Hence 42n+1 + 3n+2 is multiple of 13 when n = 1. Inductive step - Assume n = k P(k) = 42k+1 + 3k+2 P(k) = 13 m where m is some integer. Thus we assume that given expression is true for P(k). Now we will prove that is also true for P(k+1). Let, n = k +1 then P(k+1)= 42(k+1)+1 + 3(k+1)+2 We can simplify it as - = 4(2k+1)+2 + 3(k+2)+1 = 42. 42k+1+42. (3k+2 _ 3k+2) + 31.3k+2 equal to zero Taking common factors as 42 and 3k+2 we get = 42 (42k+1 + 3k+2) + 3k+2 (-42+3) =42 (13m) + 3k+2(-13). equation (1) P(k + 1) = 13 (42m - 3k+2) That is multiple of 13, for P(k + 1). This proves that given integer is multiple of 13. 9 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 10 ---
Course Code/Title: CS3401/Theory of Computation Example 1.4.9 Prove for every n > 1 by mathematical induction 2" i3 = {n(n+1)/2}2. AU: May-17, Marks 16 Solution: · Base Case Assume n = 1 then L.H.S. = (1)3 = 1 R.H.S. = [1(1+1) /2]2 = (2/2)2 = 1 As L.H.S. = R.H.S. Thus the given expression is true for n = 1. · Inductive Hypothesis Assume n = k is true Then 13+23+33 + ... + k3 = [k(k+1) /2]2 is also true, for some k in the set of positive integers. 13+23+33 + ... +k3 = [k(k+1) /2]2 ..... (1) · Inductive Step We will show that if n = k is true then it implies n = k + 1 is also true. L.H.S. = 13+23+33+ ... + k3+(k+1)3 R.H.S. = [(k+1)(k+2) /2]2 From equation (1) [k(k+1)3 /2]2 +(k+1)3 = k2(k+1)2 /4 +(k+1)3 = k2(k+1)2 +4(k+1)3 /4 = k2(k2+2k+1)+4(k3 +3k2+3k+1) = k2(k2 +2k+1)+4k3+12k2 +12k+4 /4 = k4+2k3+k2 +4k3 +12k2 +12k+4 L.H.S. = k4+6k3 +13k2 +12k+4/4 ..... (2) Now consider R.H.S. R.H.S. = [(k+1)(k+2) /2]2 = (k+1)2 (k+2)2 /4 = (k2 +2k+1)(k2 +4k+4) /4 = k4+4k3 +4k2 + 2k3 +8k2 +8k+k2 +4k+4 /4 10 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY pou LEN 175" Rank

--- Page 79 ---
Course Code/Title: CS3401/Theory of Computation 0 1 B E C F C Đ H D E H E F I G B C H H I U A E Solution: Distinguishable and indistinguishable states: If for some input string w, 8 (p w) gives an accepting state and 8 (q, w) gives a non-accepting state or vice versa then states p and q are called distinguishable states or non-equivalent states. If for some input string w, 8 (p, w) and 8 (q, w) both produces either accepting states or non-accepting states, then states p and q are called indistinguishable or equivalent states. We will apply a table filling method for minimizing the given DFA. B C X x D E X F G x H X 1 x X x X ABCDEFGH We will mark X in a final and non-final pairs. Now we will consider each pair and check for its equivalence consider pair G, H. 8 (G, 0) = H and ô (G, 1) = B 8 (H, 0) = I and ô (H, 1) = C Since pairs (H, I) and (B, C) are already marked X, pair (G, H) is not equivalent so we will mark X in pair (G, H). 79 CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 80 ---
Course Code/Title: CS3401/Theory of Computation B € x X D x E × F x x x x G x H × x × 1 X X x x ABCDEFGH Now, consider pair (E, H), § (E, 0) = F and ô (E, 1) = I 8 (H, 0) = I and ô (H, 1) = C As (F, I) and (I, C) both are final states, hence pair (E, H) is said to be equivalent and we won't put X in the pair (E, H). Now, consider pair (E, G), 8 (E, 0) = F and 8 (E, 1) = I 8 (G, 0) = H and 8 (G, 1) = B As pairs (F, H) and (I, B) have X, we put X in (E, G). Consider pair (E, F), (E, 0) = F and § (E, 1) = I § (F, 0) = G and ô (F, 1) = B Put X in pair (E, F). Hence table will be 80 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 87 ---
Course Code/Title: CS3401/Theory of Computation 3. Prove that if it is true for n, then it must be true for n + 1, as well. Q.6 How does recursive function solves Fibonacci function ? Ans .: Following recursive formula is used to solve Fibonacci function. Fib(n) = Fib(n - 1) + Fib(n - 2) Fib(0) = 0 and Fib(1) = 1 For example, to compute Fib(4) Fib(4)= 3 1 Fib(3) = 2 Fib(2) = 1 1 Fib(2) = 1 Fib(1) = 1 Fib(1) Fib(0) 1 1 Fib(1) Fib(0) 1 1 0 1 0 Q.7 What is induction principle? Give an example. AU: Dec .- 12 Ans .: The proof by mathematical induction can be carried out using following steps - 1. Basis: In this step we assume the lowest possible value. This is an initial step in the proof of mathematical induction. For example, we can prove that the result is true for n = 0 or n = 1. 2. Induction Hypothesis: In this step we assign value of n to some other value k. That mean we will check whether the result is true for n = k or not. 3. Inductive Step: In this step, if n = k is true then we check whether the result is true for n k+ 1 or not. If we get the same result at n = k + 1 then we can state that given proof is true by principle of mathematical induction. Q.8 What is the proof by contradiction? AU: May-12 Ans .: In proof by contradiction technique, for the statement of the form if A then B we start with statement A is not true and thus by assuming this false A we try to get the conclusion of statement B. When it becomes impossible to reach to statement B, then we contradict ourselves and accept that A is true. Q.9 Define NFA with & transition. Is the NFA's with & transitions more powerful than the NFA's without & transitions? AU: May-05, 18 Ans .: The NFA can be formally defined as a collection of 5 tuples. Q is a finite set of states. 2 is a finite set of inputs. 87 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY LEN nirf 175" Rank

--- Page 88 ---
Course Code/Title: CS3401/Theory of Computation 8 is called next state or transition function. q0 is initial state. F is a final state where F = Q. No. The NFA with e transitions are equivalent to the NFA without & transitions. Q.10 Construct a finite automata for the language {0" | n mod 3 = 2, n≥ 0}. AU: Dec .- 06 Ans .: While testing divisibility by 3 we group the input as D 0 0 Fig. 1.13.1 q0 remainder 0 state. qı remainder 1 state. q2 remainder 2 state. The q2 is remainder 2 state hence we will make it as final state. Q.11 What is a finite automation? Give two examples. AU: May-07, Dec .- 15, 17 Ans.: A finite automation is a collection of 5-tuples (Q, Σ, δ, q0, F) where · Q is a finite set of states, which is a non-empty one. · 2 is input alphabet, indicates input set. · qo in Q is the initial state. · F is a set of final states. · 8 is a transition function. For example - 1. The FA which accepts only those strings which start with 1 and end with 0. Co 0 A 1 B 1 Fig. 1.13.2 2. The FA which accepts the only input 101. 88 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY N39 nirf 175" Rank

--- Page 81 ---
Course Code/Title: CS3401/Theory of Computation B C × x D X E x F x X x x G x × H x X x 1 x x x × × × ABCDEF G H Consider pairs (D, H), (D, G), (D, F), (D, E) out of which 8 (D, 0) = E and ô (D, 1) = H 8 (H, 0) = I and ô (H, 1) = C Hence put X in (D, H), 8 (D, 0)=E 8 (D, 1) = H 8 (G, 0) = H ô (G, 1) = B Pair (E, H) is equivalent. For pair (H, B) we will find their input transitions. Both state H and state B yield final states we say pair (D, G) is equivalent at the same time we find pair (H, B) as equivalent. 8 (D, 0) = E and ô (D, 1) = H 8 (F, 0) =G and 8 (F, 1) = B As pair (E, G) is having X we will put X in (D, F). Consider pair (D, E), 8 (D, 0) =E 8 (D, 1) = H 8 (E, 0) =F & (E, 1) = I Pair (D, E) is not matching, put X in (D, E). Consider pair (B, G), 8(B, 0)=C ô (B, 1) = F 8(G, 0) =H ô (G, 1) = B As pair (C, H) is not equivalent put X in (B, G). Consider pair (B, F), 8(B, 0)=C ô (B, 1) = F 8(F, 0)=G ồ (F, 1) = B 81 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank --

--- Page 82 ---
Course Code/Title: CS3401/Theory of Computation As pair (C, G) is having X, we put X in (B, F). Consider pair (B, E), §(B, 0)=C ô (B, 1) = F 8(E, 0) =F & (E, 1) = I As (C, F) and (F, I) are both final states we say pair (B, E) is equivalent. Consider pair (D, B), §(B,0)=C ô (B, 1) = F § (D, 0)=E 8 (D, 1) = H As pair (E, C) have X, we put X in pair (D, B). Consider pair (A, H), (A, 0)=B ò(A, 1)=E 8 (H, 0) = I 8 (H, 1) = C As X is in pair (B, I) we put X in (A, H). Consider pair (A, G), §(A, 0)=B & (A, 1) =E 8(G, 0) = H ô (G, 1) = B As pairs (B, H) and (B, E) are equivalent. We can say pair (A, G) is equivalent. Consider pair (A, E), 8(A, 0) =B & (A, 1) = E 8 (E, 0) =F & (E, 1) = I As (B, F) have X we will put X in (A, E) pair (A, D) is already equivalent. Consider pair (A, B), 8(A, 0)=B & (A, 1) = E 8(B, 0)=C ô(B, 1) = F As pair (B, C) have X, we will put X in (A, B). Hence finally table will be, 82 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 83 ---
Course Code/Title: CS3401/Theory of Computation B x C x x D x E x x x F x x x x G x X x x H x X x × x 1 x x x x x x BCDEF GH The blank entries represent equivalent states State A = G = D State B = H = E State C = I = F Hence reduced DFA will be, Input 0 1 State T A H B B € C C A B Example 1.12.4 Minimize the Finite automaton Fig. 1.12.2 below and show both the given and the reduced one are equivalent. 83 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 84 ---
Course Code/Title: CS3401/Theory of Computation 0 0 1 0 .0.1 0 1 Fig. 1.12.2 AU May-14, Marks 10 Solution: We will first construct the transition table for given DFA - Input 0 1 State 4 94 Now we will partition the states Q in Q10 and Q20. The state q4 is a final state. Hence Q10= F = {q4} Q2º = Q- Q10 = {qo, q1, q2, q3) Io = {{q4}, {qo, q1, q2, q3} } Now we will compare qo with q1. We find that under 0 column we get q1 and q2. And q and q2 lie in the same set i.e. Q2. But under 1-column of qo and qı we get q3, q4 respectively. But q3 € Q20 and q4 € Q10. Hence they are not 1- equivalent. Similarly compare qo and q2. We will find that under 0 column of qo and q2 we get qı only. But under 1- column of qo and q2 we get q3 and q4. The q3 E Q2 and q4 E Q1. Hence q0 and q2 are not 1 - equivalent. Similarly q0 is not 1 - equivalent to q3 because of 1 column entries. 84 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGS TOUTE . TECHNOLOGY nirf 175" Rank N33

--- Page 91 ---
Course Code/Title: CS3401/Theory of Computation Q.17 Construct a DFA for the following: a) All strings that contain exactly 4 - zeros. b) All strings that don't contain the substring 110. AU: Dec .- 11 Ans. a) The DFA will be 1 0 0 0 0 b) For required DFA, we will first create a DFA that contains the substring 110. 0 1 0 0.1 0 Now change non-final state to final state and final state to non-final state. D 1 1 0 0.1 0 Q.18 Find the set of strings accepted by the finite automata. AU: Dec .- 10 0, 1 Fig. 1.13.6 Ans .: Set of strings accepted by given finite automata are S = {8, 0, 1, 00, 000, 11, 111, 01, 10, 101, 110, 001, ... ) Q.19 Define a) Finite Automaton (FA) b) Transition diagram. AU: Dec .- 12 Ans .: a) Finite Automaton (FA): Refer answer of Q.11. Ans .: A finite automation is a collection of 5-tuples (Q, 2, 8, qo, F) where · Q is a finite set of states, which is a non-empty one. · 2 is input alphabet, indicates input set. · qo in Q is the initial state. · F is a set of final states. 91 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 92 ---
Course Code/Title: CS3401/Theory of Computation · 8 is a transition function. b) Transition diagram: A transition diagram can be defined as collection of - 1) Finite set of states K 2) Finite set of symbols 3) A non empty set S of K which is called start state. 4) A set FS K of final states. 5) A transition function K x A -> K with K state and A as input from 2 *. Q.20 What is meant by DFA ? [Refer section 1.6] AU: May-13 The finite automata is called Deterministic Finite Automata if there is only one path for a specific input from current state to next state. For example, the DFA can be shown in Fig. 1.6.1. Sp S b S Fig. 1.6.1 Deterministic finite automata From state S0 for input 'a' there is only one path, going to S2. Similarly from S0 there is only one path for input b going to S1. The DFA can be represented by the same 5-tuples described in the definition of FSM. Q.21 Define the term epsilon transition. AU: May-13 Ans .: In the non deterministic finite state machine, the transition that does not require input symbols for state transition and is capable of transiting to zero or more states with & is called epsilon transition. Q.22 What is a non deterministic finite automation? [Refer section 1.7] AU: Dec .- 13 · The concept of Non-deterministic Finite Automata is exactly reverse of Deterministic Finite Automata. The Finite Automata is called NFA when there exists many paths for a specific input from current state to next state. The NFA can be shown as in Fig. 1.7.1 · Note that the NFA shows from q0 for input a there are two next states q1 and q2. Similarly, from q0 for input b the next states are q0 and q1. · Thus it is not fixed or determined that Fig. 1.7.1 Non-deterministic finite automata with a particular input where to go next. Hence this FA is called non-deterministic finite automata. 92 CHENNAI INSTITUTE OF TECHNOLOGY N39 CHENNAI INGSTOUTE . TECHNOLOGY nirf 175" Rank

--- Page 89 ---
Course Code/Title: CS3401/Theory of Computation 1 0 1 Fig. 1.13.3 Q.12 Enumerate the differences between DFA and NFA. AU: May-07,14, Dec .- 17, 18 Ans. : Sr. No. DFA 1 Every input string leads to the unique state of finite automata. 2 Conversion of regular expression to DEA is complex. Regular expression can be easily converted to NFA using Thompson's construction. 3 The DFA requires more memory for storing the state information. 4. The DFA it is not possible to move to next state without reading any symbol. NFA For the same input there can be more than one next states. The NFA requires more computations to match RE with input. In NFA we can move to next state without reading any symbol. Q.13 Construct a DFA over 2 = (a, b) which produces not more than 3a's. AU: May-08 Ans .: In the given language at the most 3a's are allowed and there is no restriction on number of b's. Such a DFA can be as shown in Fig. 1.13.4. b b b b a 47 Fig. 1.13.4 a,b Dead state The above DFA accepts {&, a, aa, aaa, ab, aba, ... }. The states qo, q1, q2 and q3 are final states but state q4 is a non-final state. Thus the above DFA accepts 3a's and not more than that. Q.14 Define the languages described by NFA and DFA. AU: Dec .- 08 [Refer sections 1.6 and 1.7 · The finite automata is called Deterministic Finite Automata if there is only one path for a specific input from current state to next state. For example, the DFA can be shown in Fig. 1.6.1. CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf N39 175" Rank 89

--- Page 90 ---
Course Code/Title: CS3401/Theory of Computation Sp S. b S, Fig. 1.6.1 Deterministic finite automata · The concept of Non-deterministic Finite Automata is exactly reverse of Deterministic Finite Automata. The Finite Automata is called NFA when there exists many paths for a specific input from current state to next state. The NFA can be shown as in Fig. 1.7.1 b b 92 Fig. 1.7.1 Non-deterministic finite automata Q.15 Construct deterministic finite automata to recognise odd number of 1's and even number of zero's. AU: May-10 Ans. : 0 9: 0 1 0 1 1 The valid strings are (1, 001, 100, 010. ] 0 Fig. 1.13.5 Q.16 Construct a DFA for the language over {0, 1}* such that it contains "000" as a substring. AU : May-11 Ans .: 1 1.0 0 0 0 90 CHENNAI CHENNAI INGSTOUTE . INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 77 ---
Course Code/Title: CS3401/Theory of Computation 8 (3, a) = 6 8 (3, b) = 7 -> Final, Non Final Pair 8 (5, a) = 7 8 (5, b) = 5 -> Final, Non Final Pair Hence put x in (3, 5) Pair (3, 6) 8 (3, a) = 6 8 (6, a) = 2 -> Final, Non Final Pair 8 (3, b) = 7 8 (6, b) = 7 Hence put x in (3, 6) Pair (4, 5) 8 (4, a) = 5 8 (5, a) = 7 -> Final, non final pair 8 (4, b) = 4 8 (5, b) = 5 Hence put x in (4, 5) Pair (4, 6) 8 (4, a) = ô (6, a) = 5 8 (4. b) = § (6. b) = 4 2 7 Final, non final pair Pair (5, 6) 8 (5, a) = 7 8 (5, b) = 5 -> Final, Non Final Pair 8 (6, a) = 2 8 (6, b) = 7 -> Final, Non Final Pair Hence put x in (5, 6) Thus we do not get any of the mentioned pairs as equivalent pair. The only equivalent pairs are (1, 2), (1, 7), (2, 7) We assume state 1 = 2 = 7. We eliminate state 2 and 7 by replacing them by 1. The minimized DFA can be given by following transition table. 77 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 78 ---
Course Code/Title: CS3401/Theory of Computation Input a b State 1 1 3 3 6 1 4 5 # 5 1 5 5 1 1 a 1 b 3 a,b 6 b b 5 b a But, since state 4 and 5 are not reachable from start state 1, we eliminate them. Also, state 6 is a dead state. Hence, minimizes DFA is b b Example 1.12.3 Define distinguishable and indistinguishable states. Using table filling method, minimize the following DFA. Draw the transition diagram of resulting DFA. 78 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 141 ---
Course Code/Title: CS3401/Theory of Computation All these languages are non-regular languages and this can be proved by pumping lemma. Q.23 Is it true that the language accepted by NFA is different from regular language? Justify your answer. Ans. The language accepted by NFA is always regular. Because any NFA is equivalent to DFA. And if we represent certain language by NFA then we can definitely represent it using DFA. The regular language is a kind of language which can be represented by DFA. Hence the language accepted by NFA is not different from regular language. Q.24 Give regular expression to describe an identifier and positive integer. Ans .: id = (a - zA - Z) (a - zA - Z 0-9)* P= (0-9)+ Q.25 Find the string of minimum length in {0,1} * NOT in the language corresponding to the given regular expressions. a) 0* (100*) * 1* b) (1+01) (0 +01) * Ans. a) {110} € 0* (100*)* 1 *. b) ¢. Q.26 Design FA that accepts {0,1} *. Ans .: 0, 1 0,1 Q.27 Write a regular expression which generates strings with at least one 0 and one 1. Ans. r.e. = = (0+ 1*) (1+ 0*) (0+1)* Q.28 Give the operations that satisfy closure properties of regular languages. Ans. The operations are union, intersection, complement, difference, reversal, concatenation, reversal, homomorphism, inverse homomorphism. Q.29 Give a regular expression for the set of all strings having odd number of 1's. Ans .: r.e. = 1(0+11)* Q.30 State the precedence of regular expression operators. Ans .: 1. The star operator is having highest precedence. 2. The dot operator (concatenation) has the next highest position, 3. Final precedence is given to + i.e. union operation. The union is an associative operator. Q.31 What does regular expression (a/b) * denote ? 141 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank --

--- Page 142 ---
Course Code/Title: CS3401/Theory of Computation Ans. The strings generated by the given expression are {&, a, b, aa, bb, ab, ba, .... } This denotes the language L= {Any number of a's and/or b's including null string}. Q.32 Give the regular expression for set of all strings ending in 00. AU: Dec .- 10 Ans. r.e. = = (0+1)* 00 Q.33 Construct NFA for regular expression a*b *. AU: May-12 Ans .: Step 1 : Step 2 : a b a.b ab Q.34 Is regular set closed under complementation? Justify. AU : May-12 Ans. Yes regular set is closed under complementation. The regular set is denoted by the regular languages and regular languages are closed under complementation. Refer answer of Q.9. Q.35 Differentiate regular expression and regular language. AU : Dec .- 12 Ans. Regular expression over 2 can be defined by the operations such as rts, rs and r* where r* and s both are regular expressions. The regular languages over 2 is defined as - 1. An empty set ¢. 3. L1 U L2 are regular languages. 2. L1 L2 are regular languages. 4. Lı* is regular language. The regular language is elaboration of regular expression. The regular expression denotes the tiny logical unit. It is typically used to represent pattern. Q.36 Name any four closure properties of regular languages. AU: May-13 Ans .: 1. The union of two regular languages is regular. 2. The intersection of two regular languages is regular. 3. The complement of regular language is regular. 4. The closure operation on a regular languages is regular. Q.37 Construct NFA equivalent to the regular expression (0 + 1) 0 1. AU: Dec .- 13 Ans. : 142 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY N39 nirf 175" Rank

--- Page 143 ---
Course Code/Title: CS3401/Theory of Computation 0.1 0 1 Q.38 What is meant by equivalent states in DFA ? AU: Dec .- 07 Ans. The two states are said to be equivalent if there are same inputs coming to the state and same output going out from the states. Q.39 Construct a finite automation for the regular expression 0*1 *. Ans. : 1 1 Fig. 2.10.11 Q.40 Prove or disprove that (r+s)* = r* +s *. Ans .: L.H.S. R.H.S. (t+ s)* r*+s* = {s. r. rr. s, ss, rs, sr .... } - {E. r. r. rrr, s. ss, sss. ... } = {[ any combination of r and s} = {E, any combination of only r or any combination of only s} Note that in R.H.S. there is no combination of r and s together. Hence L.H.S. # R.H.S. i.e. (r + s)* = r*+s *. Q.41 Write regular expression for the set of strings over {0, 1} that have at least one. AU: Dec .- 15 Ans .: r.e. = (0*1+) (0+1)* because 1+ = 11 *. That means at least one 1 is always present in regular expression. Q.42 State the pumping lemma for regular languages. AU: May-16, Dec .- 18 Ans. Let, L be regular set. Then there is a constant n such that if z is any word in L and |z| ≥ n, we can write z = uvw such that |uv| ≤ n, |v| > 1 for all i ≥ 0, then uviw is in L. Q.43 What are closure properties of regular languages. AU: Dec .- 16 Ans. Following are closure properties of regular languages: 1) The union of two regular languages is regular. 2) The intersection of two regular languages is regular. 3) The complement of a regular language is regular. 143 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY N39 nirf 175" Rank

--- Page 144 ---
Course Code/Title: CS3401/Theory of Computation 4) The difference of two regular languages is regular. Q.44 Generate NFA - & to represent a* b|c AU: May-17 Ans .: b Fig. 2.10.2 Q.45 Show whether a language L = {0" 12|n> 0} is regular or not using pumping lemma. AU: May-17 Ans .: We will map W = xyz for given string W E L = {0" 12n) Suppose W = 001111 If we map W = xyz then - W = 0 011 11 x = 0, y = 011 z = 11 ... (1) By pumping lemma if W = xy z e L then the language is regular. For equation (1) we have W = xyiz. We assume i = 2 then string becomes W = xyyz = 001101111 W = 0212 0114 € L This shows that given language is not regular. Q.46 Give language of regular expression a ? (b|c) *. AU: May-17 Ans. The ? denotes preceding character occurs for one or zero times. The * means the string occurs for any number of times. The language contains a string which may begin with single a or no a and followed by any number of b's and c's. Q.47 Write regular expression for the set of all strings of 0's and 1's not containing 101 as substring. AU: May-18 Ans. : r.e. = 0*(1*000*)*1*0* In above expression all 1's are followed by 00. This pattern is repeated for any number of times. 144 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 27 ---
Course Code/Title: CS3401/Theory of Computation Sr. No. DFA 1. Every input string leads to the unique state of finite automata. 2. Conversion of regular expression to DFA is complex. 3. The DFA requires more memory for storing the state information. 4. The DFA it is not possible to move to next state without reading any symbol. NFA For the same input there can be more than one next states. Regular expression can be easily converted to NFA using Thompson's construction. The NFA requires more computations to match RE with input. In NEA we can move to next state without reading any symbol. Example 1.7.1 Construct a NFA for a language L which accepts all the strings in which the third symbol from right end is always a. Over 2= {a, b}. Solution: · Logic: The strings in such a language are of the form Anything either a or b a a or b a or b Thus we get third symbol from right end as 'a' always. The NFA then can be · Design: a.b a 9, a,b a,b 92 93 The above figure is a NFA because in state q0 with input 'a' we can go to either state q0 or state q1. Example 1.7.2 Draw state transition diagram for FA over {a, b} containing substring aabb. Solution: The FA can be a,b a 91 a 92 b 93 AU: May- 09, Marks 4 a, b b 94 Fig. 1.7.2 Example 1.7.3 Construct NFA accepting binary strings with two consecutive 0's. AU: May-12, Marks 8 Solution: For constructing this NFA the input set 2 ={0,1}. The NFA will be - CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY nirf 175" Rank LEN 27

--- Page 28 ---
Course Code/Title: CS3401/Theory of Computation 1 0 0 92 91 1 1 0 Finite Automata with Epsilon Transitions · The E-transitions in NFA are given in order to move from one state to another without having any symbol from input set E. Consider the NFA with ¿ as: 0 2 0, € 90 1, E 92 1 Fig. 1.8.1 NFA with e - transitions · In this NFA with &, qo is a start state with input 0 one can be either in state qo or in state q1. If we get at the start a symbol 1 then with & - move we can change state from qo to qı and then with input we can be in state q1. · On the other hand, from start state qo, with input 1 we can reach to state q2. · Thus it is not definite that on input 1 whether we will be in state qı or q2. Hence it is called nondeterministic finite automata (NFA) and since there are some & moves by which we can simply change the states from one state to other. Hence it is called NFA with E. Definition of NFA with & The language L accepted by NFA with &, denoted by M = (Q, E, δ, qo, F) can be defined as: Let, M = (Q, Σ, δ, q0, F) be a NFA with ε. Where Q is a finite set of states. 2 is input set. 8 is a transition or a mapping function for transitions from Qx {2Us} to 2º. q0 is a start state. F is a set of final states such that F E Q. Example 1.8.1 Construct NFA with & which accepts a language consisting the strings of any number of a's followed by any number of b's. Followed by any number of c's. Solution: Here any number of a's or b's or c's means zero or more in number. That means there can be zero or more a's followed by zero or more b's followed by zero or more c's. Hence NFA with & can be - Normally &'s are not shown in the input string. The transition table can be - 28 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 163 ---
Course Code/Title: CS3401/Theory of Computation 3 context-Free-Grammars and languages: A language. A context-Free Grammar (CFG) is described by four tuples as, G=(V,T, P.S) grammar is means of representing a where, V is a finite set of variables T is a finite set of terminals P is a finite set of productions 5 is a Start symbol. The language of a grammar: Every string of a language is generated by applying the production rules, a finito number of times. The language of a grammar, G=(V,T, P. S ) is denoted as L (G). L (G)= >W/WET and S=> w3 01 The language generated from a context Free grammar is called a context free language. 163 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 164 ---
Course Code/Title: CS3401/Theory of Computation Language and Automata: The following describes the relation between the four types of languages and automata. Languages Automaty Typeo Type 1 Type 2 Type3 Turing Machine Linear Bounded Automaty > Pushdown Automat Finite > > Automat Ex: 1) Find the highest number which can be applied the following grammar. to a) 5-> Aa, A->C/ BL, B->abc b) 3-> AS Bld, A-JaA c) 5-> aslab 50m: a) S-> Aa, B-Ba, B-> abc - type 2 [ because productions are of the room A-soy A-Sa - type 3 [form A->a] -: highest type number is 2. 164 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY N39 nirf 175" Rank

--- Page 125 ---
Course Code/Title: CS3401/Theory of Computation Assume uvw = bbbb Take u = b v = bb w=b By pumping lemma, even if we pump v i.e. increase v then language should show the length as perfect square. = uvw = uv. VW = bbbbbb = length of b is not a perfect square Thus the behaviour of the language is not regular, as after pumping something onto it does not show the same property (being square for this example.) Example 2.6.2 Is the following language regular ? Justify your answer L= {02n|n>1} AU: May-07, Marks 2, May-13, Marks 4 Solution: This is a language length of string is always even. i.e. n= 1; L= 00 n =2 L = 00 00 and so on. Let L = uvw L = 02n |z| = 2" = u v w If we add 2n to this string length. |z| = 4n = uv. vw = even length of string. Thus even after pumping 2n to the string we get the even length. So the language L is regular language. Example 2.6.3 Show that L = {0" 1n+1 | n>0} is not regular. Solution: Let us assume that L is a regular language. |z| = |uvw| = 0" 1n+1 Length of string [z] = n + n + 1 = 2n + 1. 125 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank -- LEN

--- Page 126 ---
Course Code/Title: CS3401/Theory of Computation That means length is always odd. By pumping lemma = |uv. vw| That is if we add 2n + 1 2n+1<(2n+1)+2n+1 2n+1 < 4n+ 2 But if n = 1 then we obtain 4n+ 2 = 6 which is no way odd. Hence the language becomes irregular. Even if we add 1 to the length of |z|, then |z|=2n + 1 + 1 =2n+2 = even length of the string. So this is not a regular language. Example 2.6.4 Show that set L = (0i 1 i | i ≥ 1} is not regular. AU: May-04, Dec.-04,05, Marks 6 Solution: Assume that L = {0i 11 | i ≥ 1} is regular. Let, w = 0 1 such that |w| = 2n. By pumping lemma we can write w = xyz such that |xy| ≤ n and |y| # 0. Now if xy1z E L then the language L is said to be regular. There are many cases - i) y has only 0's ii) y has only 1's iii) y has both 0's and 1's. i) If y has only O's then the string w = 0n-k |n = xz since y = 0k and i = 0 Surely n-k # n. Hence xz € L. Hence our assumption of being L regular is wrong. ii) If y has only 1's then, for i = 0 and y = 1k w= xz = 0" 1n-k As n # n-k, xz = w € L Again L is not regular. iii) If y has 0's and 1's then w = On-k Ok jj in-j = xyiz If i = 2 then w = 0n-k 0 2k 1 2j 1 n-1 € L Hence from all these 3 cases it is clear that language L is not regular. 126 CHENNAI INGSTOUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank LEN

--- Page 135 ---
Course Code/Title: CS3401/Theory of Computation AU: May-04, Dec .- 12 Ans. a) The set of all strings of 0's and 1's is given by (0 + 1)*. Hence r.e. for L1 = (0+1)* 00. b) The set of all strings beginning with 0 can be 0 (0+1) *. Hence r.e. for L2 = 0 (0+1)* 1. Q.3 Show that (r*)* = r* for a regular expression r. AU: Dec .- 04 Ans. Consider, r=0 where Σ = {0} Now L.H.S. = (r*)* =((0)*)* = {8, 0, 00, 000, ... } ... (1) Thus the set contains string of O's including null string. Now R.H.S. = r* =(0)* From equation (1) and (2) it is proved that = {8, 0, 00, ... } ... (2) L.H.S. = R.H.S. Hence, (r*)* = r Q.4 Find the language accepted by the following automaton. AU : May-05 0 B 1 0 0.1 Fig. 2.10.1 Ans. The r.e. that can be obtained from given DFA is r.e. = 0* 1* That means the strings accepted by FA will be any number of O's followed by any number of 1's. Any number means it can be zero as well. Q.5 Define regular expression. Give an example. AU: Dec .- 05, May-13 Ans. Let 2 be an alphabet which is used to denote the input set. The regular expression over 2 can be defined as follows. 1. § is a regular expression which denotes the empty set. 2. ¿ is a regular expression and denotes the set { }. 135 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY LEN nirf 175" Rank

--- Page 136 ---
Course Code/Title: CS3401/Theory of Computation 3. For each 'a' in 2 'a' is a regular expression that denotes the set {a}. 4. If r and s are regular expressions denoting the languages L1 and L2 respectively then rts is equivalent to L1 U L2 i.e. union. rs is equivalent to L1 L2 i.e. concatenation. r* is equivalent to L1* such that if 2 = {a} then Li* = {&, a, aa, ... } For example: Write the regular expression for the language accepting all possible string over = {a}. Regular expression = a Q.6 Let R be any set of regular languages. Is U Rj regular? Prove it. AU: Dec .- 06 Ans. If R be the set of regular languages then U R is regular. Proof : Let, r = r1 + 12 where r1 and r2 be the regular expressions. There exists two NFA's. Μ1 = (Q1, Σι, δι, {f1}) M1 =(Q2, 2, 82, {f2}) L(M1)= L(r1): Means the language stated by r1 is same which is represented by M1. Similarly L(M2) = L(r2). Q1 represents the set of all the states in machine M1. Q2 represents the set of all the states in machine M2. It can be represented as, M1 Start 92 M2 Fig. 2.10.2 We assume that Q1 and Q2 are totally disjoint. We will now assume q0 as a new initial state and f0 be the new final state. Then we will form new machine M as M=((QI U Q2 U {qo, fo;), (Σι U Σ2), δ, qoffo;) This shows that L(M) = L(M1) UL(M2) i.e. L(R) = L(r) = L(r1) UL(r2) The machine designed for L(r1) U L(r2) shows that language Ri is regular. Q.7 Verify whether L = {a2n |n ≥1} is regular. AU: May-07 Ans. Refer similar example 2.6.2. 136 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN

--- Page 159 ---
Course Code/Title: CS3401/Theory of Computation Grammar Introduction: A grammar is a set of production rules for generating String in a language. A grammar generates a language with the Said of 4 elements Gr=(V,T, P. S) where, 3 V- is a finite nonempty set whose elements are called variables (Non terminals) 3 T- is a finite non empty set whose elements are called terminals 3 3 3 P- is a finite set whose elements are 2->B where 2 and B are Strings on VUT elements of P are called productions S- is a special variable called the start symbol 159 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 160 ---
Course Code/Title: CS3401/Theory of Computation Ext: construct a grammar VU for 1= hanIn is odds. soln: S = a' = 9 S = a3 = aaa -1 [ 5 ->asa] Productions S-a 5 -Jasa -> [s-sa] The grammar G=[V, T, P.S) v = { 5 } T = 493 P = {5 -> a/asa} 5 = {5 } - × - Fx2: let G=(ES}, {a, b], P.S) with the production P: 5->ash, 5->bsb, S->&- Find the language generated by the grammar. Solo: 1) S => asa => aa [s->4] 2) 5=> bs5 => bb 3) s=> bsb => basal [: s-sasa] => baab [ 5->&] 4) s = > asa => absha =>abha [5-shsb] (5->E) language cur be defined as, L= { WWR/WE Ca,bjx]] 160 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 151 ---
Course Code/Title: CS3401/Theory of Computation A-> 0A 1 | € B->1B | 1 Hence the CFG can be S -> ABC A-> 0A1 | € B-> 1B | 1 C->1C2|8 Example 3.3.8 Build a CFG for the language L = {0i 11 2k | i = j} Solution: As i = j and there is no condition on k. We can say that k is an arbitrary value. Then we rewrite L as L = 0i 1j 2k 0 i 1 j = A 2k = B S ->AB A-> 0A 1 | € B-> 2B | 8 Example 3.3.9 Obtain CFG for the language L = {0i 1 2k | j≤ k}. Solution: Here j≤ k. That means the language L has two variations. L1 = 0i 1j 2j where k = j L2 = 0i 1j 2k where k > j Hence the required CFG can be S -> AB A> 0A | 8 B-> 1B 2 | C Example 3.3.10 Consider the alphabet A = (a, b) and the language L= {bb, bab, baab, baaab, ... ) over A. AU May-10, Marks 16 i) Is A* finite or infinite? Give a brief reason for your answer. [2] ii) Write down a regular expression that represents the above language L. [4] iii) Write down a regular grammar which describes the above language L. [4] iv) Draw the DFA corresponding to the above language L. [6] CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY 151 nirf 175" Rank CEN

--- Page 152 ---
Course Code/Title: CS3401/Theory of Computation Solution: i) The A* is an infinite language, because there are any number of a's in the string and the string is starting and ending with b. This leads to an infinite strings of the language. ii) The regular expression will be = ba*b. iii) The regular grammar G = (V, T, P, S) where, V is set of non terminals = {S, X, T}. T is the set of terminals which is (a, b). P is a set of production rules as shown below - { S -> bX X -> aX|aT|b T -> b } S is a start symbol. This is a right linear grammar because the non-terminal appears at the right end of the rule. iv) The DFA will be a b b Example 3.3.11 Construct a CFG for the set {ai bi ck | i + j or j #k} As there are two cases i + j or j k. AU May-11, Marks 6 Solution: Hence the production rules will be S -> R1 R2 R1 -> aAbbBC | aaAbBC R2 -> AbBccC | AbbBcC A -> aA | 8 B -> bB | 8 C-> cC | & Example 3.3.12 If S -> aSb|aAb, A -> bAa, A -> ba is the context free grammar. Determine the context free language. AU Dec .- 11, Marks 6 Solution: The strings generated by this grammar are (a ba b, aa babb, aaa bbaa bbb, ... ) This denotes the language L = {a" b™ a™ b" |n, m≥1} Example 3.3.13 Find the context free langauge for the following grammars 1) S -> aSbs | bSaS | ¿ 2) S -> aSb | ab AU May-12, Marks 10 152 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 139 ---
Course Code/Title: CS3401/Theory of Computation The regular expression for above DFA will be r.e. = 0*(1*(00*)*)* Q.15 Prove or disprove the following for regular expression (a+b) + (cd) for 2 = {a,b,c,d}. AU: May-09 Ans .: To prove whether or not given language is regular or not following theorem is used - "For every regular language there exists a finite automata". If we could be able to build the FA for given expression then that expression is side to be regular. Hence E E C d bi Fig. 2.10.8 The NFA with & can be built for given expression. Hence the given expression is regular. Q.16 What is [10,11]* ? (Write atleast first seven terms). AU: Dec .- 09 Ans .: The {10+ 11}* is a set containing the strings of any combination of 10 and 11. This set also contains & or null string. The items of this set are {8, 10, 11, 1010, 1011, 1111, 1110, 111111, 101010, ... } Q.17 Let L= {W: W E {0, 1}* W does not contain 00 and is not empty}. Construct a regular expression that generates L} AU : May-10 Ans: We will first build a finite automata for the language accepting the strings containing 00. 0,1 0 q 0 1 Fig. 2.10.9 Now to accept the language which does not contain 00, we will change the non final state to final state and final state to non final state. The regular expression will be (01+1) + (10+1)+0. 139 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 140 ---
Course Code/Title: CS3401/Theory of Computation 1 0. 1 0 92 0 Fig. 2.10.10 Q.18 Is the set of strings over the alphabet {0} of the form 0" where n is not a prime is regular ? Prove or disprove. AU: Dec .- 11 Ans. Let us assume that L is a regular language n is not a prime number. Let zEL, L = 0n |z| = uvw, here i = 1. Now consider L = uvi w where i = 2 = u v. VW That means adding 1 to n, we get n < |uvvw| n < n+1 But (n + 1) can be a prime number. Hence our assumption becomes contradictory to it. This shows that L is not regular language. Q.19 Construct regular expression for the language over the set 2 = {a,b} in which total number of a's are divisible by 3. Ans. r.e. = (b* ab* ab* a)* Q.20 Find out the language generated by the regular expression (0+1) *. Ans .: L = {8, 0, 1, 00, 11, 01, 10,111, .... } L = {Any number of 0's and 1's including nullstring} Q.21 What is the closure property of regular set s? Ans .: If certain languages are regular and language L is formed by them by certain operations such as (union, concatenation and so on) and if we find L as regular, then this property is called closure property of regular sets. Q.22 Mention the applications of pumping lemma. Ans .: 1. Pumping lemma is used for deciding whether or not the given language is regular. 2. It is also useful for determing whether L is accepted by a regular expression or not. Example 1: L= {0"1"} is not regular. L = {0"|p-is a prime number} Example 2: 140 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN

--- Page 129 ---
Course Code/Title: CS3401/Theory of Computation W = 0 00 10000 0 = x, 00 = y, 10000 = z if i = 2 then w = xyyz i.e. = 0(00) (00) (10000) W = 05 11 04 € 0m in omtn Case ii) y has only 1's Consider w = 00110000 If we map w = xyz then By pumping lemma, w = 00 11 0000 x=00, y=11, z= 0000 w=xy1z if i = 2 then w = xyyz i.e. = 00(11) (11) (0000) = 02 14 04 € om in om+n Case iii) y has 0's and 1's Consider w = 001000 w = 0 01 000 x=0, y=01, z= 000 If we map w = xyz then By pumping lemma, w=xy`z If i = 2 then w = xyyz i.e. = 0 (01) (01) (000) = 021 011103 € 0min omtn From above three cases, it is clear that our assumption of L being regular is wrong. This proves that given L = 0min omtn is not regular. Example 2.6.8 Prove that following languages are not regular: 129 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY LEN nirf 175" Rank --

--- Page 130 ---
Course Code/Title: CS3401/Theory of Computation i) { w E {a,b}* |w = wR} ii) Set of strings of 0's and 1's beginning with 1, whose value treated as a binary number is prime. AU: Dec .- 19, Marks 13 Solution: i) Let, the given language · L = {w E {a, b} * |w = wR} is regular · Suppose there is a DFA for L with p states. Let w be word in L. We will pump the strings, such that W = xyz. . We will choose w = OP10P & L · By pumping Lemma, w =xyz, y ≥ 1,|xy|≤ p and w = xy z is in L for every i · The pumping part is at the beginning of string. Hence |xy|≤ p and xy consists of 0's only · Since |y| ≥ 1, y contains at least one 0. · If i = 2 then w = xyyz and it must E L if w = 00 0 1000 x=00 y =0 z=1000 w=xyyz w = 00001000 € L · This shows that, our assumption of being L is regular is wrong. · This proves that given language is not regular. ii) L = Binary numbers of prime numbers Let, w = xyz E L we assume that L is a binary language of prime numbers. · Assume | xyz |= n. · Assume L is regular. · By pumping lemma. w= xyz, |y| ≥ 1, |xy| ≤ p and w = xy z is in L for every i · i = 2, then w=xyyz [w] = n+1 € L This shows that our assumption of L being regular is wrong and L is non-regular. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY 130 nirf 175" Rank N39

--- Page 97 ---
Course Code/Title: CS3401/Theory of Computation Ans .: ¿ - closure {1} = {1, 2, 3, 6, 4} ¿ - closure {2} = {2, 3, 6} ¿ - closure {4} = {4} Q.34 Define & - closure. Ans .: The E - closure (p) is a set of all states which are reachable from state p on & - transitions such that: i) ¿ - closure (p) = p where p E Q. ii) If there exists & - closure (p) = {q} and 8 (q, ¿) = r then ¿ - closure (p) = {q, r} Q.35 Define & - closure (q) with an example. AU: May-12 Ans. Definition: Refer answer of Q.34. Ans .: The E - closure (p) is a set of all states which are reachable from state p on & - transitions such that: i) ¿ - closure (p) = p where p E Q. ii) If there exists & - closure (p) = {q} and 8 (q, ¿) = r then ¿ - closure (p) = {q, r} Example: Refer answer of Q.31. Ans .: The E - closure of each state means collection of & - reachable states. 0 2 ₺ Fig. 1.13.15 ¿ - closure (qo) = {qo, q1, q2} as we have & transition from qo to qo, q1, q2. & - closure (q1) = {q1, q2} & - closure {q2} = {q2} 97 INGSTOUTE . TECHNOLOGY CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 98 ---
Course Code/Title: CS3401/Theory of Computation UNIT II: Regular Expressions and Languages Syllabus Regular expression - Regular Languages- Equivalence of Finite Automata and regular expressions Proving languages to be not regular (Pumping Lemma) - Closure properties of regular languages. Regular Expression and Regular Languages AU May-13, 16, Marks 8 Let 2 be an alphabet which is used to denote the input set. The regular expression over 2 can be defined as follows. 1. § is a regular expression which denotes the empty set. 2. ¿ is a regular expression and denotes the set { } and it is a null string. 3. For each 'a' in 2 'a' is a regular expression and denotes the set {a}. 4. If r and s are regular expressions denoting the languages Li and L2 respectively, then rts is equivalent to L1 U L2 i.e. union. rs is equivalent to L1 L2 i.e. concatenation r* is equivalent to L1* i.e. closure. The r* is known as kleen closure or closure which indicates occurrence of r for oo number of times. For example if 2 = {a} and we have regular expression R = a*, then R is a set denoted by R= {&, a, aa, aaa, aaaa, ... } That is R includes any number of a's as well as empty string which indicates zero number of a's appearing, denoted by & character. Similarly there is a positive closure of L which can be shown as L+. The L+ denotes set of all the strings except the & or null string. The null string can be denoted by & or ^ If 2 = {a} and if we have regular expression R = a+ then R is a set denoted by R = {a, aa, aaa, aaaa, .... ) We can construct L* as L* = & L+ Let us try to use regular expressions with the help of some examples. Definition of Regular Language: A language is regular if it can be expressed in terms of regular expression. Example 2.1.1 Design regular expression for the language containing all the strings containing any number of a's and b's. Solution: The regular expression will be r.e. = (a + b)* This will give the set as L = {8, a, aa, ab, b, ba, bab, abab, ..... any combination of a and b}. 98 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 121 ---
Course Code/Title: CS3401/Theory of Computation In Rpi+1 has no string of less than i + 1 length. Hence w is not in set RPi+1. Hence R and QP* represent the same set. Hence it is proved that R = Q + RP has a unique solution. R = QP* Example 2.5.1 Prove (1+00*1) + (1+00*1) (0+10*1) *(0+10*1)=0*1(0+10*1) *AU: May-11, Marks 8 Solution: Let us solve L.H.S. first, (1+00*1) + (1+00*1) (0+10*1) * (0+10*1) We will take (1+00*1) as a common factor 1+00*1 (¿+(0+10*1)*(0+10*1)) (&+ R*R) where R = (0+10*1) As we know, (& + R*R) = (¿ + RR*) = R* (1+00*1) ((0+10*1)*) out of this consider (1+00*1) (0+10*1)* Taking 1 as a common factor (&+00*) 1 (0+10*1) * Applying ¿ + 00* = 0* 0*1 (0+10*1) * = R.H.S. Hence the two regular expressions are equivalent. Example 2.5.2 Show that (0*1*)* = (0+1)* Solution: Consider L.H.S. =(0*1*)* {8, 0,00,1,11,111, 01, 10, ... } = {any combination of O's, any combination of 1's, any combination of 0 and 1, ¿} Similarly, R.H.S. =(0+1)* = {8, 0, 00, 1, 11, 111, 01, 10, .... } = {&, any combination of 0's, any combination of 1's, any combination of 0 and 1} Hence, L.H.S. = R.H.S. is proved. 121 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 122 ---
Course Code/Title: CS3401/Theory of Computation Example 2.5.3 Prove that r (s+t) = rs + rt Solution: In case of L.H.S. L.H.S. = r (s+t) solving this further =r.s + r.t i.e. r is concatenated with s or with t = R.H.S. L.H.S. = R.H.S. is proved. Check for the following regular expressions for equivalence and justify. i) R1 = [(a + bb)* (b + aa)*]* ii) R2 = (a + b)* Here R1 and R2 are equivalent because the strings which can be generated by R1 can be generated by R2 or vice versa. For instance, R1 = abab - this string can be generated by selecting a from (a + bb)* then b from (b + aa)* again a from (a + bb)* and b from (b + aa) *. R2 = abab - this sting can be selecting a then b repetitively from (a + b)* Thus R1 R2 is proved ii) R1 = (a + b)* abab* R2 = b* a (a + b)* ab* Here strings generated by R1 are same as R2 For instance R1 can generate abababb. Similarly R2 can generate abababb as we will select 'null' from b* then 'null''a'; then 'bab' from (a + b)* then 'abb' from ab* Thus R1 = R2 is proved. Example 2.5.4 Prove & + 1* (011)* (1* (011)*) *= (1 + 011)* Solution: Let, L.H.S. is € + 1*(011)* (1* (011)*)* If we consider 1*(011)* as P1 then, = 8 + P1 P1* ¿ + P1P1* = P* = Pı We can put P1 = 1* (011)* then, = (1*(011)*)* 122 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 113 ---
Course Code/Title: CS3401/Theory of Computation Step 1 10+(0+11)021 Step 2 10 (0+11)0*1 Step 3 10 (0+11) 91 1 0 Step 4 10 0 O Step 5 1 0 0 1 Review Question 1. Let r be a regular expression. Then prove that there exists a NFA with & transition that accept L(r). AU: Dec .- 03, Marks 16; Dec .- 06, Marks 8; Dec .- 10, Marks 10; May-12, Marks 6 Conversion of Finite Automata to Regular Expressions AU Dec .- 03,04,05,09,11,13,16,19, May-05,06,07,08, 12, Marks 16 113 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank --

--- Page 114 ---
Course Code/Title: CS3401/Theory of Computation The Arden's theorem is useful for checking the equivalence of two regular expression as well as in conversion of DFA to r.e. Let us see its use in conversion of DFA to r.e. Following algorithm is used to build the r.e. from given DFA. 1. Let qı be the initial state. 2. There are q2, q3, q4, ... qn number of states. The final state may be some qj where j ≤ n. 3. Let aji represents the transition from qj to qi 4. Calculate qj such that qi = aji qj If qi is a start state to qi qi = aji qj + E 5. Similarly compute the final state which ultimately gives the regular expression r. Let us solve few examples based on this algorithm. Example 2.4.1 Construct r.e. from given DFA. b Start b Fig. 2.4.1 Solution: Let us write down the equations state on q1 =qı a + E Since q1 is a start state & will be added and the input a is coming to qi from qı hence we write State = Input coming to it x Source state of input Similarly q2 = q1 a + q2 b Let us simplify qı first q1 =qı a + E We can re-write it as q1 = 8 + qı a which is similar to R = Q + RP which further gets reduced to R = QP *. Assuming R = q1, Q = &, p = a We get qı = 8 . a* qı = a* 114 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 127 ---
Course Code/Title: CS3401/Theory of Computation Example 2.6.5 Which of the following languages is regular? Justify. 1) L={a" b"| n, m≥ 1} 2) L={a" b" | n≥ 1} AU: May-12, Marks 8 Solution : 1) Let, L = {an bm | n, m ≥ 1} Here the number of a's and b's can be at least one. We can write the regular expression for given L as r.e. = a+ b+ For this regular expression the FA can be As we can draw the FA for representing given language, the given L is said to be regular. a.b a,b 2) Given L= {a" b" | n ≥ 1} is not regular. Refer similar example 2.6.4. Example 2.6.6 Using pumping lemma for the regular sets, prove that the language L= {amb" | m>n} is not regular. AU: Dec .- 12, Marks 10 Solution: Let us assume that L is a regular language. The string W E L such that [W] = | xyz | = a™ bn Length of string |W| = m + n, where m > n. By pumping lemma we can write W = xyz such that |xy| ≤ n |y| # 0. Now if xy z E L then the language is said to be regular. There are many cases: i) y has only a's ii) y has only b's iii) y has both a's and b's. i) If y has only a's then assume W = a3 b2. If we map W = xy z with i = 0. Then W in equation (1) becomes W= a aa bb x=a, y=aa, z=bb W = xZ y1 = yº = 1 W = abb = a1b2 € {L = a™ b" | m>n} ii) y has only b's then assume W = a3 b2 If we map W = xy z with i = 3 then W in equation (2) becomes Regular Expressions and Languages 127 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN

--- Page 128 ---
Course Code/Title: CS3401/Theory of Computation W= aaa bb X = aaa y = b z= b W = (aaa) (bbb) (b) W = a3 b4 € {L = a™ b" | m > n} iii) y has both a's and b's. Then assume W = a3 b2. If we map W = xyiz with i = 2 then W in equation (3) becomes W = aa ab b x = aa y = ab z = b W = (aa) (abab) (b) = a3 bab2 € a™bn From above three cases it is clear that our assumption of L being regular is wrong. This proves that given L = a™b" is not regular. The simple way to know whether given language is regular or not is that try to draw finite automata for it, if you can easily draw the FA for the given L then that language is surely the regular otherwise not. Example 2.6.7 Using pumping lemma for regular sets, prove that the language L = {0min Om+n | m ≥ 1 and n ≥ 1} is not regular. AU: Dec .- 13, Marks 10 Solution: Let us assume that L is a regular language. The input string w e L such that [w] = | xyz | = 0min Omtn By pumping lemma we can write w = xyz such that |xy| ≤ n and |y| # 0. There are three cases: i) y has only 0's ii) y has only 1's iii) y has 0's and 1's. Case i) y has only 0's Consider w = 00010000 If we map w = xyz then By pumping lemma, w=xy'z 128 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 179 ---
Course Code/Title: CS3401/Theory of Computation 2.49 Theory of Computation aaSbb = aaabbb Terminal S 1 a S 1 S b - Terminal b Non terminal root of the sub-tree b LMD azabbb This is assumed. When n = 3 [(n- 1) + 1] the derivation becomes, SaSb aaaSbb = aaaabbbb => a ... S=a S 1 S b 1 a S b a S b b S => aaaabbbb => a Thus if the theorem is true for a tree of height (n - 1), then it is true for height n. 179 CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 180 ---
Course Code/Title: CS3401/Theory of Computation 2.7.2 From Derivation To Recursive Inferences Theorem Let G = (V, T, P, S) be a CFG and if there is a derivation A= a, where a is in T *. Then the recursive inference procedure applied to G determines that & is in the language of non-terminal 'A'. Proof We shall prove the theorem by the principle of mathematical induction. Basis of induction Let us consider the lowest possible input of the grammar. Let A => a, then there must be a production A -> a in G. then we can infer that a is in the language of variable A. Inductive step Assume that the theorem holds for the derivation has (n-1) number of steps and we need to prove that it holds for 'n' no. of steps. Let the derivation be stated as, A = X: X2 X3 ..... Xx . Then a can be broken as where, The derivation X =>0, can take at most 'n-1' number of steps to generate. Now we have a production, A -> X1 X2 .... Xx Infer a to be in the language of XI : Q1 204 is inferred as in the language of A. 180 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 175 ---
Course Code/Title: CS3401/Theory of Computation Example : Remove the ambiguity from the grammar, E-SE+E)a/b Soln: Given Grammar, F->E+E/a/b the grammar can be rewritten by introducing a new variance T as, E-SETT/T T-salb Parse Tree: E E + T 1 1 E + T a 1 1 - 9 1 0 -x- - 2) consider the grammar E-JE+E/EXE|CEJ/I, I->a/b i) show that the grammar is ambiguous ii) Remove the ambiguity. soln! 1) Ambiguity Grammar 1) E=> EXE E =>ITE I + E =>a+F 1 =>atEXE E 1 => a + IE I => a+bxa 1 6 F 1 I 1 a 175 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 176 ---
Course Code/Title: CS3401/Theory of Computation 2) E=> EXE E =>E+EXE => I + EXE F E =>a +I+F 1 E + F I =>a+ IXE 1 -1- 1 =>a+ b=F 1 I =>a+bxJ a 1 1 => a +bxa a 6 from 12 we have generated & different Parse free for the same string a+bxa using the Same production. - The grammar is ambiguous. ii ) Removing Ambiguity: Introducing new variable T to remove ambiguity. E-SETT/T E T-STAFF E + T F-> (E)/a/b 1 T T * F t-4 1 1 1 F -15 0 1 9 -x- 1) Whether the following grammars are ambiguous. a) 5-Josis/1505/6 b) 6-SAA A->aAb/ bsalE. 176 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY N39 nirf 175" Rank

--- Page 153 ---
Course Code/Title: CS3401/Theory of Computation Solution : 1) Let us derive some strings from given grammar. S S aSbS bSaS aaSbSbS abSaSbS bas aabSbS abaSbS ba aabbS ababS abab aSbS aabb From these derivations, we can observe that the derived strings contain equal number of a's and b's. Thus this CFG denotes the language L containing equal number of a's and b's. 2) Refer example 3.3.1. Example 3.3.14 Construct a context free grammar for {0m 1" | 1≤msn} AU: Dec .- 14, Marks 4 Solution: G = (V, T, P, S) where V = {S, A} T={0, 1} P is a production rules set. It is given by S-> 0S1 | 0A |01 A-> 1A | 1 Example 3.3.15 Construct a CFG for the regular expression (011 + 1) (01). AU May-16, Marks 6 Solution: The production rules are S -> AB A -> X | Y B -> 01 X -> 011 Y ->1 Example 3.3.16 Construct a context free grammar for the language. L= {a" | n is odd} AU: Dec .- 16, Marks 6 Solution: The context free grammar G = (V, T, P, S) Where V = (S, T) T = (a) The S is a start symbol. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank 153

--- Page 154 ---
Course Code/Title: CS3401/Theory of Computation The set of production rules P is P={ S -> aT T -> aaT | ¿ } · Simulation for string aaaaa S aT aaaT aaaaaT aaaaa Example 3.3.17 Give the regular expression of the language generated by the context free grammar (CFG) given below: S -> aS | bS | a | b. Convert regular expression to & - NFA. AU Dec .- 18, Marks 7 Solution: We will first draw the FA for given CFG. S b b The regular expression is r.e = (a+b)+ The NFA with & is as designed below E 9+ 92 93 b Parse Trees AU Dec .- 03,04,05,10,12,14,15, May-04,05,09,13,16, Marks 16 154 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY LEN nirf 175" Rank

--- Page 103 ---
Course Code/Title: CS3401/Theory of Computation Mỹ Start M2 Fig. 2.3.2 The machine M for union The construction of machine M is shows the transition from qo to fo must begin by going to qi or q2 on &. If the path goes to q1, then it follows the path in machine M1 and goes to the state f1 and then to fo on &. Similarly, if the path goes to q2, then it follows the path in machine M2 and goes to state f2 and then to f0 on ¿. Thus the L (M) = L (M1) UL (M2). That means either the path in machine M1 or M2 will be followed. Case 2: Concatenation case Let, r = r1 r2 where r1 and r2 are two regular expressions. The M1 and M2 denotes the two machines such that L(M1)=L (r1) and L (M2)=L (r2). The construction of machine M will be M =(QI U Q2, Σι U Σ2, δ,{qi}, {f2}) The mapping function & will be given as i) ô (q, a) = 81 (q, a) for q in Qi - {f1} and a in Σι Uξε} ii) ô (f1, ¿) = {q2}. iii) ô (q, a) = 82 (q, a) for q in Q2 and a in Σι U { } The machine M is shown in the Fig. 2.3.3. Start M. 4 M2 Fig. 2.3.3 Machine M for concatenation The initial state is qı by some input a the next state will be fi. And on receiving & the transition will be from fi to q2 and the final state will be f2. The transition from q2 to f2 will be on receiving some input b. Thus L (M) = ab That is a is in L (M1) and b is in L (M2). Hence we can prove L (M) = L (M1) L (M2). Case 3: Closure case Let r = rı where ri be a regular expression. 103 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 104 ---
Course Code/Title: CS3401/Theory of Computation The machine M1 is such that L(M1)=L(rı). Then construct M = (Qi U {qo, fo}, Ei, δ, qo, {fo;). The mapping function 8 is given by, i) ô (qo, ¿) = ô (f1, ¿) = {q1, fo} ii) ô (q, a) = 81 (q, a) for q in Qi - {f1} and a in Σι U { } The machine M will be Start M e Fig. 2.3.4 The machine M1 shows that from qo to qı there is a transition on receiving & similarly, from qo to fo on & there is a path. The path exists from fi to q1, a back path. Similarly a transition from fi to fo final state, on receiving ¿. The total recursion is possible. Thus one can derive ¿, a, aa, aaa, .... for the input a. Thus L (M) = L (M1)* is proved. Now based on this proof let us solve some examples. These examples illustrate how to convert given regular expression to NFA with ¿ moves. Example 2.3.1 Construct an NFA equivalent to the following regular expression ((10) + (0+1)*) 01. AU: May-06, Marks 10 Solution: Consider r.e. = (r1 + r2) r3 where rı= 10 r2=(0+1)* r3 = 01 0 Fig. 2.3.5 We will build NFA for each r1, r2 and r3. rı= 10 104 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank --

--- Page 193 ---
Course Code/Title: CS3401/Theory of Computation 1) Design a PDA That accepts a set of strings. over {a. b} with equal number of a's and bis. Soln: PDA accepting through an empty stack Transition Function 5 (90, 0, 20) =90,920) S(90, b. 20)=(90, b20) >[20, a,b) = (90, 2) S(90, b, a) =(90, &] 21 20, a, a) = (90, aa) S(90. b. b) = (90- bb) S( 90, 4, 20) = (90 %) S(90, 6.20) = (91-20) Transition diagrams 9,20/920 3.20/220 a, b) & a, a/aa 99/ 9.9 PDA accepting through final State S(40, a, 20)= (90, a20). [[90, b, 20)= (90, 620) S(90, a, b) = (90, &) S(a, b, a)=(90,4) 2 (20; a, a) = [90. au) SI to, b. b)=(90-bb) a., 20/920 4. 201620 a.b /c 4,4/4 1.0/16 4,2018 $90 PDA: n=14903,4.3},{a,b,20},S, 20,20, 1) 7h.b/bb K 4.20/20 9, M = (120, q,},{a,b}, {a, h. z.}, {, 90, 20, 19.3 CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY N39 nirf 175" Rank 193

--- Page 194 ---
Course Code/Title: CS3401/Theory of Computation 4 2) let L={a" b" cmdm/ w,m>1? Find a PDA for L. Soln: Transition Function: Step1: w = aabbed, n=2, m=/ Push ta onto the stack w = a a bhcd 770 O a a a 20 20 20 90 90 90 S [90, a, 20] = (90.920) S (90. a. a) = (90, aa) Step2: For every 'b' as input, 'a' should be popped out. w = ad bbcd b 11 a b a 4 a 20 - 20 90 9, 21 S[90, h, a] =(91. {) S(a, b, a) = (91) {) 194 CHENNAIC INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank LEN

--- Page 117 ---
Course Code/Title: CS3401/Theory of Computation A = (0+1)* ... (5) Equation (2) becomes B = A1 = (0+1)* 1 ... (6) Now by using equation (6) we get equation (3) as C=B (0+1) C= (0+1)* 1 (0+1) ... (7) Using equation (7), we get equation (4) as D=C (0+1) =(0+1)* 1 (0+1) (0+1) ... (8) As state C and D are final states equation (7) and equation (8) represent final regular expression. r.e. [(0+1)* 1 (0 + 1)]+ [(0+1)* 1 (0 + 1) (0 + 1)] Example 2.4.5 Construct the regular expression corresponding to the state diagram given in the following Fig. 2.4.5. AU: May-05, Marks 10; May-08, Marks 16, Dec .- 19, Marks 2 0 1 1 0 0 Fig. 2.4.5 Solution: We will obtain the regular expression for the given state diagram using Arden's theorem. Let us write equations for each state. q1 = q10 + q30 + & being a start state, add ¿. q2= q11+ q21+ q31 q3 = q20 Consider q2 first. q2=q11+ q21 + q31 q2 = q11 + q21 + q201 q3 = q20 q2 = q2 (1+01) + q11 R = Q + RP gives R = QP* R= q2, Q=q11, P=(1+01) q2= q11 (1+01)* Then put value of q2 in q3 and we get, q3 = q11 (1+01)* 0 117 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank -- N39

--- Page 118 ---
Course Code/Title: CS3401/Theory of Computation Now substitute value of q3 in qı q1=q10+q30+€ q1= q10 + q11 (1+01)* 00+ 8 q1 = q1 [0+1 (1+01)* 00] + ¿ R = Q + RP gives R = QP* R = q1, Q = &, P = 0+1 (1+01)* 00 we get q1= & [0+1 (1+01)* 00]* q1=(0+1 (1+01)* 00)* ER = R State qı is a final state. Hence an equation for final state becomes a regular expression for given state diagram. The r.e. = (0+1 (1+01)* 00)* Example 2.4.6 Obtain the regular expression for the finite automata. AU: May-12, Marks 8 b b Fig. 2.4.6 Solution: We will write the equations for each state, q1 = q1 a + 8 ... (1) q2 = q2a + qıb ... (2) q3 = q2b ... (3) Solving equation (1) q1 = qı a + E qı = & a* = a* R = Q + RP gives R = QP* and & R* = R* Solving equation (2) using q1 = a* we get, q2 = q2 a + a* b q2 = a* b a* Solving q3 by putting q2 = a* b a* we get q3 = q2 b 118 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 123 ---
Course Code/Title: CS3401/Theory of Computation Now consider P2 = 1 and P3 = (011) then it becomes =(P2* P3*)* (P* Q*)* = (P + Q)* =(P2 + P3)* =(1 + 011)* = R.H.S. Thus L.H.S. = R.H.S. Hence € + 1* (011)* (1* (011)*)* = (1+011)* is proved. Proving Languages to be not Regular (Pumping Lemma) AU: May-04,06,07,12, Dec .- 12,13,14,15, Marks 6 · Pumping lemma is a basic and important theorem used for checking whether given string is accepted by regular expression or not. In short, this lemma tells us whether given language is regular or not. · One key them is that any language for which it is possible to design the finite automata is definitely the regular language. Theorem: Let L be a regular set. Then there is a constant n such that if Z is any word in L and |Z| ≥ n we can write z = u v w such that |u v| ≤ n, |v| > 1 for all i > 0, u vi w is in L. The n should not be greater than the number of states. Proof: If the language L is regular it is accepted by a DFA. M = (Q, 2, 8, qo, F). With some particular number of states say, n. Consider the input can be a1, a2, a3, .... am, men. The mapping function & could be written as 8(qo, q1, q2, q3 ... qi) = qi The transition diagram is as shown in Fig. 2.6.1. Fig. 2.6.1 Pumping lemma If qm is in F i.e. q1, q2, q3, ... qm is in L(M) then a1, a2, ... aj ak+1 ak+2 ...... am is also in L(M) . Since there is path from q0 to qm that goes through qj but not around the loop labelled aj+1 .... ak. Thus 8 (qo, a1, aj ak+1 ... am) = 8 (8 (qo, q1, ... qj), ak+1 .... am) = 8 (qj, qk+1 ... qm) = 8 (qk, qk+1 ... qm) = qm That is what we have proved that given any long string can be accepted by FA, we should be able to find a substring near the beginning of the string that may be pumped i.e. repeated as many times as we like and resulting string may be accepted by FA. 123 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank -- LEN

--- Page 124 ---
Course Code/Title: CS3401/Theory of Computation · Advantage of Pumping Lemma The pumping lemma is used to check whether given language is regular or not. Example 2.6.1 Show that the set L = { bi2 |i>1} is not regular. AU: May-06, Marks 6 OR ii) Prove that L = {012 /i is an integer; i>1 is not regular. AU: Dec .- 14, 15, Marks 6 Solution: We have to prove that the language L= bi2 is not regular. This language is such that number of b's is always a perfect square. For example, if we take i = 1 L= b12 = b the length = 12 = b22 = bbbb length = 22 and so on. Now let us consider L = bn2 where length = n2 it is denoted by z. |z| = n2 By pumping lemma z = u v w where 1 ≤ |v| ≤n As z = uvi w where i = 1 Now we will pump v i.e. make i = 2. As we made i = 2 we have added one n2. 1≤ | v | ≤n. n2 +1 ≤ uvw | ≤ n+n2 i.e. n2 +1 ≤ | uvw | ≤ n2 +n+n+1 i.e. n2+1≤| uvw |≤(n+1)2 = n2 ≤ | uvw | ≤ (n+1)2 Thus the string lies between two consequetive perfect squares. But the string is not a perfect square. Hence we can say the given language is not regular. For example, L = bi2 Let i= 2 L = bbbb L = uvw 124 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank N39

--- Page 137 ---
Course Code/Title: CS3401/Theory of Computation Q.8 State pumping lemma and its advantages. AU: Nov .- 14, Dec .- 07, 08, 13, 17 Ans .: Pumping lemma: Let L be a regular set. Then there exists a constant n such that if z is any word in L such that |z| > n then we can write z = uvw such that |uv| <n and |v| > 1 for all i ≥ 0 and uv1w is in L. The n indicates number of states in FA and should not be greater than the number of states. The advantage of pumping lemma is that this theorem is used to check whether given language is regular or not. Q.9 Show that the complement of a regular language is also regular. AU: Dec .- 07 Ans. If L is a regular language, then its complement denoted by L' is also regular. To prove this, consider that language L is accepted by finite automaton M. Now all the words accepted by language L will reach to final state of machine M. If we change all the states of M to non-final states and all the non-final states of M to final state we get a new FA which is denoted by M'. Now this M' will accept all the words not belonging to L and reject all the words belonging to L. That means M' will accept the language L'. Thus there exists FA which accepts L'. Hence L' which is complement of L is also regular. Q.10 Construct a DFA for the regular expression aa*bb *. AU : May-08 Ans. The DFA for aa*bb* is - a b Fig. 2.10.3 The state q0 is a start state and state q2 is a final state. Q.11 Show that the complement of regular language is also regular. AU: Dec :- 08 Ans. Refer answer of Q.9 If L is a regular language, then its complement denoted by L' is also regular. To prove this, consider that language L is accepted by finite automaton M. Now all the words accepted by language L will reach to final state of machine M. If we change all the states of M to non-final states and all the non-final states of M to final state we get a new FA which is denoted by M'. Now this M' will accept all the words not belonging to L and reject all the words belonging to L. That means M' will accept the language L'. Thus there exists FA which accepts L'. Hence L' which is complement of L is also regular. Q.12 Show that o is e by constructing its NFA using Thomson's construction. AU: Dec .- 08 Ans. The Thomson's construction for $* will be Start c c Fig. 2.10.4 Note that is represented by qı to f1. We can reach to a final state fo from qo with input s. Hence ¢* = & is shown by above figure. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 137 nirf 175" Rank LEN

--- Page 138 ---
Course Code/Title: CS3401/Theory of Computation Q.13 Construct an NDFA for all strings over alphabet = {a, b} that contains a substring 'ab'. AU: May-09 Ans. The NDFA over E = (a, b) is - a. b b a, b b Fig. 2.10.5 The transition table will be Input a b State 91 (90. 92) 92 [g2) Q.14 Write regular expressions for the following language over the alphabet 2 = {0,1}. "The set of all strings not containing 101 as a substring". Provide justification that your regular expression is correct. AU : May-09 Ans. We will first construct DFA for the language containing 101 as substring. 0 D 1. 0 1 0 1 1 Fig. 2.10.6 Now, just change the non-final states to final states and make final state as non-final state. The DFA then becomes - 0 0 1.0 1 0 1 94 1 0 1 Fig. 2.10.7 138 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 177 ---
Course Code/Title: CS3401/Theory of Computation 2.47 Theory of Computation 2.7 RELATIONSHIP BETWEEN DERIVATION AND DERIVATION TREE 1 2.7.1 FROM TREES TO DERIVATIONS Theorem (AU- Dec 2003, Dec 2004, May 2005, Dec 2005) Let the grammar, G,- (V.T; R, S) be context free. Then Sa if and only if there is a derivation tree in grammar G, that generates the string 'a'. Proof Let S be a variable, where Se V, then S=a if and only if there is a parse tree called S-tree, starting from the root node (S) to generate the string, 'a'. . The problem can be easily proved by the principle of mathematical induction. Basis of induction Consider the lowest possible input value and prove the theorem given. So we assume that there is only one interior node [height = 1] which forms the root node, that yields the string a, by deriving the leaf nodes of S as S-> 8 22 3 . 4. The derivation tree for the generation of S=>a is given as S Height of the tree =1 - a1 82 - Thus S => a1 22 .... & => a is the input string generated from S. Inductive step Assume that the theorem is true for 'n"" input data and we need to prove the same for n=n+1 data. Hence we assume that the derivation tree can be drawn for.(n - 1) interior nodes. And we need to prove that the derivation tree can be drawn for 'n' interior nodes to derive 'a' from S. Here, we need to analyze that certain nodes shall be leaf nodes, whereas others can be interior nodes. : 177 CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 178 ---
Course Code/Title: CS3401/Theory of Computation Hence if a node X1 € T, then ar = X1 If the node X, E V. then X, 2a, is in G. The derivation tree is given as, [By inductive hypothesis] S 82 83 where, a1, 83 € V 82, 4 € T Thus, S => a, a2 .... &n => a can be obtained. Hence the theorem is proved by mathematical induction. X Example Let G=(V, T, P, S) be a CFG with productions, S -> aSb | ab Let us consider the basis of induction according to it, the lowest possible integer height of the tree should be considered. Let the height of the tree = 1. .: The grammar, S yields only one possible string S -> ab which is given by S 2md 9 a= ab [S => ab] According to the inductive step let us assume that we can generate a derivation tree of height (n - 1) and we need to prove that it.is true for the tree of height, n. Let n-1=2 S = &Sb 178 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 185 ---
Course Code/Title: CS3401/Theory of Computation PDA, P=(Q,E, F, S, 90, 20, F) 2 Q = {90, 9, 92} 2 = 29.53 5 = {20; a. b} 11 $={90} 20 = 1203 F = 492} a. 20/120 90 9, € 20/20 Instantaneous Description of a PDA: * Instantaneous description of a PDA is an information notation or description of a PDA. * If is a pictorial/ diagrammatic representation of a string processed by a PDA. * This is used to depict the processing of a String by a PDA. Ex : L= 1an 10/ mai ?. 185 CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY nirf 175" Rank LEN

--- Page 186 ---
Course Code/Title: CS3401/Theory of Computation Graphical representation (instant aneous description) 1) a/a/b/b ilp tape 1 90 20 2) a abb. - .. ilP tape a 20 90 3) Ja a/ b/b). tape a 1 a 90 120 4) Ja/a/ b/5 9, $ 0 1 20 Tuple formal- (90, aabb, 20) (90, abb, a20) (90, bb. aa) [9 1, b. (2) ( 5) a a / b/ b ] 9,1 20 6) a abb 92 1 20 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39 (92, 4, 20) 186

--- Page 109 ---
Course Code/Title: CS3401/Theory of Computation (R1 + R2 R& R3)* R2 Ri - eq 1 For state qo we have R1 for any number of times. Similarly to reach to go (from qo) we can follow R2R4"R3 path. Hence r.e. is (R1 + R2R4* R3)". To reach to final state from start state q, we require R2R4R4R4 ... any number times. This transition can then be written as R2R4" 3. If we reduce the finite automaton to only one state graph by state eliminating method then - R Start Gives t. e. = R" Fig. 2.3.13 4. After eliminating the sufficient number of states we can obtain r.e. as sum of the expressions derived from the reduced automata. Let us understand this method of obtaining regular expression with the help of some examples. Example 2.3.4 Obtain method r.e. from given FA by state elimination method. b Start b a Fig. 2.3.14 Solution: In above given FA state q3 is a dead state. Hence we will eliminate state q3. Thus the FA now becomes b Start b Fig. 2.3.15 This FA resembles the generic finite automata with two state (as given in Fig. 2.3.15) Hence required r.e. will be a*bb* As q2 is a final state we get i.e. r.e. = a*b+ Example 2.3.5 Represent the language given by the following DFA, by obtaining r.e. by state elimination method. 109 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 110 ---
Course Code/Title: CS3401/Theory of Computation a,b b a a,b Fig. 2.3.16 Solution: In above transition graph state q1 is a dead state. We can not reach to final state from this state, hence we will eliminate state q1. The FA then becomes as - a.b Fig. 2.3.17 The state q2 can be written as (a + b) *. Hence the required r.e. is a(a+b) *. This regular expression specifies a language which starts with letter a and it is followed by any number of a's and b's. Example 2.3.6 Construct Finite automaton to accept the regular expression (0+1)*(00 + 11)(0+1) *. AU: May-14, Marks 8, May-11, Marks 6 Solution: The FA for r.e. = (0+1) (00 + 11) (0+1)* as follows - Step 1 : (0+1)* (00+11) (0+1)* Step 2 : (0+1)* (00+11) (0+1)* 92 Step 3 : E 0 (00+11) 97 (0+1)* 94 1 E 110 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 101 ---
Course Code/Title: CS3401/Theory of Computation (a + b) * ( (ab)* + (ba)* + b*) ii) r. e. = (a* b a* b)* iii) The L = {&, a, b, bb, aa, ab, abb, ... } r. e. = (a*b*) Example 2.1.7 Give the Regular Expression for the following languages: i) L = { x|x € {a,b}* and x contains exactly two a's} ii) L = (a, c, ab, cb, abb, cbb, abbb, cbbb, ... } iii) L = { x|x € {a, b}* and x is any string that begins in "abb" or a} Solution: i) b* ab* ab* ii) The set contains all the string that either begin with a or c but always end with any number of b's. r.e. = (a+c) b* iii) r.e. = (abb + a) (a + b)* Review Question 1. Discuss on regular expressions. AU: May-13, Marks 8 Equivalence of Finite Automata and Regular Expressions There is a close relationship between a finite automata and the regular expression we can show this relation in Fig. 2.2.1. Can be convertod Regular expression Can be converted to Deterministic finite automata# NFA with e moves Can be converted NFA without moves Can be converted Fig. 2.2.1 Relationship between FA and regular expression The Fig. 2.2.1 shows that it is convenient to convert the regular expression to NFA with & moves. Let us see the theorem based on this conversion. Equivalence of NFA and Regular Expression AU: May-04,06,09,10,11,12,14,16,17, Dec .- 10,14,15,19, Marks 10 Theorem 1: Let r be a regular expression, then there exists a NFA with & transitions that accepts L (r). 101 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 102 ---
Course Code/Title: CS3401/Theory of Computation Proof: This theorem can be proved by induction method. The basis of induction will be by considering r has zero operators. Basis (zero operators) - Now, since r has zero operators, means r can be either & or ø or a for some a in input set Σ. The finite automata for the same can be written as Start (no path to final state) Fig. 2.3.1 Finite automata for given regular expression Induction: This theorem can be true for n number of operators. The n is greater than or equal to 1. The regular expression contains equal to or more than one operators. In any type of regular expression there are only three cases possible. 1. Union 2. Concatenation 3. Closure Let us see each, Case 1: Union case Let r = r1 + 2 where r1 and r2 be the regular expressions. There exists two NFA's M1 = (Q1, Σι, δι, {f 1}) and M2 = (Q2, 22, 82, {f2}) L (M1) = L(r1) means the language states by regular expression ri is same which is represented by M1. Similarly L (M2) = L (r2). Q1 represents the set of all the states in machine M1. Q2 represents the set of all the states in machine M2. We assume that Q1 and Q2 are totally different i.e. Q1 and Q2 are disjoint. Let q0 be new initial state and f0 be the new final state we will form M =((QI U Q2 U {qo, fo}), (Σι UE2), δ, qo {fo;) The & is denoted by, i) ô (qo, ¿) (q1, q2} ii) δ (q, a) = δι (q, a) for q in Q1 - {fi} and a in EI U {E}. iii) δ (q, a) = 82 (q, a) for q in Q2 -{f2} and a in X2 U {E}. iv) ô (f1, ¿) = 81 (f2, c) = {fo} All the moves are now present in machine M which is as shown Fig. 2.3.2. 102 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 175" Rank nirf

--- Page 167 ---
Course Code/Title: CS3401/Theory of Computation 5 Example: Let G =(v, T, P.S) be given by G = {{S, A, B} {0,13. P. 553) P: 5->AIB, A-JOA/S, B-JOB/1B/G. Solni consider the String 00101, which is to be generated by the given grammar using sentential form. S => AIB [A-SOA] =>0A 1B =>00A18 [ A-JOA] =>001B =>00 10B [B-JOB] =>00101B [B-SIB] =>00101 [B->4] Thus the string 00101 EL(G). Types of sentential Form There are 2 types of sentential forms namely, * lett sentential forms * Right sentential forms Left Sentential Forms: When the derivations are generated by expanding the leftmost symbol is called left sextential form. 167 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 168 ---
Course Code/Title: CS3401/Theory of Computation Examples: 1) consider the grammar given below 5-> AIB, A-JoA16 B-> 08/18/6. Given the lastmost derivation for the String 1001. Soln: S=> A/B 1m 1m 1B [: A-54] Im 10B [: B -JOB] => 100B [: B->OB) 1m => 1001B [=B->1B] Im => 100) [: B->&] 1m Right sentential Form: The derivation generated by performing is called Substitution on the right most variable right sentential form. rm Example. consider the Grammar, 5->A)B, A-> OALE, B->OB/ 1B/G . Generate the right most derivation for the String 1001. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY 168 nirf 175" Rank LEN

--- Page 95 ---
Course Code/Title: CS3401/Theory of Computation States 0 1 1901 190. 911 192 1921 (90, 91) 190 911 190- 92) 190. 92) 190- 911 The transition diagram can be drawn as follows. 0 Start 0 1 (4) 92) 0 1 Fig. 1.13.10 Q.29 Obtain the NFA without & transition to the following NFA with & transition. AU: Dec .- 03 0 2 E 9 Fig. 1.13.11 Ans .: Remove & transition from qo to q1. 0,1 Start E 2 Fig. 1.13.12 Now remove & transition from qo to q2. 0,1,2 Start Fig. 1.13.13 As qo to q2 is & transition qo will become start and final state both. Q.30 Obtain the & - closure of states qo and q1 in the following NFA with & transition. AU: Dec .- 04 95 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 96 ---
Course Code/Title: CS3401/Theory of Computation 8 b C 41 Fig. 1.13.14 Ans .: ¿ - closure {qo} = {qo, q1, q2} E - closure {q1} = {q1, q2} These are & - reachable states from qo and q1. Q.31 Obtain & - closure of each state in the following NFA with & move. AU: Dec .- 05 Ans .: The E - closure of each state means collection of & - reachable states. 0 2 Fig. 1.13.15 ¿ - closure (qo) = {qo, q1, q2} as we have & transition from qo to qo, q1, q2. & - closure (q1) = {q1, q2} E - closure {q2} = {q2} Q.32 Find the language accepted by the DFA given below. AU: May-06 0 0.1 0 1 Fig. 1.13.16 Ans .: The regular expression for this DFA is r.e. = 1*00*1 (0+1)* = 1*0*1 (0+1)* That means this is a language containing all the strings which consist of atleast one single pair of 01. Q.33 Find the closure of the states 1, 2 and 4 in the following transition diagram. AU: May-06 2 3 6 1 b 4 a 5 E 7 Fig. 1.13.17 96 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 191 ---
Course Code/Title: CS3401/Theory of Computation Since SB CS and by previous the orem (9, x, 2) ++ (P,Y,B) we can write (1) has (90,w3 20201) 1% (9,4, 20’) 1 Then B can be computed has (Po,w,201)th(90,w,2020)告(q,x,dz')B(P, 4, 4) Thus L = N(B) = L (A) 191 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 192 ---
Course Code/Title: CS3401/Theory of Computation 3 The language of PDA: The language of a PDA can be accepted of two ways. * Acceptance by final state * Acceptance by empty stack Acceptance By Final State: A PDA That accepts its input and enters the final accepting state is called as a PDA accepted by final state. L(P) = {w/(90, w, 20) } * (2,4,00) Acceptance By Empty Stack: Final State, NO 113. Stuck Symbols The PDA that accepts its input by emptying the stack is the PDA accepted by empty stack. N(P) = {w/(90, w, 20) 1+ (9, & Q)} Example. 1 Final State NO 1 15 Stuck fresh . . . 192 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 169 ---
Course Code/Title: CS3401/Theory of Computation 6 Soln: S => A/B rm => AJOB [: B-JOB] Ym => ALOOB [: B-JOB) rm im A1001B [: B->18] => rm A1001. [: B-> &] =S rm 100) [: A->&] -x- 1) let of be the grammar P: {5->aB/bA, A->alas/ bAA, B-> b) bs/ aB]. for the string aubbbbaa, find LMD and RMD: Soln: LMD: S => aB 1m Inaas [ B->as] I'm aabA [b] Im aabbAA [:A->bAA] 169 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 170 ---
Course Code/Title: CS3401/Theory of Computation 2. Derivation Trees/Parse Trees: The representation of a derivation in The form of a free is called a purse free/derivation tre V1-> < where VIEV an ET, then v, can have any number of children based on its production rules. If v->&, then & Should be the only child of v1. Example: For the grammar 5-> 051/01, generate derivation of the string 000111. soln: The derivation is given by, $ => 0SI => 00511 =>000111 [:5-501] The parse tree is given by 5 1 0 5 1 0 1 0 1 170 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N3 nirf 175" Rank

--- Page 171 ---
Course Code/Title: CS3401/Theory of Computation Types of Parse Tree: There are 2 types of passe free * Left most derivation tree/left derivation tree * Right most derivation tre/ Right derivation free Left most derivation Tree: A derivation free representing A=>2 is called Sa ustmost derivation tree if the production rule is Papplied only to the leftmost variable at every step. >Right most derivation Tree: A derivation tree representing A=>2 is said to be a rightmost derivation free if there the production rule is applied only to the rightmost variable at every 3 I step. Example: 31. For the grammar given below give the parse tree for lestmost and rightmost derivation of the string 1001. 3 Soln: > Left most derivation free: 1) 5 3 1 3 A 1 1 B using A->4. 12) 5 / 1 1 A 1 B 1 1 1 4 0 B using B-JOB 171 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY nirf 175" Rank LEN

--- Page 172 ---
Course Code/Title: CS3401/Theory of Computation 3) 5 / 1 A 1 B 1 1 1 4 B 0 1 0 B 4) 5 1 A B 1 1 1 0 B / 0 B / 1 1 B 5) 5 11 / A B 1 1 0 B 3 1 1 0 B 1 1 1 B using B-JOB using B-> 1B using B->4. -w 6) 5 1 $ 1 1 -w 0 B B 0 B 1 1 4. => 1001 Im 172 CHENNAI INGSTOUTE . CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 107 ---
Course Code/Title: CS3401/Theory of Computation ¿ - closure (qo) = {qo, q1, q2, q4, q7} = Call it as A 8' (A, a) = & - closure (8 (qo, q1, q2, q4, q7), a) = 8 - closure (q3, q8) = {q1, q2, q3, 94, 96, q7, q8} 8' (A, a) = Call it as state B 8' (A, a) = B 8' (A, b) = & - closure (8 (qo, q1, q2, q4, q7), b) = 8 - closure (q5) = {q1, q2, q4, q5, q6, q7} Call it as C 8' (A, b) = C 8' (B, a) = & - closure (8 (q1, q2, q3, q4, 96, q7, q8), a) = 8 - closure (q3, q8) §' (B, a) = B 8' (B, b) = & - closure (8 (q1, q2, q3, q4, q6, q7, q8), b) = 8 - closure (q5, q9) = {q1, q2, q4, 95, q6, q7, q9} Call it as D 8' (B, b) = D 8' (C, a) = & - closure (8 (q1, q2, q4, q5, q6, q7), a) = 8 - closure (q3, q8) ' (C, a) = B 8' (C, b) = & - closure (8 (q1, q2, q4, q5, q6, q7), b) = 8 - closure (q5) §' (C, b) = C 8' (D, a) = - closure (8 (q1, q2, q4, q5, q6, q7, q9), a) = 8 - closure (q3, q8) 8' (D, a) = B 8' (D, b) = 8 - closure (8 (q1, q2, q4, q5, q6, q7, q9), b) = & - closure (q5, q9) 8' (D, b) = D As no new state is getting generated the transition table will be 107 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank --

--- Page 108 ---
Course Code/Title: CS3401/Theory of Computation i/p a b State A a B A B C B B b D a b C B C C D b B D b The & transition of state B and state D is same but B is a non final state while D is a final state. So we can not merge them. We can merge state A and state C. Then the minimized DFA will be B b i/p b a B State A B A B B a b 8 D b Direct Method for Conversion of RE to FA 1. State Elimination Method This is the simplest method of obtaining regular expression. We will use following rules to obtain r.e. by eliminating states 1. Eliminate all the states except final state and start state. 2. Reduce the given finite automata and obtain the generalised transition graph with (start and end states as follows R Rx R R Fig. 2.3.12 A generic finite automaton The regular expression for this finite automata will be - 108 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 183 ---
Course Code/Title: CS3401/Theory of Computation PUSHDOWN AUTOMATA Pushdown Automata: A Push down automate shortly called as a PDA is a G-NFA with stack, which is used to define regular language. ao. a, a ..... un .. Finite State control > Accept) input tape € Reject- Definition of Pushdown Automata: Stack 7- tuples. A PDA can be formally defined by Where, P=(Q, E, F, d. 20, 20, F) Q- finite set of states. 2- Finite set of input symbols [- Fuste set of stack symbols S .- Transition function S: Qx(Eva) xr->QxTR 90 - Start State 20 - Start Symbol (Stack) F-accepting state. 183 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY LEN nirf 175" Rank

--- Page 184 ---
Course Code/Title: CS3401/Theory of Computation Transitions with stack operations 1> Read input with no-operation on stuck: The transition That reads an insul- from the input tape, but performs no-operations, on the stack is given as. S (q,, a,b)=(92, b) 2) Push operation on stack: consider that the input ='a' pushed on to the stack, Then the transition be comes S(2,a, b) = [92, a,b) - 'a' is accepted by The stack. 3) Pop oftration on stack: The transition of a POP operation is given as, 2(91, a, b) = [92, E) a cancels 'b'1 from the stick. Example: 1 ) L = { WE (a,b)*/ w is of The form a"b", h21} Soln: 2: S(20, a, 20) = (90,020) 2(%), a, a) = (90, aa). S (90, b, a) = (91, 4) S (a), b, a) = (91, &) 2(21, 4, 20) = (92, 20) 184 CHENNAI INGSTOUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 147 ---
Course Code/Title: CS3401/Theory of Computation S-> 4} If the language is 4 + 4 * 4 then we can use the production rules given by P. The start symbol is S. The number of non-terminals in the rules P is one and the only non terminal i.e. S. The terminals are +, *, (,) and 4. We are using following conventions. 1. The capital letters are used to denote the non-terminals. 2. The lower case letters are used to denote the terminals. Example: The formation of production rules for checking syntax of any English statement is SENTENCE -> NOUN VERB NOUN-> Rama / Seeta / Gopal VERB-> goes / writes / sings Thus if want to derive a string "Rama sings" then we can follow the above rules. Derivations AU May-10,11,12,16, Dec .- 11,14,16,17,18, Marks 16 The production rules are used to derive certain strings. We will now formally define the language generated by grammar G = (V, T, P, S). The generation of language using specific rules is called derivation. Definition : Let G = (V, T, P, S) be the context free grammar. If A ->ß is a production of P and a and y are any strings from non-terminals or terminals i.e. in (VUT)* then a Ay -> aßy. Suppose a1, a2, a3, ... am are strings in (VUT)*, m ≥ 1. a1 => a2 a2 => a3 . . am-1 >> am Then we can say that a1 => am The language generated by G is denoted by L(G). The language L is called context free language CFL if L(G) is for CFG. For example G = ({S, B} {a, b}, {S -> a B b, S) B -> bbb} is a grammar. Then we can derive a string abbbb as - i) We will first start from start symbol S 147 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE TECHNOLOGY LEN nirf 175" Rank

--- Page 148 ---
Course Code/Title: CS3401/Theory of Computation S=a Bb ii) Then we will replace B by bbb S => a B b =>a bbbb Thus we can obtain the desired language L by certain rules. The language L can be described as a language which starts with letter a and having 4 b's following. Let us solve some examples for derivation of CFG. Solved Examples Example 3.3.1 Try to recognize the language L for given CFG. G = [{S}, {a,b}, P, {S}] where P = {S -> aSb, S -> ab} Solution: Since S -> a S b | a b is a rule. | indicates the 'or' operator. S -> a Sb If this rule can be recursively applied then, S a Sb ↓ aa Sbb ↓ aaa Sbbb and if finally we can put S-> ab then it becomes aaaa bbbb. Thus we can have any number of a's first then equal number of b's following it. Hence we can guess the language as {L=a" b" where n>1}. The only way to recognize the language is to try out various strings from the given production rules. Simply by observing the derived strings, one can find out the language getting generated from given CFG. Example 3.3.2 Construct the CFG for the regular expression (0+1)* Solution: The CFG can be given by, P={S -> 0S | 1S S-> 8} The rules are in combination of 0's and 1's with the start symbol. Since (0+1)* indicates (8, 0, 1, 01, 10, 00, 11, ... ) in this set & is a string. So in the rules we can set the rule S -> E. Example 3.3.3 Construct a grammar generating L= w c wT where we w e {a,b}* Solution: The strings which can be generated for given L is {aacaa, bcb, abcba, bacab ... } The grammar could be S -> a Sa 148 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY LEN nirf 175" Rank

--- Page 115 ---
Course Code/Title: CS3401/Theory of Computation & R = R Substituting value of qı in q2 we get q2 = q1 b + q2 b q2 = a * b + q2 b We can compare this equation with R = Q + R P assuming R = q2, Q = a*b, P = b which gets reduced to R = QP *. q2 =a*b.b* As R R* = R+ q2 = a *. b+ From the given DFA, if we want to find out the regular expression, we normally calculate the equation for final state. Since in the given DFA q2 is a final state and q2 = a*b+. We can conclude as the DFA represents a as regular expression. Example 2.4.2 Represent the language accepted by following DFA. Start 0,1 Fig. 2.4.2 Solution: Since there is only one state in the finite automata let us solve for q0 only. qo=900+ qo1+ € qo= qo (0+1)+ € =8 . (0+1)* R=Q+RP qo = (0+1) Since q0 is a final state, q0 represents the final r.e. as r=(0+1)* L(r) = {8, 0,00,1,11,10, .... } L (r) = {any combination of 0 and 1} Example 2.4.3 Construct r.e. for the given DFA. AU May-07, Marks 6 0 1 0,1 Start 1 0 Fig. 2.4.3 Solution: Let us build the regular expression for each state. q1=q10+8 115 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 116 ---
Course Code/Title: CS3401/Theory of Computation q2 = q11 + q21 q3= q20+ q3 (0 + 1) Since final states are qı and q2, we are interested in solving qı and q2 only. Let us see qı first q1 = 8 + q10 Which is R = Q + R P equivalent so we can write qı = 8 .(0)* q1 =0* 8. R = R Substituting this value into q2, we will get q2=0*1 + q21 q2 = 0*1 (1)* R = Q + R P = QP* The regular expression is given by r= q1 + q2 =0* + 0* 1. 1* r= 0* + 0* 1+ 1.1* = 1+ Example 2.4.4 Convert the following NFA into a regular expression. AU: Dec .- 13, Marks 8 0,1 1 A 0+1 B Fig. 2.4.4 0 + 1 C D Solution: We will obtain regular expression using Arden's method. The equations for each state are - A=A0+ A1 + € = A(0 + 1) + ¿ B = A1 C=B (0+1) D= C (0+1) We will solve equation (1) A=A (0+1) + € i) R = RP + Q gives R = QP* ii) ER* = R* A= &(0+1)* = (0+1)* CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY 116 nirf 175" Rank N39

--- Page 207 ---
menframmutation 207 CHENNAI INGSTOUTE . TECHNOLOGY UNIT IV PROPERTIES OF CONTEXT FREE LANGUAGES Normal Forms for CFG - Pumping Lemma for CFL - Closure Properties of CFL - Turing Machines - Programming Techniques for TM. CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 208 ---
Course Code/Title: CS3401/Theory of Computation simplification of CFG: The preliminary simplifications, which are applied on grammars to convert them to normal forms are, * Elimination of useless symbols > Elimination of &- productions * Elimination of unit productions 1. Elimination of useless symbols: useless symbols: 1. The useless symbols are those variables/ terminals that do not appear in any derivation of a terminal String from the start string. 2. If the derivation from the start symbol to a String of terminals does not depend on a variable x then x is non-reachable symbol. Elimination: 1. The only way to Simplify a grammar containing non-generating symbol is to eliminate such symbols directly from the grammar. 2. A grammar containing non reachable symbol can simplified by deleting all the production containing be the non-reachable Symbol. CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY 208 nirf 175" Rank N39

--- Page 203 ---
Course Code/Title: CS3401/Theory of Computation 1 Convert PDA to CFG1: .M=(EP, 2}, {0,1}, {x,2}, {, q,ZA 2: S[q,1, 2) = (9, x2) &L9,1,xJ=(q, xx) 2(9 , 4, x) = (9, E) 2[9, 0,x) = (p.x) 2(P, 1,x)= (p.8) 2[P,O,2)= (9,2) Soln: CFGI G=(v, T_ P.S) v = { [ q , x , q ] , [ '℃, x , P ] [ P , X 9 ] [ P & P ] [24] [q2P] [P29][29]] + 9 4 T={0,13 S = {5} S-SELF ESIAE/IBG Px-2] P: Dp:3→[q zq] Ps: 5 -> [9 2 P] 2) 2(2, 1,2)= (2, x2) [q 2 2] -> 1 [2 × 9] [q z 2] [ q 2 9 ] - > 1 [9×P] [P.29] [9 2 p ] -> 1 [2 × 9] [q 2 P] [q 2 - I [2 xP] [12 ] 203 CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY N39 nirf 175" Rank --

--- Page 204 ---
Course Code/Title: CS3401/Theory of Computation 3) {[q, 1,x) =(9,xx) [ 9 × 9 ] = > 1 [9 × 9] [9×9] [2×9] ->|[qxp] [P×2] [ 2 × p] -> 1 [q xp][2×P] 4) 2/ 2, 4, x ) = (9, ;) [9 × 9] -> & 5) {[2, 0,x) = (p.x) [2×9]-so[Px9] [qxp] -so [px?] 6) 2( P.1, x)=(P.8) [P & P] -31 7) {[ p, 0,2)= (9, 2) [P2 2] -> o [2 2 ]] [P 2 P] => [2 ZP] final Production: 204 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY nirf 175" Rank N39

--- Page 133 ---
Course Code/Title: CS3401/Theory of Computation Proof: If L1 and L2 are regular then they can be expressed as L1 = L(R1) and L2 = L(R2) Then L1 L2 = L(R1 R2) thus we get a regular language. Hence it is proved that regular languages are closed under concatenation. Theorem 8: A homomorphism of regular languages is regular. Proof: The term homomorphism means substitution of string by some other symbols. For instance the string "aabb" can be written as 0011 under homomorphism. Clearly here, a is replaced by 0 and b is replaced by 1. Let 2 is the set of input alphabets and I be the set of substitution symbols then 2* - > I* is homomorphism. The definition of homomorphism can be extended as Let, w = a1 a2 ... an h(w) = h(a1) h(a2) ... h(an) If L is a language that belongs to set 2, then the homomorphic image of L can be defined as: h(L) = {h(w): w EL} To prove that if L is regular h(L) is also regular consider following example - Let, E = {a, b} and w = abab Let h(a) = 00 and h(b) = 11 Then we can write h(w) = h(a) h(b) h(a) h(b) = 00110011 The homomorphism to language is applied by applying homomorphism on each string of language. If L = ab*b then, L = {ab, abb, abbb, abbbb, ... ) Now h(L)=(0011, 001111, 00111111, 0011111111, ... ) h(L) = 00 (11)* As it can be represented by a regular expression, it is a regular language. Hence it is proved that if L is regular then h(L) is also regular. In other words, family of regular languages is closed under homomorphism. Theorem 9: The inverse homomorphism of regular language is regular. Proof : Let E* - > Γ* is homomorphism. The 2 is the input set and I be the substitution symbols used by homomorphic function. Let, L be the regular language where L € 2, then h(L) be homomorphic language. The inverse homomorphic language can be represented as h-1(L) Let, h-1(L) = {w | wEL} If L is regular then h(L) is also regular because regular language is closed under homomorphism. That if there exist a FA M = (Q, E, 8, qo, F) which accepts L then h(L) must also be accepted by FA M. For 133 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank CEN

--- Page 134 ---
Course Code/Title: CS3401/Theory of Computation complement of L i.e. language L' the inverse homomorphic language is h-1(L). Let M' be the FA in which all the final states of M become non-final states and all the non-final states of M become the final states. Clearly the language L' can be accepted by M' Hence h-1(L) must also be accepted by FA M'. For example - Let L = (010)* be a regular language. And L = {010, 010010, ... } Let h(0) = a and h(1) = bb be homomorphic function. Then h(L) = (abba)* This L can be represented by following DFA. 0 90 1 Le a bb 0 h(L) L a bb -1 h (L) Thus there exists a FA which accepts h-1(1). This shows that, inverse homomorphism of regular language is regular. Review Questions 1. Show that regular languages are closed under intersection and reversal. AU: Dec .- 07, 08, Marks 8 2. Prove that regular sets are closed under substitution. AU: Dec .- 10, Marks 6 3. State and prove using an example, the properties of regular language. AU: May-10, Marks 7 4. Prove any two closure properties of regular languages. AU: Dec .- 12, Marks 6 5. Discuss in detail about the closure properties of regular languages. AU: May-13, Dec .- 13, Marks 8; Dec .- 17, Marks 13 Two Marks Questions with Answers Q.1 Is it true that the language accepted by any NFA is different from the regular language? Justify your answer. AU: May-04 Ans. No, any language accepted by DFA, NFA, &-NFA is called a regular language. For any finite automaton we can construct an equivalent regular expression and the language represented by regular expressions is a regular language. Q.2 Describe the following by regular expression a) L1 = The set of all strings of 0's and 1's ending in 00. b) L2 The set of all strings of 0's and 1's beginning with 0 and ending with 1. 134 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN

--- Page 119 ---
Course Code/Title: CS3401/Theory of Computation q3 = a* ba* b As q3 represents the final state, the equation for state q3 represents the regular expression. Hence regular expression = a* ba* b. Review Questions 1. State and explain the conversion of DFA into regular expression using Arden's theorem. Illustrate with an example. AU: Dec .- 11, Marks 16 2. Discuss the basic approach to convert from NFA to regular expression. Illustrate with an example. AU Dec .- 16, Marks 16 Equivalence of Two Regular Expressions AU May-10,11, Marks 9 The two regular expressions P and Q are equivalent (denoted as P = Q) if and only if P represents the same set of strings as Q does. For showing this equivalence of regular expressions we need to show some identities of regular expressions. Let P, Q and R are regular expressions then the identity rules are as given below 1. ¿ R =R & =R 2. 8 = 8 ¿ is null string 3. ($)* = *= 8 ¢ is empty string. 4. ¢ R = R $=¢ 5.¢+R=R 6. R + R = R 7. RR* = R*R = R+ 8. (R*)* = R* 9. 8 + RR* = R* 10. (P+Q) R = PR + QR 11. (P+Q)* = (P* Q*) = (P* + Q*) 12. R* (¿ + R) = (¿ + R) R* = R* 13. (R + ¿)* = R* 14. ¿ + R *= R* 15. (PQ)* P = P (QP) 16. R*R + R = R*R Let us see one important theorem named Arden's Theorem which helps in checking the equivalence of two regular expressions. 119 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 120 ---
Course Code/Title: CS3401/Theory of Computation Arden's Theorem: Let, P and Q be the two regular expressions over the input set 2. The regular expression R is given as R = Q + RP Which has a unique solution as R = QP *. Proof : Let, P and Q are two regular expressions over the input string 2. If P does not contain & then there exists R such that R = Q + RP (2.5.1) We will replace R by QP* in equation (2.5.1) Consider R.H.S. of equation (2.5.1) = Q + QP* P =Q(&+ P* P) = QP* 0* ¿ + R* R = R* Thus R = QP* is proved. To prove that R = QP* is a unique solution, we will now replace L.H.S. of equation (2.5.1) by Q+ RP. Then it becomes Q+RP But again R can be replaced by Q + RP. Q + RP = Q+(Q+RP) P =Q+ QP + RP2 Again replace R by Q + RP. =Q+ QP+ (Q+RP) p2 =Q+ QP + QP2 + RP3 Thus if we go on replacing R by Q + RP then we get, Q + RP = Q + QP + QP2 + ... + QPi + Rpi+1 = Q(& + P+ P2 + ..... Pi) + Rpi+1 From equation (2.5.1), R = Q (& + P+ P2 + ..... + Pi) + Rpi+1 .... (2.5.2) Where i≥ 0 Consider equation (2.5.2), R = Q(& + P+ P2 + ... + pi + RPi+1 /p* R = QP* + RPi+1 Let w be a string of length i. 120 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 173 ---
Course Code/Title: CS3401/Theory of Computation Right most derivation tree: 1) 5 2) 1 5 1 1 / 1 1 A 1 B A 1 B 1 0 B 5 1 1 5) 5 A 1 B 1 0 1 A B 1 0 0 1 B / 1 0 1 B Recursive Production/ Inference: 5 3) / 1 1 A y B 1 1 0 B 1 0 B 6) 5 1 A 1 B B B 1 B 1 0 B 1 0 B 1 B 1 1 B & 1 6 => em (00) A production is said to be recursive if the left side variable occurs on the right hand side that substitutes the same production for 'n' number of times. Example: 5->as => this would lead to application of the same rule recursively as 5=>as => aus = >anas=> .... 173 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 174 ---
Course Code/Title: CS3401/Theory of Computation AMBIGUITY: Ambiguous grammar. A grammar, G is said to be ambiguous it there exists 2 different passe tree for at least one String 'w' where WE Teach passe tree with the same symbol. Example: consider the grammar given below, prove that it is ambiguous. F-> F+F/a/b. Soln: F E E + E 1 1 1 E a +1 F 1 6 a =>a+b+a. E E + + F 1 E a 0 6 => a+ b + a Since there are 2 different passe trees with same start symbol, leads to the same string WET, the grammar is ambiguous. Removing ambiguity from grammar: -x- n The only possibility of removing ambiguity from grammar is to introduce one or more different variables. 174 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank -- LEN

--- Page 195 ---
Course Code/Title: CS3401/Theory of Computation Step 3: Push 'c' onto the Stack. W = aabbcd 20 -> C . 91 20 92 2191, 2, 20) =(92) (20) &(92, c, c) = (a, cc) Step 4: popped out. W = aabbcd 7 C L For every' d'as insul 'c' should be- -> 20 20 < 92 S[ qa, d, c) = (93, E) 2(a, d, d) = 19, E). 93 step 5: string is accepted as 20 20 > 1 95 2/93, 4, 20)= (94, 4) The PDA is given us c,d M=[4,92, 994 }, {a.b.], [chen}, {a, c,20}, {. 90,20,0) 195 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY pou LEN 175" Rank

--- Page 196 ---
Course Code/Title: CS3401/Theory of Computation 5 3) let L={a' b'ck [, j,k>> [+] =x}. Provide The transition function. 1) Accepted by final state ii) Accepted by empty black. Soln: PDA: Through final State 5: PDA: Through empty stack 2. S(90,9,20) -(90-×20) 2(90, a,x)=(90,xx) 2(90, b, 20) =[21, x20) 2(90, b, x)=(21,xx) 2(a, b, x)=(21, xx) S(21, c,x)=(92,4) Q[ 92, c,x)=(92, 2) 2(2, 4, 20)= (12 }) 2(20, 8, 20) =(90-6) S(90, a, 20) = [90,×20) 2(90, a,x)=90, xx) 2/90- b, 20)=(91,x20) S/90- b,x)=(91,xx) S(21, b,x)={91, ×) 2[21, c, x) = (92, &) 2(92, c, x) = (92, &) 2(92, 4, 20)= (93, 20) 2(90, 4, 20)= [95, 20) 196 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 187 ---
Course Code/Title: CS3401/Theory of Computation Equivalance of acceptance of PDA From empty Stack to Final State Theorem: If A=(Q, E, 5, 5, 90,20, F) is a Pda accepting L by empty store, we can find a Pda B=(a), E, T', SB, 90, 20' F') which accepts L by final state (4) L=NCAJ .= L(B) Proof: B is constructed in such a way that 4.20/40 4,26/2020' Po 90 PN PI 6,2018 final State. initial Hove of B 4.2014. finite automata for well store let us define B as follows: where B = (Q; E, 5', SB, 90', 20', F) Q'- QUEPO, P+ } T' - 5 0420'} F1 - { PJ] 90' = Po 20 = Start Symbol for stack 187 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 188 ---
Course Code/Title: CS3401/Theory of Computation So is given by rules R1: SB (PO, &, 20') = (90, 20 20') R2: SB (9,a,z)=>[9,9,2) R3: SBL 9, 8, 20')=(P), &) we have to show N(A) = L(B) Let WE N(A). Then by definition of N(A). (90,10, 20) 1* (2, 4,6) By Previous the orem [2, x, s) |* [P,y, B) we get (90, w, 2020') + (9, 4, 20') Since rull store (or) empty store is a subset of &B .. we conclude that (Po, w, 20') HE (90, 00, 2020') 答(q,4, 20') (Pt,4,4) UN(A) = L(B) 188 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 157 ---
Course Code/Title: CS3401/Theory of Computation This proves that S => S1, S2, S3 .... Sn => a can be obtained. Leftmost Derivation and Rightmost Derivation AU: May-13,16, Dec .- 10,15,18, Marks 9 · The leftmost derivation is a derivation in which the leftmost non-terminal is replaced first from the sentential form. · The rightmost derivation is a derivation in which rightmost non-terminal is replaced first from the sentential form. · For example S-> XYX S-> aYX -> abX S -> aba S -> XYX S -> XYa S -> Xba S -> aba Leftmost Derivation Rightmost Derivation Note that we have replaced first X from left to right in leftmost derivation and XYX the last X i.e. the rightmost symbol. Examples for Understanding Example 3.5.1 Let, G be the grammar S -> aB | bA A -> a | aS | bAA B -> b | bS | aBB For the string baaabbabba. Find leftmost derivation, rightmost derivation and Parse tree. AU: Dec .- 10, Marks 9 Solution : PPPPPPPPPPPPPPPPPPPPPPPPPPP Example 3.5.2 Consider the following productions S -> aB bA A -> a | aS | bAA 157 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 158 ---
Course Code/Title: CS3401/Theory of Computation B -> b | bS | aBB For the string aaabbabbba, Find 1) Leftmost derivation 2) Rightmost derivation 3) Parse tree AU May-13, Marks 8 Solution: For leftmost derivation and parse tree refer answer of Q.10 from Two Marks Questions with Answers Rightmost Derivation : Example 3.5.3 Given the grammar G = (V, E, R, E) where V = {E, D, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, +, -, *, /, ()}, > = {1, 2, 3, 4, 5, 6, 7, 8, 9, 0, +, -, *, /, ()}, and R contains the following rules: D-> 0|1|2|3| ... 9 find a parse tree for the string 1+2*3. AU: Dec. 15, Marks 6 Solution: The parse tree Example 3.5.4 Show the derivation steps and construct derivation tree for the string 'ababbb' by using leftmost derivation with the grammar. S -> ABIE A -> aB B -> Sb. AU May-16, Marks 5 Solution : Ppppppppppppppppppppppppppppppppp Example 3.5.5 Consider the context free grammar (CFG) given below. Give the leftmost derivation for the string bbaa using the grammar. S-> bS | aT | ¿ T-> aT | bU | ¿ U-> aT | ¿ AU Dec .- 18, Marks 2 Solution : 158 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY LEN nirf 175" Rank

--- Page 155 ---
Course Code/Title: CS3401/Theory of Computation · Derivation trees is a graphical representation for the derivation of the given production rules for a given CFG. · It is the simple way to show how the derivation can be done to obtain some string from given set of production rules. · The derivation tree is also called parse tree. S b S b b S 1 Fig. 3.4.1 Derivation tree · Following are properties of any derivation tree - 1. The root node is always a node indicating start symbol. 2. The derivation is read from left to right. 3. The leaf nodes are always terminal nodes. 4. The interior nodes are always the non-terminal nodes. · For example, S -> bSb | a | b is a production rule. The S is a start symbol. · The above tree is a derivation tree drawn for deriving a string bbabb. By simply reading the leaf nodes we can obtain the desired string. The same tree can also be denoted by, Fig. 3.4.2 5 b S b D 8 b Fig. 3.4.2 Relationship between Derivation and Derivation Trees AU Dec .- 03, 04, 05, 10, 12, 14, 15, May-04, 05, 09, 13, 16, Marks 16 Theorem: Let G = (V, T, P, S) be a context free grammar. Then S => a if and only if there is a derivation tree in grammar G which gives the string a. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY LEN 155 nirf 175" Rank

--- Page 156 ---
Course Code/Title: CS3401/Theory of Computation OR Theorem Let G = (V, T, P, S) be a context free grammar then prove that if the recursive inference procedure tells us that terminal string w is in the language of variable A, then there is a parse tree with root A and yield w. AU: Dec .- 15, Marks 10 Proof: For a non-terminal S there exists S => w if and only if there is a derivation tree starting from root S and yielding w. To prove this we will use method of induction. Basis of induction: Assume that there is only one interior node S. The derivation tree yielding S1, S2, S3, ... ,Sn. From S is that means S = S1, S2, ... Sn => a is input string. S S. S, Fig. 3.4.3 Induction hypothesis: We assume that for K-1 nodes the derivation tree can be drawn. We then can prove that for K vertices also we can have a derivation tree. That means the input string a can be derived as S -> S1, S2, S3, .... Sk. There are two cases: either Si may be a leaf variable or Si may be an interior node yielding a. The S derives a by fewer number of K steps then a E S1 S2 S3 ... Sk. S S. S2 SK Fig. 3.4.4 If ai = Si then Si is leaf node (terminal) and if Si => aj then Si is an interior node. The tree for S1 S2 ... Sk will be shown in Fig. 3.3.4 We can also represent it as shown in Fig. 3.4.5. S S, T2 TK Fig. 3.4.5 156 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank

--- Page 197 ---
Course Code/Title: CS3401/Theory of Computation 3) Design a PDA for the language {am 60cm/m.n> 1.} using empty stack. (aa). Soln: NO-OP w = aa bcc 1 . 1 Push - POP 5: S(90, 0, 20) = (90, a20) } {[90, a, a) = [90, aa). 2(90, b. a) = (a1, a) 2(91, b, a) =(91, 9) 2 push NO- OP 2(a, c, a) = (92, E) 2192, c, Q = (92,{) 2 Pos 2(92, &, 20)= (92) }) -> empty stack M={90,9, 923, 24, c, c 3, 2,3,2, 990,1203,3 -x- 4) construct a PDA for the language accepting dy empty stack. L={am bmcn/m,n=1} Soln: w = aabbc Push Pop No-OP 197 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank --

--- Page 198 ---
Course Code/Title: CS3401/Theory of Computation From CFG'S to PDA'S: Theorem: For any context Free language L, There exist an PDA M such that L =L (M) Proof: Let G=(V,I,P.S) be a grammar. There exists a GNF then we can construct PDA which simulates left most derivations in this grammar. M= (Q, E, 5, 5, 90, 2, F) Q = 190-9, 9, } set of states E = terminals of grammar or r = v 0 42 } F = {92} final state. The transition function will inculde 2(90, 6, 2) = (9,52), so that after first move of M, the stack contains S. In addition, the set of transition rules S(a, a, A) = {a, 2} for each A-> 2 in P S(z, a, a) = { (4, 4)} for each atE 198 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank -- LEN

--- Page 149 ---
Course Code/Title: CS3401/Theory of Computation S-> b Sb S -> c Since the language L = w c wT where we (a + b)* Hence S -> a S a or S -> b S b. The string abcba can be generated from given production rules as S a Sa ab Sb abcba b b bc b b Thus any of this kind of string could be derived from the given production rules. Example 3.3.4 Construct CFG for the language L which has all the strings which are all palindrome over > = {a,b} AU: Dec .- 17, Marks 7 Solution: As we know the strings are palindrome if they posses same alphabets from forward as well as from backward. For example, the string "madam" is a palindrome because madam read read It is the same ! Since the language L is over = (a, b). We want the production rules to be build a's and b's. As & can be the palindrome, a can be palindrome even b can be palindrome. So we can write the production rules as G = ({S}, {a, b}, P, S) P can be S -> a Sa S -> b Sb S -> a S -> b The string abaaba can be derived as CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN 149 nirf 175" Rank

--- Page 150 ---
Course Code/Title: CS3401/Theory of Computation S a Sa a ba aba abSba dbaaba aba Saba abadba abamba abaaba which is a palindrome. Example 3.3.5 Construct CFG for the language containing all the strings of different first and last symbols over E = {0,1}. Solution: Since the problem statement says as if the string starts with 0 it should end with 1 or if the string starts with 1 it should end with 0. S->0A1 |A0 A-> 0A | 1 A | ¿ Thus clearly, in the above CFG different start and end symbols are maintained. The non terminal A -> 0A | 1A | € indicates (0+1) *. Thus the given CFG is equivalent to the regular expression [0 (0+1)*1+1(0+1)*0] Example 3.3.6 Construct the CFG for the language L= a" b2n where n ≥ 1. Solution: Here the number of b's are doubled than a's so we can write S -> aSbb | abb It is as simple as this! Example 3.3.7 Build a CFG for the language L = {0i 1i 2k | j>i+k}. Solution: As the language L = 0i 1j 2k such that j > i + k. Hence we can rewrite L as p 1 0 1 1 2 This will be greater than i+k We can again rewrite L for simplification as - L= 1P 1k 2K deline rule using NTA define rule using B define rule using C CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY 150 nirf 175" Rank LEN

--- Page 145 ---
Course Code/Title: CS3401/Theory of Computation Unit III: Context Free Grammar and Push Down Automata Syllabus Types of Grammar - Chomsky's hierarchy of languages - Context - Free Grammar (CFG) and Languages - Derivations and Parse trees - Ambiguity in grammars and languages - Push Down Automata (PDA): Definition - Moves - Instantaneous descriptions - Languages of pushdown automata- Equivalence of pushdown automata and CFG - CFG to PDA - PDA to CFG - Deterministic Pushdown Automata. Types of Grammar and Chomsky's Hi Hierarchy of Languages The Chomsky's Hierarchy represents the class of languages that are accepted by different machine. The category of languages in Chomsky's Hierarchy is as given below - Language class Type 3 Type 2 Type 1 Type 0 Language Grammar Regular Regular grammar Context free Context free grammar Decidable languages Context sensitive grammar Computable languages Unrestricted grammar Machine One example FSM i.e. NFA or a*b* DFA PDA Linear bounded automata Turing machine n! This is a hierarchy therefore every language of type 3 is also of type 2, 1 and 0. Similarly every language of type 2 is also of type 1 and 0 etc. Computable languages Context Context Regular language Fig. 3.1.1 Chomsky hierarchy Type 3 Regular languages Regular languages are those languages which can be described using regular expressions. These languages can be modelled by NFA or DFA. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY nirf 175" Rank - LEN 145

--- Page 146 ---
Course Code/Title: CS3401/Theory of Computation Type 2 Context free languages The context free languages are the languages which can be represented by Context Free Grammar (CFG). The production rule is of the form A -> a where A is any single non-terminal and a is any combination of terminals and non-terminals. A NFA or DFA cannot recognize strings of this language because these automata are not having "stack" to memorize. Instead of it the Push Down Automata can be used to represent these languages. Type 1 - Context sensitive languages The context sensitive grammars are used to represent context sensitive languages. The context sensitive grammar is follows the following rules - 1. The context sensitive grammar may have more than one symbol on the left hand side of their production rules. 2. The number of symbols on the left hand side must not exceed the number of symbols on the right hand side. 3. The rule of the form A -> & is not allowed unless A is a start symbol. It does not occur on the right hand side of any rule. The automaton which recognizes context sensitive languages is called linear bounded automaton. While deriving using context sensitive grammar the sentential form must always increase in length every time a production rule is applied. Thus the size of a sentential form is bounded by a length of the sentence we are deriving. Type 0 Unrestricted languages There is no restriction on the grammar rules of these type of languages. These languages can be effectively modeled by Turing machines. Context-Free Grammar (CFG) and Languages Definition : The context free grammar can be formally defined as a set denoted by G = (V, T, P, S) where V and T are set of non-terminals and terminals respectively. P is set of production rules, where each production rule is in the form of non-terminal -> non-terminals or non-terminal -> terminals S is a start symbol. For example, P = {S-> S + S S-> S * S S-> (S) 146 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 209 ---
Course Code/Title: CS3401/Theory of Computation Example: consider the grammar {s-aA/bB)a/b, A-JAa/a, B-168 eliminate useless symbols. 1 som: B->bB is a recursive grammar that substitution tos 1 resulting is endless 5 =>bB => bbB =>bbbB=> .... So eliminating the production for B->bB 6-JaA/a/b A-JAala - 2) consider the grammar {S-AB/CA, B->BC)AB. (-> aB/b, A->a}. eliminate useless symbols. Soln: Here B is useless symbol. so eleminate & production & variable S->CA C->b A-Ja 209 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN

--- Page 210 ---
Course Code/Title: CS3401/Theory of Computation Elimination of Null Production: Null productions are those productions that are the from: x->E. EX1: 5-Jable Here S-sas and 5->E are the productions 5->as doesn't contuis null production. But s-SE is an These are also called as &-production. null production. - S is said to be nullable variable. The nullable variable used in the production is removed. Ex2: Remove & production from the grammar, P={S->ABA, A-> 6, B-> &3. Soln: A, B and s are nullable variables After eliminating &, S-SABA) BAJAS/AB/A/B Fx3: p={5->as/AB, A->&,B->6} 5-Sas/AB/A/B Soln: 210 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY LEN nirf 175" Rank

--- Page 189 ---
Course Code/Title: CS3401/Theory of Computation 1 1. 1) construct a PDA for Sani'm amtn? 4) construct a PDA for fam bmc?] 2) convert the grammar 5-> asb/A, A-> bsa/s/s do a PDA that accept the same language by empty Stack. check whether the string arbabb is accept or not . 0. convot the grammar 5-> asblaAb, A->bAalba check whether the string abbaab is accept or not. PDA- 2) convert the grammar 5-Jos1/ A, A-> LAO/s )& into PDA that accepts the same language 34 empty stack. check whether 0101 belongs to N(H). 90 %, a b .202 9. 20 3) let p= [2P.23, 50,1}, {x, 20,2 2,9,200) where 5 is given by S(2,1, 20)= {(q,x201} 2[q, 1, x) ={(2,xx)} 2(2,0,x)= {pix)} 2/9, E, x)= 19,4)} 2(Pットッメノニュアッシア 5 (P, 0, 20)=2 (1. 2013. CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY HIu LEN 175" Rank PS/ 90,3.20 )=EL90,22003 {[90,1,20)={(10->>3 S[90, 1,20)={(10, 22)} 2(90,b, 2)={(90,22)3. 2(20,9,2)= 3.(21,2)} 2(a, b, 2)={(9,2)} /2/2/19, 20) =2(20,20] 189

--- Page 190 ---
Course Code/Title: CS3401/Theory of Computation Equivalence of acceptance of PDA from final state to empty stack: Theorem: If A = [Q, E, 5, 2, 90, 20, F) accepts L by final State, we can find PDAB, accepting L by emily stuck (6) L=L(A)=N(B) Proof: B is constructed from A in such a way that 4, 20/2020 90 P PF 6/any/ 4 finite automata for final state acceptance B is as follows: B = (QU & POP}, {, 5 0520'}, {B, PO, 20', 0 ) SB is defined by R1, R2, h3 & R4 as: R1: SB (Po, &, 20') = (10,2020') R2: SB[P, 4,2) = (8)}) R3: SB (9, a,z)= 5/2, 9,2) ·R4: SB (P, 2, 2) = 5/2, 1, 2) we have to show that L (A)= N(B). Then (90, w, 20) HA (9, 1, 2) ->. 1 190 CHENNAI INGSTOUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY HIu LEN 175" Rank

--- Page 229 ---
Course Code/Title: CS3401/Theory of Computation .1) computable languages 1) Design a Turing Machine that accepts all strings. of the form a" b" for n21 and reject all other strings. Soln: M =(Q,E, F, S, 90, B, F) Q = 190, 91, 92, 93, 94) 1 1 E = {a, b] c 5 = {a, b, B, X,y] 1 1 90 = 9903 B = 4B3 . F = 9943 5: a a (91, x, R) 9, (9, 9, 4) 92 - - (93, Y, R) 95 3 (93, a, L) 9 - (91, b,R) (93 b,L) × - (90, X,R) 4 B (94, Y, N) (92,4,L) (92, BL) - - - - ×24 $ 9 9 9 Transition diagram 4/4, N x/XP -> 950/X.R 4/4,2 B/B,L b/ 4,R 92 9: a/a,R b/ b.R a/a, L blair CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank 23

--- Page 230 ---
Course Code/Title: CS3401/Theory of Computation 3 2) Design TM That recognizes strings of the form Lan bn cn \ n213 over = = {0}}. Soln: n= 2 a abb cc aabbCCBfxabbccB/-xabBccB J-xayb ccB ) 1 90 9, 2, 1 92 xayb (CB)-XaybCCBY-xayb&CBLxayb2CBt 92 1 93 12 93 xayb2CB + xayb2CB-Xayb2CB-xx462cBr 1 93 1 95 1 70 9, XxY 62LB X XX442 CB ) XXYYZ CB XX Y Z B 2, 92 72 95 Xx+422BY XxYY22B+XXYY22B/XXYY22B+ 1 93 Kč 93 xx4722B /XXYY22BYXX4422 B-XXYY22B1-XX4422 B 24 , 14 1 1 1 14 14 xx4422B / XxYY22 B1=>Halt 95 a a. 9 C x 4 2 B 90 (2,X,R) 9, (2,4,R) - (92,4,R) - (a, b,R) - [24, x,R) (1, 4, R) - - - 1 - - 92 - - (93, 2, 2) - - (92,2, 2) - 93 (93, 9, L) (93,b, c) - (90, x, R) [a3,4, 2) (a 3, 2, R) - 94 - - - (4x 4, 4R) (44, 4,R) (9 4, 2, H) (25, RN) 95. Q à $ 4 4 9 1 9 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank 24

--- Page 105 ---
Course Code/Title: CS3401/Theory of Computation r2=(0+1)* ľ3 = 01 e 0 E E 1 C Fig. 2.3.6 0 912 1 Fig. 2.3.7 Now we will design the final NFA for r. (See Fig. 2.3.8 on next page) 0 Fig. 2.3.8 1 0 90 913 1 Start 1 9g (I+00) (1+0) = 97 Example 2.3.2 Construct a NFA equivalent to (0 + 1)* (00 + 11). AU May-04, Marks 8 Solution: Consider r.e. = (0+ 1)* (00 + 11) rı=(0+1) r2=(00+11) The NFA for r1 can be drawn as follows. 105 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 106 ---
Course Code/Title: CS3401/Theory of Computation 0 92 1 94 € Fig. 2.3.9 The NFA for r2 can be 0 0 E 9+2 1 1 E 914 Fig. 2.3.10 The NFA for the given regular expression can be (See Fig. 2.3.11 on next page). Example 2.3.3 Construct NFA with epsilon for the RE = (a|b)* ab and convert into DFA and further find the minimized DFA. AU May-17, Marks 16 Solution: The NFA with & can be constructed as follows e a b b O 0 0 Fig. 2.3.11 1 412 Start The above NFA can be converted to DFA as follows - 106 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY pou LEN 175" Rank

--- Page 99 ---
Course Code/Title: CS3401/Theory of Computation The (a + b)* means any combination with a and b even a null string. Example 2.1.2 Write r.e. to denote a language L which accepts all the strings which begin or end with either 00 or 11. Solution: The r.e. can be categorized into two subparts. R = L1 + L2 L1 = The strings which begin with 00 or 11. L2 = The strings which end with 00 or 11. Let us find out L1 and L2. L1 = (0011) (any number of O's and 1's) L1=(0011) (0+1)* Similarly, L2 = (any number of 0's and 1's) (00 + 11) =(0+1)* (00 + 11) Hence R= [(00+11) (0+1)*] + [(0+1)* (00+11)] Example 2.1.3 Write the r.e. to denote the language L over 2 = {a, b} such that all the strings do not contain the substring "ab". Solution: The L = {8, a, b, bb, aa, ba, baa ... ) The r.e. = (b* a*) In this regular expression if we substitute a* = & we get all combination of b's and similarly if we substitute b* = & then we will get all combinations of a's. We have strictly maintained a condition for not to have ab as substring by giving the regular expression as b* a *. Example 2.1.4 What is regular expression? Write a regular expression for set of strings that consists of alternating 0's and 1's. AU May-16, Marks 8 Solution: Regular expression - Refer section 2.1. Regular Expression Let 2 be an alphabet which is used to denote the input set. The regular expression over 2 can be defined as follows. 1. ¢ is a regular expression which denotes the empty set. 2. ¿ is a regular expression and denotes the set {a} and it is a null string. 3. For each 'a' in 2 'a' is a regular expression and denotes the set {a}. 4. If r and s are regular expressions denoting the languages L1 and L2 respectively, then r+s is equivalent to Li U L2 i.e. union. 99 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE & TECHNOLOGY nirf 175" Rank -- LEN

--- Page 100 ---
Course Code/Title: CS3401/Theory of Computation rs is equivalent to L1 L2 i.e. concatenation r* is equivalent to L1* i.e. closure. The r* is known as kleen closure or closure which indicates occurrence of r for oo number of times. For example if 2 = {a} and we have regular expression R = a*, then R is a set denoted by R= {8, a, aa, aaa, aaaa, ... } r.e. = (0+ 1+ 0+ 1+)* Example 2.1.5 Write regular expression to describe following languages: i) {w = {0, 1}*}: w corresponds to binary encoding, without leading 0's, of natural numbers that are evenly divisible by 4} ii) {w = {0, 1}}* : w corresponds to binary encoding, without leading 0's of natural numbers that are powers of 4} iii) {w = {0-9}* : w corresponds to the decimal encoding, without leading 0's of an odd natural number}. Solution: i) The valid strings for given language are (100, 1000, 1100, 11100, ... ) For ex: 100 = 4 Hence regular expression will be r.e. = (1(0+1)* 00) + 0 ii) The vaild strings for given language are (100, 10000, 1000000, ... } Since (i) 100 in binary = 1×22 + 0×21 + 0×2º = 4 + 0 + 0 = 4 in decimal (ii) 10000 in binary = 1×24 + 0×23 + 0×22 + 0×21 + 0×2º = 16 in decimal Thus we want 4, 16, 64 ... in binary representation as valid string. Hence the regular expression is r.e. = 1(00)* iii) The odd natural numbers are those numbers that end with either 1, 3, 5 or 9. Hence regular expression using the set (0-9)* will be as follows r.e. = (¿ +((1-9)(0-9)*))(1+3+5+7+9) Example 2.1.6 Write regular expressions for the following languages over the alphabet 2 = (a, b) i) All strings that do not end with 'aa'. ii) All strings that contain an even number of 'b's. iii) All strings which do not contain the substring 'ba'. Solution: i) The language contains all the strings that do not end with aa means it may end with bb, ba or ab. Hence r. e. will be 100 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY nirf 175" Rank -- LEN

--- Page 223 ---
Course Code/Title: CS3401/Theory of Computation 6. Intersection: Theorem: L1 and L2 are context-free does not closed under intersection. i.e., Lin L2 is not possible for context-free languages. Example: The languages L1 = {0m]"0"/m, n > 0} and L2 = {0m1"0"/m,n > 0}, are both context-free languages. Then Lin L2 is not possible. Because L1 requires m number of l's and L2 requires number of 1's. The non-context-free intersection L1 n L2 = {0m1m+"0"/m, n > 0} is not possible. Theorem: The class of context-free languages is closed under intersection with regular languages, that is, for every context-free language L and regular language R, the language Ln R is context-free. Proof: Let the language L be context-free language and the language R be regular language, Finite Automata Input AND Accept/ Reject Pushdown Automata Stack To run the two automaton "in parallel" and result in another PDA. Let P = (Qp, E, Γ, δp, qp,Zo, Fp) be a PDA that accepts L by final state. Let A = (QA, E, δA, qA, FA) be a DFA for regular language L. Construct a new PDA, intersection of P and A P' = (Qp X QA, E,Γ, δ, {qp,qA),Zo,Fp xFA) where ô((p, q), a, x) is the set of all pairs ((r, s), y), such that i. r = ô^(p, a) in Finite automata ii. (r, y) = §^ (q, a X) in Pushdown automata CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE & TECHNOLOGY nirf 175" Rank LEN 17

--- Page 224 ---
Course Code/Title: CS3401/Theory of Computation Each move of PDA P and FA A, then there should be corresponding move in PDA P'. In finite automata i. If a is any value, then o^(p, a) = "(p) for some states. ii. ii. If a = & then, then "(p. a) , A does not change the state. In PDA (qp, w, Zo) +* (q, E, y) If and only if ((qp, qA) w, Zo) IF* ((q, p)E, y) where (q, p) is an accepting state of P', if and only if q is an accepting state of FA A and P is an accepting state of PDA P. To conclude that, P' accepts w if and only if both P and A accepts w, where w is in L NR Difference: Let the language L be context-free and the language R be regular. Then the language L-R is context-free. L-R=LnR Complement: Let L context-free does not imply that L is context-free LUR =LnR LnR =LUR Inverse Homomorphism: L is any language, h is homomorphism, then h-1 (L) is the set of strings w such that h(w) is in L. To construct a PDA to accept the inverse homomorphism of given PDA accepts. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN 18

--- Page 227 ---
Course Code/Title: CS3401/Theory of Computation Instantaneous Description (ID) too TM: Instantaneous description for TH is the. snapshot of how the input string is processed by the Turing Machine It describes, * The input string * Position of the head * State of the Machine Ex: L = Lanon/ n=1] Instantaneous Description: n=2 BaabbB 1 90 BxabbB 2, Bxa|b|b|B 9, BxabbB 1 2, . BxabbB 1 2(a, B) =(92, B. L) 1 2, BrabbB F 92 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY s(90, a)= (91, x, R) 2(a .; a) =[2, 9, R) 2 (a, b) = (21, b, x) 2(qi,b)=[q,,b, R) 2(92, b) = (93, Y, R) N39 nirf 175" Rank -- 21

--- Page 228 ---
Course Code/Title: CS3401/Theory of Computation 1. BxabyB 1 93 B|x|a|b\\\B 73 BXabyB 1 93 BXabyB 1 90 BXXBYB 9, BX|X|b|4|1 1 2 [11, Y) =(9g, 4, 2) 1 B.Xx/3/4/B 92 BXXYY|B 93 BXXYYB 1 90 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE & TECHNOLOGY S(93, b)=(93, b,L) S(93, 41)=(93, 9,2) S(93, x)=(90, x, R) {[90,a)= (9, x, R) 2(a, b) = (a, b,R) 2192, b) =(93, y, L) S(93, x)= (90, x,R) S(90,y)=(94, Y, R) LEN nirf 175" Rank 22

--- Page 245 ---
XL (M) = { if M does not accept w 3 * [ (M) = L it M accept w The Turing Machine M' is a two tape Turing Machine. 1) One Tape is used to simulates M on w 2) The other Tape of M' is used to simulates M2 on the input x to M'. The Turing Machine M' is constructed to perform following, -> Simulate M on input w. The String 'w' is not the input to M', rather M' writes M and i on to one of its tupe and simulates the universal turing Machine'U' on that fair -> It M does not accept w, Then M' does not perform anything. M' never accept its own ilp x. -> It M accept w, then M' accept its own [ 18 x. This algorithm is a reduction of Lu to LP, and proves that the property P is undecidable and unsolvable. 4) Post's correspondence Problem (PCP): The undecidability of strings is determined with help of post's correspondence problem (PCP). Let us define the PCP. The Post's correspondance problem consert of two lists of strings that use of equal length over the input s. 39 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 246 ---
The two hit are. A = w2, 43, us, ---- , and B = x,X2, x3, ... x, then there exists a non empty set of i, soils :. In such that- w1, 0%; w3 .... ,un=x1,13,93,- orgas to find we = " then we say that per has a Solution Example problem: -- 1) let = = 40,1} and let , and is be the lists defined as follows, = List B wi xi 8 2 10 3 Find the instance of pc Solution: w1=1: x1 = 111 W2 =1011 2 12=10 . Les W3 = 10 4 x3=0. Now food instance of PCP : --- (w , wa; w," ... . w. = x1, 12, x3 let us take m= 4 and take combinationg 2,1,1, 3 · xn Wy w,w, w3=xxxix,03 = tonmno Instance IL na CHENNAI INGSTOUTE . CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank N33 40

--- Page 211 ---
Course Code/Title: CS3401/Theory of Computation Elimination of unit production: The unit productions are those productions of the form x->4 where xxy are variable of the grammar. Elimination * Select the unit production x-Sy * Add the production x->d to the grammar since x->y and y->w Remove the whit production, x-by from the grammar. Example: Eliminate the unit production from the grammar, 1) P=45-JABA/BA/AA/AB/A/B, A-SOAJO, B-1B/14 Soln: 5->ABA/ BA/AA/AB/0A/0/1B/1 A-30110 B->1B/1 2) P={E->F+TIT, T-> TXFIF, F-> (E)], [->a/b/Ia/ Ibl 2015 soln: E->ETT|TXF|CE)|a|b|so)Ib\SolI, F-> (E)/a/b/Ja(Ib)JOLI, I-> a/b/Ja/IblJo II, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nif 175" Rank -- 5

--- Page 212 ---
Course Code/Title: CS3401/Theory of Computation 2) simply The following grammar 5->ASB) E A-JaAsla B -> Sbs|A|bb Soln: 1) Elimination of Null Production 5->ASB/AB A-JaAS Ja JaA B-> Sbs/sb/ b5/6/A/b ii) Elimination of unit production The unit production in the above A B-SA Final Production grammar > 5-JASB/AB A-SaAS/a/aA B->sbs/sb/b5)blaAs/a/aA/b , , There is no useless symbols. -x- 3) Simply the following grammar 3 S->aAa/ bBb BB , A-SC B -> SIA c-> Sla > soln: , 1) Elimination of null production ( -> 4 , A -> 6, 8->4. 5->ana/aa/bBb/bb\BB) B A-SC , B ->A (-> S CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN 6

--- Page 277 ---
construction of H1: M. Halting Machine: Hil w SHALT SLOOP FOREVER Consider, a string describing M and 1/3 string, W for M. * Let H1 generates "halt" if H, determines that the turing machine, M stops after accepting the 1/9, 10. otherwise H, loops forever when, M doesn't Stop on processing wo. construction of H2: M Halting Machine H2 1 HALT Loop * H2 is constructed with both the inputs being M. * H2 determines M and halts if M halts otherwise Loops forever. Construction of H3: M H2 HALT- Loops H3. LOOP HALT * Let H3 be constructed from the outputs of H2 * If the olp of H2 are HALT then H3, loops forever ELSE if the olp of Ha is loop forever, then H3 halts. Thus H3 acts contractor to that of H2. This halting Problem is undecidable .... CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank 71

--- Page 215 ---
Course Code/Title: CS3401/Theory of Computation A->Caca cb => A-Scal2 B->CrcDA => (1->CbA (2-> cacb B-> Cbc1 B->ChCb casa cb-Jb final CNF grammar: 5-> AB/ Cbc1/cbcb/caB A->Caco B-> Cbc1/ChCb 91-> cbA (2-> cacb cu-Sa cb-sb CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank 9

--- Page 216 ---
Course Code/Title: CS3401/Theory of Computation Greibach Normal Form [GNF] A context Free grammar is in GNF it all productions Definition have the form A->ad ; A-Ja Where. A is the variable, a is the terminal and 2 is any number of non terminals. In GNF, there is no restrictions on the length of right side of the production. Example: 1) Find GNF for the following grammar. 5-> AB; A->BS|b; B->SA/a soln: To write the above grammar G is to GNF. follow the following steps: Step1: check the given grammar of whether it is in CNF, It is already in CNF. Step 2: Replace the variables S=A1; A=A2; B= A3 A1 -> A2 A3 A2-S AzAILb A3 -> A, Aala In the First production) <2, then there is no change. In the second In the fwird production 2-3. production 3>1, A3-> A1 A2 /a CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank N39 10

--- Page 231 ---
Course Code/Title: CS3401/Theory of Computation 4/4,R. a/a, R 212,2 b) bik 90 a/ X,R 9, 3/4/2 92 > XIX,R 4/4/R C/2,R 93 > 94 B ) BIN 98) ala,L b/bic 1/4,2 212,2 4/4/1 212,R 3) construct a TM to make a copy of a string over = = {0,13. 3 4) Design a TH which recognizes palindrome over == [a,b3." Lafwwx) we a.bjx? 5) Design a "TH that accept The language L={anmich | no ?. I Computable Functions: It is capable of perform any sort of computations such as, * Addition * 1'3 complementation. * subtraction * a's complementation * Multiplication * squaring number * Division comparing two numbers Function: IM with + ₺ 1 Hw) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank -- 25

--- Page 232 ---
Course Code/Title: CS3401/Theory of Computation 2) construct & TM to compute the function +: N- N. such that flaj= x+1. soln: x == = > 2+3=3 DOBROOB LOOOB 1 90 9, 40 5: C B 90 (20,0,R) [2, 0, N) 91 - - M = (Q, E. T. S. %, B.F) $ 20 1 1 00 R { 00 # OB Q = {0 1.}, { = 103_ R ={0.1| |82 10=403 F 1.3 Transition Diagram 018:RO & BIO,N, >90 3) Design a IM to compute proper subtraction (w. m-n for man o for men 4. Design a two to compute proper Multiplication () mxn : +lm, ns = mxn. 2010 5. Design a TM to compute the function fixes = 2x CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY pou LEN 175" Rank --- 26

--- Page 251 ---
where A and B are the list generated by (For. Now derivations pare,. 5- A ->W1 W12 5-> B -> xx, xx2 xim qim ... 92, The solution is Wim adm . dir. we, wiswimxx, x12xim This implies that GAB is ambiguous. COMPUTABLE FUNCTIONS : Primitive Recursive Functions: Basic Definitions ·· Partial Function : × f : Y . X1€ 14, x2 < p x3K 14 partial function, + from & toy is a function that assigns every elements of X to at most one element of y. Ex: f(min) = man is a partial function [since m-) generate +ves-ve 2. Total Function: x 8: y *3K M2. 45 CHENNAI INGSTOUTE . CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY pou LEN 175" Rank

--- Page 252 ---
Total function, & from x toy is defined as the function that assigns every element of x to unique element of y 2 Ex: +(m, n) = m+n [generates only five values] 3. Initial Function: The initial function include i) constant Function ii) successor Function iii) projection function i) constant function: A function of the form, Cc: NK->N for K20 sazo is called constant General Form: Function Example: Ca (x) = a for XENK ( (5) = 0 -5 2090 function [ (4) = 1 -> unet function C (10) = 10 - constant Function ii) Successor Function [S]: A function, &LDIN+ 3: N-IN defined bys(x)= x+1 is said be successor function. Example: S(4) =5 5(1) =2 S[20] =2), ... 46 CHENNAI INGSTOUTE . CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 165 ---
Course Code/Title: CS3401/Theory of Computation 4 1) Construct a CFG for the language, 2={weWR/WE labsx] Soln: The possible strings generated from I can be 2 c, aca, bcb, aacaq, bbcbb, abc ba, bacab .... } : Production rules are defined to be, 5-SC S-saba 5->bsb Thus the grammar, G=(V.T. P.S) where. V =153 T = > a, b,c} P={5->asa/h5blc} 5 = {53 2) construct a CFG - -x- dos. L={an bnIn20} Soln: The possible productions are {{, ab, aabb, a 424, ...? productions are 5-36 s-sabb Thus the CFG, G=(V,T, P.S) is given by v = {5} T={9,63 p = {5-> asbl&3 3 = {5} 165 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 166 ---
Course Code/Title: CS3401/Theory of Computation Derivations and languages: Derivations: Derivations are the best of strings that we derived from the start symbol, after applying the production rules, a finite number of times. S=> W/WET* Representation of derivations: Derivations are represented two forms: * Sentential Form * Parse free Form in either of the Types of Derivations: There are two types of derivations * Leftmost derivation [LMD] Rightmost derivation [RND] 1. Sentential Form: sentential form is the derivation, derived from the start symbol, by applying the rules. let G=(V,T, P.S) be a CFGy then 5 => a 0 0 sentential form, where at (TUV)+ 166 CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE & TECHNOLOGY nirf 175" Rank LEN

--- Page 225 ---
Course Code/Title: CS3401/Theory of Computation Input Buffer Pustoown Automata Sizol After reading a, applying homomorphism h(a) is placed in a buffer. The symbols d h(a) are used one at a time. When the buffer is empty. constructed PDA read mode d' its input symbols and apply the homomorphism to it. Theorem: Let L be a CFL and h be a homomorphism, then h-1(L) is & CFL. Proof: Construct a PDA P = (Q. E. Γ. δ. qo,Zo. F) that accepts L by tinal state. his homomorphism applies to symbols of alphabet E and produces string s in To L is a language over alphabet T. We construct a new PDA. where Ρ' = (Ο'. Σ.Γ.δ. (Δρ.€), Ζο. F x {ελ) Q' is the set of pairs (q. x) such that. i. q is a state in Q ii. x is a suffix of some string h(a) for input symbol a in E. First component of the stack of P' is the state of P. and the second component is the buffer. §' is defined by i. 6'(q. s), a, x) = {(q. (h(a)). X)} where a in Y. q in Q and X in l'. ii. 6(q. b. x) = (p. )) where bis in Torb = g. then 8'(q. bx), c, X) = ((P. x), >) P' starts in the start state of P with an empty buffer. Accepting states of P. is 30 accepting state of P. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank 19

--- Page 226 ---
Course Code/Title: CS3401/Theory of Computation 1 wurg Machine. A turing Machine is an automatic machine that manipulates the input Strings according to the transition rule. Model of Turing Machine: B a,. a2 ... |b1 / b2 B -5 Infinite i)p tupe Read/write head inputs 1 Empty cell Fuite control Definition of TM: A Turing Machine, M is a 7 tuple given by M=(Q, E, F, S, 90, B, F) where, Q - Finte set of states, E-Finite set of inputs I - Finite set of tape symbols [EUB] S- Transition function given by. a(2, a)= (9', b, M) [M-> movement- Left, Right, 90 - Initial State no movement] B- Blank symbol F - set of final states CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY N39 nirf 175" Rank 20

--- Page 267 ---
0 6 1 c 1 + e , 0 2 8 i 1 1 a 0 6 1 2 1 0 f 2 0 3 0 9 , 0 2 0 i No class problem: problems that can be stored in non-delermint 1 Polynominal time is called as NP-class problems These types of NP problems are known as instructables meine alicante Dproblems Example: Towers of Hanoi, Traveling Salesman problem, Graph colouring problem, Hamiltonian circuit problem satisstability problem , etc . Prople na caixa Example of NO class problem. 6 Traveling salesman's problem (isp) \ -> Given a set of cities and cost to travel blw each Part of cities, determine whether there is a Ruth That visit every city once and returns to the 61 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 268 ---
the first city. Such that the cost travelledis less 2 3 1 c 8 5 6 2 1 4 1 3 b 4 7 9 . The tour path will be : a- b-d-e-c- a The total cost of town will be : 16 NP-complete problem: A Problem is said to be NP-complete it it belongs to NO class problem, and can be solved in polynominal time. IfThey are also called polynominal- time posso reducible problem. NP- Hard Problem: medlong dienste is said to be NP hard it theo -> A Problem" exists an algorithm for solving it and it can be translated into one for solving another NP problem paulovert- - 1) The problem is an N.Piclairs problem 2) For any other problem, P2 in NP, there is a polynomi time reduction of L2 to LI -> Every "NP complete problem must be NP-hoord problem. 62 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY LEN nirf 175" Rank

--- Page 213 ---
Course Code/Title: CS3401/Theory of Computation Normal Forms For CFby: Let the grammar G=(V,T, P.S) be a context free grammar. If the production rules in or satisfy some restrictions, then they are said to be in normal form. There are two types of normal forms: * chomsky Normal Form [CNF] * Grekbach Normal form [GNF] Chomsky Normal Form [CNF]; A context free grammar, G=(V, T,P.S) is said to be in CNF if each production in G1, is of the form x->Y2; x->2, where x,Y, Z EV and LET. Algorithm to convert a CFG to CNF: 1. Eliminate the useless symbols, unit and null productions from the grammar. 2. Each variable should be having a derivation of length 2 or more, having only variable as of the form, A-> x Where |2|>2, & EV. 3. The production have three more than three variables as derivation, must be broken down into a cascade of productions with derivations containing at most 2 variables. -*- Ex: convert the grammar 5->AB/aB, A-> aaB/c, B-sbbA in to CNF. CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank 7

--- Page 214 ---
Course Code/Title: CS3401/Theory of Computation 14 Soln * Elimination of &-production: 5-> AB) B) aB A-> aa B B-> bbA) bb * Elimination of unit production 5->AB/bbA/bblaB A-> aa B B->bbA/bb Theo is no useless symbol in the x conversion to CNF grammar. 1) Adding production to terminals Casa cb-sb 2) Rewriting the grammar 5-SAB/COCHA/cbcb/CaB A -> Cala cb B->CbcbA/cbcb conversion the grammar to CNF gives 5-SAB 5->AbcbA => 5->cbc, 61-> CbA 8+746 400 SEESCRIB 5-5 cbch S-ScaB CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank 8

--- Page 111 ---
Course Code/Title: CS3401/Theory of Computation Step 4 : 0 (00+11) 4 910 + Step 5 : We will remove e - transitions, 0,1 0,1 00+11 Step 6 : Step 7 : 0,1 0,1 0,1 0 0 0.1 00 1 11 is the required FA. Example 2.3.7 Construct Finite Automata equivalent to the regular expression (ab +a) *. AU: Dec .- 15, Marks 6 Solution: First of all we will construct NFA for given regular expression. a b C The NFA without & is b a Example 2.3.8 Draw a non-deterministic automata to accept string containing the substring 0101. AU May- 16, Marks 2 111 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 112 ---
Course Code/Title: CS3401/Theory of Computation Solution: The regular expression is (0+1)* 0101 (0+1)* The non-deterministric finite automata is 0.1 0.1 0 1 0 1 Example 2.3.9 Construct a finite automata for the regular expression 10 + (0+11)0*1 AU Dec .- 19, Marks 6 Solution : 112 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N39

--- Page 201 ---
Course Code/Title: CS3401/Theory of Computation TIENE OF ULOT by M. 2(2, 010000, 5) - (9, 010000, OB) + 1.9, 10000, BB) + (9, 10.000, ISB) + (9,0000, SB) + (2,0000, OBB) + [9,000, BBB) + (2,000, OBB) 119, 00-BB) 1- [9.00, OB) + (9,0, 8) + (2, 0, 0) + (9, E) .: The string 010" is accepted by MEL. 201 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY LEN nirf 175" Rank

--- Page 202 ---
Course Code/Title: CS3401/Theory of Computation From PDA'S to CFG'S: Theorem: If 2 is N(M) for some PDA M, then L is CFL= Proof: 1. It has single final state of ift the stack is empty. 2. All transitions must have the form. S[ai, a, A) = {c1, c2) \ cni}, where S(9), a,A)=(9;, &) -- >1) S[ai, a, A) = (9), BC) - - >@ Given M=(Q,E, T, S, 90, 20,.19)} Statistics the condition 1 & 2 G=[V,T,P.S) T = E V- elements of the form. [9, A, P], qSp in Q and A is T 5- start symbol 5 -> [90, 20, 9] for each q in Q P consist of : u, AV { {" A, XETX, 91,9; E.Q. (2), uv, Ax)+=(2+, v,x) implas (9), u,A)->u consider (9), A, 9x)->a(9;, B,9,)(4, 2, 9x) the corresponding transition for PDA is s[9], a, A)={(9;,BC) ... ] similarly if (9), A,q;)->a. Then the transition is [4], a, A)= : The conclusion is (0, 0, 20) }(9, &, 4) is true if(90,20,9+)=>w consequently L(M) =1(or) 202 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY pou LEN 175" Rank --

--- Page 249 ---
Exemple: 5 1). let us convert the Turing Machine, M=[191,929, 20,13, {0.1.83, S. 2., B, 19.3). where & is 2[21. 01 2 (1:1) 2, (2, 1,2) (22, 0,L) 92 (93, 0,2 ) 93 - (21, 0, R) - -... (92, 1, 1) (92, 0, R); And the input string w=01. Find solution. solo: $ # 0-# 1 * list B # 2:01 # source ·W= 01 q1= Initial state 0 Tupe Symbols and the separator 1 # # s appenabal. 210 .192 09, 1 2200 191 2210 02,# 9,01# 92 11# 09,0 9300 1 920 9310 221 02, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank 2(a);0) =(92), x) a[2,1) = (92,0, 2) S(a, B) =(2, 1,2) 3(920)=(93, 0,2) $ ( 2 , 1 ) = (9 , 0, R ) 43

--- Page 250 ---
List A list B 92# 092# 0930 93 0 931 93 1930 93 1931 93 93 193 93 . 930 93 931 93 93## # . w=01 source 2 (92, B)=(92, 0,R) 93 is an Accepting State. 93 is an Accepting state. 9,01+1921 - 109, -19201 -93101//accepted 5. Unsolvable problems of CFG: Theorem: It is undecidable whether CFG is ambiguous .. proof: we have to prove that "GAB is ambiguous It and only If instance (A,B) of PCP dias a solution. It part: . there are two derivation GAB as A -> W,Aa,|waAa2) -.. |wkak/w1a1) ... /wkau B -> X, Ba, \x, B42) ... |xxBak|x1 a1) ... |xxx CHENNAI INGSTOUTE CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank 44

--- Page 241 ---
UNSOLVABLE PROBLEMS AND COMPUTABLE FUNCTIONS UNSOLVABLE PROBLEMS: A problem whose language is recursive is said to be decidable otherwise, the problem is undecidable. That is, a problem is undecidable if there is no algorithm that takes as insul- an instance of the problem and determines whether the answer to that instance is yes or no. Ex: can you develop an algorithm which will correctly tell us when a person will die? Whatever you want may be taken as input .. person's name and other detail Death knowing algorithm .. Date and time of death Undecidability of Death For unsolvable problem, let us see the following problem. 1. Unsolvable problem of a Non Recursive language 2. Unsolvable problem of Reduction 3. Unsolvable Problem in Rice's Theorem 4. Post correspondence problem 5. Unsolvable problems of context Free Grammes CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank 35

--- Page 242 ---
1) Unsolvable problem bois Non Recursive Language : . .. If a language is recursively enumerable then it is non Recursive. So now we have find a non recursive language that is unsolvable. Let us see the following languages 1. Empty language (Le) 2. Non Empty language (Lne) 3. Non belt Accepting language (NSA) c 4. Self Accepty language (SA) 5. 0 1. Empty language "Le": If (MI) = 0, that is M2 does not accept: any i/p, then w is in Le. Thus, Le is the language = consisting of all TH'S whose language is empty. U 5 Le = { M / L(M) = 0} 2. Non Empty language [Lne): If (Mi) +9, then w is in Line. Thus Lhe is the language for TH that accept at least one input string Lne ={M/L(M)} 3. Non self Accepting Language (NSA): The Non self Accepting Language (NSA) is defined as. NSA={WE[0,1]*/w == (T)} for some Twing Machine , and the i/p strung w /L(+) 4. Selt Accepting Language (SA): The belt Accepting language (SA) is defined as 1 SA={WE20,13*/W === (+)} for some Turing Machine T, and the ilp String WEL(T) CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank 36

--- Page 181 ---
Course Code/Title: CS3401/Theory of Computation 3 Leftmost derivation of a tree yields string Theorem Let G=(V, T, P, S) be a CFG. Suppose there is a parse tree with root labeled by A | A E V and with yield a where a € Tº. Then there is a leftmost derivation Ama in G. Proof The theorem is proved by mathematical induction on the height of the parse tree. Basis of induction Let the height of the tree be 1, the tree looks like, Height of the . Ạ tree= 1 Here the root is labeled A, AEV and the children are the leaf nodes cz, o2 ...... which are read from the left to right to generatea. Thus Ama is proved. Inductive step ! We assume the height of the tree as'n | n>1, then Ama stands. We need to prove the same for n = n+1. There are two issues: The child of A can be terminal / variable as given A X1 X2 1 - X, 181 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 182 ---
Course Code/Title: CS3401/Theory of Computation . 1. If A; € T, then Xi => a; [0] => terminal] 2. If A; E V, then there must be some leftmost derivation, A =a, For n nodes, a = a, a, a, .... A=A,A,A, ... A. (for i=1,2,3, ..... ) If Ar is a terminal, then If A¡ is a variable, then and A=>008, A, A. .... A. A, A. A, a, a, B, A, . Thus, Thus the theorem has been proved. 2.8 SIMPLIFICATION OF CFG . The need for simplifying CFG is to make it easier to analyze and prove facts about CFL. 182 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 131 ---
Course Code/Title: CS3401/Theory of Computation Closure Properties of Regular Languages AU: May-06,10,13,14, Dec .- 07.08,10,12,13,17, Marks 8 If certain languages are regular and language L is formed from them by certain operations (such as union or concatenation) then L is also regular. These properties are called closure properties of regular languages. Such languages represent the class of regular languages which is closed under the certain specific operations. The closure properties express the idea that when one or many languages are regular then certain related languages are also regular. The closure properties of regular languages are as given below. 1. The union of two regular languages is regular. 2. The intersection of two regular languages is regular. 3. The complement of a regular languages is regular. 4. The difference of two regular languages is regular. 5. The reversal of a regular languages is regular. 6. The closure operation on a regular language is regular. 7. The concatenation of regular language is regular. 8. A homomorphism of regular languages is regular. 9. The inverse homomorphism of regular language is regular. Theorem 1: If L1 and L2 are two languages then L1 U L2 is regular. Proof: If L1 and L2 are regular then they have regular expression L1 = L (R1) and L2 = L (R2) Then Li UL2 = L (R1+ R2) thus we get Li U L2 as regular language. (any language given by some regular expression is regular). Theorem 2: The complement of regular language is regular. AU May-14, Marks 6, Dec .- 08, Marks 2 Proof : Consider L1 be regular language which is accepted by a DFA M = (Q, 2, 8, qo, F) The complement of regular language is Li which is accepted by M'= (Q, 2, 8, qo, Q - F) That means M is a DFA with final states E F and M' is a DFA in which all the non-final states of M become final. In other words, we can say that the strings that are accepted by M are rejected by M' similarly, the strings rejected by M are accepted by M'. Thus as Li is accepted by DFA M', it is regular. Theorem 3: If L1 and L2 are two regular languages then Lin L2 is regular. AU: Dec .- 07, 08, Marks 4 Proof: Consider that languages Li is regular. That means there exists some DFA M1 that accepts L1. We can write M = (Q1, Σ, δι, qi, F1) Similarly being L2 regular there is another DFA M2 (Q2, E, 82, q2, F2) Let, L be the language obtained from L1 n L2. We can the simulate M = (Q, 2, 8, q, F) where Q = Q10 Q2 8 = 81 0 82 a mapping function derived from both the DFAS. q E Q which is initial state of machine M. 131 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY nirf 175" Rank LEN

--- Page 132 ---
Course Code/Title: CS3401/Theory of Computation F = F1 n F2, the set of final states, which is common for M1 and M2 both. It is clear that there exists some DFA which accepts L1 / L2 i.e. L. Hence L is a regular language. This proves that if L1 and L2 are two regular languages then Li is regular. In other words the regular language is closed under intersection. Theorem 4: If L1 and L2 are two regular languages then L1 - L2 is regular. Proof: The L1 - L2 can also be denoted as L1 U Ļ2 Consider Li be regular language which is accepted by DFA M =(Q,2,8,qo,F) The complement of regular language Li is Li which is accepted by M'=(Q,2,8,qo,Q-F) That means M is a DFA with final state's set F and M' is a DFA in which all the non final states of M become final states and all the final states of M become non-final states. Thus L1 and L2 are two regular languages. That also means: these languages are accepted by regular expressions. If L1 = L(R1) and [2 = L(R`2). Then Li UL2 = L (R1 + R'2). This ultimately shows that L1 U L2 is regular. In other words L1 - L2 is regular. Thus regular languages are closed under difference. Theorem 5: The reversal of a regular languages is regular. AU: Dec .- 07, 08, Marks 4 Proof: Reversal of a string means obtaining a string which is written from backward that is w is denoted as reversal of string w. That means L (wR) = (L(w))R. This proof can be done with basis of induction. Basis: If w= & or ø then wR is also & or ¢ i.e. (¿)R = ¿ and (¢)R = ¢ Hence L(wR) is also regular. Induction : Case 1: If w = w1+ W2 then W = (W1)R + (w)R As the regular language is closed under union. Then w is also regular. Case 2: If w = w1 + W2 Consider w1 = (ab, bb) and W2 = (bbba, aa) The wR1 = (ba, aa) wR2 = (aaab, bb) then w - wR1 wR2 (ba, aa, aaab, bb) is also regular. Thus given language is regular one. Theorem 6: The closure operation on a regular language is regular. Proof: If language Li is regular then it can be expressed as L1 = L(R1) Thus for a closure operation a language can be expressed as a language of regular expressions. Hence Li is said to be a regular language. Theorem 7: If L1 and L2 are two languages then L1 L2 is regular. In other words regular languages are closed under concatenation. 132 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 247 ---
2) let 2= 40,13 and ABB be the list as 4. liest A list B 1 wi xi 1 10 (0) 2 011 1) 3 101 011 Find the instance of : Pcp Modified PCP: An intermediate version of PCP is modified PCP (NPCP), there is the additional requirement on solution that the first pair on the A and B list must be the first pair in the solution. An instance of MPCP is. two lists; A= w,wa, ... uk B = x1 x2, .... xx And -solution is a list of 0 or more integer 11, 12 .... I'm such that W, W2, W12 ··· wim=x,xx,xx2, ··· Xim Example problem: consider the following list A & B and find instance List A list B W1 xi 1 10 10 2 110 3 " 01 1 10 110 0. 0 11 Som: Now the sequence taken is 2,3 13,2 50 W1 W2 W3 =x1 ×2 ×3 1011011= 104011 Instance of MPCP = 2,3 41 CHENNAY CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY N39 nirf 175" Rank

--- Page 248 ---
Completion of the proof of PCD undecidability: Rules: 1) The First Pour is List A List B # # now# .(2) Tape Symbols and separator # can be appended - - List A list B X X #1 X 3) To simulate a move of M list A. ilost B 2 x, 302 nypas M 22x Pz4 i YP#xx 2# 29# Pz4#= It &(a, B) = (P.4,2) Transitem + Q(2,x)=(P,4,R). it S(a,x) = (8,Y,L) 2 is tape symbol IF (a, B)=(PMIR) 4) For each q. in F, then for all hope symbols x andy: list A West B ×94 9 4 94 9 5) we Finally, we use the final four to complete the Solution CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY List A list B q ## # 2 nirf 175" Rank CEN 42

--- Page 221 ---
Course Code/Title: CS3401/Theory of Computation Applications of Substitution Theorem: · Union · Concatenation · Closure (*) and positive closure (+) · Homomorphism. 1. Union: Let L1 and L2 be CFL's. Then the union of L1 and L2, S (L) = L1 UL2 where L is the language {1,2} and S is the substitution given by S(1)= L1 and S(2) = L2. 2. Concatenation: Let L1 and L2 be CFL's. Then the concatenation of L1 and L2, S(L) =L1.L2 where L is the language {1,2} and S is the substitution given by S(1) = L1|and S(2) = L2. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank 15

--- Page 222 ---
Course Code/Title: CS3401/Theory of Computation 3. Closure: Kleene closure Let L1 is a CFL's. Then the kleene closure of L1 is given by, S (L) = L1+ where S(1) = L, L = {1} *. Positive closure Let L1 is a CFL's. Then the positive closure of L1 is given by, S (L) = L+ where S(1) = L,L={1}+ 4. Homomorphism: Let L be a CFL over alphabet E and h is a homomorphism on E. Let S be the substitution that replaces each symbol a in E, by one string h (a). S (a) = {h(a)} for all a in E Thus h (L) = S (L) 5. Reversal: Theorem: If L is a CFL, then is also a CFL. Proof: Let L = L (G) for some CFL. The CFL generated from CFG G = (V, T, P, S). Then construct the reverse of the grammar GR = (V, T, PR, S), where PR is reverse of each production in P. G A GR A a1 a2 an an 1 1 1 an-1 ª1 L(GR) = LR All the sentential forms of GR are reverse of the sentential forms of G. CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY LEN nirf 175" Rank 16

--- Page 235 ---
Course Code/Title: CS3401/Theory of Computation 2. Multiple tracks TH: 1 It is also possible that a IM input tape can be divided into several trucks. Each track can hold one symbol, and the tape alphabet of the I'M consists of taples with one component for each track. 010115BB BBBB10BBBB .. BB1011BB Finite control A three track Turing Machine 6 Ex: Design a. TH to check whether the given input is prince or not using multiple tracks Ex: 5 5 2 1 5 5 2 -> 3 € 5 2 increase the second track value by 1 1 5 3 1 5 5 4 1 5 5 increase the second truck 3 value by 1 2 5 increase the second truck 4 value by 1 1 5 Here number on II track = number on III track 5 5 - . The given number is a prime number CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE & TECHNOLOGY N39 nirf 175" Rank - 29

--- Page 236 ---
Course Code/Title: CS3401/Theory of Computation 3. Subroutine: 7 subroutines are sub functions that can be used to execute repeated tasks for any. number of times depending on the applications. In such case, the Turing Machine has to be designed that handles subroutines. 11 1 Tuing Machine : Subroutine called by main () The subroutine has two states *Initial state * Return State When the main function is executed, the subroutine is called. 4. Checking off symbols: TM can be enterded by using checking of symbol this method is used to by The TH for the language that contain repeated, Strings and sonoputa compute the length of two sufialy Sub String CHENNAIC INGSTOUTE & TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank -- LEN 30

--- Page 233 ---
Course Code/Title: CS3401/Theory of Computation 1) Design a TH to compute addition of two numbers Soln: (01) +(x+y)=x+y w = 2,3 >> 2+3 = 5 ID: 1 BOO # 00OB -300*DOOB -300#00OB -3000000B 1 90 90 90 9, 1.BOOOOOOB -3000000B- 30000008-300000BB- 9, 22 9, +B00008BB =>Half. 93 5: M[Q, E, F, S, 90, B,F) Q 0 # B Q = { 90, 91, 98, 933 . "0 (90,0, R) (2, 0,R) - E = {0; # 3 9, (9, 0,R) - 92, B.L 5 = {0; #, B}" [%,0, 4) 90=90 92 [93, B, N) - - B = B 93 $ 9 4 F= {93] 90- O/0,R #10,12 O/0,R B/BIL>/92 0 % BIN 95 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank -- LEN 27

--- Page 234 ---
Course Code/Title: CS3401/Theory of Computation 4 Programming Techniques For Turing Machine Construction: "A tiring Machine is also as powerful as a conventional computer. The following are the different techniques of construction a TM to the meet high-level needs. 1. Storage in the finite control (r) state 2. Multiple tracks 3. Subroutines 4. checking of symbols. 5. Two-way infinite tape TH 1. Storage in state (0) Storage in Finita control: The finite control: can also be used to OxiedARM hold a finite amount of information along with the bask of - representing a position in the program. The state is written as a pair of elements, one for control and other storing a symbol. States i Storage sport Ground but ai a2 93/ 94. V Storage in turite confront A CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN 28

--- Page 257 ---
Properties of Recursive And RE languages: 9 1) The Union of two Recursively Enumerable (RE] is Recursively Enumerable. H. 2) The language 2 and it's complement I are. recursively enumerable, then 2 and I are recursive 3) The complement of a recursive language is also recursive. 4) The union of two recursive language is recursive ·5) The intersection of two recursive language is recursive. B) The union este two recursively enumerate langenegger is recursively enumerable. 5) The intersection of two recursively enumerable language is recursively enumerable, was Hi Theorem : If'L'is a recursive language, then L' is also a Recursive language. It"I is recursive language, so "L"?" The complement of recursive language is also , recursive language . (or) walt- 3X3 Proof: To prove I isirecursive then L' is also recursive n vitou + 1 let us construct the complement of the Twing Machine M as M' such that L'=L(MI) and it's shown below CHENNAIC CHENNAI INGSTOUTE . INSTITUTE OF TECHNOLOGY TECHNOLOGY 51 nirf 175" Rank N39

--- Page 258 ---
Turing Machines' comput w Tuning Machine H ACCOPE > Reject V Accept Reject 12. Construction of M' accepting the complement of M M' just behaves like M. The Turing Machine M is constructed as follows? 1) The Accepting States of M are made, as ny accepting states of it with no transitions 2) M' has hi a new accepting state 's' and there are no transitions from'r' Since M is guaranteed to halt, then M' is also guaranteed to halt. The Twing Machine M' exactly accepts those strings that are not accepted by THE recursive, · So the complement of recursive Language is also Theorem: It a language" L and L' are Recursively primerable (RE) Then L' is Recursive. ( or ) If both a language & and its complement L' are RE then L'is recursive language. Proof: To prove that will 1 "1"and I' is RF, then + is Recursive. Let 1= '(M) and L' = L(M2) for some Turing, Machine M1 and H2: Both M1 and M2 are simulated in CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N33 nirf 175" Rank 52

--- Page 217 ---
Course Code/Title: CS3401/Theory of Computation The replace A1 with its productions A3-> Az AzAala After replacement, the production has 3>2. Then replace A3-JA3 A1 AzA2/bAZAZla In the above production, 1=1, 3=3, the Introduce the new Variable B. A2, A3->bA3A2/a/bA3A2 B3 |a B3 B3 ->A,A3A2/A,A3A2 B3 At this stage the grammar now looks like: A1-> A2 A3 A2-> A3 A1/b A3->bA3A2/a/bA3A2B3 |aB= = >GNF B3 -> A, A3 A2/A, A3A2 B3 GNF conversion: A3 is already in GNF. Apply As value to A2 A2 ->bA3 A2A1|QA1 |bA3A2 B3A1| aB3A1/b Now A& is in GNF. Apply A2 value to A1 A1 -SbA3A2A,A3|9A,A3/bA3A2B,A, A3/9B3A,A3/bA3 Now A) is in GNF. Apply A, value to Bs B3->bA3 A2 A,A3 A3A2|GAIA3A3A2/bA3A2B3A, A3A3A2/ a B3 A1 A3 A3 A2 bA3 A3 A2 / b A3A2A, A3A3 A& B3 / a A1 A3A3A2B3|bA3 A2B3 A, A3 A3 A2 B3|GB3 A, A3 A3 Abs b A3A3 A2 B3 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank LEN 11

--- Page 218 ---
Course Code/Title: CS3401/Theory of Computation The resultant grammar is: P: A)->bA3A2A,A3|QA,A3|bA3A23A,A3|aB,A,A3|bA3 5 - A3-> KAZAa/a/ bA3 A2 B3) aBs B3-> bA3A2A, A3A3Aa aA, A3A3A2|bA3A2B,A,A,A3A313 A B3A, A3A3A2/bA3A3A2 /bA3A2A, A3A3 A2B3/ 9A, AZAZA,B, /bAZAZBAIA,AZAZA2B3|aB3A, AZAZAZ b A3A3A1B3. Bs/ IN GNF, G=(V,T, P.S) V= [A1, A2, A3, B3 } T=44,63 P=1 3 5 = [AT] CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank 12

--- Page 259 ---
Parallel by a Turing Maschine M. And it is thath below strony co Mr . (5.9) 2. Tuning Machine Mrj Turing Machine M Accept- > Accept Reject simulation of Two TH accepting & lining s'ils complement Here we w convating the two tape"THEM in to One tape IM 'M' to make simulation easy. IN't * one tape of H simulates the tape - of H1 " The other tape of M Simulates the hope of M2 * The states of M,B 16, are each components of the State of M. * If input w & M is in L, then M, will also accept. then M accept and husets. 7000 * It ispart w is not is L, then M hults without accepting. - we can conclude that, Lis Recursive language. Theorem: 1 If 4 and L2 are recursively enumerable; language over E, then L, Uly is also RE. 2. Not Passt (or) 1 union of two recursively enunomble language is RG. CHENNAI INGSTOUTE CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN 53

--- Page 260 ---
Proof: To prove LI UL2 is also Recursively enimerable, let us construct turing Machine for language 21 and L2. T1 = TM for the language 20 T2 = TM for the language L2 The transition function of T, is & (P,x)=[7,M,L) The transition function of T2 is, &(9, y) =(5; N, R) Then the transition of THO Tot will be", , The Simulation of the machine is given below, 3 TI 2140000 > Acceptit to 20% ( 2x X 6 Infinite. LOOP : to aolite wir x OR 1 ACCEPT I doy T=TIVE SIL ל T2 > Infinite ~ 200 1 al Loop SALCODE ₹ TTOT2 is also" Recursively Enumerable. Here the tuing machine + accept the string is accepted byany of 9, and 12 and I enter in to infinite loop it both , and to reject the string, Henle T= T1 UT2 and its language 2 =SUL2 recursively enumerable isi 54 CHENNAI INGSTOUTE . CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 243 ---
Theorem: Non Empty language "Line" is Recursively Enumerable. Proof: : 1 Lne . Now we have to construct- a nsluet- 9 TM that accepts "M" takes as conuts a tureog Machine cole" Mi 1) Using it's Nondeloministic capability, i guesses an input w that Mi night accept. 2) M tests whether Mi accept .W. It Hisaccepts 'w' then M also accepts its own input 'w'. This is done by M, simulating the universal TH "U" that accept Lu .. 3) It Hi access w, then M accepts its own input, which is Mi Mi 1 Guessed String w Turing Machine U -> ACCEDI- > ALLORA , 6. 0 (M) + Gold. OM for Lne - Thus if Turing Machine code Mi accepts even one string, M will guess that "string and accept Mi. However, it 1 (Mi]=Q, then no guess w leads to acceptance by Mis so M does not accept Mi. Thus L (N) = Line and Ine is recursively enumerable. x. it and past 37 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY LEN nirf 175" Rank

--- Page 244 ---
2) Unsolvable Problem of Reduction: Formally, a reduction from P1 to P2 is a TH that takes an instance of P1 written on its tape and halls with an instance of P2 on its tape. so reduction takes an instance of P, as input and produces an instance of 12 as output c C C C c Yes NO c Toyes O No P, 92 3) Unsolvable problem in Rice's Theorem: Rico's Theorem: Every Non Trivial property of the RE language is undecidable and unsolvable. proof: The design of M' is shown below L(M) is & if M does not accept w, and L(M1) = L it M accept w. N Turing Machine. M Accept Input × Start · Turing Machine ML S'Accept- SACCOPER Turing Machine M construction of M' for Rice Theorem 38 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 237 ---
Course Code/Title: CS3401/Theory of Computation Ex: Design TH for L={annn/ n21}. Liraxbxb4 5. Two- way infinite tape TH: A TH, M= (Q, 2, 5; S, 90, B, F) is said to be a two way infinite tape TH if the input type is infinite to the left and right. FB 90 a1 92 ... |a. ... |ak B-S Infinite Length at the left side of the tap Input Symbols 5 Infinite length at the rightside of the input- It can be viewed as a finite Sequence of input symboles. with infinite sequence of blank symbols to the left and right of the input. AS with the standard IM, there is fixed left end, the two way infinite tape TH has no lift end. Hence it can more as far as possible towards left as well as right. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN nirf 175" Rank -- 31

--- Page 238 ---
Course Code/Title: CS3401/Theory of Computation MULTI HEAD TURING MACHINE : . 8 A) Turing Machine with a single tape can have any number of heads to provide simultaneous accesses over the tapet Input 91/92/93 tape bs b3. C1 |C2 C3 |. .. Head, Heads Heads Finite Control The finite control processes two or more tape heads to access the input tape for performing multiple reads/writes in a simultaneous and independbut manner. The transition behaviour of a HORA 2 headed tage is given as S(a, H1(a), H2(a))=(2", +(b),MI),(H2(b), Ma) where? q' 2" - > states of a H1(a) -> symbol to be processed (read) by , Ha[a]-> symbol to be processed [read) by H3 Hi (b) -> symbol to be replaced by Hy Ha(b)-> symbol to be replaced by A2(0) M1 -> Movement of H1 (1) RYN) : M2 -> Movement of HalLIRIN). CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY LEN nirf 175" Rank 32

--- Page 273 ---
Example: 1) Find the code of The Turing Machine M be, M={2,92, 93}, {0,1] {0,1, 8} SI, B 1.923] Where & consist of the following rules, a( 2 1) = ( 92 0 , K) S[a30)=/2, (, R) 2[931)=(92,0,R) &[93, B)- (931,L) soln: According to the code for Turing Machine the assumptions ares som (1) The states are 9, ->0 homes of that afmais ix 92-100 1, 2 93-5000 2) The Tape symbols are O.1, B. Assume 0-5x,-50 1-5x2-500 B-> x3-3000 . 1 3) The Directions care of the form of Left-52-SD1 =>0 Right -> R- De ->80 Now the transitions are 1) S[21, 1]=(93, 0,R) WORK1 =8'100 103 10/ 10 2 (2=01001,00010100 2) {[9,0]= (9,1,R) (2)= 0001010,100100 14: 1 67 CHENNAI INGSTOUTE . CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY CEN nirf 175" Rank

--- Page 274 ---
3) 2[ 93,1) =(92, 0, R) C3=00010010010100 4) &(a, B) =(93, 1, 2): (4=000100010001000 12 S S S 5 Complete Code= C++ C2 d13 11 24) Code = 010010001010011 0001010100100 11 00010010010100 1) 00010010010100 110001000100010010 , 2) write the cycle for the following TM transitions, S(a1,a) =(91, x,R), s(a1, 4) = (9++, Rx, s/2;x)=(9,0,R) S(91, 1) =(92,4,2), [8,4)=(914; R) ,S(92,0) =(92,0,L) Slq2jx)=(1,x,R), slax,y)=[q2 y L), 2(93 4)={s,4, R) 2[93, B)= (93.B.R). 4) Enumerating the Binary Strings: In this binary string enumeration, wer Assign the interests , to all the binary string, so that each string 0.541 corresponds to one integer and each integen corresponds to one string .... 1 EtFirst string 4 0 -> second string !- Third String 00-> Fourth String 01-> Fifth String 10 - sixth string and so on 68 CHENNAI INGSTOUTE . CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 269 ---
PROBLEMS ABOUT TURING MACHINE: In problems about tuing machine, we are .: going to see the following concepts 1) Decidable Problemy 2) Undecidable problems 1 3) codes for the Turing Machine 4) Enumerating the Binary String of Turing Mackerel 5) Undecidable problems, about Furing machine 63 CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N33 nirf 175" Rank

--- Page 270 ---
Problem types, A There are basically three types of Problems namely, Pay * Decidable solvable recursive * undecidable/ unsolvable * semi decidable /partial solvable/recursively Decidable/ Solvable problems: enumerable A problem, P is said to be decidable if there exists a boring machine, IM that decides P ... This P is said to be recursive. - Consider a TH, M that halts with either yes' or 'no' after computing the input. WEST Mais YES (IfWEL) No (it. w. Et Jee The machine is defined as, if p(w) Fp(w) =[1 Lo It rp(w) Undecidable Problem: A problem; P is said to be undecidable if there is a Tuing machine, IM that doesn't decides P. 64 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 263 ---
MEASURING AND CLASSIFYING COMPLEXITY: Growth Rates of Functions; 12 Input Machine . Algorithm (> Space complexity Tinie complexity Complexity problem is classified in two types * space complexity. * Time complexity -- >The Space complexity is the amount of "memory Space required by the algorithm) problem it's computation() 2. 000 -> The time complexity is the amount of time required to run the program of the problem The growth rates of the functions can also be compared using the assumpotic notations like * Big-oh [0] * Big-Omega [2] * Big-Thela [o]. to complete CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN 57

--- Page 264 ---
TRACTABLE AND INTRACTABLE PROBLEMS; C Tractable Problems/ Languages: c The language that can be regno recognized by a TM in finite time with reasonable space constraint is said to tractable. The Tractable problems wo those that can 5 C C C be solved in polynominal time period ... C Intractable Problem: C The language that cannot be recognized by a TM with reasonable Space and time constraint. is called as, intractable problems. These problems cannot be solved in Polynominal time period 6 Tractable and Possibly Intractable Problems: Pand NP:6 There are two groups in which a Problem can be classified Computational complexity Problems P-class NP-class NP-Complete NP-nord 58 CHENNAIC INGSTOUTE TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY nirf 175" Rank N32

--- Page 253 ---
y Projection function [D] A function of the room, Pi- NK-IN defined byp(xxxxxxxx) = x1 For K ≥1 and 1 Li {x Example: P,3 [1,3,5) = 1. P3" (2,4,6,8) = 6 R2 (0,1) = 1, etc 4. Complex primitive Recursive Functions: complex functions are obtained by applycity certain operations on the initial functions, They are, 1) composition 1 ii) Primitive Recursion + 1. composition: is let "I be a partial function defined as NA-SN, for 1 LiIK and 'g' is a partial function from N"-> N, then the composition of ' and g is a partial function, defined by 'b' as that is, h(x) = +(g(x); 92(2), -. 9x(x) []xEN] h(x)=f(g(x, xx ... an); 92(x2+2x) ==== 9k [x1, 72-3) ii. Primitive Recursion; *** A function 'f' over N is recursive it there exists a constant, 'x and a function 'h' (x, 4) such that FCOJECK. +(n+1)= h(n, ten) 4/ CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY LEN nirf 175" Rank

--- Page 254 ---
let 9 be function of n', variable and 'h' be another function with '(nta)' variables. c c The primitive Recursion function is obtained as : N"+1 -IN, f(x,0)=g(x) f(x, k+1)=h(x,k,f(x,k),XENn, K20 c That is, f(x1,x2, ... xn,0) =g(x,22, ... an) and C C € 6 f(x1, x2, ... an,k+1)=h(x1,x2,·xn,k,+(x,in, -. znak)) 5 Example problems: 1) let fi(x, y) = x+y , f2 (x,y) =x, f3(x,y)=xy, g (x, y, 2) = x+4+2. Find the composition of g with . Si, fa, +3. 6 € Som: 6 6 The composition function, h(x,y) is given as, 6 n(x)=9[f(x,y), +2(x), +3(2,4)) =g(x+y,2x,x4) = x+4+2x+14 3(x,y)=x+4+2xxxy Here, f1, f2, 29 are total function that I also total function 2) Given: f1(x,4)=x-4, +2(x,y)=y-x, g(x,y)=x+y obtain h(x,4) over y and 2. som: = 9(x-9,x1-4) h (x,y)= x-y +x-4 h(x,y)=9(+1(2,4), +2(2,4) 48 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 219 ---
Course Code/Title: CS3401/Theory of Computation 1) PUMPING LEMMA FOR CFL: Prove That L={a+b+c2|>>1} is not CFL. · Soln: 12 1) let us assume that Lis CFL 2) Let us Pick up; W="a"b"ch, where his constant 3) w is rewritten as uvxyz, where. i) luxyl = n ii) vy + 4 iii) for all i, uve x1 2 EL If vy contains all three symbols a, h.c V= ab/bc/ac or y=ab/Dc/ac If i = 2, uvdxy+ 2 =>uve xy2 2 casel it = ab and Y=c case 2: uv2x+2 =(ab)2℃=>uvexyz&L' if v .= a and 4= bc uv2xy22 =a/b[]2=>uv++++2+L Hence L is not CFL. CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY LEN nirf 175" Rank 13

--- Page 220 ---
Course Code/Title: CS3401/Theory of Computation 1 2) prove that L=gal1 21 31|1218]>1} is not- context-free. Soln: 1) Let us assume that itis ) 2) Let w- 0", "2"3", where his a constant 3) Let wo is rewritten as uvx42 where i) luxyl &n ii) vy+ 4 13. iii) uvixyi 2EL for i= 0.1.2 .;. Let- vy takes the symbols 012 or 123. i ) if v = 0 , and y = 2 At i=2, uv2xy2 2 =; (01)2/2je3; ii) it v=12 and 4=3" b.000 - 8 At 1=2, uve×4×2=(12)(3)-2 From 18& There are unequal number of 012 and 3. Hence L' is not a CFL. PROPERTIES OF CFL !! A CFL is closed under the following operations union * concatenation * kleene stor A CFL is not closed under * Intersection * comple mentation * Reversal. CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank -- 14

--- Page 271 ---
Serul decidable partial solvable/recursively enumerable A problem, P is said to be semi decidable, if p is recursively enumerate. $ A problem is RE if M terminates with 'YES' If it accepts WE L', and doesn't halt it w&L WEET M SYES (it WEL) SLOOP FOREVER (it WEL) partial solvability of machine is defined as Fp(w) =f1 lit Plaj (undefined if ri(w) . PURT 4 65 CHENNAI TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N33 nirf 175" Rank

--- Page 272 ---
3. Code for Twing Machine . " Now we are going to generate a binary code for the Twing machine So that each Tuing machine with input alphabet 40,13 may be as a binary String let as assume the states 21,92,93, ... 91 some value of K. For Example 2,-50 92-500 93-5000 p&C 1) Assume that tape symbols as x, xxx3 ... Xm for some value 'm' such that los? xi always will be symbol 0, Xe always will be symboli1 sets x3 dways will be 'B' 2) Assume The directions, 'E'idus D, and 'R' as D2 Let -SC -D, ->0 Right - R - De -100 00 € we can encode the transition & as follows Q(q1;x) = qn,x, - Dm) code for the string = 1'10 1 of 1 glas The code for the entire Tiring machine. M consists of all the codes for the transitions in some order complete code = C 11 CQ 11 C3 1151- 11 QUE 11 CON where each ( is the code for one transition the Turing machine & M'. of 66 CHENNAY INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 261 ---
1 1 1 1 3 1 3 1 Theorem: . The union of two recursive language recursive. Proof: is 11 Let L, and L2 be wo recursive language that are accepted by the Turing machine M, and M2, 3 given by L(M) = L, 2 (M2) = L2 Let 43 be the Turing Machine constructed by the , Whim of M, and M2, M3 is constructed as, M1 w M2 Yes NO yes > yes M3 > NO * If WE L1, then M, accepts and thus M3 also accepts Since [M3)= L(H,JUL(M2) *If M, reject, [WL,], then M3 Simulates M2. , M3 halt with "yes" if M2 accepts "w" else returns"No". Hence Mg, M2, M, halt with either yes or NO. Thus the union of two recursive language is also recursive. , , , , , 55 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGTOUTE . TECHNOLOGY N39 nif 175" Rank

--- Page 262 ---
Universal Turing Machine: The universal Turing machine, Tu takes over the program and input set to process the program The program and the inputs are encoded and stored on different types of multi-lage TH. -> The To this take up [+, w> where T is the Special purpose TM that passes the program in the form of binary string, w is the data set that is to be processed by T. Finite control Input LT,w> TapeofH 0001010000101 ... State of M · OVO . . . OBB .. . The universal language Lu, is. If all busary string (ai), Where & represents the ordered Pair <T, W> where T- Turing Macherie W-Jany ionut string accepted by T It can also be represented as a ={{(T), e(w> the set 56 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 205 ---
Course Code/Title: CS3401/Theory of Computation 10 .2) Give. CFG for the PDA, M=[{9,9] 10-1},{20,x},{, 90, 20, 9) S.[90, 1,20)=[90, ×20) 2(90,1, x)=(90, xx) 2190,0,xJ=(q1, x) 21. 9, 4, 201=(90.8) &(q,1, x) = (91,6) 2/91,0,20)= (90.20) Soln: 1) G=(V,T, P.S) v =[%2090] [9020 9] [9, 2090] [9,209;] [90 × 90] [90×9,] [9,x90] [q,×9,]} T= {0,13 5 = { 5 }. P: 1) 5-> [90 20 9] 9.20 9, 5-> [90 20 91] x 2) S(90, 1,20)=(90,×20) [ 90 20 90 ] -> 1 [90 × 90] [% 20 90] [90 2090]->1[90×91] [2,.20 %]x [% 20 9] ;- )) [90×90] [90 20 2]x [90 20 9; ] -> 1 [90 × 9, ] [2, 20 2] x 90 × 9, CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY LEN 205 nirf 175" Rank -

--- Page 206 ---
Course Code/Title: CS3401/Theory of Computation 3) 2(90,1,x)= (90,xx) [10×90]=>1[90×90][90×90] [90×90] >1[20×91] [91 ×90]x [ 90 × 9 ; ] -> 1 [90x90] [20 ×9.]x [ 90 × 9,] -) 1[90×9,] [91 ×9,]x 4) 2( 90,0,x) =(q1x) [20×90]=>0[2×90] [90×91] =>[9, ×9,]x 5) 2/90, 6, 20) =(90, ℃) [20, 20 90 ] => Ee. 6) {(21, 1, x) =(2, 4) [21 ×90]->1 7) 2(91, 0,20)=[90,20) [2,20 90] ->0[90209.] [9.20 91] ->o [90209,]x Final Production: 5->[2.2%] 5 [902090] =>1[90×90] [10 20 90] [90×90] =>0[91×20] [202090] =>& :[9,2090]-so[902090] 206 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE TECHNOLOGY N39 nirf 175" Rank

--- Page 265 ---
13 P-class Problem Problems that can be solved in Polynominal time. Ex: Searching element, sorting element, Multiplication of integers, finding all-pair-Shortest path, finding minimum spanning tree, etc Example of P class problem: Kruskal's Algorithm:> In Kruskal's alg. the minimum weight is obtained. -> In this aly also the circuit should not be formed. - Each time the edge of minimum weight is selected. Example 1: 8 1 3 13 V1 2 . 12 2 V2 8 V5 7 12 V3 10 9 14 som: 1) 7 1 v2 2) 2 VI 1 V5 59 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 266 ---
3) 2 1 toda 3 4) - V6 V 8 2 3 . 013 y V 5 9 Ju weight=2) Example 2: Find the minimum spanning tree Kruskal's algorithm. using a 3 fo 2 b 2 2 2 , 3 3 e 1 1 1 bd 3. 3 5 2 2 3 3 9 4 3 i Soln: 5 b 1 0 1 1 2 5 w/1): 6 ao , 3 00 0 1 00 1 e 1 PC 0 RO 2) 5 0 3) a 5 w(+1)=9 3 5 2 w(73)= 60 CHENNAI INGSTOUTE . CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY nirf 175" Rank LEN

--- Page 199 ---
Course Code/Title: CS3401/Theory of Computation Equivalance of PDA and CFG: 1 For a given CFG, G= (V,T, P.S), we call construct a PDA, M That simulates The left-most derivation of Gr. The PDA accepting 2(a) by empty Stack is given by M=[[9], T, VUT_ 2.9, 5, 0) LÉGI Problems: PDA hy empty stack PDAhy final state 1) Find a PDA for the given Solni grammar 5-5 051/00/11 S(2, E, s) = {[9, 051), (2, 00 }, {9, 11)} 2[9,0.0)={[q,4)} 219,1, 1)={(q,a)} s 7 String [000011]. 0 St optical 10011 199 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY nirf 175" Rank N39

--- Page 200 ---
Course Code/Title: CS3401/Theory of Computation 2) Construct a PDA for the grammar 3-JaB/bA, A-Salas/baA, B->b/bs)aBB soln: where & is given by S (a, E. S) = {{q, aB), (a, bA) } S (a, E, A) = {2, a). [9, us), (a, bAA)} {[2, E, B) = {[a, b), (9, bs), (9, aBB)} S(a, a, a) = [9, 4)} 2(2, b. b) = [9, 4)} M=[{},{a.b}, { a,h, S,A,B}_ S.9, 5, $). Set 1 3) construct a PDA for S-SaAA, A ->as/ bs)a. - TabsA abano 4) construct a PDA for 5-> asublab. 5) construct a PDA for G={{ s. A}{a,b}_P.S) with. [tp: 5->AA/a, A->SA/b. (6. let G be the grammar given by 5-JOBB B->05/15/0. construct a PDA. Test it 0104 is is the language. Soln: 2: S(2, E, s) = {q, OBB) } S(a, E, B) = {[2, 05), (9, 15), (2,0)} (9, 0,0) = (g.w), [21,1) =(9, 0) M=([2}, {0,1}, {0-1,5-B},{,a,s,o). 200 CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank

--- Page 161 ---
CHENNAI TECHNOLOGY Type Grammar/Language Form of production to grammar Example unrestricted grammar (or) Phase structure grammar DAY -5024 Automata A grammar without any restrictions 1) abAcd->abAB bed with the following Production from; where 0 INSTITUTE OF TECHNOLOGY CHENNAI LEN 175" Rank nirf where A - variable 9 - left context 4) - right context $24=>replacement String ab -let context- bed - right content 2-AB 2) AC-SA where A -> left context 1 - right context 2-1 context sensitive grammar (00) A production of the form DAY-> Px 4 is called type) Production it &#1. In types Productions erasing of A is not permitted 1 (oxtext dependent grammer Tuning Machine (TH) Types of Grammar: Course Code/Title: CS3401/Theory of Computation 1) a Abc D-SabCDDeD Linear where a - left context bcD-right context Ausreplaced by bcD +x 2) AB -> AbBC Where A - left context 1- right content x = bBc = > 161 Bounded Automata 2

--- Page 162 ---
CHENNAI INSTITUTE OF TECHNOLOGY INSTITUTE ® CHENNAI TECHNOLOGY Type N39 175" Rank nirf 2 Grammar/language context Free Grammar (CFG) 3 Regular Grammar 162 Form of production in grammar S-> Aa A production of the form A->2, Where AEV and XE (VUE)* is called a type 2 grammar A = L.H.SEV 2=R.H.SE (VUE)+ A production of the form A-la 5-1 b/c or A-> B where A, BEV and 5-> bA aEE is called a typo 3 production A production S->> is allowed to type 3, but in this case s does not appear on the right Example A-Ja B->abc A-SI 5->a A=L.H.SEV R.H.SE (vva) Automata push down Automata (PDA) Course Code/Title: CS3401/Theory of Computation Finite Automaty

--- Page 275 ---
CHOMSKKAN DERAACHY 5) Undecidable Problems about Turing Machine. The process of undecidable problems about Tuing machine says that any non biral property of Twing machine that depends only on the language the turing machine accepts must be undertable " dair?). . yes 10 yes NO Pi O NO Formally, a reduction from p, to Po is a TH that takes an instance of P, written on its tape and halts with an instances of P2 on it's tage : so reduction takes an instance of P1 as input and produces an instance of P2 as actout 80 CHONSKIAN HIERARCHY OF LANGUAGES Language maintenir Bist ur Automata 15 Type ? Type 1 Type 2 Type a. CHENNAIC CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY LEN nirf 175" Rank Taking Machine Lined Bounded Autorita Push down Automata Finite Automata

--- Page 276 ---
The Halting Problem: The halting problem is the problem of finding If the program/machine halls of loop forever. The halting problem is undecidable over 9 Turing Machines. Ex: while (1) * : 3 printf ("Halting Problem"); The above code goes to infinite loop since The argument of while loop is true forever. Thus it doesn't halls. Honce Turing problem is the example of undecidability. Representation of the hatting set The halting set is represented as, where, h (M, w)= it M halls on copul w 0 otherwise M-> Twing Machine w -> Input string U. ts Theorem: ND Halting problem of Turing machine is unsolvable undecidable. Proof: 2 The theorem is proved by the method of proof by contradiction: let us assume that TH is solvable/decidable. 70 CHENNAI INGSTOUTE . CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY N39 nirf 175" Rank

--- Page 255 ---
8 :3) let Li[x, xx)=x,xx, +2 (x, xx)= >, +3 (2, 22 ) = x1, y (2,1x3,73)=2273- Generate composition function (2,20). ix h(ama) og (fi(2, 22), +2(x1, 20), +1=1,2,3} 1 39.2314 = 12, h(21jaa) = >21 =2, 4) Show that +1(2,4)= x+y is primitive recursive. Soln: 2.4 \ BomA. To prove that f, is primitive recursive. let I be a function of single variable. and I be a function of three variables when 9=0; Le (X)O) =270" =x = f(x) =U. (a) ONT seit when yegt) fi (2, 4) = +1 (x, y+1) F (x, y+1 ) = h (x,y, +, (x,y) Avisoex = 0,3 (x, y, z ) 7. 9 and I are primitive recursive Hence f1 (x, y) is primitive recursive. --- 2) Show that f (x,y)= xwey is primitive Recursive: 49 CHENNAI INGSTOUTE . TECHNOLOGY CHENNAI INSTITUTE OF TECHNOLOGY N39 nirf 175" Rank

--- Page 256 ---
RECURSIVE AND RECURSIVELY ENUMERABLE LANGUAGES start Recursive language: A language 'L'is said to be Recursive language It L=1(H) for some; Twins Machine M such that 1) If w is in 1, Then i accept 2) If w is not in 2, then M reject, but it halt 30 1 is called decidable language if It is a Recursive language InDit Twing Machine M -> Accept/yes Reject No Recursive Enumerable Language TREJ. A language is Recursively Enumerable it there exists a Turing Machine That accepts every String present in the language and does not accept the string and It may cause the turing martine to enter into an infinite loop. Input w CHENNAIC INGSTOUTE . CHENNAI INSTITUTE OF TECHNOLOGY TECHNOLOGY Tuning Machine M nirf 175" Rank LEN >Accept/yes > Reject /NO Infinito 100 P 50

--- Page 239 ---
Course Code/Title: CS3401/Theory of Computation T MULTI TAPE TORING MACHINE : q) *A multitape IM is finite control with more than one tape for read/writing symbols storing States etcl nt. * Fach tape has in number of individual * The first tape is mostly used to store the .. cells input symbols of the string, w. * The second and third taxes are normally used . to store the states and state-changes. - ... cuit Tape 1 Tapez Tapes Finite Control COM (The transition of a two tape TM is given as, S(9, a1, a) = (9's Hi(b), M1 ); Halb), M= ) where; 2-> current state to be processed q'S heart state to be reached (a1 ) symbol read by tape, a2 -> symbol read by tape 2" H(b)->-symbol to be written by tape, H2(b)-> Symbol to be written by tape 2 M1 -> Movement by tapail (R)LIN) Ma -Movement by fupea (RIL[N] CHENNAI CHENNAI INSTITUTE OF TECHNOLOGY INGSTOUTE . TECHNOLOGY N39 nirf 175" Rank 33

--- Page 240 ---
UNIT V UNDECIDABILITY Non Recursive Enumerable (RE) Language - Undecidable Problem with RE - Undecidable Problems about TM - Post's Correspondence Problem, The Class P and NP.

--- Page 15 ---
Publish - subscribe A messaging pattern used in software architecture to facilitate asynchronous communication between different components or systems. The Publish-Subscribe (Pub-Sub) pattern is a messaging pattern where senders (Publishers) send messages without knowing who will receive them, and receivers (Subscribers) receive messages without knowing who sent them.

--- Page 13 ---
How MVC Works . User interacts with the View (e.g., clicks a button) · View sends the request to the Controller · Controller processes input and updates the Model . Model changes and notifies the View · View updates the Ul accordingly

--- Page 14 ---
Design Principles of MVC · Divide and conquer · Increase cohesion · Reduce coupling · Increase reuse · Design for flexibility

--- Page 1 ---
Design Pattern - Types · Creational Design Pattern · Structural Design Pattern · Behavioral Design Pattern

--- Page 2 ---
Creational Design Patterns · Focuses on the process of object creation or problems related to object creation. · Helps in making a system independent of how its objects are created, composed and represented.

--- Page 9 ---
Model [Data and Logic layer ] · Manages Data and Business Logics · Does not directly interconnected with the User · Sends notification to "View" , incase of any changes in the data. Ex) User Model -> Updates, fetches & validates information from database.

--- Page 10 ---
View [UI layer ] · Displays data from the Model in a user-friendly way. · Only renders information, without modifying data or business logic. · Updates automatically when the Model changes. Ex) Web Page

--- Page 5 ---
Adapter Pattern 01 07 Bridge Pattern Structural Design Patterns 02 Composite Pattern 03 04 9e Flyweight Pattern 06 Proxy Pattern 05 Facade Pattern Decorator Pattern

--- Page 6 ---
Behavioral Design Patterns · Concerned with algorithms and the assignment of responsibilities between objects. · Describe not just patterns of objects or classes but also the patterns of communication between them.

--- Page 3 ---
Singleton Pattern 01 Factory Method Pattern 02 Creational Design Pattern Abstract Factory Pattern 05 C Prototype Pattern 04 Builder Pattern

--- Page 4 ---
Structural Design Patterns · Solves problems related to how classes and objects are composed/assembled to form larger structures. · Uses inheritance to compose interfaces or implementations.

--- Page 11 ---
Controller [ Input and Logic Layer ] · Handles user input and communicates between Model and View. · Updates the Model when an action is performed. · Instructs the View to refresh when the Model changes. Ex) Login Controller - validates user credentials and updates the session

--- Page 12 ---
Model 0 View · Handles data logic . Interacts with Database Fetch Data Request O o - Database · Handles data presentation Dynamically rendered Fetch presentation . Handles request flow · Never handles data logic O Response End User Controller DG

--- Page 7 ---
01 Observer Pattern 02 Strategy Pattern 03 State Pattern 04 Command Pattern 05 Chain of Responsibility Behavioral Design Patterns Template Pattern 06 Interpreter Pattern 07 Visitor Pattern 08 Mediator Pattern 09 Memento Pattern 10 se

--- Page 8 ---
MVC( Model - View Controller) · Software design Pattern · Separates application into three components as: Model View Controller

